{"id": "2510.01931", "categories": ["cs.CG", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.01931", "abs": "https://arxiv.org/abs/2510.01931", "authors": ["Bubai Manna"], "title": "Minimum Selective Subset on Unit Disk Graphs and Circle Graphs", "comment": null, "summary": "In a connected simple graph G = (V(G),E(G)), each vertex is assigned one of c\ncolors, where V(G) can be written as a union of a total of c subsets\nV_{1},...,V_{c} and V_{i} denotes the set of vertices of color i. A subset S of\nV(G) is called a selective subset if, for every i, every vertex v in V_{i} has\nat least one nearest neighbor in $S \\cup (V(G) \\setminus V_{i})$ that also lies\nin V_{i}. The Minimum Selective Subset (MSS) problem asks for a selective\nsubset of minimum size.\n  We show that the MSS problem is log-APX-hard on general graphs, even when\nc=2. As a consequence, the problem does not admit a polynomial-time\napproximation scheme (PTAS) unless P = NP. On the positive side, we present a\nPTAS for unit disk graphs, which works without requiring a geometric\nrepresentation and applies for arbitrary c. We further prove that MSS remains\nNP-complete in unit disk graphs for arbitrary c. In addition, we show that the\nMSS problem is log-APX-hard on circle graphs, even when c=2.", "AI": {"tldr": "MSS seeks a minimum-size selective subset of vertices under a per-color locality constraint; the problem is hard to approximate on general graphs (log-APX-hard even for c=2) and has a PTAS in unit disk graphs, with MSS NP-complete there for arbitrary c; also log-APX-hard on circle graphs (c=2).", "motivation": "To understand the computational and approximation complexity of selecting a small subset of vertices that ensures, within each color class, every vertex has a same-color neighbor either in the subset or outside its color class.", "method": "Prove hardness via reductions showing log-APX-hardness on general graphs (and circle graphs); establish a PTAS for unit disk graphs that does not rely on an explicit geometric embedding and extends to arbitrary c; prove NP-completeness of MSS in unit disk graphs for arbitrary c.", "result": "Hardness results: MSS is log-APX-hard on general graphs, even when c=2; no PTAS unless P=NP. Positive result: a PTAS exists for unit disk graphs (not requiring geometric input) for any c. MSS is NP-complete in unit disk graphs for arbitrary c. Additional result: MSS is log-APX-hard on circle graphs, even for c=2.", "conclusion": "MSS exhibits strong inapproximability in general graphs, with limited positive tractability in structured geometric graphs like unit disk graphs (where a PTAS exists). The problem remains hard in circle graphs, and NP-complete in unit disk graphs for arbitrary color counts, indicating nuanced complexity across graph classes."}}
{"id": "2510.01939", "categories": ["cs.CG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.01939", "abs": "https://arxiv.org/abs/2510.01939", "authors": ["Sariel Har-Peled"], "title": "Bifurcation: How to Explore a Tree", "comment": null, "summary": "Avraham et al. [AFK+15] presented an alternative approach to parametric\nsearch, called \\emph{bifurcation}, that performs faster under certain\ncircumstances. Intuitively, when the underlying decider execution can be rolled\nback cheaply and the decider has a near-linear running time. For some problems,\nthis leads to fast algorithms that beat the seemingly natural lower bound\narising from distance selection.\n  Bifurcation boils down to a tree exploration problem. You are given a binary\n(unfortunately implicit) tree of height $n$ and $k$ internal nodes with two\nchildren (all other internal nodes have a single child), and assume each node\nhas an associated parameter value. These values are sorted in the inorder\ntraversal of the tree. Assume there is (say) a node (not necessarily a leaf)\nthat is the target node that the exploration needs to discover.\n  The player starts from the root. At each step, the player can move to\nadjacent nodes to the current location (i.e., one of the children or the\nparent). Alternatively, the player can call an oracle on the current node,\nwhich returns either that it is the target (thus, mission accomplished!) or\nwhether the target value is strictly smaller or larger than the current one.\n  A naive algorithm explores the whole tree, in $O(n k)$ time, then performs\n$O(\\log k n)$ calls to the oracle to find the desired leaf. Avraham \\etal\nshowed that this can be improved to $O(n \\sqrt{k} )$ time, and $O( \\sqrt{k}\n\\log n)$ oracle calls.\n  Here, we improve this to $O(n \\sqrt{k} )$ time, with only $ O( \\sqrt{k} +\n\\log n)$ oracle calls. We also show matching lower bounds, under certain\nassumptions. We believe our interpretation of bifurcation as a tree exploration\nproblem, and the associated algorithm, are of independent interest.", "AI": {"tldr": "A bifurcation-based approach to parametric search recast as a tree exploration problem yields near-optimal efficiency on a specific tree with k branching nodes: time O(n sqrt(k)) and oracle calls O(sqrt(k) + log n).", "motivation": "To accelerate parametric search by exploiting a reversible exploration (bifurcation) framework that leverages cheap rollbacks and near-linear decider time, potentially beating standard distance-selection lower bounds.", "method": "Model the problem as exploring a binary implicit tree of height n with k internal two-child nodes; use a bifurcation-based algorithm that minimizes oracle calls by performing selective exploration and rollback, achieving O(n sqrt(k)) time and O(sqrt(k) + log n) oracle calls.", "result": "Improved bound from O(n sqrt(k)) time and O(sqrt(k) log n) oracle calls to O(n sqrt(k)) time and O(sqrt(k) + log n) oracle calls; matching lower bounds under certain assumptions.", "conclusion": "Viewing bifurcation as a tree exploration yields a simple, potentially broadly useful algorithmic tool for parametric search, with provable efficiency gains in the stated model and potential for independent application."}}
{"id": "2510.01348", "categories": ["cs.RO", "x", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.01348", "abs": "https://arxiv.org/abs/2510.01348", "authors": ["Michal Werner", "David \u010capek", "Tom\u00e1\u0161 Musil", "Ond\u0159ej Fran\u011bk", "Tom\u00e1\u0161 B\u00e1\u010da", "Martin Saska"], "title": "Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge", "comment": "8 pages", "summary": "Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied\nenvironments is challenging: integrating odometry leads to drift, loop closures\nare unavailable in previously unseen areas and embedded platforms provide\nlimited computational power. We present a fully onboard UAV system developed\nfor the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km\nlong-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS\nor prior dense mapping. The system integrates perception, mapping, planning,\nand control with a lightweight drift-correction method that matches\nLiDAR-derived local heightmaps to a prior geo-data heightmap via\ngradient-template matching and fuses the evidence with odometry in a clustered\nparticle filter. Deployed during the competition, the system executed\nkilometer-scale flights across urban, forest, and open-field terrain and\nreduced drift substantially relative to raw odometry, while running in real\ntime on CPU-only hardware. We describe the system architecture, the\nlocalization pipeline, and the competition evaluation, and we report practical\ninsights from field deployment that inform the design of GNSS-denied UAV\nautonomy.", "AI": {"tldr": "Fully onboard, CPU-only GNSS-denied UAV system for long-range navigation (9 km) using LiDAR heightmap matching to a geo-data map, fused with odometry via a clustered particle filter; achieved kilometer-scale flights and reduced drift in real time.", "motivation": "Enable reliable long-range UAV operation in GNSS-denied environments where odometry drifts and loop closures are unreliable, under strict compute constraints.", "method": "Integrated perception, mapping, planning, and control. Introduces drift-correction by gradient-template matching LiDAR-derived local heightmaps to a prior geo-data heightmap; fuses this evidence with odometry in a clustered particle filter; fully onboard CPU-only implementation and evaluation in the SPRIN-D challenge.", "result": "Executed kilometer-scale flights across urban, forest, and open-field terrain; substantially reduced drift compared with raw odometry; real-time performance on CPU; detailed system architecture and localization pipeline; evaluation and deployment insights.", "conclusion": "Shows the feasibility of GNSS-denied UAV autonomy with lightweight, onboard localization using geo-data heightmaps and particle-filter fusion, providing practical guidelines for design and deployment in real-world environments."}}
{"id": "2510.01339", "categories": ["cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01339", "abs": "https://arxiv.org/abs/2510.01339", "authors": ["Alessio Spagnoletti", "Andr\u00e9s Almansa", "Marcelo Pereyra"], "title": "LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration", "comment": "23 pages, 12 figures", "summary": "Computational imaging methods increasingly rely on powerful generative\ndiffusion models to tackle challenging image restoration tasks. In particular,\nstate-of-the-art zero-shot image inverse solvers leverage distilled\ntext-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy\nand perceptual quality with high computational efficiency. However, extending\nthese advances to high-definition video restoration remains a significant\nchallenge, due to the need to recover fine spatial detail while capturing\nsubtle temporal dependencies. Consequently, methods that naively apply\nimage-based LDM priors on a frame-by-frame basis often result in temporally\ninconsistent reconstructions. We address this challenge by leveraging recent\nadvances in Video Consistency Models (VCMs), which distill video latent\ndiffusion models into fast generators that explicitly capture temporal\ncausality. Building on this foundation, we propose LVTINO, the first zero-shot\nor plug-and-play inverse solver for high definition video restoration with\npriors encoded by VCMs. Our conditioning mechanism bypasses the need for\nautomatic differentiation and achieves state-of-the-art video reconstruction\nquality with only a few neural function evaluations, while ensuring strong\nmeasurement consistency and smooth temporal transitions across frames.\nExtensive experiments on a diverse set of video inverse problems show\nsignificant perceptual improvements over current state-of-the-art methods that\napply image LDMs frame by frame, establishing a new benchmark in both\nreconstruction fidelity and computational efficiency.", "AI": {"tldr": "A zero-shot, plug-and-play solver (LVTINO) for high-definition video restoration using Video Consistency Models (VCMs). It overcomes frame-by-frame limitations of image LDM priors by enforcing temporal causality, achieving state-of-the-art reconstruction quality with few neural evaluations and strong measurement consistency.", "motivation": "To address temporal inconsistency and limited spatial detail when applying image-based diffusion priors frame-by-frame to HD video restoration; leverage Video Consistency Models to capture temporal dependencies and improve efficiency.", "method": "Develop LVTINO, a zero-shot/plug-and-play inverse solver conditioned on VCM priors. It bypasses automatic differentiation, uses a lightweight conditioning mechanism, and relies on few neural function evaluations to ensure measurement consistency and smooth temporal transitions across frames.", "result": "Significant perceptual improvements over frame-by-frame image LDM methods, achieving state-of-the-art video reconstruction quality and improved computational efficiency; establishes a new benchmark in reconstruction fidelity and efficiency across diverse video inverse problems.", "conclusion": "LVTINO demonstrates that integrating Video Consistency Models into zero-shot video restoration yields superior temporal coherence and detail with low computational cost, enabling practical, high-quality HD video restoration using pre-trained LDM priors."}}
{"id": "2510.01206", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01206", "abs": "https://arxiv.org/abs/2510.01206", "authors": ["Hung Le", "Sherif Abbas", "Minh Hoang Nguyen", "Van Dai Do", "Huu Hiep Nguyen", "Dung Nguyen"], "title": "Accelerating Long-Term Molecular Dynamics with Physics-Informed Time-Series Forecasting", "comment": "16 pages, preprint", "summary": "Efficient molecular dynamics (MD) simulation is vital for understanding\natomic-scale processes in materials science and biophysics. Traditional density\nfunctional theory (DFT) methods are computationally expensive, which limits the\nfeasibility of long-term simulations. We propose a novel approach that\nformulates MD simulation as a time-series forecasting problem, enabling\nadvanced forecasting models to predict atomic trajectories via displacements\nrather than absolute positions. We incorporate a physics-informed loss and\ninference mechanism based on DFT-parametrised pair-wise Morse potential\nfunctions that penalize unphysical atomic proximity to enforce physical\nplausibility. Our method consistently surpasses standard baselines in\nsimulation accuracy across diverse materials. The results highlight the\nimportance of incorporating physics knowledge to enhance the reliability and\nprecision of atomic trajectory forecasting. Remarkably, it enables stable\nmodeling of thousands of MD steps in minutes, offering a scalable alternative\nto costly DFT simulations.", "AI": {"tldr": "A time-series forecasting framework for MD that predicts atomic displacements using a physics-informed loss with DFT-parametrised Morse potentials, achieving accurate, scalable simulations faster than traditional DFT.", "motivation": "DFT-based MD is computationally expensive, limiting long-term, large-system simulations; integrating physics knowledge into ML can improve physical plausibility and reliability.", "method": "Formulate MD as a time-series forecasting task; use advanced forecasting models to predict atomic displacements rather than absolute positions; incorporate a physics-informed loss and an inference mechanism derived from DFT-parametrised pairwise Morse potentials to penalize unphysical atomic proximity.", "result": "Consistently outperforms standard baselines in simulation accuracy across diverse materials; enables stable modeling of thousands of MD steps in minutes, offering a scalable alternative to costly DFT simulations.", "conclusion": "Embedding physics knowledge into ML-based MD forecasting improves reliability and precision and provides a scalable, efficient alternative to conventional DFT-based approaches."}}
{"id": "2510.01253", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01253", "abs": "https://arxiv.org/abs/2510.01253", "authors": ["Jianzhang Zhang", "Jialong Zhou", "Chuang Liu"], "title": "OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models", "comment": null, "summary": "Large language models (LLMs) demonstrate strong mathematical reasoning, but\nreliance on closed-source APIs for OR tasks raises privacy concerns, and\ntraining open-source models from scratch incurs high compute costs. We\nintroduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a\nsemi-automatic data synthesis pipeline that generates diverse OR problem-answer\npairs and augments the model with external solvers to produce API calls. On\nthree of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution\naccuracy, exceeding size-matched baselines by over 4.3%. In zero-shot\nevaluation on two unseen OR problem types, it attains 54% average accuracy, a\n21 percentage-point improvement over the strongest baseline. These findings\nvalidate the efficacy of tool-augmented fine-tuning LLMs for accurate and\ngeneralizable OR problem modeling and solving.", "AI": {"tldr": "OR-Toolformer fine-tunes Llama-3.1-8B-Instruct via a semi-automatic data synthesis pipeline and solver augmentation to produce API calls, achieving strong OR performance and zero-shot generalization without relying on closed APIs.", "motivation": "Address privacy concerns with closed-source APIs for optimization and reasoning tasks and the high compute costs of training open models from scratch; enable accurate, generalizable OR solving with open models.", "method": "Fine-tune Llama-3.1-8B-Instruct using a semi-automatic data synthesis pipeline to generate diverse OR problem-answer pairs, and augment the model with external solvers to produce API calls.", "result": "On three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot evaluation on two unseen OR problem types, it attains 54% average accuracy, a 21-point improvement over the strongest baseline.", "conclusion": "Tool-augmented fine-tuning of LLMs is effective for accurate and generalizable OR problem modeling and solving, offering privacy-preserving and cost-efficient alternatives to closed APIs."}}
{"id": "2510.01357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01357", "abs": "https://arxiv.org/abs/2510.01357", "authors": ["Alejandro Gonzalez-Garcia", "Wei Xiao", "Wei Wang", "Alejandro Astudillo", "Wilm Decr\u00e9", "Jan Swevers", "Carlo Ratti", "Daniela Rus"], "title": "Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels", "comment": "IROS 2025", "summary": "Safe motion planning is essential for autonomous vessel operations,\nespecially in challenging spaces such as narrow inland waterways. However,\nconventional motion planning approaches are often computationally intensive or\noverly conservative. This paper proposes a safe motion planning strategy\ncombining Model Predictive Control (MPC) and Control Barrier Functions (CBFs).\nWe introduce a time-varying inflated ellipse obstacle representation, where the\ninflation radius is adjusted depending on the relative position and attitude\nbetween the vessel and the obstacle. The proposed adaptive inflation reduces\nthe conservativeness of the controller compared to traditional fixed-ellipsoid\nobstacle formulations. The MPC solution provides an approximate motion plan,\nand high-order CBFs ensure the vessel's safety using the varying inflation\nradius. Simulation and real-world experiments demonstrate that the proposed\nstrategy enables the fully-actuated autonomous robot vessel to navigate through\nnarrow spaces in real time and resolve potential deadlocks, all while ensuring\nsafety.", "AI": {"tldr": "Proposes a MPC-CBF safe motion planning framework for autonomous vessels in narrow waterways using adaptive time-varying ellipse obstacles to reduce conservativeness and enable real-time performance.", "motivation": "To enable safe, real-time navigation for autonomous vessels in constrained inland waterways where traditional planners are either too conservative or computationally heavy.", "method": "Use time-varying inflated ellipse obstacles with an inflation radius that depends on the vessel\u2013obstacle relative position and attitude. Solve an approximate motion plan via Model Predictive Control (MPC) and enforce safety with high-order Control Barrier Functions (CBFs) that account for the varying inflation. Validate through simulations and real-world experiments.", "result": "The approach enables fully actuated autonomous vessels to navigate through narrow spaces in real time, reduces conservativeness compared to fixed-ellipse representations, and helps resolve potential deadlocks while maintaining safety.", "conclusion": "MPC combined with high-order CBFs and an adaptive obstacle-inflation scheme provides a practical, less conservative, real-time safe motion-planning solution for autonomous vessels in challenging environments; the framework shows promise for broader robot navigation tasks in constrained spaces."}}
{"id": "2510.01347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01347", "abs": "https://arxiv.org/abs/2510.01347", "authors": ["Shuochen Chang"], "title": "Image Generation Based on Image Style Extraction", "comment": null, "summary": "Image generation based on text-to-image generation models is a task with\npractical application scenarios that fine-grained styles cannot be precisely\ndescribed and controlled in natural language, while the guidance information of\nstylized reference images is difficult to be directly aligned with the textual\nconditions of traditional textual guidance generation. This study focuses on\nhow to maximize the generative capability of the pretrained generative model,\nby obtaining fine-grained stylistic representations from a single given\nstylistic reference image, and injecting the stylistic representations into the\ngenerative body without changing the structural framework of the downstream\ngenerative model, so as to achieve fine-grained controlled stylized image\ngeneration. In this study, we propose a three-stage training style\nextraction-based image generation method, which uses a style encoder and a\nstyle projection layer to align the style representations with the textual\nrepresentations to realize fine-grained textual cue-based style guide\ngeneration. In addition, this study constructs the Style30k-captions dataset,\nwhose samples contain a triad of images, style labels, and text descriptions,\nto train the style encoder and style projection layer in this experiment.", "AI": {"tldr": "Three-stage style-extraction method for injecting a single-reference style into pretrained text-to-image models, via a style encoder and style projection layer, enabling fine-grained style-guided generation; introduces Style30k-captions dataset.", "motivation": "Fine-grained styles are hard to describe with natural language; aligning stylized reference images with textual guidance is challenging; objective to maximize generative capability without altering model architecture.", "method": "Three-stage training; use a style encoder and style projection layer to align style representations with textual representations; train on Style30k-captions dataset consisting of image, style labels, and text descriptions to supervise the components.", "result": "Not stated in the abstract.", "conclusion": "Not stated in the abstract."}}
{"id": "2510.01218", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01218", "abs": "https://arxiv.org/abs/2510.01218", "authors": ["Sergey Troshin", "Wafaa Mohammed", "Yan Meng", "Christof Monz", "Antske Fokkens", "Vlad Niculae"], "title": "Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs", "comment": "Second Conference on Language Modeling, 2025", "summary": "Diversity is an essential metric for evaluating the creativity of outputs\ngenerated by language models. Temperature-based sampling is a common strategy\nto increase diversity. However, for tasks that require high precision, e.g.,\nmathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$\nor top-$p$, degrades reasoning quality. We demonstrate that the loss of\naccuracy is caused by sampling incorrect continuations in sensitive decoding\npositions. To address this, in this paper, we propose \\textbf{selective\nsampling}, a method that dynamically switches between greedy and\nhigh-temperature sampling based on a sampling risk metric. This risk metric\nestimates the likelihood of output errors when applying high-temperature\nsampling on the current token position. To predict sampling risk, we train a\nlightweight classifier on a small subset of verifiable problems. The trained\nclassifier can be integrated with the base language model with minimal latency\noverhead. Experiments on mathematical reasoning tasks demonstrate that\nselective sampling enhances the quality-diversity trade-off, even in\nhigh-temperature settings.", "AI": {"tldr": "Selective sampling dynamically switches between greedy and high-temperature sampling using a sampling-risk metric, guided by a lightweight classifier trained on verifiable problems, to improve the quality\u2013diversity trade-off in math reasoning tasks with minimal latency overhead.", "motivation": "High-temperature sampling boosts diversity but hurts precision in tasks requiring exact reasoning (e.g., math). There is a need for risk-aware sampling that preserves accuracy while maintaining diversity.", "method": "Introduce selective sampling that computes a sampling risk metric at each token position and dynamically chooses between greedy decoding and high-temperature sampling. Train a lightweight classifier on a small set of verifiable problems to predict sampling risk and integrate it with the base language model with minimal latency. The classifier informs when to apply high-temperature sampling based on estimated risk.", "result": "Experiments on mathematical reasoning tasks show an improved quality\u2013diversity trade-off under high-temperature settings, indicating that selective sampling reduces error-prone risky sampling while preserving diversity.", "conclusion": "Selective sampling provides a practical, low-overhead approach to risk-aware decoding that can be integrated with existing LMs to improve accuracy in precision-demanding tasks while maintaining diversity."}}
{"id": "2510.01272", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01272", "abs": "https://arxiv.org/abs/2510.01272", "authors": ["Kunal Jha", "Aydan Yuenan Huang", "Eric Ye", "Natasha Jaques", "Max Kleiman-Weiner"], "title": "Modeling Others' Minds as Code", "comment": null, "summary": "Accurate prediction of human behavior is essential for robust and safe\nhuman-AI collaboration. However, existing approaches for modeling people are\noften data-hungry and brittle because they either make unrealistic assumptions\nabout rationality or are too computationally demanding to adapt rapidly. Our\nkey insight is that many everyday social interactions may follow predictable\npatterns; efficient \"scripts\" that minimize cognitive load for actors and\nobservers, e.g., \"wait for the green light, then go.\" We propose modeling these\nroutines as behavioral programs instantiated in computer code rather than\npolicies conditioned on beliefs and desires. We introduce ROTE, a novel\nalgorithm that leverages both large language models (LLMs) for synthesizing a\nhypothesis space of behavioral programs, and probabilistic inference for\nreasoning about uncertainty over that space. We test ROTE in a suite of\ngridworld tasks and a large-scale embodied household simulator. ROTE predicts\nhuman and AI behaviors from sparse observations, outperforming competitive\nbaselines -- including behavior cloning and LLM-based methods -- by as much as\n50% in terms of in-sample accuracy and out-of-sample generalization. By\ntreating action understanding as a program synthesis problem, ROTE opens a path\nfor AI systems to efficiently and effectively predict human behavior in the\nreal-world.", "AI": {"tldr": "ROTE uses behavioral programs synthesized with LLMs and probabilistic inference to predict human behavior, treating action understanding as program synthesis; it outperforms baselines in gridworld and a household sim.", "motivation": "Existing human-modeling approaches are data-hungry or brittle due to strong rationality assumptions and heavy computation; many everyday interactions follow predictable routines that can be efficiently modeled as scripts to reduce cognitive load.", "method": "ROTE leverages LLMs to generate a hypothesis space of behavioral programs (scripts) and employs probabilistic inference to reason about uncertainty over that space, tested on gridworld tasks and a large-scale embodied household simulator.", "result": "ROTE achieves higher predictive accuracy than competitive baselines (including behavior cloning and LLM-based methods), with improvements up to 50% in both in-sample accuracy and out-of-sample generalization.", "conclusion": "Viewing action understanding as program synthesis offers a scalable, efficient path for predicting human behavior in real-world human-AI collaboration."}}
{"id": "2510.01381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01381", "abs": "https://arxiv.org/abs/2510.01381", "authors": ["Spencer Teetaert", "Sven Lilge", "Jessica Burgner-Kahrs", "Timothy D. Barfoot"], "title": "A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots", "comment": "17 pages, 11 figures. Submitted to IEEE Transactions on Robotics", "summary": "State estimation techniques for continuum robots (CRs) typically involve\nusing computationally complex dynamic models, simplistic shape approximations,\nor are limited to quasi-static methods. These limitations can be sensitive to\nunmodelled disturbances acting on the robot. Inspired by a factor-graph\noptimization paradigm, this work introduces a continuous-time stochastic state\nestimation framework for continuum robots. We introduce factors based on\ncontinuous-time kinematics that are corrupted by a white noise Gaussian process\n(GP). By using a simple robot model paired with high-rate sensing, we show\nadaptability to unmodelled external forces and data dropout. The result\ncontains an estimate of the mean and covariance for the robot's pose, velocity,\nand strain, each of which can be interpolated continuously in time or space.\nThis same interpolation scheme can be used during estimation, allowing for\ninclusion of measurements on states that are not explicitly estimated. Our\nmethod's inherent sparsity leads to a linear solve complexity with respect to\ntime and interpolation queries in constant time. We demonstrate our method on a\nCR with gyroscope and pose sensors, highlighting its versatility in real-world\nsystems.", "AI": {"tldr": "A probabilistic, continuous-time state estimation framework for continuum robots using Gaussian-process\u2013based factors in a factor-graph, enabling robust, sparsity-friendly estimation of pose, velocity, and strain with continuous-time interpolation and resilience to disturbances.", "motivation": "Current state estimators for continuum robots rely on computationally heavy dynamics, crude shape models, or quasi-static assumptions, making them sensitive to unmodeled disturbances and data dropouts.", "method": "Introduce continuous-time kinematic factors corrupted by a Gaussian process noise. Use a simple robot model with high-rate sensing; perform factor-graph optimization to estimate the mean and covariance of pose, velocity, and strain, with continuous-time interpolation in time or space. The interpolation scheme also allows incorporating measurements on states that are not explicitly estimated. The formulation yields sparsity, giving linear solve complexity with respect to time and interpolation queries.", "result": "Validated on a continuum robot equipped with gyroscope and pose sensors, demonstrating adaptability to external forces and data dropouts, and providing interpretable mean/covariance estimates that can be interpolated in time/space.", "conclusion": "The proposed GP-based continuous-time factor-graph framework offers robust, scalable, and interpolation-capable state estimation for continuum robots, resilient to disturbances and imperfect data, suitable for real-world deployment."}}
{"id": "2510.01362", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01362", "abs": "https://arxiv.org/abs/2510.01362", "authors": ["Shijia Feng", "Michael Wray", "Walterio Mayol-Cuevas"], "title": "EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels", "comment": "10 pages", "summary": "The ability to determine when a person struggles during skill acquisition is\ncrucial for both optimizing human learning and enabling the development of\neffective assistive systems. As skills develop, the type and frequency of\nstruggles tend to change, and understanding this evolution is key to\ndetermining the user's current stage of learning. However, existing\nmanipulation datasets have not focused on how struggle evolves over time. In\nthis work, we collect a dataset for struggle determination, featuring 61.68\nhours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle\nsegments collected from 76 participants. The dataset includes 18 tasks grouped\ninto four diverse activities -- tying knots, origami, tangram puzzles, and\nshuffling cards, representing different task variations. In addition,\nparticipants repeated the same task five times to capture their evolution of\nskill. We define the struggle determination problem as a temporal action\nlocalization task, focusing on identifying and precisely localizing struggle\nsegments with start and end times. Experimental results show that Temporal\nAction Localization models can successfully learn to detect struggle cues, even\nwhen evaluated on unseen tasks or activities. The models attain an overall\naverage mAP of 34.56% when generalizing across tasks and 19.24% across\nactivities, indicating that struggle is a transferable concept across various\nskill-based tasks while still posing challenges for further improvement in\nstruggle detection. Our dataset is available at\nhttps://github.com/FELIXFENG2019/EvoStruggle.", "AI": {"tldr": "A dataset and approach for struggle detection in skill learning using temporal action localization; EvoStruggle shows transferable struggle cues across tasks with room for improvement.", "motivation": "Understanding how struggle evolves during skill acquisition to optimize learning and enable adaptive assistance; existing datasets neglect the temporal evolution of struggle.", "method": "Collected 61.68 hours of video (2,793 videos, 5,385 annotated struggle segments) from 76 participants across 18 tasks in four activities (tying knots, origami, tangram puzzles, shuffling cards). Participants repeated tasks five times to capture skill evolution. Framed as temporal action localization to identify start/end struggle segments and evaluated generalization to unseen tasks/activities.", "result": "Temporal action localization models detected struggle segments with cross-task mAP of 34.56% and cross-activity mAP of 19.24%, indicating transferable yet task/activity-dependent struggle cues. Dataset is publicly available.", "conclusion": "Struggle signals transfer across diverse skill domains but remain challenging to detect; EvoStruggle provides a valuable resource for advancing adaptive learning and assistive systems, with temporal localization as a viable detection method."}}
{"id": "2510.01235", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01235", "abs": "https://arxiv.org/abs/2510.01235", "authors": ["Subham Ghosh", "Abhishek Tewari"], "title": "Automated Extraction of Material Properties using LLM-based AI Agents", "comment": null, "summary": "The rapid discovery of materials is constrained by the lack of large,\nmachine-readable datasets that couple performance metrics with structural\ncontext. Existing databases are either small, manually curated, or biased\ntoward first principles results, leaving experimental literature\nunderexploited. We present an agentic, large language model (LLM)-driven\nworkflow that autonomously extracts thermoelectric and structural-properties\nfrom about 10,000 full-text scientific articles. The pipeline integrates\ndynamic token allocation, zeroshot multi-agent extraction, and conditional\ntable parsing to balance accuracy against computational cost. Benchmarking on\n50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91\nfor thermoelectric properties and 0.82 for structural fields), while GPT-4.1\nMini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction\nof the cost, enabling practical large scale deployment. Applying this workflow,\nwe curated 27,822 temperature resolved property records with normalized units,\nspanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity,\npower factor, and thermal conductivity, together with structural attributes\nsuch as crystal class, space group, and doping strategy. Dataset analysis\nreproduces known thermoelectric trends, such as the superior performance of\nalloys over oxides and the advantage of p-type doping, while also surfacing\nbroader structure-property correlations. To facilitate community access, we\nrelease an interactive web explorer with semantic filters, numeric queries, and\nCSV export. This study delivers the largest LLM-curated thermoelectric dataset\nto date, provides a reproducible and cost-profiled extraction pipeline, and\nestablishes a foundation for scalable, data-driven materials discovery beyond\nthermoelectrics.", "AI": {"tldr": "Large-scale, LLM-driven workflow autonomously extracts thermoelectric and structural properties from ~10k full-text articles, achieving high accuracy and delivering a public dataset and tools to enable scalable data-driven materials discovery beyond thermoelectrics.", "motivation": "Addresses the scarcity of large, machine-readable datasets linking performance with structural context; current databases are small, manual, or biased toward first-principles results, underutilizing experimental literature.", "method": "An agentic, multimodal LLM workflow with dynamic token allocation, zero-shot multi-agent extraction, and conditional table parsing. Benchmarked on 50 curated papers (GPT-4.1 best, GPT-4.1 Mini cost-efficient). Curated 27,822 temperature-resolved property records (ZT, Seebeck, conductivity, resistivity, power factor, thermal conductivity) and structural attributes (crystal class, space group, doping). Provided an interactive web explorer with semantic filters and CSV export.", "result": "GPT-4.1 achieved F1 of 0.91 for thermoelectric properties and 0.82 for structural fields; GPT-4.1 Mini achieved F1 of 0.89 and 0.81 respectively. Dataset includes normalized records; analysis recapitulates known thermoelectric trends (alloys outperform oxides, p-type doping advantage) and reveals broader structure\u2013property correlations. The workflow is reproducible, cost-profiled, and scalable, culminating in the largest LLM-curated thermoelectric dataset to date and an accessible community explorer.", "conclusion": "Demonstrates scalable, data-driven material discovery beyond thermoelectrics, providing a reproducible extraction pipeline, a large curated dataset, and tools to facilitate community access and further discovery."}}
{"id": "2510.01293", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01293", "abs": "https://arxiv.org/abs/2510.01293", "authors": ["Zekun Jiang", "Chunming Xu", "Tianhang Zhou"], "title": "Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery", "comment": null, "summary": "The rapid advancement of artificial intelligence (AI) has demonstrated\nsubstantial potential in chemical engineering, yet existing AI systems remain\nlimited in interdisciplinary collaboration and exploration of uncharted\nproblems. To address these issues, we present the Cyber Academia-Chemical\nEngineering (CA-ChemE) system, a living digital town that enables self-directed\nresearch evolution and emergent scientific discovery through multi-agent\ncollaboration. By integrating domain-specific knowledge bases, knowledge\nenhancement technologies, and collaboration agents, the system successfully\nconstructs an intelligent ecosystem capable of deep professional reasoning and\nefficient interdisciplinary collaboration. Our findings demonstrate that\nknowledge base-enabled enhancement mechanisms improved dialogue quality scores\nby 10-15% on average across all seven expert agents, fundamentally ensuring\ntechnical judgments are grounded in verifiable scientific evidence. However, we\nobserved a critical bottleneck in cross-domain collaboration efficiency,\nprompting the introduction of a Collaboration Agent (CA) equipped with ontology\nengineering capabilities. CA's intervention achieved 8.5% improvements for\ndistant-domain expert pairs compared to only 0.8% for domain-proximate pairs -\na 10.6-fold difference - unveiling the \"diminished collaborative efficiency\ncaused by knowledge-base gaps\" effect. This study demonstrates how carefully\ndesigned multi-agent architectures can provide a viable pathway toward\nautonomous scientific discovery in chemical engineering.", "AI": {"tldr": "A knowledge-base\u2013enhanced multi-agent CA-ChemE system enables autonomous discovery in chemical engineering, with improved dialogue quality and cross-domain collaboration via a Collaboration Agent that mitigates knowledge-gap inefficiencies.", "motivation": "To address limited interdisciplinary collaboration and exploration of uncharted problems in AI-augmented chemical engineering by enabling self-directed research evolution through a living digital town of collaborating agents.", "method": "Integrate domain-specific knowledge bases, knowledge-enhancement technologies, and collaboration agents into a multi-agent system; employ ontology engineering within the Collaboration Agent to bridge cross-domain gaps.", "result": "Dialogue quality scores improved by 10\u201315% across seven expert agents; Collaboration Agent yielded 8.5% gains for distant-domain pairs vs 0.8% for domain-proximate pairs (a 10.6\u00d7 difference), revealing a knowledge-base-gap\u2013driven bottleneck that CA helps alleviate.", "conclusion": "A carefully designed multi-agent architecture with knowledge-based enhancements can support autonomous scientific discovery in chemical engineering, though cross-domain collaboration remains bottlenecked by knowledge-base gaps without targeted collaboration agents."}}
{"id": "2510.01388", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01388", "abs": "https://arxiv.org/abs/2510.01388", "authors": ["Arthur Zhang", "Xiangyun Meng", "Luca Calliari", "Dong-Ki Kim", "Shayegan Omidshafiei", "Joydeep Biswas", "Ali Agha", "Amirreza Shaban"], "title": "VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation", "comment": "9 pages, 6 figures, 3 tables", "summary": "Robots must adapt to diverse human instructions and operate safely in\nunstructured, open-world environments. Recent Vision-Language models (VLMs)\noffer strong priors for grounding language and perception, but remain difficult\nto steer for navigation due to differences in action spaces and pretraining\nobjectives that hamper transferability to robotics tasks. Towards addressing\nthis, we introduce VENTURA, a vision-language navigation system that finetunes\ninternet-pretrained image diffusion models for path planning. Instead of\ndirectly predicting low-level actions, VENTURA generates a path mask (i.e. a\nvisual plan) in image space that captures fine-grained, context-aware\nnavigation behaviors. A lightweight behavior-cloning policy grounds these\nvisual plans into executable trajectories, yielding an interface that follows\nnatural language instructions to generate diverse robot behaviors. To scale\ntraining, we supervise on path masks derived from self-supervised tracking\nmodels paired with VLM-augmented captions, avoiding manual pixel-level\nannotation or highly engineered data collection setups. In extensive real-world\nevaluations, VENTURA outperforms state-of-the-art foundation model baselines on\nobject reaching, obstacle avoidance, and terrain preference tasks, improving\nsuccess rates by 33% and reducing collisions by 54% across both seen and unseen\nscenarios. Notably, we find that VENTURA generalizes to unseen combinations of\ndistinct tasks, revealing emergent compositional capabilities. Videos, code,\nand additional materials: https://venturapath.github.io", "AI": {"tldr": "VENTURA is a vision-language navigation system that generates a visual plan (path mask) via diffusion models and grounds it with a lightweight behavior-cloning policy to execute robust, context-aware robotic navigation guided by natural language.", "motivation": "Bridge the gap between powerful vision-language models and robotics control, addressing misalignment in action spaces and pretraining objectives; enable fine-grained, context-aware navigation with scalable, self-supervised training.", "method": "Finetune internet-pretrained image diffusion models to produce path masks in image space (visual plans) rather than low-level actions; learn a lightweight behavior-cloning policy to convert plans into executable trajectories; supervise on path masks derived from self-supervised tracking models paired with VLM-augmented captions, avoiding manual pixel-level annotation; evaluate on real-world tasks including object reaching, obstacle avoidance, and terrain preference.", "result": "Outperforms state-of-the-art foundation-model baselines in real-world object-reaching, obstacle avoidance, and terrain preference tasks; 33% higher success rates and 54% fewer collisions across seen and unseen scenarios; demonstrates generalization to unseen combinations of distinct tasks (emergent compositionality).", "conclusion": "Visual plans produced by diffusion models, when grounded by a lightweight policy, offer scalable, transferable navigation capable of following natural language and generalizing to new task combinations; the approach reduces data labeling needs and improves safety and performance in open-world settings."}}
{"id": "2510.01370", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.01370", "abs": "https://arxiv.org/abs/2510.01370", "authors": ["Abu Bucker Siddik", "Diane Oyen", "Alexander Most", "Michal Kucer", "Ayan Biswas"], "title": "SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs", "comment": null, "summary": "We introduce Small PDE U-Net Solver (SPUS), a compact and efficient\nfoundation model (FM) designed as a unified neural operator for solving a wide\nrange of partial differential equations (PDEs). Unlike existing\nstate-of-the-art PDE FMs-primarily based on large complex transformer\narchitectures with high computational and parameter overhead-SPUS leverages a\nlightweight residual U-Net-based architecture that has been largely\nunderexplored as a foundation model architecture in this domain. To enable\neffective learning in this minimalist framework, we utilize a simple yet\npowerful auto-regressive pretraining strategy which closely replicates the\nbehavior of numerical solvers to learn the underlying physics. SPUS is\npretrained on a diverse set of fluid dynamics PDEs and evaluated across 6\nchallenging unseen downstream PDEs spanning various physical systems.\nExperimental results demonstrate that SPUS using residual U-Net based\narchitecture achieves state-of-the-art generalization on these downstream tasks\nwhile requiring significantly fewer parameters and minimal fine-tuning data,\nhighlighting its potential as a highly parameter-efficient FM for solving\ndiverse PDE systems.", "AI": {"tldr": "A lightweight residual U-Net-based foundation model (SPUS) learns via autoregressive pretraining to solve a wide range of PDEs, achieving state-of-the-art generalization on unseen tasks with far fewer parameters and minimal fine-tuning.", "motivation": "Current PDE foundation models rely on large transformers, which are resource-intensive. There is a need for compact, parameter-efficient models that generalize across diverse PDEs and require limited fine-tuning data.", "method": "Propose SPUS, a compact residual U-Net architecture pretrained autoregressively to mimic numerical solver behavior, trained on diverse fluid dynamics PDEs, and evaluated on six unseen downstream PDEs from various physical systems.", "result": "SPUS achieves state-of-the-art generalization on unseen PDEs while using significantly fewer parameters and minimal fine-tuning data.", "conclusion": "SPUS demonstrates the viability of a parameter-efficient FM for solving diverse PDEs, highlighting the potential of lightweight architectures for broad PDE solvers."}}
{"id": "2510.01240", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01240", "abs": "https://arxiv.org/abs/2510.01240", "authors": ["Zukang Xu", "Xing Hu", "Qiang Wu", "Dawei Yang"], "title": "RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing tasks. However, their exponentially\nincreasing parameters pose significant challenges for deployment on\nresource-constrained devices. Vector Quantization (VQ) shows great promise for\nlow-bit quantization (e.g., 2 to 4 bits), but existing work faces two key\nchallenges: unconstrained direction error and suboptimal bit allocation. In\nthis paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit\nquantization for LLMs. RSAVQ introduces two geometry-driven innovations that\neffectively mitigate above limitations: (1) Error Direction Sensitivity\nGuidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced\nRiemannian metric to project quantization errors onto low-sensitivity\ndirections in the parameter space. Specifically, this projection is performed\nalong the negative natural gradient direction, which effectively suppresses\nerror expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which\nconstructs a channel-wise sensitivity metric via FIM curvature analysis to\ndynamically guide bit resource allocation. The approach facilitates a globally\noptimal quantization solution within prescribed bit constraints. Experiments\ndemonstrate that RSAVQ outperforms existing methods for LLMs. For example, in\n2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by\n0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a\npractical solution for constrained environments and a theoretical bridge\nbetween information geometry and the quantization of neural networks, advancing\nefficient deep learning.", "AI": {"tldr": "RSAVQ uses information-geometry-guided error direction and channel sensitivity to achieve extremely low-bit quantization (2-bit) for LLMs, outperforming prior methods.", "motivation": "LLMs have enormous parameters making deployment on resource-constrained devices challenging. Vector quantization to ultra-low bits (2\u20134 bits) helps, but existing methods suffer from unconstrained error directions and suboptimal bit allocation.", "method": "Introduce two geometry-driven components based on Fisher Information Matrix: (1) Error Direction Sensitivity Guidance (EDSG) projects quantization errors along low-sensitivity directions using the negative natural gradient to suppress error expansion; (2) Weight Channel Sensitivity Guidance (WCSG) builds a channel-wise sensitivity metric via FIM curvature to adaptively allocate bit precision per channel, aiming for a globally optimal quantization under bit constraints.", "result": "RSAVQ achieves better performance than baselines on LLM quantization. In 2-bit quantization of LLaMA-3 8B, it improves perplexity by 0.4 points and zero-shot accuracy by about 1.5 percentage points compared with VPTQ and QuIP#.", "conclusion": "The approach offers a practical solution for deploying LLMs in constrained environments and provides a theoretical link between information geometry and neural network quantization, advancing efficient deep learning."}}
{"id": "2510.01295", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.01295", "abs": "https://arxiv.org/abs/2510.01295", "authors": ["Zarreen Reza"], "title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent\n  Abilities, and Scaling", "summary": "As Large Language Models (LLMs) transition from static tools to autonomous\nagents, traditional evaluation benchmarks that measure performance on\ndownstream tasks are becoming insufficient. These methods fail to capture the\nemergent social and cognitive dynamics that arise when agents communicate,\npersuade, and collaborate in interactive environments. To address this gap, we\nintroduce a novel evaluation framework that uses multi-agent debate as a\ncontrolled \"social laboratory\" to discover and quantify these behaviors. In our\nframework, LLM-based agents, instantiated with distinct personas and\nincentives, deliberate on a wide range of challenging topics under the\nsupervision of an LLM moderator. Our analysis, enabled by a new suite of\npsychometric and semantic metrics, reveals several key findings. Across\nhundreds of debates, we uncover a powerful and robust emergent tendency for\nagents to seek consensus, consistently reaching high semantic agreement ({\\mu}\n> 0.88) even without explicit instruction and across sensitive topics. We show\nthat assigned personas induce stable, measurable psychometric profiles,\nparticularly in cognitive effort, and that the moderators persona can\nsignificantly alter debate outcomes by structuring the environment, a key\nfinding for external AI alignment. This work provides a blueprint for a new\nclass of dynamic, psychometrically grounded evaluation protocols designed for\nthe agentic setting, offering a crucial methodology for understanding and\nshaping the social behaviors of the next generation of AI agents. We have\nreleased the code and results at\nhttps://github.com/znreza/multi-agent-LLM-eval-for-debate.", "AI": {"tldr": "Proposes a psychometrically grounded, multi-agent LLM debate framework as a dynamic evaluation lab, revealing emergent consensus, persona effects, and moderator influence across topics, with code released.", "motivation": "Traditional benchmarks fail to capture social-cognitive dynamics when LLMs operate as autonomous agents; there is a need for controlled environments to study debate, persuasion, collaboration, and alignment risks in agentic settings.", "method": "Instantiate multiple LLM-based agents with distinct personas and incentives, supervise debates with an LLM moderator, develop psychometric and semantic metrics, run hundreds of debates across topics, and analyze results to reveal emergent social dynamics and alignment-relevant patterns.", "result": "The framework reveals a robust emergent tendency for agents to seek consensus, achieving high semantic agreement (mu > 0.88) without explicit instruction; personas produce stable psychometric cognitive profiles; the moderator\u2019s persona can substantially shape debate outcomes by structuring the environment; the approach provides scalable evaluation tooling and insights.", "conclusion": "Provides a blueprint for dynamic, psychometrically grounded evaluation in agentic settings, aiding understanding and shaping the social behaviors of next-generation AI agents; code and results have been released for external use."}}
{"id": "2510.01389", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01389", "abs": "https://arxiv.org/abs/2510.01389", "authors": ["Ulas Berk Karli", "Ziyao Shangguan", "Tesca FItzgerald"], "title": "INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models", "comment": null, "summary": "Recent Vision-Language-Action (VLA) models show strong generalization\ncapabilities, yet they lack introspective mechanisms for anticipating failures\nand requesting help from a human supervisor. We present \\textbf{INSIGHT}, a\nlearning framework for leveraging token-level uncertainty signals to predict\nwhen a VLA should request help. Using $\\pi_0$-FAST as the underlying model, we\nextract per-token \\emph{entropy}, \\emph{log-probability}, and Dirichlet-based\nestimates of \\emph{aleatoric and epistemic uncertainty}, and train compact\ntransformer classifiers to map these sequences to help triggers. We explore\nsupervision regimes for strong or weak supervision, and extensively compare\nthem across in-distribution and out-of-distribution tasks. Our results show a\ntrade-off: strong labels enable models to capture fine-grained uncertainty\ndynamics for reliable help detection, while weak labels, though noisier, still\nsupport competitive introspection when training and evaluation are aligned,\noffering a scalable path when dense annotation is impractical. Crucially, we\nfind that modeling the temporal evolution of token-level uncertainty signals\nwith transformers provides far greater predictive power than static\nsequence-level scores. This study provides the first systematic evaluation of\nuncertainty-based introspection in VLAs, opening future avenues for active\nlearning and for real-time error mitigation through selective human\nintervention.", "AI": {"tldr": "INSIGHT uses token-level uncertainty signals from a VLA model to predict when to request human help. Temporal modeling with transformers outperforms static scores; strong supervision yields fine-grained triggers while weak supervision remains viable with aligned training/evaluation, across ID and OOD settings.", "motivation": "Vision-Language-Action models generalize well but lack introspective mechanisms to anticipate failures or solicit human assistance. There is a need for reliable, scalable triggers for human intervention and for enabling active learning and real-time error mitigation.", "method": "From a pi_0-FAST base model, extract per-token entropy, log-probability, and Dirichlet-based estimates of aleatoric and epistemic uncertainty. Train compact transformer classifiers to map token-level uncertainty sequences to help-trigger signals. Compare strong vs weak supervision regimes and evaluate under in-distribution and out-of-distribution tasks, emphasizing the temporal evolution of uncertainty rather than static scores.", "result": "Strong supervision captures fine-grained uncertainty dynamics for reliable help triggers; weak supervision, though noisier, remains competitive when training and evaluation are aligned, offering scalable annotation. Modeling temporal evolution of token-level uncertainty with transformers yields substantially higher predictive power than static sequence-level metrics. This constitutes the first systematic evaluation of uncertainty-based introspection in VLAs, enabling active learning and real-time selective human intervention.", "conclusion": "Uncertainty-based introspection is viable for VLA systems, with clear trade-offs between supervision strength and scalability. The study points to future work in active learning and real-time error mitigation through selective human collaboration, leveraging temporal uncertainty dynamics for better help-trigger decisions."}}
{"id": "2510.01399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01399", "abs": "https://arxiv.org/abs/2510.01399", "authors": ["Shubhankar Borse", "Farzad Farhadzadeh", "Munawar Hayat", "Fatih Porikli"], "title": "DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation", "comment": null, "summary": "State-of-the-art text-to-image models excel at realism but collapse on\nmulti-human prompts - duplicating faces, merging identities, and miscounting\nindividuals. We introduce DisCo (Reinforcement with Diversity Constraints), the\nfirst RL-based framework to directly optimize identity diversity in multi-human\ngeneration. DisCo fine-tunes flow-matching models via Group-Relative Policy\nOptimization (GRPO) with a compositional reward that (i) penalizes intra-image\nfacial similarity, (ii) discourages cross-sample identity repetition, (iii)\nenforces accurate person counts, and (iv) preserves visual fidelity through\nhuman preference scores. A single-stage curriculum stabilizes training as\ncomplexity scales, requiring no extra annotations. On the DiverseHumans\nTestset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global\nIdentity Spread - surpassing both open-source and proprietary methods (e.g.,\nGemini, GPT-Image) while maintaining competitive perceptual quality. Our\nresults establish DisCo as a scalable, annotation-free solution that resolves\nthe long-standing identity crisis in generative models and sets a new benchmark\nfor compositional multi-human generation.", "AI": {"tldr": "DisCo is an RL-based framework that optimizes identity diversity in multi-human generation, outperforming existing models on DiverseHumans with high unique-face accuracy and strong identity spread, without extra annotations.", "motivation": "Current text-to-image models produce realistic images but fail on prompts involving multiple humans, causing duplicated faces, merged identities, and miscounts. There is a need for a scalable, annotation-free method that ensures identity diversity and accurate counts.", "method": "DisCo fine-tunes flow-matching models using Group-Relative Policy Optimization (GRPO) with a compositional reward: penalize intra-image facial similarity, discourage cross-sample identity repetition, enforce correct person counts, and preserve visual fidelity via human-preference signals; trained with a single-stage curriculum to stabilize learning and requires no additional annotations.", "result": "On the DiverseHumans Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global Identity Spread, outperforming both open-source and proprietary methods (e.g., Gemini, GPT-Image) while maintaining competitive perceptual quality.", "conclusion": "DisCo provides a scalable, annotation-free solution that resolves the identity crisis in generative models and sets a new benchmark for compositional multi-human generation."}}
{"id": "2510.01261", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01261", "abs": "https://arxiv.org/abs/2510.01261", "authors": ["Vedant Palit"], "title": "Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks", "comment": "16 pages, 10 figures", "summary": "Federated learning is vulnerable to poisoning and backdoor attacks under\npartial observability. We formulate defence as a partially observable\nsequential decision problem and introduce a trust-aware Deep Q-Network that\nintegrates multi-signal evidence into client trust updates while optimizing a\nlong-horizon robustness--accuracy objective. On CIFAR-10, we (i) establish a\nbaseline showing steadily improving accuracy, (ii) show through a Dirichlet\nsweep that increased client overlap consistently improves accuracy and reduces\nASR with stable detection, and (iii) demonstrate in a signal-budget study that\naccuracy remains steady while ASR increases and ROC-AUC declines as\nobservability is reduced, which highlights that sequential belief updates\nmitigate weaker signals. Finally, a comparison with random, linear-Q, and\npolicy gradient controllers confirms that DQN achieves the best\nrobustness--accuracy trade-off.", "AI": {"tldr": "A defense for federated learning against poisoning/backdoor under partial observability using a trust-aware DQN, achieving better robustness-accuracy trade-off on CIFAR-10.", "motivation": "Federated learning is vulnerable to poisoning/backdoor attacks, especially when client observability is partial; there is a need to fuse multi-signal evidence and sequential decision-making to update client trust and improve robustness without sacrificing accuracy.", "method": "Formulates defense as a partially observable sequential decision problem; proposes a trust-aware Deep Q-Network that updates client trust from multiple signals and optimizes a long-horizon robustness-accuracy objective; empirical CIFAR-10 experiments with Dirichlet overlap, signal-budget analysis, and controller comparisons.", "result": "Empirical results show baseline accuracy improvement; increased client overlap improves accuracy and reduces ASR with stable detection; decreasing observability keeps accuracy steady but ASR rises and ROC-AUC falls, showing sequential belief updates help with weaker signals; DQN achieves best robustness-accuracy trade-off vs random, linear-Q, and policy gradient baselines.", "conclusion": "Trust-aware DQN yields superior robustness-accuracy trade-off under partial observability for federated learning; sequential belief updates mitigate weak signals and improve trust estimation compared to baselines."}}
{"id": "2510.01304", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01304", "abs": "https://arxiv.org/abs/2510.01304", "authors": ["Yu Zeng", "Wenxuan Huang", "Shiting Huang", "Xikun Bao", "Yukun Qi", "Yiming Zhao", "Qiuchen Wang", "Lin Chen", "Zehui Chen", "Huaian Chen", "Wanli Ouyang", "Feng Zhao"], "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models", "comment": null, "summary": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .", "AI": {"tldr": "AGILE treats jigsaw solving as an interactive agentic learning loop where the model generates executable code to act in a visual environment; it yields large gains on simple jigsaws and generalizes across tasks, offering a scalable approach to improving perception and reasoning in vision-language models.", "motivation": "Current large vision-language models show limited perceptual and reasoning abilities. Simple jigsaw tasks expose core deficits. The scarcity and limited scalability of high-quality multimodal reinforcement learning data hinder progress. There is a need for data-efficient, scalable methods that can enhance perception and reasoning through interaction.", "method": "AGILE formulates jigsaw solving as an interactive process. At each step, the model generates executable code to perform an action based on the current state. The environment provides fine-grained visual feedback to guide task completion. Through iterative observation-action cycles, the model explores and receives feedback to progressively improve perceptual and reasoning capabilities.", "result": "In 2x2 jigsaw settings, accuracy improves from 9.5% to 82.8%. Across 9 general vision tasks, AGILE achieves an average improvement of 3.1%. The code and datasets are available at the provided URL, indicating reproducibility and scalability.", "conclusion": "AGILE introduces a novel interactive learning paradigm for enhancing perception and reasoning in multimodal models. It offers a scalable, data-efficient approach to improve core capabilities and generalization, addressing multimodal RL data scarcity and potentially broadening the applicability of VLMs to reasoning tasks."}}
{"id": "2510.01402", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01402", "abs": "https://arxiv.org/abs/2510.01402", "authors": ["Hun Kuk Park", "Taekyung Kim", "Dimitra Panagou"], "title": "Beyond Collision Cones: Dynamic Obstacle Avoidance for Nonholonomic Robots via Dynamic Parabolic Control Barrier Functions", "comment": "The first two authors contributed equally to this work. Project page:\n  https://www.taekyung.me/dpcbf", "summary": "Control Barrier Functions (CBFs) are a powerful tool for ensuring the safety\nof autonomous systems, yet applying them to nonholonomic robots in cluttered,\ndynamic environments remains an open challenge. State-of-the-art methods often\nrely on collision-cone or velocity-obstacle constraints which, by only\nconsidering the angle of the relative velocity, are inherently conservative and\ncan render the CBF-based quadratic program infeasible, particularly in dense\nscenarios. To address this issue, we propose a Dynamic Parabolic Control\nBarrier Function (DPCBF) that defines the safe set using a parabolic boundary.\nThe parabola's vertex and curvature dynamically adapt based on both the\ndistance to an obstacle and the magnitude of the relative velocity, creating a\nless restrictive safety constraint. We prove that the proposed DPCBF is valid\nfor a kinematic bicycle model subject to input constraints. Extensive\ncomparative simulations demonstrate that our DPCBF-based controller\nsignificantly enhances navigation success rates and QP feasibility compared to\nbaseline methods. Our approach successfully navigates through dense\nenvironments with up to 100 dynamic obstacles, scenarios where collision\ncone-based methods fail due to infeasibility.", "AI": {"tldr": "Introduces Dynamic Parabolic Control Barrier Function (DPCBF) to relax safety constraints for nonholonomic robots in dynamic clutter. The parabola-based CBF adapts to obstacle distance and relative speed, improving QP feasibility and navigation success in dense scenarios (up to 100 dynamic obstacles) over collision-cone baselines, validated on a kinematic bicycle model.", "motivation": "Collision-cone/velocity-obstacle constraints are conservative for nonholonomic robots, often causing CBF-QP infeasibility in dense, dynamic environments. There is a need for a safety boundary that adapts to motion context.", "method": "Formulate a dynamic parabolic safe set; adapt parabola vertex and curvature as functions of distance to obstacle and magnitude of relative velocity; derive CBF conditions; prove CBF validity for a kinematic bicycle with input constraints; conduct extensive simulations comparing to baselines.", "result": "Demonstrates higher navigation success rates and QP feasibility; handles dense environments with up to 100 dynamic obstacles; outperforms collision-cone-based methods which fail due to infeasibility in dense settings.", "conclusion": "DPCBF provides a less conservative safety boundary enabling safer and more feasible control for nonholonomic robots in dynamic clutter; shows significant performance gains and broad potential applicability."}}
{"id": "2510.01448", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01448", "abs": "https://arxiv.org/abs/2510.01448", "authors": ["Angel Daruna", "Nicholas Meegan", "Han-Pang Chiu", "Supun Samarasekera", "Rakesh Kumar"], "title": "GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings", "comment": "preprint under review", "summary": "Worldwide visual geo-localization seeks to determine the geographic location\nof an image anywhere on Earth using only its visual content. Learned\nrepresentations of geography for visual geo-localization remain an active\nresearch topic despite much progress. We formulate geo-localization as aligning\nthe visual representation of the query image with a learned geographic\nrepresentation. Our novel geographic representation explicitly models the world\nas a hierarchy of geographic embeddings. Additionally, we introduce an approach\nto efficiently fuse the appearance features of the query image with its\nsemantic segmentation map, forming a robust visual representation. Our main\nexperiments demonstrate improved all-time bests in 22 out of 25 metrics\nmeasured across five benchmark datasets compared to prior state-of-the-art\n(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional\nablation studies support the claim that these gains are primarily driven by the\ncombination of geographic and visual representations.", "AI": {"tldr": "Proposes a hierarchical geographic embedding for worldwide visual geo-localization and fuses query appearance with a semantic segmentation map to form a robust representation, achieving state-of-the-art performance across 25 metrics on five benchmarks, with ablations showing gains mainly from combining geographic and visual cues.", "motivation": "To improve localizing a photo to its exact global location using only visual content, addressing limitations of existing learned geographic representations and the need for robust, scalable representations that can leverage both appearance and semantic information.", "method": "Model the world as a hierarchy of geographic embeddings and align the visual query with a learned geographic representation. Introduce a method to efficiently fuse the query's appearance features with its semantic segmentation map, creating a unified visual representation. Evaluate across five benchmark datasets, comparing to prior SOTA methods and LVLMs, with ablation studies.", "result": "Achieves improved all-time bests in 22 out of 25 metrics across five benchmarks compared to prior SOTA methods and recent LVLMs. Ablation studies indicate gains arise from the combination of geographic and visual representations.", "conclusion": "Hierarchical geographic embeddings combined with appearance-segmentation fusion provide a robust and scalable improvement for worldwide visual geo-localization, surpassing prior SOTA and demonstrating the value of integrating geographic structure with rich visual/semantic cues."}}
{"id": "2510.01262", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01262", "abs": "https://arxiv.org/abs/2510.01262", "authors": ["Koyena Chowdhury", "Paramita Koley", "Abhijnan Chakraborty", "Saptarshi Ghosh"], "title": "RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction", "comment": null, "summary": "Accurate prediction of train delays is critical for efficient railway\noperations, enabling better scheduling and dispatching decisions. While earlier\napproaches have largely focused on forecasting the exact delays of individual\ntrains, recent studies have begun exploring station-level delay prediction to\nsupport higher-level traffic management. In this paper, we propose the\nRailway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed\nto forecast average arrival delays of all the incoming trains at railway\nstations for a particular time period. Our approach incorporates several\narchitectural innovations and novel feature integrations, including train\nfrequency-aware spatial attention, which significantly enhances predictive\nperformance. To support this effort, we curate and release a comprehensive\ndataset for the entire Indian Railway Network (IRN), spanning 4,735 stations\nacross 17 zones - the largest and most diverse railway network studied to date.\nWe conduct extensive experiments using multiple state-of-the-art baselines,\ndemonstrating consistent improvements across standard metrics. Our work not\nonly advances the modeling of average delay prediction in large-scale rail\nnetworks but also provides an open dataset to encourage further research in\nthis critical domain.", "AI": {"tldr": "Proposes Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN) to predict average station arrival delays in a large-scale Indian Rail Network; introduces train-frequency-aware spatial attention; releases a large IRN dataset; demonstrates improvements over baselines.", "motivation": "Accurate delay forecasting is crucial for efficient railway operations and higher-level traffic management. The shift from predicting individual train delays to station-level arrivals enables broader decision support, and the authors address a data and scalability gap by building a large, diverse IRN dataset.", "method": "RSTGCN combines spatio-temporal graph convolutional networks with railway-specific features, notably train-frequency-aware spatial attention, to forecast the average arrival delays of all incoming trains at each station for a given time period. The study uses data from 4,735 stations across 17 zones and benchmarks against multiple state-of-the-art baselines.", "result": "The approach achieves consistent improvements over baselines on standard metrics across the large-scale IRN setting, and the authors release the dataset to support further research in railway delay forecasting.", "conclusion": "RSTGCN advances station-level delay prediction in large railway networks and provides a valuable open dataset to spur future research and better traffic management in rail systems."}}
{"id": "2510.01346", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01346", "abs": "https://arxiv.org/abs/2510.01346", "authors": ["Tudor Achim", "Alex Best", "Kevin Der", "Math\u00efs F\u00e9d\u00e9rico", "Sergei Gukov", "Daniel Halpern-Leister", "Kirsten Henningsgard", "Yury Kudryashov", "Alexander Meiburg", "Martin Michelsen", "Riley Patterson", "Eric Rodriguez", "Laura Scharff", "Vikram Shanker", "Vladmir Sicca", "Hari Sowrirajan", "Aidan Swope", "Matyas Tamas", "Vlad Tenev", "Jonathan Thomm", "Harold Williams", "Lawrence Wu"], "title": "Aristotle: IMO-level Automated Theorem Proving", "comment": null, "summary": "We introduce Aristotle, an AI system that combines formal verification with\ninformal reasoning, achieving gold-medal-equivalent performance on the 2025\nInternational Mathematical Olympiad problems. Aristotle integrates three main\ncomponents: a Lean proof search system, an informal reasoning system that\ngenerates and formalizes lemmas, and a dedicated geometry solver. Our system\ndemonstrates state-of-the-art performance with favorable scaling properties for\nautomated theorem proving.", "AI": {"tldr": "Hybrid AI system Aristotle merges Lean-based proof search, informal lemma generation/formalization, and a geometry solver to reach gold-medal-equivalent performance on 2025 IMO problems, with favorable scaling in automated theorem proving.", "motivation": "To bridge formal verification with human-like informal mathematical reasoning for challenging problems and to improve automation and scalability in theorem proving.", "method": "Architecture with three components: (1) Lean proof search for formal verification; (2) informal reasoning system that generates and formalizes lemmas; (3) a dedicated geometry solver. Integration yields an end-to-end problem-solving pipeline and improved scalability.", "result": "Achieves state-of-the-art performance on IMO-style problems, attaining gold-medal-equivalent results on the 2025 IMO problems; demonstrates favorable scaling properties compared to baselines.", "conclusion": "Demonstrates that a hybrid formal+informal reasoning approach with a geometry solver can yield strong automated theorem proving performance and scalability, suggesting potential extensions to other domains and future integration refinements."}}
{"id": "2510.01404", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01404", "abs": "https://arxiv.org/abs/2510.01404", "authors": ["Lexi Foland", "Thomas Cohn", "Adam Wei", "Nicholas Pfaff", "Boyuan Chen", "Russ Tedrake"], "title": "How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?", "comment": "Under review. 8 pages, 3 figures, 3 tables. Additional results\n  available at https://diffusion-learns-kinematic.github.io", "summary": "Diffusion policies have shown impressive results in robot imitation learning,\neven for tasks that require satisfaction of kinematic equality constraints.\nHowever, task performance alone is not a reliable indicator of the policy's\nability to precisely learn constraints in the training data. To investigate, we\nanalyze how well diffusion policies discover these manifolds with a case study\non a bimanual pick-and-place task that encourages fulfillment of a kinematic\nconstraint for success. We study how three factors affect trained policies:\ndataset size, dataset quality, and manifold curvature. Our experiments show\ndiffusion policies learn a coarse approximation of the constraint manifold with\nlearning affected negatively by decreases in both dataset size and quality. On\nthe other hand, the curvature of the constraint manifold showed inconclusive\ncorrelations with both constraint satisfaction and task success. A hardware\nevaluation verifies the applicability of our results in the real world. Project\nwebsite with additional results and visuals:\nhttps://diffusion-learns-kinematic.github.io", "AI": {"tldr": "Diffusion policies learn only a coarse approximation of kinematic constraint manifolds; data size and quality influence learning; curvature shows no clear link to constraint satisfaction or task success; hardware validation supports findings.", "motivation": "To understand whether diffusion-based policies truly learn and obey kinematic constraint manifolds, beyond just achieving task-level success, and to identify factors that affect this learning.", "method": "Case study on a bimanual pick-and-place task that enforces a kinematic constraint. Systematically examined three factors\u2014dataset size, dataset quality, and manifold curvature\u2014through simulations and a hardware evaluation.\"", "result": "Diffusion policies converge to a coarse representation of the constraint manifold. Reductions in dataset size and quality degrade learning of the constraint. Curvature of the manifold shows inconclusive correlations with constraint satisfaction and task success.", "conclusion": "Learned constraint representations by diffusion policies are data-dependent and only approximate the true manifold; curvature effects are inconclusive. Hardware experiments validate the relevance of the findings for real-world robotic applications."}}
{"id": "2510.01454", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01454", "abs": "https://arxiv.org/abs/2510.01454", "authors": ["Nilay Naharas", "Dang Nguyen", "Nesihan Bulut", "Mohammadhossein Bateni", "Vahab Mirrokni", "Baharan Mirzasoleiman"], "title": "Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories", "comment": "30 pages, 10 figures, 5 tables, link:\n  https://bigml-cs-ucla.github.io/XMAS-project-page/", "summary": "Data-efficient learning aims to eliminate redundancy in large training\ndatasets by training models on smaller subsets of the most informative\nexamples. While data selection has been extensively explored for vision models\nand large language models (LLMs), it remains underexplored for Large\nVision-Language Models (LVLMs). Notably, none of existing methods can\noutperform random selection at different subset sizes. In this work, we propose\nthe first principled method for data-efficient instruction tuning of LVLMs. We\nprove that examples with similar cross-modal attention matrices during\ninstruction tuning have similar gradients. Thus, they influence model\nparameters in a similar manner and convey the same information to the model\nduring training. Building on this insight, we propose XMAS, which clusters\nexamples based on the trajectories of the top singular values of their\nattention matrices obtained from fine-tuning a small proxy LVLM. By sampling a\nbalanced subset from these clusters, XMAS effectively removes redundancy in\nlarge-scale LVLM training data. Extensive experiments show that XMAS can\ndiscard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while\nfully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and\nspeeding up its training by 1.2x. This is 30% more data reduction compared to\nthe best baseline for LLaVA-665k. The project's website can be found at\nhttps://bigml-cs-ucla.github.io/XMAS-project-page/.", "AI": {"tldr": "A data-efficient, principled subset selection method for LVLM instruction tuning named XMAS, clustering examples by cross-modal attention trajectory to reduce data without sacrificing performance.", "motivation": "Data redundancy in large LVLM instruction-tuning datasets and the lack of effective data selection methods for LVLMs; aim to reduce dataset size and training cost while maintaining performance.", "method": "Compute attention matrices for examples during fine-tuning on a small proxy LVLM; measure top singular value trajectories; cluster by these trajectories; sample a balanced subset from clusters; train LVLM on the subset (XMAS).", "result": "Can discard 50% of LLaVA-665k and 85% of Vision-Flan; preserve LLaVA-1.5-7B performance across 10 downstream tasks; training speedup of 1.2x; about 30% more data reduction than the best baseline for LLaVA-665k.", "conclusion": "XMAS demonstrates a principled approach to data-efficient LVLM instruction tuning via gradient-informed redundancy reduction based on cross-modal attention dynamics; enables substantial data reduction with maintained performance and faster training."}}
{"id": "2510.01263", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01263", "abs": "https://arxiv.org/abs/2510.01263", "authors": ["Yaron Meirovitch", "Fuming Yang", "Jeff Lichtman", "Nir Shavit"], "title": "Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency", "comment": null, "summary": "Most pruning methods remove parameters ranked by impact on loss (e.g.,\nmagnitude or gradient). We propose Budgeted Broadcast (BB), which gives each\nunit a local traffic budget (the product of its long-term on-rate $a_i$ and\nfan-out $k_i$). A constrained-entropy analysis shows that maximizing coding\nentropy under a global traffic budget yields a selectivity-audience balance,\n$\\log\\frac{1-a_i}{a_i}=\\beta k_i$. BB enforces this balance with simple local\nactuators that prune either fan-in (to lower activity) or fan-out (to reduce\nbroadcast). In practice, BB increases coding entropy and decorrelation and\nimproves accuracy at matched sparsity across Transformers for ASR, ResNets for\nface identification, and 3D U-Nets for synapse prediction, sometimes exceeding\ndense baselines. On electron microscopy images, it attains state-of-the-art F1\nand PR-AUC under our evaluation protocol. BB is easy to integrate and suggests\na path toward learning more diverse and efficient representations.", "AI": {"tldr": "Budgeted Broadcast (BB) is a sparsity method that uses per-unit traffic budgets to optimize coding entropy, balancing selectivity and audience, yielding higher entropy, decorrelation, and often improved accuracy across diverse models and tasks.", "motivation": "Standard pruning ranks parameters by loss impact but may neglect global resource constraints and representation diversity. BB targets a global sparsity budget while maximizing coding entropy, aiming for a more balanced and informative sparse structure that decorrelates features and preserves performance.", "method": "Define a per-unit traffic budget a_i (activity) times fan-out k_i; perform a constrained-entropy analysis to maximize coding entropy under a global budget; derive a per-unit selectivity-audience balance log((1\u2212a_i)/a_i)=\u03b2 k_i; implement simple local actuators to prune fan-in (reduce activity) or fan-out (reduce broadcast); evaluate on Transformers for ASR, ResNets for face identification, 3D U-Nets for synapse prediction, and electron microscopy images.", "result": "BB increases coding entropy and decorrelation in practice; yields improved accuracy at matched sparsity across multiple architectures and tasks, sometimes surpassing dense baselines; achieves state-of-the-art F1 and PR-AUC on electron microscopy under the authors\u2019 protocol; easy to integrate and promotes more diverse and efficient representations.", "conclusion": "BB provides a practical, integrable pruning framework that links sparsity to information-theoretic entropy, encouraging diversity in representations and potentially better generalization. It suggests a path toward learning representations with higher entropy and reduced redundancy across varied domains."}}
{"id": "2510.01353", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01353", "abs": "https://arxiv.org/abs/2510.01353", "authors": ["Darshan Deshpande", "Varun Gangal", "Hersh Mehta", "Anand Kannappan", "Rebecca Qian", "Peng Wang"], "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments", "comment": "Accepted to NeurIPS 2025 SEA Workshop", "summary": "Recent works on context and memory benchmarking have primarily focused on\nconversational instances but the need for evaluating memory in dynamic\nenterprise environments is crucial for its effective application. We introduce\nMEMTRACK, a benchmark designed to evaluate long-term memory and state tracking\nin multi-platform agent environments. MEMTRACK models realistic organizational\nworkflows by integrating asynchronous events across multiple communication and\nproductivity platforms such as Slack, Linear and Git. Each benchmark instance\nprovides a chronologically platform-interleaved timeline, with noisy,\nconflicting, cross-referring information as well as potential\ncodebase/file-system comprehension and exploration. Consequently, our benchmark\ntests memory capabilities such as acquistion, selection and conflict\nresolution. We curate the MEMTRACK dataset through both manual expert driven\ndesign and scalable agent based synthesis, generating ecologically valid\nscenarios grounded in real world software development processes. We introduce\npertinent metrics for Correctness, Efficiency, and Redundancy that capture the\neffectiveness of memory mechanisms beyond simple QA performance. Experiments\nacross SoTA LLMs and memory backends reveal challenges in utilizing memory\nacross long horizons, handling cross-platform dependencies, and resolving\ncontradictions. Notably, the best performing GPT-5 model only achieves a 60\\%\nCorrectness score on MEMTRACK. This work provides an extensible framework for\nadvancing evaluation research for memory-augmented agents, beyond existing\nfocus on conversational setups, and sets the stage for multi-agent,\nmulti-platform memory benchmarking in complex organizational settings", "AI": {"tldr": "A benchmark, MEMTRACK, evaluates long-term memory and state tracking in multi-platform enterprise agent environments using asynchronous platform-interleaved timelines, with metrics Correctness, Efficiency, and Redundancy; results show current models struggle with long-horizon memory (GPT-5 ~ 60% correctness).", "motivation": "Address the gap in memory evaluation for dynamic, real-world enterprise workflows where memory must integrate across multiple platforms and asynchronous events, beyond traditional conversational benchmarks.", "method": "Manual expert-driven design plus scalable agent-based synthesis to generate ecologically valid scenarios across Slack, Linear, and Git. Benchmarks provide chronologically platform-interleaved timelines with noisy, conflicting, cross-referring information and possible codebase/file-system exploration. Evaluate memory via acquisition, selection, and conflict resolution across long horizons. Dataset built both by experts and synthetic agents.", "result": "Experiments with state-of-the-art LLMs and memory backends reveal challenges in long-horizon memory usage, cross-platform dependencies, and contradiction resolution. Best model (GPT-5) achieves about 60% Correctness on MEMTRACK, highlighting substantial room for improvement.", "conclusion": "MEMTRACK offers an extensible framework for memory-augmented agents beyond conversational benchmarks, enabling multi-agent, multi-platform memory benchmarking in complex organizational settings and motivating further research into memory mechanisms."}}
{"id": "2510.01433", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01433", "abs": "https://arxiv.org/abs/2510.01433", "authors": ["Anukriti Singh", "Kasra Torshizi", "Khuzema Habib", "Kelin Yu", "Ruohan Gao", "Pratap Tokekar"], "title": "AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation", "comment": null, "summary": "Vision-based robot learning often relies on dense image or point-cloud\ninputs, which are computationally heavy and entangle irrelevant background\nfeatures. Existing keypoint-based approaches can focus on manipulation-centric\nfeatures and be lightweight, but either depend on manual heuristics or\ntask-coupled selection, limiting scalability and semantic understanding. To\naddress this, we propose AFFORD2ACT, an affordance-guided framework that\ndistills a minimal set of semantic 2D keypoints from a text prompt and a single\nimage. AFFORD2ACT follows a three-stage pipeline: affordance filtering,\ncategory-level keypoint construction, and transformer-based policy learning\nwith embedded gating to reason about the most relevant keypoints, yielding a\ncompact 38-dimensional state policy that can be trained in 15 minutes, which\nperforms well in real-time without proprioception or dense representations.\nAcross diverse real-world manipulation tasks, AFFORD2ACT consistently improves\ndata efficiency, achieving an 82% success rate on unseen objects, novel\ncategories, backgrounds, and distractors.", "AI": {"tldr": "Efficient, affordance-guided keypoint policy AFFORD2ACT distills semantic 2D keypoints from a text prompt and a single image into a compact 38D state, enabling fast, data-efficient manipulation with strong generalization (82% success on unseen objects).", "motivation": "Vision-based robot learning with dense inputs is computationally heavy and entangles background features. Existing keypoint methods either rely on manual heuristics or task-specific selection, hindering scalability and semantic understanding.", "method": "A three-stage pipeline: (1) affordance filtering from a text prompt to select relevant capabilities, (2) construction of category-level keypoints, and (3) transformer-based policy learning with embedded gating to reason about the most relevant keypoints, yielding a 38-dimensional state; training time is about 15 minutes.", "result": "In real-world manipulation tasks, AFFORD2ACT improves data efficiency and enables real-time operation without proprioception or dense representations, achieving an 82% success rate on unseen objects, novel categories, backgrounds, and distractors.", "conclusion": "AFFORD2ACT provides a scalable, lightweight, and semantically grounded framework for manipulation policies by distilling keypoints from affordances, enabling robust generalization across objects and backgrounds."}}
{"id": "2510.01478", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01478", "abs": "https://arxiv.org/abs/2510.01478", "authors": ["R\u0103zvan-Andrei Mati\u015fan", "Vincent Tao Hu", "Grigory Bartosh", "Bj\u00f6rn Ommer", "Cees G. M. Snoek", "Max Welling", "Jan-Willem van de Meent", "Mohammad Mahdi Derakhshani", "Floor Eijkelboom"], "title": "Purrception: Variational Flow Matching for Vector-Quantized Image Generation", "comment": null, "summary": "We introduce Purrception, a variational flow matching approach for\nvector-quantized image generation that provides explicit categorical\nsupervision while maintaining continuous transport dynamics. Our method adapts\nVariational Flow Matching to vector-quantized latents by learning categorical\nposteriors over codebook indices while computing velocity fields in the\ncontinuous embedding space. This combines the geometric awareness of continuous\nmethods with the discrete supervision of categorical approaches, enabling\nuncertainty quantification over plausible codes and temperature-controlled\ngeneration. We evaluate Purrception on ImageNet-1k 256x256 generation. Training\nconverges faster than both continuous flow matching and discrete flow matching\nbaselines while achieving competitive FID scores with state-of-the-art models.\nThis demonstrates that Variational Flow Matching can effectively bridge\ncontinuous transport and discrete supervision for improved training efficiency\nin image generation.", "AI": {"tldr": "Purrception uses variational flow matching for vector-quantized image generation, combining continuous transport with discrete codebook supervision to enable uncertainty estimation and faster training.", "motivation": "To bridge continuous transport methods and discrete vector-quantized supervision, improving efficiency and offering uncertainty quantification in code assignments.", "method": "Extend Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in a continuous embedding space; supports temperature-controlled generation and uncertainty quantification.", "result": "Faster training convergence than both continuous and discrete flow matching baselines; competitive FID on ImageNet-1k 256x256 against state-of-the-art models.", "conclusion": "Variational Flow Matching can effectively merge continuous transport with discrete supervision to enhance training efficiency in image generation."}}
{"id": "2510.01264", "categories": ["cs.LG", "cs.RO", "68T42 (Primary) 68T40, 68T05 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.01264", "abs": "https://arxiv.org/abs/2510.01264", "authors": ["Isaac Peterson", "Christopher Allred", "Jacob Morrey", "Mario Harper"], "title": "A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab", "comment": "8 page, 9 figures, code https://github.com/DIRECTLab/IsaacLab-HARL", "summary": "Multi-Agent Reinforcement Learning (MARL) is central to robotic systems\ncooperating in dynamic environments. While prior work has focused on these\ncollaborative settings, adversarial interactions are equally critical for\nreal-world applications such as pursuit-evasion, security, and competitive\nmanipulation. In this work, we extend the IsaacLab framework to support\nscalable training of adversarial policies in high-fidelity physics simulations.\nWe introduce a suite of adversarial MARL environments featuring heterogeneous\nagents with asymmetric goals and capabilities. Our platform integrates a\ncompetitive variant of Heterogeneous Agent Reinforcement Learning with Proximal\nPolicy Optimization (HAPPO), enabling efficient training and evaluation under\nadversarial dynamics. Experiments across several benchmark scenarios\ndemonstrate the framework's ability to model and train robust policies for\nmorphologically diverse multi-agent competition while maintaining high\nthroughput and simulation realism. Code and benchmarks are available at:\nhttps://github.com/DIRECTLab/IsaacLab-HARL .", "AI": {"tldr": "IsaacLab-HARL: a scalable adversarial MARL framework built on IsaacLab with heterogeneous agents and HAPPO integration, enabling training of robust adversarial policies in high-fidelity simulations, with open-source benchmarks.", "motivation": "Adversarial interactions are crucial for real-world robotic applications (pursuit-evasion, security, competitive manipulation). There is a need for scalable, realistic training in high-fidelity physics to develop robust policies.", "method": "Extend the IsaacLab platform to support adversarial MARL, create a suite of environments with heterogeneous agents and asymmetric goals/capabilities, and integrate a competitive variant of Heterogeneous Agent Proximal Policy Optimization (HAPPO) for efficient training under adversarial dynamics.", "result": "The framework can model and train robust policies for morphologically diverse multi-agent competition while maintaining high throughput and realistic simulations; experiments across benchmark scenarios demonstrate effectiveness.", "conclusion": "The proposed IsaacLab-HARL framework enables scalable adversarial MARL in high-fidelity physics, facilitating robust policy learning for competitive, heterogeneous multi-agent systems; code and benchmarks are openly available at the referenced repository."}}
{"id": "2510.01363", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01363", "abs": "https://arxiv.org/abs/2510.01363", "authors": ["Leon Garza", "Anantaa Kotal", "Michael A. Grasso", "Emre Umucu"], "title": "Retrieval-Augmented Framework for LLM-Based Clinical Decision Support", "comment": null, "summary": "The increasing complexity of clinical decision-making, alongside the rapid\nexpansion of electronic health records (EHR), presents both opportunities and\nchallenges for delivering data-informed care. This paper proposes a clinical\ndecision support system powered by Large Language Models (LLMs) to assist\nprescribing clinicians. The system generates therapeutic suggestions by\nanalyzing historical EHR data, including patient demographics, presenting\ncomplaints, clinical symptoms, diagnostic information, and treatment histories.\nThe framework integrates natural language processing with structured clinical\ninputs to produce contextually relevant recommendations. Rather than replacing\nclinician judgment, it is designed to augment decision-making by retrieving and\nsynthesizing precedent cases with comparable characteristics, drawing on local\ndatasets or federated sources where applicable. At its core, the system employs\na retrieval-augmented generation (RAG) pipeline that harmonizes unstructured\nnarratives and codified data to support LLM-based inference. We outline the\nsystem's technical components, including representation representation\nalignment and generation strategies. Preliminary evaluations, conducted with\nde-identified and synthetic clinical datasets, examine the clinical\nplausibility and consistency of the model's outputs. Early findings suggest\nthat LLM-based tools may provide valuable decision support in prescribing\nworkflows when appropriately constrained and rigorously validated. This work\nrepresents an initial step toward integration of generative AI into real-world\nclinical decision-making with an emphasis on transparency, safety, and\nalignment with established practices.", "AI": {"tldr": "LLM-powered clinical decision support using a retrieval-augmented generation pipeline to assist prescribing by fusing unstructured EHR narratives with structured data, emphasizing safety, alignment, and validation.", "motivation": "To address growing complexity in clinical decision-making and the expansion of EHR data, enabling data-informed prescribing without replacing clinician judgment.", "method": "A retrieval-augmented generation (RAG) pipeline that combines natural language processing with structured clinical inputs (demographics, complaints, symptoms, diagnostics, treatment history) and supports local or federated data sources. Representation alignment and generation strategies are used to generate contextually relevant recommendations. Evaluation uses de-identified/synthetic datasets to assess plausibility and consistency.", "result": "Preliminary evaluations indicate plausibility and consistency of outputs. Early findings suggest LLM-based tools can provide valuable decision support in prescribing workflows when constrained and rigorously validated.", "conclusion": "An initial step toward integrating generative AI into real-world clinical decision-making, prioritizing transparency, safety, and alignment with established practices."}}
{"id": "2510.01438", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01438", "abs": "https://arxiv.org/abs/2510.01438", "authors": ["Minglun Wei", "Xintong Yang", "Yu-Kun Lai", "Ze Ji"], "title": "Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation", "comment": "4 pages", "summary": "Robotic automation is accelerating scientific discovery by reducing manual\neffort in laboratory workflows. However, precise manipulation of powders\nremains challenging, particularly in tasks such as transport that demand\naccuracy and stability. We propose a trajectory optimisation framework for\npowder transport in laboratory settings, which integrates differentiable\nphysics simulation for accurate modelling of granular dynamics, low-dimensional\nskill-space parameterisation to reduce optimisation complexity, and a\ncurriculum-based strategy that progressively refines task competence over long\nhorizons. This formulation enables end-to-end optimisation of contact-rich\nrobot trajectories while maintaining stability and convergence efficiency.\nExperimental results demonstrate that the proposed method achieves superior\ntask success rates and stability compared to the reinforcement learning\nbaseline.", "AI": {"tldr": "A differentiable-physics, curriculum-guided trajectory optimization framework for powder transport in lab settings that improves stability and task success over a reinforcement learning baseline.", "motivation": "Powders are notoriously hard to manipulate precisely in automated labs; achieving stable, accurate transport is essential for scalable powder-handling workflows.", "method": "Integrates differentiable physics simulation to model granular dynamics, uses a low-dimensional skill-space parameterization to lower optimization complexity, and employs a curriculum-based strategy to progressively master long-horizon tasks, enabling end-to-end optimization of contact-rich robot trajectories.", "result": "Experiments show higher task success rates and improved stability compared with a reinforcement learning baseline.", "conclusion": "The proposed framework enables robust, efficient end-to-end optimization for granular powder transport in lab automation, illustrating effective combination of differentiable physics, skill-space reduction, and curriculum learning for complex contact-rich manipulation."}}
{"id": "2510.01498", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01498", "abs": "https://arxiv.org/abs/2510.01498", "authors": ["Yuxuan Ou", "Ning Bi", "Jiazhen Pan", "Jiancheng Yang", "Boliang Yu", "Usama Zidan", "Regent Lee", "Vicente Grau"], "title": "AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging", "comment": null, "summary": "While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic\naneurysms (AAA), the required iodinated contrast agents pose significant risks,\nincluding nephrotoxicity, patient allergies, and environmental harm. To reduce\ncontrast agent use, recent deep learning methods have focused on generating\nsynthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a\nmulti-stage pipeline that first generates images and then performs\nsegmentation, which leads to error accumulation and fails to leverage shared\nsemantic and anatomical structures. To address this, we propose a unified deep\nlearning framework that generates synthetic CECT images from NCCT scans while\nsimultaneously segmenting the aortic lumen and thrombus. Our approach\nintegrates conditional diffusion models (CDM) with multi-task learning,\nenabling end-to-end joint optimization of image synthesis and anatomical\nsegmentation. Unlike previous multitask diffusion models, our approach requires\nno initial predictions (e.g., a coarse segmentation mask), shares both encoder\nand decoder parameters across tasks, and employs a semi-supervised training\nstrategy to learn from scans with missing segmentation labels, a common\nconstraint in real-world clinical data. We evaluated our method on a cohort of\n264 patients, where it consistently outperformed state-of-the-art single-task\nand multi-stage models. For image synthesis, our model achieved a PSNR of 25.61\ndB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,\nit improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus\nDice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to\nmore accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm\nfrom 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to\nnnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.", "AI": {"tldr": "AortaDiff: a unified conditional diffusion + multi-task framework that jointly synthesizes CECT from NCCT and segments aortic lumen and thrombus; achieves higher PSNR and Dice than baselines; uses semi-supervised training; evaluated on 264 patients; code available.", "motivation": "Reduce risks and costs of iodinated contrast in abdominal aortic aneurysm CT by enabling end-to-end synthetic imaging and segmentation that leverage shared anatomy, avoiding error accumulation from staged pipelines and handling missing labels in real-world data.", "method": "A unified conditional diffusion model integrated with multi-task learning to simultaneously synthesize synthetic CECT from NCCT and segment the aortic lumen and thrombus. The model shares encoder/decoder across tasks, requires no initial coarse segmentation, and uses semi-supervised training to learn from scans with missing segmentation labels.", "result": "On 264-patient cohort, synthetic CECT achieved PSNR 25.61 dB vs 23.80 dB for single-task CDM; lumen Dice 0.89 vs 0.87; thrombus Dice 0.53 vs 0.48; lumen diameter MAE 4.19 mm vs 5.78; thrombus area error 33.85% vs 41.45% (nnU-Net). Code: GitHub link.", "conclusion": "The end-to-end, semi-supervised, multi-task diffusion framework outperforms single-task and multi-stage baselines, improving image quality and segmentation accuracy, and yielding more accurate clinical measurements, potentially enabling contrast reduction in AAA assessment."}}
{"id": "2510.01265", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01265", "abs": "https://arxiv.org/abs/2510.01265", "authors": ["Ali Hatamizadeh", "Syeda Nahida Akter", "Shrimai Prabhumoye", "Jan Kautz", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Yejin Choi"], "title": "RLP: Reinforcement as a Pretraining Objective", "comment": "RLP introduces a new paradigm for RL-based Pretraining", "summary": "The dominant paradigm for training large reasoning models starts with\npre-training using next-token prediction loss on vast amounts of data.\nReinforcement learning, while powerful in scaling reasoning, is introduced only\nas the very last phase of post-training, preceded by supervised fine-tuning.\nWhile dominant, is this an optimal way of training? In this paper, we present\nRLP, an information-driven reinforcement pretraining objective, that brings the\ncore spirit of reinforcement learning -- exploration -- to the last phase of\npretraining. The key idea is to treat chain-of-thought as an exploratory\naction, with rewards computed based on the information gain it provides for\npredicting future tokens. This training objective essentially encourages the\nmodel to think for itself before predicting what comes next, thus teaching an\nindependent thinking behavior earlier in the pretraining. More concretely, the\nreward signal measures the increase in log-likelihood of the next token when\nconditioning on both context and a sampled reasoning chain, compared to\nconditioning on context alone. This approach yields a verifier-free dense\nreward signal, allowing for efficient training for the full document stream\nduring pretraining. Specifically, RLP reframes reinforcement learning for\nreasoning as a pretraining objective on ordinary text, bridging the gap between\nnext-token prediction and the emergence of useful chain-of-thought reasoning.\nPretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an\neight-benchmark math-and-science suite by 19%. With identical post-training,\nthe gains compound, with the largest improvements on reasoning-heavy tasks such\nas AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2\nincreases the overall average from 42.81% to 61.32% and raises the average on\nscientific reasoning by 23%, demonstrating scalability across architectures and\nmodel sizes.", "AI": {"tldr": "Information-driven reinforcement pretraining (RLP): treat chain-of-thought as exploratory action during pretraining; rewards information gain in predicting the next token; yields notable gains across models and math-science benchmarks.", "motivation": "The dominant training pipeline (pretraining with next-token prediction, then supervised fine-tuning, then RL in a final phase) may be suboptimal for emergent reasoning. The paper proposes injecting exploration and reasoning earlier in pretraining by using a reinforcement-like objective driven by information gain.", "method": "RLP defines a reward as the increase in log-likelihood of the next token when conditioning on context plus a sampled reasoning chain versus conditioning on context alone. This dense, verifier-free reward turns RL for reasoning into a pretraining objective on ordinary text, enabling scalable training across full document streams. The approach is evaluated by pretraining Qwen3-1.7B-Base and applying RLP to Nemotron-Nano-12B-v2.", "result": "For Qwen3-1.7B-Base, pretraining with RLP lifts average performance across an eight-benchmark math-and-science suite by 19%, with largest gains on AIME25 and MMLU-Pro. Applying RLP to Nemotron-Nano-12B-v2 increases overall average from 42.81% to 61.32%, and raises scientific reasoning average by 23%.", "conclusion": "RLP offers an information-driven pretraining objective that fuses exploration with next-token modeling, yielding scalable gains in reasoning capabilities across architectures and sizes. It suggests a promising direction to integrate reasoning emergence into pretraining rather than relegating it to post-training RL, while leaving room for further ablations, broader benchmarks, and analysis of reward design."}}
{"id": "2510.01367", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01367", "abs": "https://arxiv.org/abs/2510.01367", "authors": ["Xinpeng Wang", "Nitish Joshi", "Barbara Plank", "Rico Angell", "He He"], "title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort", "comment": null, "summary": "Reward hacking, where a reasoning model exploits loopholes in a reward\nfunction to achieve high rewards without solving the intended task, poses a\nsignificant threat. This behavior may be explicit, i.e. verbalized in the\nmodel's chain-of-thought (CoT), or implicit, where the CoT appears benign thus\nbypasses CoT monitors. To detect implicit reward hacking, we propose TRACE\n(Truncated Reasoning AUC Evaluation). Our key observation is that hacking\noccurs when exploiting the loophole is easier than solving the actual task.\nThis means that the model is using less `effort' than required to achieve high\nreward. TRACE quantifies effort by measuring how early a model's reasoning\nbecomes sufficient to pass a verifier. We progressively truncate a model's CoT\nat various lengths, force the model to answer, and measure the verifier-passing\nrate at each cutoff. A hacking model, which takes a shortcut, will achieve a\nhigh passing rate with only a small fraction of its CoT, yielding a large area\nunder the accuracy-vs-length curve. TRACE achieves over 65% gains over our\nstrongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B\nmonitor in coding. We further show that TRACE can discover unknown loopholes\nduring training. Overall, TRACE offers a scalable unsupervised approach for\noversight where current monitoring methods prove ineffective.", "AI": {"tldr": "TRACE is a scalable, unsupervised method to detect implicit reward hacking by truncating a model's chain-of-thought (CoT) and measuring how early the reasoning suffices to pass a verifier, with higher area under the accuracy-vs-length curve indicating shortcut use.", "motivation": "Reward hacking poses a serious threat, including implicit leakage where CoT appears benign. Existing monitors struggle with unseen loopholes and lack scalable oversight; there is a need for an unsupervised detector that can reveal shortcuts in reasoning.", "method": "Progressively truncate a model's CoT at multiple lengths, force a final answer, and measure the verifier-passing rate at each cutoff. Compute the area under the accuracy-vs-length curve (AUC). A short-cut (hacking) model will pass with only a small fraction of CoT, yielding a large AUC. Evaluated on math reasoning (with a 72B CoT monitor) and coding (with a 32B monitor),TRACE shows significant gains and can identify unknown loopholes during training.", "result": "TRACE achieved >65% gains over the strongest 72B CoT monitor in math reasoning and >30% gains over a 32B monitor in coding, demonstrating substantial improvement over existing monitors and the ability to uncover unknown loopholes during training.", "conclusion": "TRACE offers a scalable, unsupervised approach for oversight that is effective against implicit reward hacking and loopholes in CoT, improving monitoring where current methods are ineffective."}}
{"id": "2510.01452", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01452", "abs": "https://arxiv.org/abs/2510.01452", "authors": ["Laura Connolly", "Tamas Ungi", "Adnan Munawar", "Anton Deguet", "Chris Yeung", "Russell H. Taylor", "Parvin Mousavi", "Gabor Fichtinger Keyvan Hashtrudi-Zaad"], "title": "Touching the tumor boundary: A pilot study on ultrasound based virtual fixtures for breast-conserving surgery", "comment": null, "summary": "Purpose: Delineating tumor boundaries during breast-conserving surgery is\nchallenging as tumors are often highly mobile, non-palpable, and have\nirregularly shaped borders. To address these challenges, we introduce a\ncooperative robotic guidance system that applies haptic feedback for tumor\nlocalization. In this pilot study, we aim to assess if and how this system can\nbe successfully integrated into breast cancer care.\n  Methods: A small haptic robot is retrofitted with an electrocautery blade to\noperate as a cooperatively controlled surgical tool. Ultrasound and\nelectromagnetic navigation are used to identify the tumor boundaries and\nposition. A forbidden region virtual fixture is imposed when the surgical tool\ncollides with the tumor boundary. We conducted a study where users were asked\nto resect tumors from breast simulants both with and without the haptic\nguidance. We then assess the results of these simulated resections both\nqualitatively and quantitatively.\n  Results: Virtual fixture guidance is shown to improve resection margins. On\naverage, users find the task to be less mentally demanding, frustrating, and\neffort intensive when haptic feedback is available. We also discovered some\nunanticipated impacts on surgical workflow that will guide design adjustments\nand training protocol moving forward.\n  Conclusion: Our results suggest that virtual fixtures can help localize tumor\nboundaries in simulated breast-conserving surgery. Future work will include an\nextensive user study to further validate these results and fine-tune our\nguidance system.", "AI": {"tldr": "A haptic-guided cooperative robotic system using virtual fixtures improves tumor boundary localization in simulated breast-conserving surgery, reducing cognitive load and improving margins compared to non-haptic guidance.", "motivation": "Difficulties in delineating mobile, non-palpable, and irregular breast tumors during breast-conserving surgery necessitate improved guidance to achieve clean surgical margins.", "method": "Retrofit a small haptic robot with an electrocautery blade for cooperative control; use ultrasound and electromagnetic navigation to identify tumor boundaries; impose a forbidden-region virtual fixture when the tool hits the boundary; compare tumor resections in simulants with and without haptic guidance; assess results qualitatively and quantitatively.", "result": "Virtual fixture guidance improves resection margins; users report lower mental demand, frustration, and perceived effort with haptic guidance; identification of unanticipated workflow impacts informing design/training.", "conclusion": "Virtual fixtures appear beneficial for localizing tumor boundaries in simulated breast-conserving surgery; future work includes extensive user studies and system refinements to validate and optimize the guidance system."}}
{"id": "2510.01513", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.01513", "abs": "https://arxiv.org/abs/2510.01513", "authors": ["Basem Rizk", "Joel Walsh", "Mark Core", "Benjamin Nye"], "title": "From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding", "comment": null, "summary": "Analysis of multi-modal content can be tricky, computationally expensive, and\nrequire a significant amount of engineering efforts. Lots of work with\npre-trained models on static data is out there, yet fusing these opensource\nmodels and methods with complex data such as videos is relatively challenging.\nIn this paper, we present a framework that enables efficiently prototyping\npipelines for multi-modal content analysis. We craft a candidate recipe for a\npipeline, marrying a set of pre-trained models, to convert videos into a\ntemporal semi-structured data format. We translate this structure further to a\nframe-level indexed knowledge graph representation that is query-able and\nsupports continual learning, enabling the dynamic incorporation of new\ndomain-specific knowledge through an interactive medium.", "AI": {"tldr": "Proposes a framework to rapidly prototype multi-modal video analysis pipelines by converting videos into a temporal semi-structured format and a frame-level, queryable knowledge graph that supports continual learning.", "motivation": "Multi-modal content analysis is hard, computationally heavy, and requires substantial engineering. While many pretrained models exist for static data, fusing them for complex data like videos remains challenging and inefficient.", "method": "Introduce a framework and a candidate pipeline recipe that combines pretrained models to convert videos into a temporal semi-structured data format, then translate this into a frame-level indexed knowledge graph that is queryable and supports continual learning through interactive knowledge integration.", "result": "A proposed framework that enables efficient prototyping of multi-modal pipelines, producing a temporal semi-structured representation and a frame-level knowledge graph that supports querying and continual learning, with interactive knowledge incorporation.", "conclusion": "The framework lowers engineering overhead for multi-modal video analysis and enables dynamic integration of domain-specific knowledge via an interactive medium, facilitating rapid prototyping and continual learning."}}
{"id": "2510.01269", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01269", "abs": "https://arxiv.org/abs/2510.01269", "authors": ["Rohan Vitthal Thorat", "Juhi Singh", "Rajdip Nayek"], "title": "Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance", "comment": "Paper accepted for presentation at ICCMS 2025. The submission\n  includes 10 pages and 6 figures", "summary": "Structural vibrations induced by external excitations pose significant risks,\nincluding safety hazards for occupants, structural damage, and increased\nmaintenance costs. While conventional model-based control strategies, such as\nLinear Quadratic Regulator (LQR), effectively mitigate vibrations, their\nreliance on accurate system models necessitates tedious system identification.\nThis tedious system identification process can be avoided by using a model-free\nReinforcement learning (RL) method. RL controllers derive their policies solely\nfrom observed structural behaviour, eliminating the requirement for an explicit\nstructural model. For an RL controller to be truly model-free, its training\nmust occur on the actual physical system rather than in simulation. However,\nduring this training phase, the RL controller lacks prior knowledge and it\nexerts control force on the structure randomly, which can potentially harm the\nstructure. To mitigate this risk, we propose guiding the RL controller using a\nLinear Quadratic Regulator (LQR) controller. While LQR control typically relies\non an accurate structural model for optimal performance, our observations\nindicate that even an LQR controller based on an entirely incorrect model\noutperforms the uncontrolled scenario. Motivated by this finding, we introduce\na hybrid control framework that integrates both LQR and RL controllers. In this\napproach, the LQR policy is derived from a randomly selected model and its\nparameters. As this LQR policy does not require knowledge of the true or an\napproximate structural model the overall framework remains model-free. This\nhybrid approach eliminates dependency on explicit system models while\nminimizing exploration risks inherent in naive RL implementations. As per our\nknowledge, this is the first study to address the critical training safety\nchallenge of RL-based vibration control and provide a validated solution.", "AI": {"tldr": "A hybrid model-free vibration control approach combines RL with an LQR baseline to safely train on real hardware without an accurate model, showing performance gains over uncontrolled cases even when the LQR model is incorrect.", "motivation": "Structural vibrations pose safety risks and maintenance costs. Traditional LQR requires accurate system models, and pure model-free RL requires risky real-world training. A safe, model-free training framework is needed.", "method": "Develop a hybrid controller where the LQR policy is derived from a randomly chosen model and used to guide RL during training on the physical system. The approach remains model-free because the LQR does not rely on the true model, eliminating the need for explicit system identification.", "result": "RL guided by the LQR policy outperforms the uncontrolled case and reduces exploration risks; the framework removes dependence on explicit structural models and is claimed to be the first to address training safety in RL-based vibration control; validated solution.", "conclusion": "This work provides a safe, model-free path for RL-based vibration control, reducing the need for system identification and mitigating training risks, marking a novel contribution to safe RL in structural control."}}
{"id": "2510.01375", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01375", "abs": "https://arxiv.org/abs/2510.01375", "authors": ["Humaid Ibrahim", "Nikolai Rozanov", "Marek Rei"], "title": "Fine-tuning with RAG for Improving LLM Learning of New Skills", "comment": "Under review at ICLR 2026", "summary": "Large language model (LLM) agents deployed for multi-step tasks frequently\nfail in predictable ways: attempting actions with unmet preconditions, issuing\nredundant commands, or mishandling environment constraints. While\nretrieval-augmented generation (RAG) can improve performance by providing\nruntime guidance, it requires maintaining external knowledge databases and adds\ncomputational overhead at every deployment. We propose a simple pipeline that\nconverts inference-time retrieval into learned competence through distillation.\nOur approach: (1) extracts compact, reusable hints from agent failures, (2)\nuses these hints to generate improved teacher trajectories via one-shot\nretrieval at episode start, and (3) trains student models on these trajectories\nwith hint strings removed, forcing internalization rather than memorization.\nAcross two interactive benchmarks, ALFWorld (household tasks) and WebShop\n(online shopping), distilled students consistently outperform baseline agents,\nachieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving\nWebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens\nthan retrieval-augmented teachers depending on the environment. The approach\ngeneralizes across model scales (7B/14B parameters) and agent architectures\n(ReAct/StateAct), demonstrating that retrieval benefits can be effectively\ninternalized through targeted fine-tuning without permanent runtime\ndependencies.", "AI": {"tldr": "A simple distillation pipeline converts retrieval-guided performance into internal competence by extracting failure hints, generating improved teacher trajectories via one-shot retrieval at episode start, and training students with hints removed, yielding strong gains with fewer tokens and no runtime retrieval dependencies.", "motivation": "LLM agents often fail due to unmet preconditions, redundancy, or poor environment handling. While retrieval-augmented generation can help, it relies on external memory and adds runtime cost. The work seeks a lightweight alternative that preserves retrieval benefits without ongoing dependencies.", "method": "1) Extract compact failure-derived hints from agent failures. 2) Use these hints to produce enhanced teacher trajectories through one-shot retrieval at episode start. 3) Train student models on these trajectories with hint strings removed to force internalization rather than memorization. Evaluated on ALFWorld and WebShop across 7B/14B models and ReAct/StateAct architectures.", "result": "Distilled students outperform baselines on both benchmarks (ALFWorld: 91% vs 79%; WebShop: 72 vs 61). They use 10\u201360% fewer tokens than retrieval-augmented teachers. The approach generalizes across model scales and architectures, demonstrating that retrieval benefits can be internalized via targeted fine-tuning without permanent runtime dependencies.", "conclusion": "Retrieval advantages can be effectively internalized through distillation, enabling robust multi-step task performance without external retrieval during deployment. This offers a scalable, architecture-agnostic path to improve efficiency and reduce runtime overhead while preserving gains from retrieval-informed guidance."}}
{"id": "2510.01483", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01483", "abs": "https://arxiv.org/abs/2510.01483", "authors": ["Mohamad Al Mdfaa", "Svetlana Lukina", "Timur Akhtyamov", "Arthur Nigmatzyanov", "Dmitrii Nalberskii", "Sergey Zagoruyko", "Gonzalo Ferrer"], "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Vision-language models (VLMs) have shown potential for robot navigation but\nencounter fundamental limitations: they lack persistent scene memory, offer\nlimited spatial reasoning, and do not scale effectively with video duration for\nreal-time application. We present VL-KnG, a Visual Scene Understanding system\nthat tackles these challenges using spatiotemporal knowledge graph construction\nand computationally efficient query processing for navigation goal\nidentification. Our approach processes video sequences in chunks utilizing\nmodern VLMs, creates persistent knowledge graphs that maintain object identity\nover time, and enables explainable spatial reasoning through queryable graph\nstructures. We also introduce WalkieKnowledge, a new benchmark with about 200\nmanually annotated questions across 8 diverse trajectories spanning\napproximately 100 minutes of video data, enabling fair comparison between\nstructured approaches and general-purpose VLMs. Real-world deployment on a\ndifferential drive robot demonstrates practical applicability, with our method\nachieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5\nPro performance while providing explainable reasoning supported by the\nknowledge graph, computational efficiency for real-time deployment across\ndifferent tasks, such as localization, navigation and planning. Code and\ndataset will be released after acceptance.", "AI": {"tldr": "VL-KnG builds a spatiotemporal knowledge-graph-based visual scene understanding system for robot navigation, enabling persistent object memory and explainable spatial reasoning; it introduces WalkieKnowledge benchmark and demonstrates real-time feasibility with competitive performance.", "motivation": "To address persistent memory, limited spatial reasoning, and poor scalability of current VLM-based navigation systems; need for persistent scene graphs and efficient querying for real-time robotic tasks.", "method": "Process video in chunks using VLMs to build a persistent, identity-maintaining spatiotemporal knowledge graph; enable queryable graph-based spatial reasoning; introduce WalkieKnowledge benchmark with ~200 questions over 8 trajectories (~100 min); validate on a differential-drive robot for localization, navigation, planning; compare against Gemini 2.5 Pro; report accuracy and success metrics.", "result": "Achieves 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5 Pro; provides explainable reasoning via knowledge graph; computational efficiency suitable for real-time deployment; scalable across tasks; benchmark and dataset to be released.", "conclusion": "VL-KnG offers a practical, scalable framework for vision-language navigation with persistent scene understanding and explainable reasoning, demonstrated on real robots and new benchmark; promises broad applicability and future release of code/dataset."}}
{"id": "2510.01524", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01524", "abs": "https://arxiv.org/abs/2510.01524", "authors": ["Viraj Prabhu", "Yutong Dai", "Matthew Fernandez", "Jing Gu", "Krithika Ramakrishnan", "Yanqi Luo", "Silvio Savarese", "Caiming Xiong", "Junnan Li", "Zeyuan Chen", "Ran Xu"], "title": "WALT: Web Agents that Learn Tools", "comment": null, "summary": "Web agents promise to automate complex browser tasks, but current methods\nremain brittle -- relying on step-by-step UI interactions and heavy LLM\nreasoning that break under dynamic layouts and long horizons. Humans, by\ncontrast, exploit website-provided functionality through high-level operations\nlike search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),\na framework that reverse-engineers latent website functionality into reusable\ninvocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust\nimplementations of automations already designed into websites -- spanning\ndiscovery (search, filter, sort), communication (post, comment, upvote), and\ncontent management (create, edit, delete). Tools abstract away low-level\nexecution: instead of reasoning about how to click and type, agents simply call\nsearch(query) or create(listing). This shifts the computational burden from\nfragile step-by-step reasoning to reliable tool invocation. On VisualWebArena\nand WebArena, WALT achieves higher success with fewer steps and less\nLLM-dependent reasoning, establishing a robust and generalizable paradigm for\nbrowser automation.", "AI": {"tldr": "WALT turns website functionality into reusable, high-level tools to automate browser tasks, reducing brittle step-by-step reasoning.", "motivation": "Current web automation relies on fragile UI interactions and heavy LLM reasoning; humans leverage built-in website features (search, filter, sort, etc.).", "method": "Reverse-engineer latent website functionality into invocable tools (e.g., search(query), create(listing), post, comment). Tools encapsulate low-level actions; agents invoke tools rather than clicking/typing.", "result": "WALT achieves higher success with fewer steps and less LLM-dependent reasoning on VisualWebArena and WebArena, demonstrating robustness and generalizability.", "conclusion": "A robust, generalizable paradigm for browser automation by exposing website-designed functionalities as reusable tools."}}
{"id": "2510.01271", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01271", "abs": "https://arxiv.org/abs/2510.01271", "authors": ["Arend Hintze", "Asadullah Najam", "Jory Schossau"], "title": "Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations", "comment": null, "summary": "Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is\ncrucial for advancing their interpretability and improving their design. This\nstudy introduces an innovative information-theoretic method to identify and\nanalyze information-transfer nodes within RNNs, which we refer to as\n\\textit{information relays}. By quantifying the mutual information between\ninput and output vectors across nodes, our approach pinpoints critical pathways\nthrough which information flows during network operations. We apply this\nmethodology to both synthetic and real-world time series classification tasks,\nemploying various RNN architectures, including Long Short-Term Memory (LSTM)\nnetworks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns\nof information relay across different architectures, offering insights into how\ninformation is processed and maintained over time. Additionally, we conduct\nnode knockout experiments to assess the functional importance of identified\nnodes, significantly contributing to explainable artificial intelligence by\nelucidating how specific nodes influence overall network behavior. This study\nnot only enhances our understanding of the complex mechanisms driving RNNs but\nalso provides a valuable tool for designing more robust and interpretable\nneural networks.", "AI": {"tldr": "Introduces an information-theoretic framework to identify information-relay nodes (information relays) in RNNs by measuring mutual information between inputs and outputs across nodes; validates on synthetic and real time-series tasks with LSTM/GRU; uses node knockout to gauge importance; aims to improve interpretability and design of RNNs.", "motivation": "Enhance interpretability of RNNs by revealing internal information-flow structure and identify key nodes that preserve/transfer information, with potential to guide robust model design.", "method": "Define information relays as nodes with significant mutual information transfer. Compute mutual information between input and output vectors across nodes during network operation. Apply to various RNN architectures (LSTM, GRU) on synthetic and real-world time-series classification tasks. Perform node knockout experiments to test functional importance of relays.", "result": "Expose distinct information-relay patterns across architectures, revealing how information is processed and retained over time. Node knockout confirms functional importance of identified nodes. The approach provides a practical explainability tool and insights to design more robust/interpretable networks.", "conclusion": "The study advances understanding of RNN internal dynamics and contributes to explainable AI by offering a method to map information flow and guide architecture design."}}
{"id": "2510.01398", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01398", "abs": "https://arxiv.org/abs/2510.01398", "authors": ["Yang Liu", "Zaid Abulawi", "Abhiram Garimidi", "Doyeong Lim"], "title": "Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents", "comment": null, "summary": "Modern engineering increasingly relies on vast datasets generated by\nexperiments and simulations, driving a growing demand for efficient, reliable,\nand broadly applicable modeling strategies. There is also heightened interest\nin developing data-driven approaches, particularly neural network models, for\neffective prediction and analysis of scientific datasets. Traditional\ndata-driven methods frequently involve extensive manual intervention, limiting\ntheir ability to scale effectively and generalize to diverse applications. In\nthis study, we propose an innovative pipeline utilizing Large Language Model\n(LLM) agents to automate data-driven modeling and analysis, with a particular\nemphasis on regression tasks. We evaluate two LLM-agent frameworks: a\nmulti-agent system featuring specialized collaborative agents, and a\nsingle-agent system based on the Reasoning and Acting (ReAct) paradigm. Both\nframeworks autonomously handle data preprocessing, neural network development,\ntraining, hyperparameter optimization, and uncertainty quantification (UQ). We\nvalidate our approach using a critical heat flux (CHF) prediction benchmark,\ninvolving approximately 25,000 experimental data points from the OECD/NEA\nbenchmark dataset. Results indicate that our LLM-agent-developed model\nsurpasses traditional CHF lookup tables and delivers predictive accuracy and UQ\non par with state-of-the-art Bayesian optimized deep neural network models\ndeveloped by human experts. These outcomes underscore the significant potential\nof LLM-based agents to automate complex engineering modeling tasks, greatly\nreducing human workload while meeting or exceeding existing standards of\npredictive performance.", "AI": {"tldr": "LLM agents automate end-to-end data-driven regression modeling using two frameworks (multi-agent collaboration and ReAct-based single agent). They autonomously preprocess data, build/train models, optimize hyperparameters, and quantify uncertainty; evaluated on CHF data (~25k points); outperform CHF lookup tables and match/better Bayesian-optimized DNNs developed by humans.", "motivation": "Reduce manual intervention and scale data-driven engineering modeling to diverse applications by leveraging LLM-based agents to automate end-to-end workflows.", "method": "Compare two LLM-agent frameworks: a multi-agent system with specialized, collaborative agents, and a single-agent ReAct-based system. Both autonomously perform data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification on a CHF regression benchmark using ~25,000 data points. Baselines include traditional CHF lookup tables and state-of-the-art Bayesian-optimized DNNs.", "result": "The LLM-agent models surpass traditional CHF lookup tables and achieve predictive accuracy and uncertainty quantification on par with state-of-the-art Bayesian-optimized deep neural networks designed by human experts.", "conclusion": "LLM-based agents can robustly automate complex engineering modeling tasks, reducing human workload while delivering predictive performance that meets or exceeds current standards."}}
{"id": "2510.01485", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01485", "abs": "https://arxiv.org/abs/2510.01485", "authors": ["Nicholas B. Andrews", "Yanhao Yang", "Sofya Akhetova", "Kristi A. Morgansen", "Ross L. Hatton"], "title": "Pose Estimation of a Thruster-Driven Bioinspired Multi-Link Robot", "comment": "8 pages, 8 figures, 3 tables", "summary": "This work demonstrates pose (position and shape) estimation for a\nfree-floating, bioinspired multi-link robot with unactuated joints,\nlink-mounted thrusters for control, and a single gyroscope per link, resulting\nin an underactuated, minimally sensed platform. Through a proof-of-concept\nhardware experiment and offline Kalman filter analysis, we show that the\nrobot's pose can be reliably estimated. State estimation is performed using an\nunscented Kalman filter augmented with Gaussian process residual learning to\ncompensate for non-zero-mean, non-Gaussian noise. We further show that a filter\ntrained on a multi-gait dataset (forward, backward, left, right, and turning)\nperforms comparably to one trained on a larger forward-gait-only dataset when\nboth are evaluated on the same forward-gait test trajectory. These results\nreveal overlap in the gait input space, which can be exploited to reduce\ntraining data requirements while enhancing the filter's generalizability across\nmultiple gaits.", "AI": {"tldr": "Pose/shape estimation for a free-floating, bioinspired multi-link robot with unactuated joints and per-link gyros using an unscented Kalman filter augmented with Gaussian-process residual learning; shows data-efficient multi-gait generalization and reliable state estimation in hardware and offline analysis.", "motivation": "Address reliable pose estimation for underactuated, minimally sensed robots where joints are unactuated and sensing is sparse; explore how GP residual learning and gait diversity can improve estimation robustness and reduce training data needs.", "method": "Hardware proof-of-concept platform with link-mounted thrusters and a single gyroscope per link. State estimation via an Unscented Kalman Filter augmented with Gaussian process residual learning to model non-zero-mean, non-Gaussian noise. Training includes a multi-gait dataset (forward, backward, left, right, turning) and comparison against a forward-gait-only dataset. Evaluation performed on a forward-gait test trajectory.", "result": "The pose estimation is reliable; GP residual learning compensates for non-ideal noise, and the multi-gait-trained filter generalizes well to a forward gait similar to the larger forward-only dataset, indicating overlap in the gait input space and enabling reduced training data.", "conclusion": "UKF with GP residual learning is effective for underactuated, minimally sensed bioinspired robots; gait diversity can improve generalization and data efficiency, suggesting practical benefits for controlling such systems and guiding future work to broaden gait coverage and sensing."}}
{"id": "2510.01532", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01532", "abs": "https://arxiv.org/abs/2510.01532", "authors": ["Meilong Xu", "Xiaoling Hu", "Shahira Abousamra", "Chen Li", "Chao Chen"], "title": "MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation", "comment": "20 pages, 6 figures. Accepted by NeurIPS 2025", "summary": "In semi-supervised segmentation, capturing meaningful semantic structures\nfrom unlabeled data is essential. This is particularly challenging in\nhistopathology image analysis, where objects are densely distributed. To\naddress this issue, we propose a semi-supervised segmentation framework\ndesigned to robustly identify and preserve relevant topological features. Our\nmethod leverages multiple perturbed predictions obtained through stochastic\ndropouts and temporal training snapshots, enforcing topological consistency\nacross these varied outputs. This consistency mechanism helps distinguish\nbiologically meaningful structures from transient and noisy artifacts. A key\nchallenge in this process is to accurately match the corresponding topological\nfeatures across the predictions in the absence of ground truth. To overcome\nthis, we introduce a novel matching strategy that integrates spatial overlap\nwith global structural alignment, minimizing discrepancies among predictions.\nExtensive experiments demonstrate that our approach effectively reduces\ntopological errors, resulting in more robust and accurate segmentations\nessential for reliable downstream analysis. Code is available at\n\\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.", "AI": {"tldr": "A semi-supervised segmentation framework (MATCH) that enforces topological consistency across perturbed predictions and uses a novel cross-prediction feature matching strategy to improve histopathology segmentation.", "motivation": "In histopathology, densely packed structures and limited labeled data make it hard to capture meaningful topology; robust semi-supervised methods are needed to distinguish real biological structures from artifacts.", "method": "Leverages multiple perturbed predictions via stochastic dropout and temporal training snapshots; enforces topological consistency across outputs; introduces a matching strategy that blends spatial overlap with global structural alignment to align topology across predictions.", "result": "Extensive experiments show reduced topological errors and more robust, accurate segmentations; code is released.", "conclusion": "The approach enables more reliable downstream analysis in histopathology by preserving topology under semi-supervision."}}
{"id": "2510.01278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01278", "abs": "https://arxiv.org/abs/2510.01278", "authors": ["Hengwei Zhao", "Zhengzhong Tu", "Zhuo Zheng", "Wei Wang", "Junjue Wang", "Rusty Feagin", "Wenzhe Jiao"], "title": "Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning", "comment": null, "summary": "Positive-Unlabeled (PU) learning aims to train a binary classifier (positive\nvs. negative) where only limited positive data and abundant unlabeled data are\navailable. While widely applicable, state-of-the-art PU learning methods\nsubstantially underperform their supervised counterparts on complex datasets,\nespecially without auxiliary negatives or pre-estimated parameters (e.g., a\n14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the\nchallenge of learning discriminative representations under unreliable\nsupervision. To tackle this challenge, we propose NcPU, a non-contrastive PU\nlearning framework that requires no auxiliary information. NcPU combines a\nnoisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns\nintra-class representations despite unreliable supervision, with a phantom\nlabel disambiguation (PLD) scheme that supplies conservative negative\nsupervision via regret-based label updates. Theoretically, NoiSNCL and PLD can\niteratively benefit each other from the perspective of the\nExpectation-Maximization framework. Empirically, extensive experiments\ndemonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive\nperformance; and (2) NcPU achieves substantial improvements over\nstate-of-the-art PU methods across diverse datasets, including challenging\ndatasets on post-disaster building damage mapping, highlighting its promise for\nreal-world applications. Code: Code will be open-sourced after review.", "AI": {"tldr": "NcPU is a non-contrastive PU learning framework using NoiSNCL and PLD to learn discriminative representations under unreliable supervision without auxiliary negatives, achieving substantial improvements over state-of-the-art PU methods across diverse datasets.", "motivation": "PU learning suffers from large performance gaps to supervised methods due to unreliable supervision and limited negative information; existing methods often rely on auxiliary negatives or pre-estimated parameters, hindering performance on complex datasets (e.g., CIFAR-100). A learning framework that can operate with only positive and unlabeled data and improve representation quality is highly desirable for real-world tasks.", "method": "NcPU combines two components: (1) NoiSNCL, a noisy-pair robust supervised non-contrastive loss that aligns intra-class representations despite unreliable supervision, and (2) PLD (phantom label disambiguation), a regret-based scheme that provides conservative negative supervision by updating labels. The two components are designed to iteratively benefit each other in an EM-like framework.", "result": "Theoretically, NoiSNCL and PLD reinforce each other under an EM perspective. Empirically, NoiSNCL enables simple PU methods to achieve competitive performance, and NcPU delivers substantial improvements over state-of-the-art PU methods across diverse datasets, including challenging post-disaster building damage mapping scenarios.", "conclusion": "NcPU offers a practical, parameter-light approach to PU learning that closes much of the gap with supervised methods, with strong empirical performance and potential real-world impact; code is planned to be open-sourced after review."}}
{"id": "2510.01409", "categories": ["cs.AI", "I.2.7; I.2.6; I.2.4"], "pdf": "https://arxiv.org/pdf/2510.01409", "abs": "https://arxiv.org/abs/2510.01409", "authors": ["Luca Cotti", "Idilio Drago", "Anisa Rula", "Devis Bianchini", "Federico Cerutti"], "title": "OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models", "comment": "20 pages, 6 tables, 7 figures", "summary": "System logs represent a valuable source of Cyber Threat Intelligence (CTI),\ncapturing attacker behaviors, exploited vulnerabilities, and traces of\nmalicious activity. Yet their utility is often limited by lack of structure,\nsemantic inconsistency, and fragmentation across devices and sessions.\nExtracting actionable CTI from logs therefore requires approaches that can\nreconcile noisy, heterogeneous data into coherent and interoperable\nrepresentations. We introduce OntoLogX, an autonomous Artificial Intelligence\n(AI) agent that leverages Large Language Models (LLMs) to transform raw logs\ninto ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a\nlightweight log ontology with Retrieval Augmented Generation (RAG) and\niterative correction steps, ensuring that generated KGs are syntactically and\nsemantically valid. Beyond event-level analysis, the system aggregates KGs into\nsessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level\nlog evidence to higher-level adversarial objectives. We evaluate OntoLogX on\nboth logs from a public benchmark and a real-world honeypot dataset,\ndemonstrating robust KG generation across multiple KGs backends and accurate\nmapping of adversarial activity to ATT&CK tactics. Results highlight the\nbenefits of retrieval and correction for precision and recall, the\neffectiveness of code-oriented models in structured log analysis, and the value\nof ontology-grounded representations for actionable CTI extraction.", "AI": {"tldr": "OntoLogX uses LLMs to convert raw logs into ontology-grounded knowledge graphs via a lightweight log ontology, retrieval augmented generation (RAG), and iterative corrections, enabling aggregation into sessions and MITRE ATT&CK tactic prediction; validated on benchmark data and honeypot logs; shows improved precision/recall and actionable CTI extraction.", "motivation": "System logs are rich but unstructured and heterogeneous; current fragmentation hinders actionable CTI. A unified, ontology-based, LLM-assisted approach can reconcile data into interoperable representations and link evidence to attacker objectives.", "method": "Develop OntoLogX: a framework combining a lightweight log ontology, Retrieval Augmented Generation (RAG), and iterative correction steps; generate ontology-grounded KGs from logs; aggregate event-level KGs into sessions; use LLM to map evidence to MITRE ATT&CK tactics; evaluate across multiple KG backends on benchmark and real-world honeypot datasets.", "result": "Demonstrates robust KG generation across KG backends; accurate mapping of adversarial activity to ATT&CK tactics; retrieval and correction improve precision and recall; code-oriented models effective for structured log analysis; ontology-grounded representations beneficial for actionable CTI.", "conclusion": "Shows feasibility and value of combining ontologies, retrieval-augmented LLMs, and iterative corrections for structured, interoperable CTI extraction from logs; supports linking low-level evidence to high-level objectives and enhances CTI usefulness."}}
{"id": "2510.01519", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01519", "abs": "https://arxiv.org/abs/2510.01519", "authors": ["Wei Han Chen", "Yuchen Liu", "Alexiy Buynitsky", "Ahmed H. Qureshi"], "title": "Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments", "comment": null, "summary": "Robot navigation in large, complex, and unknown indoor environments is a\nchallenging problem. The existing approaches, such as traditional\nsampling-based methods, struggle with resolution control and scalability, while\nimitation learning-based methods require a large amount of demonstration data.\nActive Neural Time Fields (ANTFields) have recently emerged as a promising\nsolution by using local observations to learn cost-to-go functions without\nrelying on demonstrations. Despite their potential, these methods are hampered\nby challenges such as spectral bias and catastrophic forgetting, which diminish\ntheir effectiveness in complex scenarios. To address these issues, our approach\ndecomposes the planning problem into a hierarchical structure. At the high\nlevel, a sparse graph captures the environment's global connectivity, while at\nthe low level, a planner based on neural fields navigates local obstacles by\nsolving the Eikonal PDE. This physics-informed strategy overcomes common\npitfalls like spectral bias and neural field fitting difficulties, resulting in\na smooth and precise representation of the cost landscape. We validate our\nframework in large-scale environments, demonstrating its enhanced adaptability\nand precision compared to previous methods, and highlighting its potential for\nonline exploration, mapping, and real-world navigation.", "AI": {"tldr": "Hierarchical planning that combines a sparse global graph with a neural-field local planner solving the Eikonal PDE to improve ANTFields, addressing spectral bias and forgetting for scalable, accurate navigation in large, unknown indoor environments.", "motivation": "Traditional sampling-based methods struggle with resolution control and scalability; imitation learning requires large demonstration sets; Active Neural Time Fields (ANTFields) suffer from spectral bias and catastrophic forgetting, hindering performance in complex environments.", "method": "A two-level approach: (1) high-level sparse graph captures global connectivity; (2) low-level neural-field planner uses a physics-informed Eikonal PDE solver to navigate local obstacles and produce a smooth cost landscape, mitigating spectral bias and fitting difficulties.", "result": "Validated in large-scale environments, showing enhanced adaptability and precision over prior methods, with strong potential for online exploration, mapping, and real-world navigation.", "conclusion": "A hierarchical, physics-informed neural-field framework improves robustness and scalability of ANTFields for complex indoor navigation, addressing key shortcomings and enabling more reliable online planning."}}
{"id": "2510.01540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01540", "abs": "https://arxiv.org/abs/2510.01540", "authors": ["Jiamu Bai", "Xin Yu", "Meilong Xu", "Weitao Lu", "Xin Pan", "Kiwan Maeng", "Daniel Kifer", "Jian Wang", "Yu Wang"], "title": "Towards Better Optimization For Listwise Preference in Diffusion Models", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has proven effectiveness\nfor aligning text-to-image (T2I) diffusion models with human preferences.\nAlthough Direct Preference Optimization (DPO) is widely adopted for its\ncomputational efficiency and avoidance of explicit reward modeling, its\napplications to diffusion models have primarily relied on pairwise preferences.\nThe precise optimization of listwise preferences remains largely unaddressed.\nIn practice, human feedback on image preferences often contains implicit ranked\ninformation, which conveys more precise human preferences than pairwise\ncomparisons. In this work, we propose Diffusion-LPO, a simple and effective\nframework for Listwise Preference Optimization in diffusion models with\nlistwise data. Given a caption, we aggregate user feedback into a ranked list\nof images and derive a listwise extension of the DPO objective under the\nPlackett-Luce model. Diffusion-LPO enforces consistency across the entire\nranking by encouraging each sample to be preferred over all of its lower-ranked\nalternatives. We empirically demonstrate the effectiveness of Diffusion-LPO\nacross various tasks, including text-to-image generation, image editing, and\npersonalized preference alignment. Diffusion-LPO consistently outperforms\npairwise DPO baselines on visual quality and preference alignment.", "AI": {"tldr": "Diffusion-LPO introduces listwise preference optimization for diffusion models using the Plackett\u2013Luce model, leveraging ranked human feedback to improve text-to-image generation, editing, and personalization beyond pairwise DPO baselines.", "motivation": "Human feedback often contains richer, implicit ranked information. Pairwise DPO underutilizes this signal; a listwise approach can better capture user preferences and improve alignment.", "method": "Aggregate caption-aligned feedback into a ranked list of images, derive a listwise DPO objective under Plackett\u2013Luce, enforce consistency by promoting each sample over all lower-ranked ones, and apply to diffusion models for T2I, editing, and personalized alignment.", "result": "Diffusion-LPO consistently outperforms pairwise DPO baselines in both visual quality and alignment with user preferences across multiple tasks.", "conclusion": "Listwise preference optimization is a practical and effective enhancement for RLHF in diffusion models, suggesting potential extensions beyond current tasks and prompting further investigation into listwise signals and ranking models."}}
{"id": "2510.01288", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01288", "abs": "https://arxiv.org/abs/2510.01288", "authors": ["Rui Melo", "Rui Abreu", "Corina S. Pasareanu"], "title": "Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours", "comment": "9 main pages, 13 appendix pages", "summary": "We draw inspiration from microsaccades, tiny involuntary eye movements that\nreveal hidden dynamics of human perception, to propose an analogous probing\nmethod for large language models (LLMs). Just as microsaccades expose subtle\nbut informative shifts in vision, we show that lightweight position encoding\nperturbations elicit latent signals that indicate model misbehaviour. Our\nmethod requires no fine-tuning or task-specific supervision, yet detects\nfailures across diverse settings including factuality, safety, toxicity, and\nbackdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate\nthat these perturbation-based probes surface misbehaviours while remaining\ncomputationally efficient. These findings suggest that pretrained LLMs already\nencode the internal evidence needed to flag their own failures, and that\nmicrosaccade-inspired interventions provide a pathway for detecting and\nmitigating undesirable behaviours.", "AI": {"tldr": "Proposes microsaccade-inspired perturbations of LLM position encodings to surface internal signals of misbehavior without fine-tuning, achieving detection of factual, safety, toxicity, and backdoor issues across models efficiently.", "motivation": "Analogize perceptual micro-movements to probing LLMs to reveal hidden dynamics of behavior and enable self-flagging of failures without task-specific supervision.", "method": "Apply lightweight perturbations to position encoding; measure latent signals; no fine-tuning; universal probing across tasks; detect misbehavior.", "result": "Experiments on multiple state-of-the-art LLMs show the perturbations surface misbehaviors; method is computationally efficient; robust across factuality, safety, toxicity, backdoor detection.", "conclusion": "Pretrained LLMs contain internal evidence to flag failures; microsaccade-inspired interventions offer a pathway for detection and mitigation of undesirable behaviors; no additional supervision needed."}}
{"id": "2510.01427", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01427", "abs": "https://arxiv.org/abs/2510.01427", "authors": ["Sipeng Zhang", "Longfei Yun", "Zilong Wang", "Jingbo Shang", "Letian Peng"], "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining", "comment": null, "summary": "At the core of Deep Research is knowledge mining, the task of extracting\nstructured information from massive unstructured text in response to user\ninstructions. Large language models (LLMs) excel at interpreting such\ninstructions but are prohibitively expensive to deploy at scale, while\ntraditional pipelines of classifiers and extractors remain efficient yet\nbrittle and unable to generalize to new tasks. We introduce Falconer, a\ncollaborative framework that combines the agentic reasoning of LLMs with\nlightweight proxy models for scalable knowledge mining. In Falconer, LLMs act\nas planners, decomposing user instructions into executable pipelines, and as\nannotators, generating supervision to train small proxies. The framework\nunifies classification and extraction into two atomic operations, get label and\nget span, enabling a single instruction-following model to replace multiple\ntask-specific components. To evaluate the consistency between proxy models\nincubated by Falconer and annotations provided by humans and large models, we\nconstruct new benchmarks covering both planning and end-to-end execution.\nExperiments show that Falconer closely matches state-of-the-art LLMs in\ninstruction-following accuracy while reducing inference cost by up to 90% and\naccelerating large-scale knowledge mining by more than 20x, offering an\nefficient and scalable foundation for Deep Research.", "AI": {"tldr": "Falconer combines LLMs as planners/annotators with lightweight proxy models to perform scalable knowledge mining, achieving near-SOTA instruction-following accuracy with substantial cost reductions and speedups.", "motivation": "To enable scalable, accurate knowledge mining from massive unstructured text while avoiding the high cost and brittleness of end-to-end LLMs and large, task-specific pipelines.", "method": "Falconer uses LLMs as planners to decompose user instructions into executable pipelines and as annotators to generate supervision for small proxy models. It unifies classification and extraction into two atomic operations (get label and get span) and trains lightweight proxies with LLM-/human-generated supervision, evaluating with new planning and end-to-end benchmarks.", "result": "Falconer closely matches state-of-the-art instruction-following accuracy while reducing inference cost by up to 90% and accelerating large-scale knowledge mining by more than 20x.", "conclusion": "Falconer provides an efficient, scalable foundation for Deep Research by combining agentic LLM reasoning with lightweight proxies, reducing reliance on expensive LLMs and enabling robust, generalizable knowledge mining."}}
{"id": "2510.01592", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01592", "abs": "https://arxiv.org/abs/2510.01592", "authors": ["Shun Niijima", "Ryoichi Tsuzaki", "Noriaki Takasugi", "Masaya Kinoshita"], "title": "Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion", "comment": "8 pages, 12 figures, This work has been submitted to the IEEE for\n  possible publication. Copyright may be transfered without notice, after which\n  this version may no longer be accessible", "summary": "This paper proposes a real-time multi-plane segmentation method based on\nGPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion.\nExisting online planar mapping approaches struggle to balance accuracy and\ncomputational efficiency: direct depth image segmentation from specific sensors\nsuffers from poor temporal integration, height map-based methods cannot\nrepresent complex 3D structures like overhangs, and voxel-based plane\nsegmentation remains unexplored for real-time applications. To address these\nlimitations, we develop a novel framework that integrates vertex-based\nconnected component labeling with random sample consensus based plane detection\nand convex hull, leveraging GPU parallel computing to rapidly extract planar\nregions from point clouds accumulated in high-resolution 3D voxel maps.\nExperimental results demonstrate that the proposed method achieves fast and\naccurate 3D multi-plane segmentation at over 30 Hz update rate even at a\nresolution of 0.01 m, enabling the detected planes to be utilized in real time\nfor locomotion tasks. Furthermore, we validate the effectiveness of our\napproach through experiments in both simulated environments and physical legged\nrobot platforms, confirming robust locomotion performance when considering 3D\nplanar structures.", "AI": {"tldr": "A GPU-accelerated voxel-based method combines vertex-connected component labeling with RANSAC plane detection and convex hulls to perform fast, real-time multi-plane segmentation from dense point clouds, enabling robust legged locomotion tasks.", "motivation": "To overcome limitations of online planar mapping: depth-only segmentation has poor temporal integration, height-map representations cannot capture overhangs, and voxel-plane segmentation has not been explored for real-time use in legged robotics.", "method": "A framework that uses vertex-based connected component labeling, RANSAC-based plane detection, and convex hull computation, implemented on GPU to extract planar regions from high-resolution 3D voxel maps built from accumulated point clouds.", "result": "Achieves fast and accurate 3D multi-plane segmentation at over 30 Hz update rate at 0.01 m resolution, demonstrated in both simulated environments and physical legged robots, enabling real-time use of detected planes in locomotion.", "conclusion": "The proposed GPU-accelerated voxel mapping and plane detection pipeline enables robust real-time exploitation of 3D planar structures for legged locomotion, addressing prior trade-offs between accuracy and efficiency and expanding real-time 3D perception capabilities."}}
{"id": "2510.01546", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01546", "abs": "https://arxiv.org/abs/2510.01546", "authors": ["Hanyu Wang", "Jiaming Han", "Ziyan Yang", "Qi Zhao", "Shanchuan Lin", "Xiangyu Yue", "Abhinav Shrivastava", "Zhenheng Yang", "Hao Chen"], "title": "Growing Visual Generative Capacity for Pre-Trained MLLMs", "comment": "Project page: https://hywang66.github.io/bridge/", "summary": "Multimodal large language models (MLLMs) extend the success of language\nmodels to visual understanding, and recent efforts have sought to build unified\nMLLMs that support both understanding and generation. However, constructing\nsuch models remains challenging: hybrid approaches combine continuous\nembeddings with diffusion or flow-based objectives, producing high-quality\nimages but breaking the autoregressive paradigm, while pure autoregressive\napproaches unify text and image prediction over discrete visual tokens but\noften face trade-offs between semantic alignment and pixel-level fidelity. In\nthis work, we present Bridge, a pure autoregressive unified MLLM that augments\npre-trained visual understanding models with generative ability through a\nMixture-of-Transformers architecture, enabling both image understanding and\ngeneration within a single next-token prediction framework. To further improve\nvisual generation fidelity, we propose a semantic-to-pixel discrete\nrepresentation that integrates compact semantic tokens with fine-grained pixel\ntokens, achieving strong language alignment and precise description of visual\ndetails with only a 7.9% increase in sequence length. Extensive experiments\nacross diverse multimodal benchmarks demonstrate that Bridge achieves\ncompetitive or superior results in both understanding and generation\nbenchmarks, while requiring less training data and reduced training time\ncompared to prior unified MLLMs.", "AI": {"tldr": "Bridge is a pure autoregressive unified multimodal LLM that uses a Mixture-of-Transformers to enable both understanding and generation from a single next-token model, with a semantic-to-pixel discrete representation that combines compact semantic tokens with fine-grained pixel tokens. This yields strong language alignment and precise visual descriptions with only a 7.9% increase in sequence length, achieving competitive or superior performance on multimodal tasks while using less training data and time than previous unified MLLMs.", "motivation": "There is a need for unified multimodal language models that can perform both image understanding and generation within a single autoregressive framework. Existing approaches either rely on hybrids that disrupt autoregressive properties or on discrete-token generation that sacrifices either semantic alignment or pixel fidelity. A pure autoregressive, unified MLLM with efficient, high-fidelity generation and strong alignment is highly desirable.", "method": "Introduce Bridge, a pure autoregressive unified MLLM built with a Mixture-of-Transformers architecture that augments pre-trained visual understanding models with generative capability, enabling both image understanding and generation within a single next-token prediction pipeline. To improve visual generation fidelity, adopt a semantic-to-pixel discrete representation that combines compact semantic tokens with fine-grained pixel tokens, increasing sequence length by 7.9% but preserving strong language alignment and detailed visual description.", "result": "Extensive experiments across diverse multimodal benchmarks show Bridge achieves competitive or superior performance in both understanding and generation tasks, while requiring less training data and reduced training time compared to prior unified MLLMs.", "conclusion": "Bridge demonstrates the viability of pure autoregressive unified MLLMs and the effectiveness of a semantic-to-pixel discrete representation, achieving strong language alignment and detailed visual fidelity with improved efficiency, and offering a promising direction for future research in unified multimodal modeling."}}
{"id": "2510.01290", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01290", "abs": "https://arxiv.org/abs/2510.01290", "authors": ["Akshat Ramachandran", "Marina Neseem", "Charbel Sakr", "Rangharajan Venkatesan", "Brucek Khailany", "Tushar Krishna"], "title": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models", "comment": null, "summary": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines.", "AI": {"tldr": "ThinKV compresses the KV cache for long-output reasoning by leveraging thought-aware sparsity and a hybrid quantization-eviction policy, achieving near-lossless accuracy with <5% of the original KV cache and up to 5.8x throughput.", "motivation": "Long chain-of-thought generation expands the KV cache, exhausting GPU memory and hindering inference throughput. A memory-efficient, accurate KV management strategy is needed for long-context reasoning in LLMs.", "method": "A hybrid quantization-eviction framework driven by attention-sparsity-derived thought types. Tokens are quantized according to thought importance, and tokens from less critical thoughts are progressively evicted as reasoning proceeds. Implemented a kernel extending PagedAttention to reuse memory slots of evicted tokens, removing compaction overhead.", "result": "Across mathematics and coding benchmarks on DeepSeek-R1-Distill, GPT-OSS, and NVIDIA AceReason, ThinKV achieves near-lossless accuracy with <5% of the original KV cache and up to 5.8x higher inference throughput compared with state-of-the-art baselines.", "conclusion": "ThinKV provides a memory-efficient, high-throughput KV management solution for long-context reasoning that preserves accuracy while drastically reducing memory footprint."}}
{"id": "2510.01432", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01432", "abs": "https://arxiv.org/abs/2510.01432", "authors": ["Sarath Sreedharan", "Kelsey Sikes", "Nathaniel Blanchard", "Lisa Mason", "Nikhil Krishnaswamy", "Jill Zarestky"], "title": "On the Role of Domain Experts in Creating Effective Tutoring Systems", "comment": "Accepted to AIED 2025 Blue Sky Track", "summary": "The role that highly curated knowledge, provided by domain experts, could\nplay in creating effective tutoring systems is often overlooked within the AI\nfor education community. In this paper, we highlight this topic by discussing\ntwo ways such highly curated expert knowledge could help in creating novel\neducational systems. First, we will look at how one could use explainable AI\n(XAI) techniques to automatically create lessons. Most existing XAI methods are\nprimarily aimed at debugging AI systems. However, we will discuss how one could\nuse expert specified rules about solving specific problems along with novel XAI\ntechniques to automatically generate lessons that could be provided to\nlearners. Secondly, we will see how an expert specified curriculum for learning\na target concept can help develop adaptive tutoring systems, that can not only\nprovide a better learning experience, but could also allow us to use more\nefficient algorithms to create these systems. Finally, we will highlight the\nimportance of such methods using a case study of creating a tutoring system for\npollinator identification, where such knowledge could easily be elicited from\nexperts.", "AI": {"tldr": "This paper argues for leveraging highly curated expert knowledge and explainable AI to automatically generate lessons and adaptive tutoring systems, demonstrated via a pollinator-identification case study.", "motivation": "There is an underutilized potential for domain-expert knowledge in AI for education; combining expert-curated rules with XAI can create scalable, explainable, adaptive educational tools.", "method": "Discuss two avenues: (1) using expert-specified solving rules with XAI to automatically generate lessons; (2) using expert-curated curricula to drive adaptive tutoring systems and enable more efficient algorithms; supports with a pollinator-identification case study.", "result": "Conceptual framework with illustrative case study; no empirical results reported.", "conclusion": "Integrating expert knowledge with XAI in education can enhance tutoring systems and learner experiences; highlights need for expert elicitation and further research."}}
{"id": "2510.01603", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01603", "abs": "https://arxiv.org/abs/2510.01603", "authors": ["Sharfin Islam", "Zewen Chen", "Zhanpeng He", "Swapneel Bhatt", "Andres Permuy", "Brock Taylor", "James Vickery", "Pedro Piacenza", "Cheng Zhang", "Matei Ciocarlie"], "title": "MiniBEE: A New Form Factor for Compact Bimanual Dexterity", "comment": null, "summary": "Bimanual robot manipulators can achieve impressive dexterity, but typically\nrely on two full six- or seven- degree-of-freedom arms so that paired grippers\ncan coordinate effectively. This traditional framework increases system\ncomplexity while only exploiting a fraction of the overall workspace for\ndexterous interaction. We introduce the MiniBEE (Miniature Bimanual\nEnd-effector), a compact system in which two reduced-mobility arms (3+ DOF\neach) are coupled into a kinematic chain that preserves full relative\npositioning between grippers. To guide our design, we formulate a kinematic\ndexterity metric that enlarges the dexterous workspace while keeping the\nmechanism lightweight and wearable. The resulting system supports two\ncomplementary modes: (i) wearable kinesthetic data collection with self-tracked\ngripper poses, and (ii) deployment on a standard robot arm, extending dexterity\nacross its entire workspace. We present kinematic analysis and design\noptimization methods for maximizing dexterous range, and demonstrate an\nend-to-end pipeline in which wearable demonstrations train imitation learning\npolicies that perform robust, real-world bimanual manipulation.", "AI": {"tldr": "A compact two-armed bimanual system (MiniBEE) with two 3-DOF arms coupled to preserve the relative gripper pose, guided by a kinematic dexterity metric to maximize dexterous workspace; supports wearable data collection and deployment on a standard robot arm; demonstrated via kinematic analysis, design optimization, and an end-to-end imitation-learning pipeline for real-world bimanual manipulation.", "motivation": "Traditional bimanual manipulation relies on two full six- or seven-DOF arms, which adds complexity and underutilizes workspace. A compact, wearable, low-DOF core that preserves relative gripper positioning can expand dexterous capabilities while reducing mass and complexity.", "method": "Introduce MiniBEE: two 3-DOF arms coupled into a kinematic chain that maintains full relative gripper positioning. Formulate a kinematic dexterity metric to enlarge the dexterous workspace. Perform kinematic analysis and design optimization to maximize dexterous range. Provide an end-to-end pipeline where wearable demonstrations train imitation-learning policies, and validate both wearable data collection and deployment on a standard robot arm.", "result": "Development of a kinematic dexterity metric-guided design for maximizing dexterous range, along with kinematic analyses and optimization. Demonstration of an end-to-end pipeline where wearable demonstrations train imitation-learning policies that perform robust, real-world bimanual manipulation, and validation of the approach.", "conclusion": "MiniBEE enables a compact, lightweight bimanual system that expands the dexterous workspace. The kinematic dexterity metric guides design decisions and allows extending dexterity across the robot\u2019s workspace. The framework supports two modes\u2014wearable data collection and deployment on standard robot arms\u2014broadening applicability for learning-based bimanual manipulation."}}
{"id": "2510.01547", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01547", "abs": "https://arxiv.org/abs/2510.01547", "authors": ["Akshay Bhagwan Sonawane", "Lena D. Swamikannan", "Lakshman Tamil"], "title": "Robust Classification of Oral Cancer with Limited Training Data", "comment": null, "summary": "Oral cancer ranks among the most prevalent cancers globally, with a\nparticularly high mortality rate in regions lacking adequate healthcare access.\nEarly diagnosis is crucial for reducing mortality; however, challenges persist\ndue to limited oral health programs, inadequate infrastructure, and a shortage\nof healthcare practitioners. Conventional deep learning models, while\npromising, often rely on point estimates, leading to overconfidence and reduced\nreliability. Critically, these models require large datasets to mitigate\noverfitting and ensure generalizability, an unrealistic demand in settings with\nlimited training data. To address these issues, we propose a hybrid model that\ncombines a convolutional neural network (CNN) with Bayesian deep learning for\noral cancer classification using small training sets. This approach employs\nvariational inference to enhance reliability through uncertainty\nquantification. The model was trained on photographic color images captured by\nsmartphones and evaluated on three distinct test datasets. The proposed method\nachieved 94% accuracy on a test dataset with a distribution similar to that of\nthe training data, comparable to traditional CNN performance. Notably, for\nreal-world photographic image data, despite limitations and variations\ndiffering from the training dataset, the proposed model demonstrated superior\ngeneralizability, achieving 88% accuracy on diverse datasets compared to 72.94%\nfor traditional CNNs, even with a smaller dataset. Confidence analysis revealed\nthat the model exhibits low uncertainty (high confidence) for correctly\nclassified samples and high uncertainty (low confidence) for misclassified\nsamples. These results underscore the effectiveness of Bayesian inference in\ndata-scarce environments in enhancing early oral cancer diagnosis by improving\nmodel reliability and generalizability.", "AI": {"tldr": "A hybrid CNN-Bayesian deep learning model with variational inference is proposed for oral cancer classification on small datasets, yielding high accuracy and uncertainty estimates to improve reliability and generalization in data-scarce settings.", "motivation": "Oral cancer has high mortality, especially in regions with limited healthcare access. Early diagnosis is critical but constrained by scarce training data, inadequate infrastructure, and practitioner shortages. Conventional DL can be overconfident and data-hungry, reducing reliability in resource-limited environments.", "method": "A hybrid architecture combining a convolutional neural network with Bayesian deep learning using variational inference to quantify uncertainty. Trained on smartphone-captured color images and evaluated on three distinct test datasets, comparing performance to traditional CNNs.", "result": "Achieved 94% accuracy on a test dataset similar to training data (comparable to CNNs). Demonstrated superior generalizability on diverse datasets with 88% accuracy versus 72.94% for traditional CNNs, even with smaller training data. Uncertainty analysis showed low uncertainty for correctly classified samples and high uncertainty for misclassified ones.", "conclusion": "Bayesian inference enhances reliability and generalizability of deep learning models for early oral cancer diagnosis in data-scarce environments, enabling more trustworthy decision-making in real-world, resource-limited settings."}}
{"id": "2510.01292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01292", "abs": "https://arxiv.org/abs/2510.01292", "authors": ["Xiaobo Ma", "Hyunsoo Noh", "James Tokishi", "Ryan Hatch"], "title": "Network-Level Vehicle Delay Estimation at Heterogeneous Signalized Intersections", "comment": "arXiv admin note: text overlap with arXiv:2503.20113", "summary": "Accurate vehicle delay estimation is essential for evaluating the performance\nof signalized intersections and informing traffic management strategies. Delay\nreflects congestion levels and affects travel time reliability, fuel use, and\nemissions. Machine learning (ML) offers a scalable, cost-effective alternative;\nHowever, conventional models typically assume that training and testing data\nfollow the same distribution, an assumption that is rarely satisfied in\nreal-world applications. Variations in road geometry, signal timing, and driver\nbehavior across intersections often lead to poor generalization and reduced\nmodel accuracy. To address this issue, this study introduces a domain\nadaptation (DA) framework for estimating vehicle delays across diverse\nintersections. The framework separates data into source and target domains,\nextracts key traffic features, and fine-tunes the model using a small, labeled\nsubset from the target domain. A novel DA model, Gradient Boosting with\nBalanced Weighting (GBBW), reweights source data based on similarity to the\ntarget domain, improving adaptability. The framework is tested using data from\n57 heterogeneous intersections in Pima County, Arizona. Performance is\nevaluated against eight state-of-the-art ML regression models and seven\ninstance-based DA methods. Results demonstrate that the GBBW framework provides\nmore accurate and robust delay estimates. This approach supports more reliable\ntraffic signal optimization, congestion management, and performance-based\nplanning. By enhancing model transferability, the framework facilitates broader\ndeployment of machine learning techniques in real-world transportation systems.", "AI": {"tldr": "Domain-adaptive gradient boosting for cross-site vehicle delay estimation; outperforms baselines on 57 intersections.", "motivation": "Real-world heterogeneity across intersections causes distribution shifts; need transferable ML models for delay estimation.", "method": "Domain adaptation framework: separate source/target domains, extract features, fine-tune with a small labeled target subset; Gradient Boosting with Balanced Weighting (GBBW) reweights source samples by similarity to target; evaluation against eight ML regressors and seven DA methods on data from 57 intersections.", "result": "GBBW achieves more accurate and robust delay estimates; better reliability for signal optimization and congestion management; demonstrates transferability of ML in transportation.", "conclusion": "Proposes a scalable framework that enhances cross-intersection transferability of ML-based delay estimation, enabling broader real-world deployment."}}
{"id": "2510.01444", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01444", "abs": "https://arxiv.org/abs/2510.01444", "authors": ["Rui Liu", "Dian Yu", "Tong Zheng", "Runpeng Dai", "Zongxia Li", "Wenhao Yu", "Zhenwen Liang", "Linfeng Song", "Haitao Mi", "Pratap Tokekar", "Dong Yu"], "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce $\\textbf{VOGUE (Visual Uncertainty Guided\nExploration)}$, a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.", "AI": {"tldr": "VOGUE introduces visual-uncertainty driven exploration for multimodal RL from RLVR, shifting exploration to input space and using a KL-based uncertainty signal to improve reasoning without sacrificing exploitation.", "motivation": "To address exploration failure in multimodal LLMs under RL from verifiable rewards, where visual inputs are treated as fixed and deterministic, leading to brittle policies under plausible visual variations.", "method": "Treat the image as a stochastic context and quantify policy sensitivity to visual perturbations via the symmetric KL divergence between a 'raw' and a 'noisy' visual branch. Use this uncertainty signal to form an uncertainty-proportional bonus in the learning objective, augmented with a token-entropy bonus and an annealed sampling schedule. Implemented within the GRPO framework on two model scales (Qwen2.5-VL-3B/7B).", "result": "On three visual math benchmarks and three general-domain reasoning benchmarks, pass@1 improves by ~2.6% and ~3.7% respectively (averages across tasks); pass@4 also improves and exploration decay during RL fine-tuning is mitigated.", "conclusion": "Grounding exploration in the inherent uncertainty of visual inputs is an effective strategy for enhancing multimodal reasoning in RL-finetuned LLMs."}}
{"id": "2510.01607", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01607", "abs": "https://arxiv.org/abs/2510.01607", "authors": ["Qiyuan Zeng", "Chengmeng Li", "Jude St. John", "Zhongyi Zhou", "Junjie Wen", "Guorui Feng", "Yichen Zhu", "Yi Xu"], "title": "ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations", "comment": "technique report. The website is available at\n  https://activeumi.github.io", "summary": "We present ActiveUMI, a framework for a data collection system that transfers\nin-the-wild human demonstrations to robots capable of complex bimanual\nmanipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized\ncontrollers that mirror the robot's end-effectors, bridging human-robot\nkinematics via precise pose alignment. To ensure mobility and data quality, we\nintroduce several key techniques, including immersive 3D model rendering, a\nself-contained wearable computer, and efficient calibration methods.\nActiveUMI's defining feature is its capture of active, egocentric perception.\nBy recording an operator's deliberate head movements via a head-mounted\ndisplay, our system learns the crucial link between visual attention and\nmanipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies\ntrained exclusively on ActiveUMI data achieve an average success rate of 70\\%\non in-distribution tasks and demonstrate strong generalization, retaining a\n56\\% success rate when tested on novel objects and in new environments. Our\nresults demonstrate that portable data collection systems, when coupled with\nlearned active perception, provide an effective and scalable pathway toward\ncreating generalizable and highly capable real-world robot policies.", "AI": {"tldr": "ActiveUMI enables portable, head-motion\u2013aware data collection to train generalizable bimanual robotic policies, achieving 70% success on in-distribution tasks and 56% on novel objects/environments.", "motivation": "To scale real-world robot learning for complex manipulation by bridging human demonstrations to robots and leveraging active perception (operator head motion) as a crucial signal.", "method": "A portable VR teleoperation setup with sensorized controllers mapped to robot end-effectors; precise pose alignment; immersive 3D model rendering; self-contained wearable computer; calibration methods; records egocentric perception by tracking head movements via a head-mounted display during operation.", "result": "Evaluated on six bimanual tasks; policies trained on ActiveUMI data achieved 70% success on in-distribution tasks and 56% on novel objects/environments, demonstrating strong generalization.", "conclusion": "Portable data collection plus learned active perception provides an effective, scalable pathway to generalizable, capable real-world robot policies."}}
{"id": "2510.01559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01559", "abs": "https://arxiv.org/abs/2510.01559", "authors": ["Renrong Shao", "Wei Zhang", "Kangyang Luo", "Qin Li", "and Jun Wang"], "title": "Consistent Assistant Domains Transformer for Source-free Domain Adaptation", "comment": null, "summary": "Source-free domain adaptation (SFDA) aims to address the challenge of\nadapting to a target domain without accessing the source domain directly.\nHowever, due to the inaccessibility of source domain data, deterministic\ninvariable features cannot be obtained. Current mainstream methods primarily\nfocus on evaluating invariant features in the target domain that closely\nresemble those in the source domain, subsequently aligning the target domain\nwith the source domain. However, these methods are susceptible to hard samples\nand influenced by domain bias. In this paper, we propose a Consistent Assistant\nDomains Transformer for SFDA, abbreviated as CADTrans, which solves the issue\nby constructing invariable feature representations of domain consistency.\nConcretely, we develop an assistant domain module for CADTrans to obtain\ndiversified representations from the intermediate aggregated global attentions,\nwhich addresses the limitation of existing methods in adequately representing\ndiversity. Based on assistant and target domains, invariable feature\nrepresentations are obtained by multiple consistent strategies, which can be\nused to distinguish easy and hard samples. Finally, to align the hard samples\nto the corresponding easy samples, we construct a conditional multi-kernel max\nmean discrepancy (CMK-MMD) strategy to distinguish between samples of the same\ncategory and those of different categories. Extensive experiments are conducted\non various benchmarks such as Office-31, Office-Home, VISDA-C, and\nDomainNet-126, proving the significant performance improvements achieved by our\nproposed approaches. Code is available at\nhttps://github.com/RoryShao/CADTrans.git.", "AI": {"tldr": "CADTrans introduces a Consistent Assistant Domains Transformer for SFDA that generates invariable feature representations via an assistant domain module and a CMK-MMD objective, improving robustness to hard samples and domain bias across standard benchmarks.", "motivation": "In source-free domain adaptation, source data are unavailable, making it hard to obtain invariant features. Existing methods struggle with hard samples and domain bias when aligning target to source-like representations.", "method": "CADTrans builds an assistant domain module to extract diversified representations from aggregated global attentions. It derives invariable feature representations using multiple consistent strategies across the assistant and target domains to distinguish easy vs hard samples. A conditional multi-kernel MMD (CMK-MMD) aligns hard samples with easy samples by considering same-category vs different-category pairs.", "result": "Extensive experiments on Office-31, Office-Home, VISDA-C, and DomainNet-126 show significant performance gains over baselines. Code is provided at the authors' repository.", "conclusion": "CADTrans offers a robust SFDA framework by constructing domain-consistent representations through an assistant domain module and employing CMK-MMD to handle hard samples and domain bias, achieving strong generalization across multiple benchmarks."}}
{"id": "2510.01296", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01296", "abs": "https://arxiv.org/abs/2510.01296", "authors": ["Emma McMillian", "Abhirup Banerjee", "Alfonso Bueno-Orovio"], "title": "From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review", "comment": null, "summary": "Deep learning-based 3-dimensional (3D) shape reconstruction from\n2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly\nimportant in medical disease diagnosis, treatment planning, and computational\nmodeling. This review surveys the methodological landscape of 3D MRI\nreconstruction, focusing on 4 primary approaches: point cloud, mesh-based,\nshape-aware, and volumetric models. For each category, we analyze the current\nstate-of-the-art techniques, their methodological foundation, limitations, and\napplications across anatomical structures. We provide an extensive overview\nranging from cardiac to neurological to lung imaging. We also focus on the\nclinical applicability of models to diseased anatomy, and the influence of\ntheir training and testing data. We examine publicly available datasets,\ncomputational demands, and evaluation metrics. Finally, we highlight the\nemerging research directions including multimodal integration and\ncross-modality frameworks. This review aims to provide researchers with a\nstructured overview of current 3D reconstruction methodologies to identify\nopportunities for advancing deep learning towards more robust, generalizable,\nand clinically impactful solutions.", "AI": {"tldr": "A comprehensive survey of deep learning-based 3D MRI reconstruction across four model classes (point cloud, mesh-based, shape-aware, volumetric), detailing state-of-the-art techniques, datasets, metrics, clinical applicability, and future directions.", "motivation": "3D reconstruction from 2D MRI is crucial for diagnosis, treatment planning, and computational modeling; a structured survey is needed to map methods, data, evaluation, and translation to clinical practice.", "method": "Categorize methods into four model classes (point cloud, mesh-based, shape-aware, volumetric). Synthesize state-of-the-art techniques and their methodological foundations; discuss limitations, anatomical applications (cardiac, neurological, lung), data influence, public datasets, computational demands, and evaluation metrics; examine clinical applicability and cross-modality training/testing considerations; outline emerging directions including multimodal integration.", "result": "Provides a structured overview and synthesis of current 3D MRI reconstruction methods; identifies gaps and opportunities to improve robustness, generalization, and clinical impact; highlights multimodal and cross-modality integration as promising directions.", "conclusion": "This review aims to guide researchers toward more robust, generalizable, and clinically impactful 3D reconstruction methods, emphasizing data quality, standardized evaluation, and integration across modalities."}}
{"id": "2510.01474", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01474", "abs": "https://arxiv.org/abs/2510.01474", "authors": ["Bill Marino", "Rosco Hunter", "Zubair Jamali", "Marinos Emmanouil Kalpakos", "Mudra Kashyap", "Isaiah Hinton", "Alexa Hanson", "Maahum Nazir", "Christoph Schnabl", "Felix Steffek", "Hongkai Wen", "Nicholas D. Lane"], "title": "AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance", "comment": null, "summary": "As governments move to regulate AI, there is growing interest in using Large\nLanguage Models (LLMs) to assess whether or not an AI system complies with a\ngiven AI Regulation (AIR). However, there is presently no way to benchmark the\nperformance of LLMs at this task. To fill this void, we introduce AIReg-Bench:\nthe first benchmark dataset designed to test how well LLMs can assess\ncompliance with the EU AI Act (AIA). We created this dataset through a two-step\nprocess: (1) by prompting an LLM with carefully structured instructions, we\ngenerated 120 technical documentation excerpts (samples), each depicting a\nfictional, albeit plausible, AI system - of the kind an AI provider might\nproduce to demonstrate their compliance with AIR; (2) legal experts then\nreviewed and annotated each sample to indicate whether, and in what way, the AI\nsystem described therein violates specific Articles of the AIA. The resulting\ndataset, together with our evaluation of whether frontier LLMs can reproduce\nthe experts' compliance labels, provides a starting point to understand the\nopportunities and limitations of LLM-based AIR compliance assessment tools and\nestablishes a benchmark against which subsequent LLMs can be compared. The\ndataset and evaluation code are available at\nhttps://github.com/camlsys/aireg-bench.", "AI": {"tldr": "AIReg-Bench is introduced as the first benchmark dataset to evaluate how well LLMs can assess compliance with the EU AI Act, created by prompting an LLM to generate 120 fictional AI system documents and having legal experts annotate compliance; aims to benchmark and understand the capabilities and limits of LLM-based AIR compliance tools.", "motivation": "There is no existing benchmark to measure LLM performance on AI Regulation compliance assessment. This work fills that gap by providing a labeled dataset and a framework to compare frontier LLMs against expert annotations.", "method": "Two-step creation: (1) prompt an LLM with structured instructions to generate 120 plausible technical excerpts describing AI systems; (2) legal experts annotate each sample for violations of specific AIA Articles. The authors evaluate whether frontier LLMs can reproduce the expert labels, and provide dataset + evaluation code.", "result": "A labeled benchmark dataset and evaluation framework that enables benchmarking LLMs on AIR compliance. Frontier LLMs are evaluated against expert annotations, establishing a starting point to understand capabilities and limitations.", "conclusion": "AIReg-Bench offers a valuable resource to benchmark and compare LLM-based AIR compliance tools, highlighting opportunities and limitations, and providing code and data for future research."}}
{"id": "2510.01642", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01642", "abs": "https://arxiv.org/abs/2510.01642", "authors": ["Zijun Lin", "Jiafei Duan", "Haoquan Fang", "Dieter Fox", "Ranjay Krishna", "Cheston Tan", "Bihan Wen"], "title": "FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models", "comment": "Project Page: https://jimntu.github.io/FailSafe/", "summary": "Recent advances in robotic manipulation have integrated low-level robotic\ncontrol into Vision-Language Models (VLMs), extending them into\nVision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve\nstrong performance in downstream robotic applications, supported by large-scale\ncrowd-sourced robot training data, they still inevitably encounter failures\nduring execution. Enabling robots to reason about and recover from\nunpredictable and abrupt failures remains a critical challenge. Existing\nrobotic manipulation datasets, collected in either simulation or the real\nworld, primarily provide only ground-truth trajectories, leaving robots unable\nto recover once failures occur. Moreover, the few datasets that address failure\ndetection typically offer only textual explanations, which are difficult to\nutilize directly in VLA models. To address this gap, we introduce FailSafe, a\nnovel failure generation and recovery system that automatically produces\ndiverse failure cases paired with executable recovery actions. FailSafe can be\nseamlessly applied to any manipulation task in any simulator, enabling scalable\ncreation of failure-action data. To demonstrate its effectiveness, we fine-tune\nLLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results\nshow that FailSafe-VLM successfully helps robotic arm detect and recover from\npotential failures, improving the performance of three state-of-the-art VLA\nmodels pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several\ntasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different\nspatial configurations, camera viewpoints, and robotic embodiments. We plan to\nrelease the FailSafe code to the community.", "AI": {"tldr": "FailSafe introduces a failure generation and recovery system for Vision-Language-Action models in robotic manipulation, enabling scalable creation of failure-action data, improving robustness and generalization, and boosting VLA performance by up to 22.6% on Maniskill tasks.", "motivation": "Current robotic VLA datasets provide only ground-truth trajectories or textual explanations of failures, lacking actionable recovery data and robust failure-explanation utilities; robots need to anticipate and recover from unpredictable failures to be reliable.", "method": "Propose FailSafe: a failure generation and recovery pipeline that automatically produces diverse failure cases paired with executable recovery actions. It can be applied to any manipulation task in any simulator. Fine-tune LLaVa-OneVision-7B to obtain FailSafe-VLM. Train on generated data; evaluate on various VLA models.", "result": "FailSafe-VLM enables detection and recovery from potential failures and improves three state-of-the-art VLA models (pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across Maniskill tasks. Demonstrates generalization across spatial configurations, viewpoints, and embodiments.", "conclusion": "FailSafe provides scalable failure-action data generation for robust robotic VLA systems, generalizes across settings, and should be released as open-source to support community adoption."}}
{"id": "2510.01576", "categories": ["cs.CV", "cs.AI", "cs.HC", "I.2.m; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.01576", "abs": "https://arxiv.org/abs/2510.01576", "authors": ["Ricardo Gonzalez Penuela", "Felipe Arias-Russi", "Victor Capriles"], "title": "Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations", "comment": "7 pages, 2 figure, 2 tables, CV4A11y Workshop at ICCV 2025", "summary": "Multimodal large language models (MLLMs) have been integrated into visual\ninterpretation applications to support Blind and Low Vision (BLV) users because\nof their accuracy and ability to provide rich, human-like interpretations.\nHowever, these applications often default to comprehensive, lengthy\ndescriptions regardless of context. This leads to inefficient exchanges, as\nusers must go through irrelevant details rather than receiving the specific\ninformation they are likely to seek. To deliver more contextually-relevant\ninformation, we developed a system that draws on historical BLV users\nquestions. When given an image, our system identifies similar past visual\ncontexts from the VizWiz-LF dataset and uses the associated questions to guide\nthe MLLM generate descriptions more relevant to BLV users. An evaluation with\nthree human labelers who revised 92 context-aware and context-free descriptions\nshowed that context-aware descriptions anticipated and answered users'\nquestions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of\ncomparisons (50 out of 92). Our paper reviews, and data analysis are publicly\navailable in a Github repository at\nhttps://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .", "AI": {"tldr": "A retrieval-based approach guides multimodal LLMs to produce context-relevant, BLV-friendly image descriptions by leveraging historical questions from VizWiz-LF, improving relevance and user satisfaction.", "motivation": "BLV users often receive long, generic descriptions from multimodal LLMs that do not align with their immediate information needs; leveraging historical BLV questions can tailor explanations to user contexts.", "method": "For each image, identify similar past visual contexts in VizWiz-LF and use their associated questions to steer the MLLM's description generation. Evaluate with three human labelers revising 92 descriptions (context-aware vs. context-free) to assess relevance and preference.", "result": "Context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70/92) and were preferred in 54.4% (50/92). The GitHub repository with data and analysis is publicly available.", "conclusion": "Using historical BLV questions to guide MLLM generation can produce more context-relevant descriptions for BLV users; the work provides publicly available data and analysis to support future improvements."}}
{"id": "2510.01303", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01303", "abs": "https://arxiv.org/abs/2510.01303", "authors": ["Rishi Sonthalia", "Michael Murray", "Guido Mont\u00fafar"], "title": "Low Rank Gradients and Where to Find Them", "comment": null, "summary": "This paper investigates low-rank structure in the gradients of the training\nloss for two-layer neural networks while relaxing the usual isotropy\nassumptions on the training data and parameters. We consider a spiked data\nmodel in which the bulk can be anisotropic and ill-conditioned, we do not\nrequire independent data and weight matrices and we also analyze both the\nmean-field and neural-tangent-kernel scalings. We show that the gradient with\nrespect to the input weights is approximately low rank and is dominated by two\nrank-one terms: one aligned with the bulk data-residue , and another aligned\nwith the rank one spike in the input data. We characterize how properties of\nthe training data, the scaling regime and the activation function govern the\nbalance between these two components. Additionally, we also demonstrate that\nstandard regularizers, such as weight decay, input noise and Jacobian\npenalties, also selectively modulate these components. Experiments on synthetic\nand real data corroborate our theoretical predictions.", "AI": {"tldr": "Gradient w.r.t input weights in a two-layer network trained on anisotropic, spiked data is approximately low-rank, collapsing to two rank-one terms (bulk-residue and spike interactions) across mean-field and NTK regimes; regularizers can modulate the components; empirical evidence supports the theory.", "motivation": "To understand the gradient structure and training dynamics of neural networks when standard isotropy and independence assumptions are relaxed, and to assess how data geometry, scaling, and activation shape the low-rank decomposition of gradients.", "method": "Theoretical analysis in a spiked data model with anisotropic, possibly ill-conditioned bulk. Examines both mean-field and neural tangent kernel scalings for a two-layer network, without requiring independent data/weights. Derives that the gradient with respect to input weights decomposes into two dominant rank-one terms aligned with (i) the bulk data-residue and (ii) the rank-one spike in the input data. Studies how data properties, scaling, and activation influence the balance between terms. Also analyzes how regularizers (weight decay, input noise, Jacobian penalties) selectively modulate these components. Empirical validation on synthetic and real datasets.", "result": "The gradient with respect to input weights is approximately low-rank, dominated by two rank-one components: one aligned with the bulk data-residue and another with the input spike. The relative strength of these components is governed by data properties, scaling regime, and activation function. Standard regularizers selectively modulate these components. Experiments on synthetic and real data corroborate the theoretical predictions.", "conclusion": "Low-rank structure in gradient dynamics persists under relaxed data/parameter isotropy and across MF and NTK regimes. This reveals robust, interpretable components of learning signals (bulk-residue vs. spike) and suggests potential for regularization strategies and computational efficiency by exploiting the low-rank gradient decomposition."}}
{"id": "2510.01500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01500", "abs": "https://arxiv.org/abs/2510.01500", "authors": ["Abhinav Madahar"], "title": "Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates", "comment": null, "summary": "Modern deployments increasingly allocate large test-time compute (thousands\nof tokens or many node expansions) to boost reliability. Under such budgets,\nstandard Tree-of-Thoughts-style search exhibits two pathologies: breadth\nsaturation (additional samples mostly produce near-duplicates, so width stops\ngrowing) and depth myopia (noisy short-horizon utilities prune branches whose\npayoff appears after a few more steps). We propose Lateral Tree-of-Thoughts\n(LToT), a drop-in controller that separates utility from logical consistency\nand treats low-utility but consistent candidates as assets rather than waste.\nThe frontier is split into mainlines (high-utility candidates used for\nexploitation) and laterals (consistent, initially low-utility candidates that\nreceive short, cheap probes before judgment). LToT explores laterals via\nLateral Racing with Short-Circuit (LR--SC): a capped successive-halving race\nthat spreads tiny probes across a very wide lateral set, uses width-aware\nthresholds with repeat-to-confirm, and immediately promotes a branch once its\nenvelope clears the mainline bar; mainlines are kept intentionally narrow so\nsurplus compute is invested where width is cheap. We prove a pseudolinear\nlateral cost $\\Theta(N_0 \\log_{\\eta} N_0)$ with logarithmically many rungs\n(initial lateral width $N_0$; culling factor $\\eta>1$), in contrast to the\nexponential growth of uncapped mainlines. Empirical evaluations on benchmark\ntasks are in preparation and will be added in a future revision. In short, LToT\nturns large test-time budgets into principled diversity while preserving\npromotion discipline, mitigating saturation and myopia without inflating\ncompute.", "AI": {"tldr": "LToT separates utility from logical consistency in Tree-of-Thoughts, using mainlines for high-utility paths and laterals for diverse, consistent candidates; laterals are explored via Lateral Racing with Short-Circuit (LR-SC), a capped successive-halving across a wide lateral set; mainlines are kept narrow to allocate compute cheaply to width expansion; claims a pseudolinear lateral cost \u0398(N0 log_eta N0) vs exponential growth without caps; empirical validation is forthcoming.", "motivation": "Standard Tree-of-Thoughts search under large test-time budgets suffers two pathologies: breadth saturation (increasing samples yield near-duplicates) and depth myopia (noisy short-horizon utilities prune potentially payoff-rich branches). A principled way to harvest diversity without wasting compute is needed.", "method": "Introduce Lateral Tree-of-Thoughts (LToT) that splits the frontier into mainlines (high-utility candidates used for exploitation) and laterals (consistent but initially low-utility candidates). Apply Lateral Racing with Short-Circuit (LR-SC): a capped successive-halving race that quickly probes a very wide lateral set with width-aware thresholds and repeat-to-confirm, promoting a branch once its envelope clears the mainline bar. Keep mainlines intentionally narrow; surrogate compute is spent on a cheap exploration of width via laterals. Theoretical bound: pseudolinear lateral cost \u0398(N0 log_eta N0) (eta>1) contrasted with exponential growth of uncapped mainlines.", "result": "The abstract notes empirical evaluations on benchmark tasks are in preparation for a future revision; it also provides a theoretical cost bound and qualitative claims about diversity and promotion discipline.", "conclusion": "LToT offers a drop-in controller that turns large test-time budgets into principled diversity while preserving promotion discipline, mitigating breadth saturation and depth myopia without inflating overall compute."}}
{"id": "2510.01648", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01648", "abs": "https://arxiv.org/abs/2510.01648", "authors": ["Seungwon Choi", "Donggyu Park", "Seo-Yeon Hwang", "Tae-Wan Kim"], "title": "Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation", "comment": null, "summary": "A fundamental challenge in robust visual-inertial odometry (VIO) is to\ndynamically assess the reliability of sensor measurements. This assessment is\ncrucial for properly weighting the contribution of each measurement to the\nstate estimate. Conventional methods often simplify this by assuming a static,\nuniform uncertainty for all measurements. This heuristic, however, may be\nlimited in its ability to capture the dynamic error characteristics inherent in\nreal-world data. To improve this limitation, we present a statistical framework\nthat learns measurement reliability assessment online, directly from sensor\ndata and optimization results. Our approach leverages multi-view geometric\nconsistency as a form of self-supervision. This enables the system to infer\nlandmark uncertainty and adaptively weight visual measurements during\noptimization. We evaluated our method on the public EuRoC dataset,\ndemonstrating improvements in tracking accuracy with average reductions of\napproximately 24\\% in translation error and 42\\% in rotation error compared to\nbaseline methods with fixed uncertainty parameters. The resulting framework\noperates in real time while showing enhanced accuracy and robustness. To\nfacilitate reproducibility and encourage further research, the source code will\nbe made publicly available.", "AI": {"tldr": "Online learning of measurement reliability for robust visual-inertial odometry by leveraging multi-view geometric consistency as self-supervision, improving accuracy and robustness in real time.", "motivation": "Static, uniform uncertainty in VIO measurements fails to capture dynamic, real-world error characteristics; a data-driven approach is needed to adapt measurement weighting during optimization.", "method": "A statistical framework that learns measurement reliability online from sensor data and optimization results, using multi-view geometric consistency as self-supervision to infer landmark uncertainty and adaptively weight visual measurements during optimization.", "result": "Demonstrates significant accuracy improvements on EuRoC: ~24% reduction in translation error and ~42% reduction in rotation error compared to baselines with fixed uncertainty; operates in real time and enhances robustness.", "conclusion": "Online learning of measurement reliability can substantially improve VIO performance, enabling adaptive weighting of measurements; code will be publicly available to support reproducibility."}}
{"id": "2510.01582", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01582", "abs": "https://arxiv.org/abs/2510.01582", "authors": ["Krishna Teja Chitty-Venkata", "Murali Emani"], "title": "ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models", "comment": "Preprint", "summary": "We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the\ndevelopment of Vision Language Models (VLMs) with explicit reasoning\ncapabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,\nproviding structured thinking tokens and corresponding answers. Our synthetic\ndataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and\nKimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of\nthinking-answer sequences, creating a resource for training and evaluating\nmultimodal reasoning models. We capture the step-by-step reasoning process of\nVLMs and the final descriptive answers. Our goal with this dataset is to enable\nthe development of more robust VLMs while contributing to the broader\nunderstanding of multimodal reasoning mechanisms. The dataset and evaluation\nbenchmarks will be publicly available to aid research in reasoning/thinking\nmultimodal VLMs.", "AI": {"tldr": "A synthetic multimodal reasoning dataset, ImageNet-Think, uses 250k ImageNet21k images with two thinking-answer sequences per image, generated by two VLMs, to train/evaluate explicit multimodal reasoning in Vision-Language Models; aims for public release.", "motivation": "Address the need for explicit, step-by-step multimodal reasoning in VLMs; current models struggle with interpretability and robust reasoning; leveraging synthetic thinking tokens to supervise reasoning.", "method": "Construct dataset by prompting two state-of-the-art VLMs to generate thinking tokens and final descriptive answers per image; two thinking-answer sequences per image; based on ImageNet21k; collect and structure tokens; provide benchmarks.", "result": "Proposes and constructs a public dataset with ~250k images and two thinking-answer sequences per image (approx. 500k sequences) to train/evaluate multimodal reasoning; public benchmarks to assess model reasoning capabilities.", "conclusion": "ImageNet-Think offers a resource to foster development of VLMs with explicit multimodal reasoning and to advance understanding of multimodal reasoning mechanisms; public release planned."}}
{"id": "2510.01335", "categories": ["cs.LG", "cond-mat.dis-nn", "math.MG", "physics.data-an", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.01335", "abs": "https://arxiv.org/abs/2510.01335", "authors": ["Aritra Das", "Joseph T. Iosue", "Victor V. Albert"], "title": "Quantum-inspired Benchmark for Estimating Intrinsic Dimension", "comment": "19 figures, 35 pages", "summary": "Machine learning models can generalize well on real-world datasets. According\nto the manifold hypothesis, this is possible because datasets lie on a latent\nmanifold with small intrinsic dimension (ID). There exist many methods for ID\nestimation (IDE), but their estimates vary substantially. This warrants\nbenchmarking IDE methods on manifolds that are more complex than those in\nexisting benchmarks. We propose a Quantum-Inspired Intrinsic-dimension\nEstimation (QuIIEst) benchmark consisting of infinite families of topologically\nnon-trivial manifolds with known ID. Our benchmark stems from a quantum-optical\nmethod of embedding arbitrary homogeneous spaces while allowing for curvature\nmodification and additive noise. The IDE methods tested were generally less\naccurate on QuIIEst manifolds than on existing benchmarks under identical\nresource allocation. We also observe minimal performance degradation with\nincreasingly non-uniform curvature, underscoring the benchmark's inherent\ndifficulty. As a result of independent interest, we perform IDE on the fractal\nHofstadter's butterfly and identify which methods are capable of extracting the\neffective dimension of a space that is not a manifold.", "AI": {"tldr": "Proposes QuIIEst, a quantum-inspired, curvature-adjusted benchmark for intrinsic-dimension estimation (IDE) on complex, topologically non-trivial manifolds, revealing IDE difficulties and potential non-manifold dimension extraction.", "motivation": "Current IDE benchmarks underrepresent manifold complexity and curvature variation; IDE estimates vary widely across methods, necessitating a benchmark that introduces richer geometry (topology, curvature, noise) to stress-test IDE methods and reveal their limitations.", "method": "Introduce QuIIEst: an infinite family of topologically non-trivial manifolds with known intrinsic dimension, constructed via a quantum-optical embedding of homogeneous spaces with tunable curvature and additive noise. Evaluate existing IDE methods on these manifolds and on the Hofstadter butterfly fractal, to assess performance on spaces that are not smooth manifolds.", "result": "IDE methods generally perform worse on QuIIEst manifolds than on existing benchmarks under equal resources; curvature heterogeneity causes only modest additional degradation, highlighting benchmark difficulty. Some IDE methods can extract effective dimension from non-manifold spaces like Hofstadter butterfly, showing partial robustness.", "conclusion": "QuIIEst provides a challenging, informative benchmark for IDE methods, promoting development of robust estimators that handle complex geometry and non-manifold spaces; extending IDE evaluation to fractal-like structures broadens applicability of intrinsic-dimension concepts in ML."}}
{"id": "2510.01528", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01528", "abs": "https://arxiv.org/abs/2510.01528", "authors": ["Daniel Zhao", "Abhilash Shankarampeta", "Lanxiang Hu", "Tajana Rosing", "Hao Zhang"], "title": "Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation", "comment": null, "summary": "We propose a novel method that leverages sparse autoencoders (SAEs) and\nclustering techniques to analyze the internal token representations of large\nlanguage models (LLMs) and guide generations in mathematical reasoning tasks.\nOur approach first trains an SAE to generate sparse vector representations for\ntraining tokens, then applies k-means clustering to construct a graph where\nvertices represent token clusters and weighted edges capture sequential token\ntransitions. Using this graph, we define an edge-weight based reward function\nto quantify adherence to established reasoning traces, thereby identifying\nexploitative reasoning trajectories. Additionally, we measure generation\ndiversity from clustering to assess the extent of exploration. Our findings\nindicate that balancing both exploitation and exploration is crucial for\nachieving high accuracy in mathematical reasoning tasks. During generation, the\nSAE can serve as a scalable reward model to guide generations, ensuring a\nbalanced trade-off between exploitation and exploration. This prevents extreme\nbehaviors in either direction, ultimately fostering a higher-quality reasoning\nprocess in LLMs.", "AI": {"tldr": "SAEs + clustering to analyze LLM token representations for guided, balanced mathematical reasoning; uses sparse autoencoders and k-means to build a token-graph and an edge-weighted reward to enforce adherence to reasoning traces; measures diversity via clustering; balances exploitation and exploration to improve accuracy.", "motivation": "to quantify internal token representations and transitions in LLMs in order to steer reasoning and avoid extreme behaviors, thereby improving mathematical reasoning accuracy.", "method": "train a sparse autoencoder to generate sparse token representations; apply k-means clustering to form token clusters and construct a graph where vertices are clusters and edges capture sequential token transitions; define an edge-weighted reward based on adherence to established reasoning traces; assess generation diversity via clustering; use the SAE as a scalable reward model during generation to balance exploitation and exploration.", "result": "balancing exploitation and exploration improves accuracy in mathematical reasoning tasks; SAE-guided generation prevents extreme behaviors and yields higher-quality reasoning; the approach offers a scalable framework for reward shaping in LLM reasoning.", "conclusion": "SAE-based reward shaping and clustering-derived guidance can steer LLM generations toward robust mathematical reasoning by maintaining a balance between following established reasoning traces and exploring alternative paths."}}
{"id": "2510.01661", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01661", "abs": "https://arxiv.org/abs/2510.01661", "authors": ["Yifei Simon Shao", "Yuchen Zheng", "Sunan Sun", "Pratik Chaudhari", "Vijay Kumar", "Nadia Figueroa"], "title": "Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation", "comment": "CoRL 2025 Learning Effective Abstractions for Planning (LEAP)\n  Workshop Best Paper Award (https://sites.google.com/view/symskill)", "summary": "Multi-step manipulation in dynamic environments remains challenging. Two\nmajor families of methods fail in distinct ways: (i) imitation learning (IL) is\nreactive but lacks compositional generalization, as monolithic policies do not\ndecide which skill to reuse when scenes change; (ii) classical task-and-motion\nplanning (TAMP) offers compositionality but has prohibitive planning latency,\npreventing real-time failure recovery. We introduce SymSkill, a unified\nlearning framework that combines the benefits of IL and TAMP, allowing\ncompositional generalization and failure recovery in real-time. Offline,\nSymSkill jointly learns predicates, operators, and skills directly from\nunlabeled and unsegmented demonstrations. At execution time, upon specifying a\nconjunction of one or more learned predicates, SymSkill uses a symbolic planner\nto compose and reorder learned skills to achieve the symbolic goals, while\nperforming recovery at both the motion and symbolic levels in real time.\nCoupled with a compliant controller, SymSkill enables safe and uninterrupted\nexecution under human and environmental disturbances. In RoboCasa simulation,\nSymSkill can execute 12 single-step tasks with 85% success rate. Without\nadditional data, it composes these skills into multi-step plans requiring up to\n6 skill recompositions, recovering robustly from execution failures. On a real\nFranka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented\nand unlabeled play data, is capable of performing multiple tasks simply by goal\nspecifications. The source code and additional analysis can be found on\nhttps://sites.google.com/view/symskill.", "AI": {"tldr": "SymSkill is a unified learning framework that merges imitation learning and task-and-motion planning to enable compositional generalization and real-time failure recovery in multi-step manipulation, by offline learning of predicates, operators, and skills from unlabeled, unsegmented demonstrations and runtime planning and recovery.", "motivation": "Imitation learning struggles with compositional generalization because monolithic policies can't adapt to changing scenes; classical TAMP offers compositionality but suffers prohibitive runtime and real-time recovery challenges.", "method": "Offline joint learning of predicates, operators, and skills from unlabeled/unsegmented demonstrations. At execution, a symbolic planner composes/reorders learned skills to meet symbolic goals; real-time motion and symbolic recovery via a compliant controller.", "result": "In RoboCasa simulation, executes 12 single-step tasks with 85% success; can compose up to 6 skill recompositions for multi-step plans with robust failure recovery. On a real Franka robot, learns from 5 minutes of unlabeled play data to perform multiple tasks via goal specifications. Code and analyses available online.", "conclusion": "SymSkill enables safe, uninterrupted execution under human/environmental disturbances by combining IL and TAMP for real-time compositional generalization and failure recovery in dynamic manipulation."}}
{"id": "2510.01608", "categories": ["cs.CV", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.01608", "abs": "https://arxiv.org/abs/2510.01608", "authors": ["Roman Jacome", "Romario Gualdr\u00f3n-Hurtado", "Leon Suarez", "Henry Arguello"], "title": "NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems", "comment": "25 pages, 12 tables, 10 figures. Accepted to NeurIPS 2025", "summary": "Imaging inverse problems aims to recover high-dimensional signals from\nundersampled, noisy measurements, a fundamentally ill-posed task with infinite\nsolutions in the null-space of the sensing operator. To resolve this ambiguity,\nprior information is typically incorporated through handcrafted regularizers or\nlearned models that constrain the solution space. However, these priors\ntypically ignore the task-specific structure of that null-space. In this work,\nwe propose \\textit{Non-Linear Projections of the Null-Space} (NPN), a novel\nclass of regularization that, instead of enforcing structural constraints in\nthe image domain, promotes solutions that lie in a low-dimensional projection\nof the sensing matrix's null-space with a neural network. Our approach has two\nkey advantages: (1) Interpretability: by focusing on the structure of the\nnull-space, we design sensing-matrix-specific priors that capture information\northogonal to the signal components that are fundamentally blind to the sensing\nprocess. (2) Flexibility: NPN is adaptable to various inverse problems,\ncompatible with existing reconstruction frameworks, and complementary to\nconventional image-domain priors. We provide theoretical guarantees on\nconvergence and reconstruction accuracy when used within plug-and-play methods.\nEmpirical results across diverse sensing matrices demonstrate that NPN priors\nconsistently enhance reconstruction fidelity in various imaging inverse\nproblems, such as compressive sensing, deblurring, super-resolution, computed\ntomography, and magnetic resonance imaging, with plug-and-play methods,\nunrolling networks, deep image prior, and diffusion models.", "AI": {"tldr": "Non-Linear Projections of the Null-Space (NPN) learn a low-dimensional, sensing-matrix-specific regularizer that constrains solutions to the projected null-space, improving reconstruction across diverse imaging inverse problems and compatible with plug-and-play, unrolled networks, DIPs, and diffusion models.", "motivation": "Imaging inverse problems are ill-posed, with infinite solutions in the null-space of the sensing operator. Traditional priors (handcrafted or learned) often ignore the task-specific structure of that null-space. NPN aims to capture null-space information tailored to the sensing process to resolve ambiguity.", "method": "Propose Non-Linear Projections of the Null-Space (NPN) as a regularization that uses a neural network to impose a low-dimensional projection of the sensing matrix's null-space. Integrates with reconstruction frameworks via plug-and-play methods, unrolled networks, deep image priors, and diffusion models. Provides theoretical convergence and reconstruction guarantees within these schemes.", "result": "Empirical results across diverse sensing matrices show that NPN priors consistently enhance reconstruction fidelity in imaging inverse problems\u2014including compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging\u2014when used with plug-and-play methods, unrolled networks, deep image priors, and diffusion models.", "conclusion": "NPN priors offer interpretable, flexible, and sensing-matrix-specific regularization that complements traditional image-domain priors. They enable improved reconstructions across a range of inverse problems and are compatible with multiple modern reconstruction frameworks, with theoretical convergence guarantees."}}
{"id": "2510.01337", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01337", "abs": "https://arxiv.org/abs/2510.01337", "authors": ["S\u00e9bastien Lachapelle"], "title": "On the Identifiability of Latent Action Policies", "comment": "10 pages", "summary": "We study the identifiability of latent action policy learning (LAPO), a\nframework introduced recently to discover representations of actions from video\ndata. We formally describe desiderata for such representations, their\nstatistical benefits and potential sources of unidentifiability. Finally, we\nprove that an entropy-regularized LAPO objective identifies action\nrepresentations satisfying our desiderata, under suitable conditions. Our\nanalysis provides an explanation for why discrete action representations\nperform well in practice.", "AI": {"tldr": "Analyzes identifiability in latent action policy learning (LAPO), proposes desiderata for action representations, and proves that an entropy-regularized LAPO objective identifiably yields representations meeting these desiderata under certain conditions, explaining why discrete action representations perform well in practice.", "motivation": "To understand what makes latent action representations identifiable in LAPO and to explain the empirical success of discrete action representations.", "method": "Formalizes desiderata for LAPO representations, analyzes statistical benefits and sources of unidentifiability, and proves an identifiability result for an entropy-regularized LAPO objective under suitable conditions.", "result": "Under suitable conditions, entropy-regularized LAPO identifies action representations that satisfy the stated desiderata, providing a theoretical explanation for the practical effectiveness of discrete action representations.", "conclusion": "Entropy regularization can resolve identifiability issues in LAPO; the findings offer theoretical grounding for the success of discretized action representations in practice."}}
{"id": "2510.01530", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01530", "abs": "https://arxiv.org/abs/2510.01530", "authors": ["Navapat Nananukul", "Yue Zhang", "Ryan Lee", "Eric Boxer", "Jonathan May", "Vibhav Giridhar Gogate", "Jay Pujara", "Mayank Kejriwal"], "title": "LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning", "comment": null, "summary": "High-assurance reasoning, particularly in critical domains such as law and\nmedicine, requires conclusions that are accurate, verifiable, and explicitly\ngrounded in evidence. This reasoning relies on premises codified from rules,\nstatutes, and contracts, inherently involving defeasible or non-monotonic logic\ndue to numerous exceptions, where the introduction of a single fact can\ninvalidate general rules, posing significant challenges. While large language\nmodels (LLMs) excel at processing natural language, their capabilities in\nstandard inference tasks do not translate to the rigorous reasoning required\nover high-assurance text guidelines. Core reasoning challenges within such\ntexts often manifest specific logical structures involving negation,\nimplication, and, most critically, defeasible rules and exceptions. In this\npaper, we propose a novel neurosymbolically-grounded architecture called\nLOGicalThought (LogT) that uses an advanced logical language and reasoner in\nconjunction with an LLM to construct a dual symbolic graph context and\nlogic-based context. These two context representations transform the problem\nfrom inference over long-form guidelines into a compact grounded evaluation.\nEvaluated on four multi-domain benchmarks against four baselines, LogT improves\noverall performance by 11.84% across all LLMs. Performance improves\nsignificantly across all three modes of reasoning: by up to +10.2% on negation,\n+13.2% on implication, and +5.5% on defeasible reasoning compared to the\nstrongest baseline.", "AI": {"tldr": "A neurosymbolically-grounded LogT architecture combines dual symbolic graph and logic-based contexts with LLMs to tackle high-assurance reasoning (defeasible rules, negation, implication), achieving ~11.84% gains across four benchmarks and LLMs.", "motivation": "High-assurance domains (law, medicine) rely on codified rules that involve non-monotonic reasoning and numerous exceptions; single facts can invalidate general rules. LLMs excel at language but struggle with rigorous, evidence-grounded inference, necessitating a hybrid approach that grounds reasoning in symbolic logic.", "method": "Introduce LOGicalThought (LogT), a neurosymbolic architecture that uses an advanced logical language and reasoner alongside an LLM to construct two context representations: a dual symbolic graph context and a logic-based context. These contexts transform long-form guideline inference into a compact, grounded evaluation. Evaluated on four multi-domain benchmarks against four baselines.", "result": "LogT yields an overall improvement of 11.84% across all LLMs. Gains are substantial across reasoning modes: negation +10.2%, implication +13.2%, and defeasible reasoning +5.5% over the strongest baseline.", "conclusion": "The study demonstrates that a neurosymbolic, dual-context approach improves high-assurance reasoning tasks involving defeasible rules and non-monotonic logic. The two-context representation enables grounded, compact evaluation and outperforms baselines across multiple domains and LLMs; future work may explore scalability, broader domain coverage, and deeper integration with existing guidelines."}}
{"id": "2510.01675", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01675", "abs": "https://arxiv.org/abs/2510.01675", "authors": ["Jaewoo Lee", "Dongjae Lee", "Jinwoo Lee", "Hyungyu Lee", "Yeonjoon Kim", "H. Jin Kim"], "title": "Geometric Backstepping Control of Omnidirectional Tiltrotors Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances", "comment": null, "summary": "This work presents a geometric backstepping controller for a variable-tilt\nomnidirectional multirotor that explicitly accounts for both servo and rotor\ndynamics. Considering actuator dynamics is essential for more effective and\nreliable operation, particularly during aggressive flight maneuvers or recovery\nfrom sudden disturbances. While prior studies have investigated actuator-aware\ncontrol for conventional and fixed-tilt multirotors, these approaches rely on\nlinear relationships between actuator input and wrench, which cannot capture\nthe nonlinearities induced by variable tilt angles. In this work, we exploit\nthe cascade structure between the rigid-body dynamics of the multirotor and its\nnonlinear actuator dynamics to design the proposed backstepping controller and\nestablish exponential stability of the overall system. Furthermore, we reveal\nparametric uncertainty in the actuator model through experiments, and we\ndemonstrate that the proposed controller remains robust against such\nuncertainty. The controller was compared against a baseline that does not\naccount for actuator dynamics across three experimental scenarios: fast\ntranslational tracking, rapid rotational tracking, and recovery from sudden\ndisturbance. The proposed method consistently achieved better tracking\nperformance, and notably, while the baseline diverged and crashed during the\nfastest translational trajectory tracking and the recovery experiment, the\nproposed controller maintained stability and successfully completed the tasks,\nthereby demonstrating its effectiveness.", "AI": {"tldr": "Actuator-aware geometric backstepping control for a variable-tilt omnidirectional multirotor achieving exponential stability and robustness under actuator dynamics, outperforming a non-actuator-aware baseline in challenging maneuvers.", "motivation": "Actuator dynamics introduce nonlinear effects, especially with variable tilt; existing actuator-aware controls rely on linear actuator-wrench relationships and large-scale approximations, limiting performance during aggressive maneuvers and disturbances.", "method": "Exploit cascade structure between rigid-body dynamics and nonlinear actuator dynamics to design a backstepping controller; prove exponential stability; experimentally identify actuator model uncertainty and test robustness; compare to a baseline without actuator dynamics across fast translational tracking, rapid rotational tracking, and disturbance recovery.", "result": "The proposed controller yields better tracking; baseline diverges/crashes under fastest translational and disturbance recovery; the proposed controller maintains stability and completes tasks.", "conclusion": "Modeling actuator dynamics is essential for reliable control of variable-tilt multirotors; the proposed actuator-aware backstepping controller is robust to model uncertainty and enhances performance during aggressive maneuvers and disturbance rejection."}}
{"id": "2510.01618", "categories": ["cs.CV", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2510.01618", "abs": "https://arxiv.org/abs/2510.01618", "authors": ["Zijun Li", "Jinchang Zhang", "Ming Zhang", "Guoyu Lu"], "title": "Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics", "comment": null, "summary": "We propose an automated genomic interpretation module that transforms raw DNA\nsequences into actionable, interpretable decisions suitable for integration\ninto medical automation and robotic systems. Our framework combines Chaos Game\nRepresentation (CGR) with a Concept Bottleneck Model (CBM), enforcing\npredictions to flow through biologically meaningful concepts such as GC\ncontent, CpG density, and k mer motifs. To enhance reliability, we incorporate\nconcept fidelity supervision, prior consistency alignment, KL distribution\nmatching, and uncertainty calibration. Beyond accurate classification of HIV\nsubtypes across both in-house and LANL datasets, our module delivers\ninterpretable evidence that can be directly validated against biological\npriors. A cost aware recommendation layer further translates predictive outputs\ninto decision policies that balance accuracy, calibration, and clinical\nutility, reducing unnecessary retests and improving efficiency. Extensive\nexperiments demonstrate that the proposed system achieves state of the art\nclassification performance, superior concept prediction fidelity, and more\nfavorable cost benefit trade-offs compared to existing baselines. By bridging\nthe gap between interpretable genomic modeling and automated decision-making,\nthis work establishes a reliable foundation for robotic and clinical automation\nin genomic medicine.", "AI": {"tldr": "An interpretable genomic interpretation framework that maps raw DNA to actionable decisions via Chaos Game Representation and a Concept Bottleneck Model, enforcing concept-level predictions (GC content, CpG density, k-mer motifs) with calibration and cost-aware decision policies, achieving state-of-the-art HIV subtype classification and interpretable evidence for robotic/clinical automation.", "motivation": "There is a need for end-to-end, interpretable, and reliable genomic decision systems that can be integrated into automated medical robotics. By constraining predictions to biologically meaningful concepts and calibrating uncertainty, the system aims to provide trustworthy, actionable outputs aligned with biological priors.", "method": "Encode sequences using Chaos Game Representation (CGR) and route predictions through a Concept Bottleneck Model (CBM) with concepts such as GC content, CpG density, and k-mer motifs. Add concept fidelity supervision, prior consistency alignment, KL distribution matching, and uncertainty calibration. Include a cost-aware layer that translates predictions into decision policies suitable for automation.", "result": "The framework achieves state-of-the-art HIV subtype classification on both in-house and LANL datasets, with superior concept prediction fidelity and favorable cost-benefit trade-offs compared to baselines. It delivers interpretable, biology-aligned evidence that can be validated against priors and supports automated decision-making.", "conclusion": "This work bridges interpretable genomic modeling with automated decision-making, laying a foundation for reliable robotic and clinical automation in genomic medicine by providing both high predictive performance and concept-level interpretability validated against biological priors."}}
{"id": "2510.01345", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01345", "abs": "https://arxiv.org/abs/2510.01345", "authors": ["Akhlaqur Rahman Sabby", "Yi Sui", "Tongzi Wu", "Jesse C. Cresswell", "Ga Wu"], "title": "Self-Supervised Representation Learning as Mutual Information Maximization", "comment": null, "summary": "Self-supervised representation learning (SSRL) has demonstrated remarkable\nempirical success, yet its underlying principles remain insufficiently\nunderstood. While recent works attempt to unify SSRL methods by examining their\ninformation-theoretic objectives or summarizing their heuristics for preventing\nrepresentation collapse, architectural elements like the predictor network,\nstop-gradient operation, and statistical regularizer are often viewed as\nempirically motivated additions. In this paper, we adopt a first-principles\napproach and investigate whether the learning objective of an SSRL algorithm\ndictates its possible optimization strategies and model design choices. In\nparticular, by starting from a variational mutual information (MI) lower bound,\nwe derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint\nMI (JMI), each imposing distinct structural constraints and covering a set of\nexisting SSRL algorithms. SDMI inherently requires alternating optimization,\nmaking stop-gradient operations theoretically essential. In contrast, JMI\nadmits joint optimization through symmetric architectures without such\ncomponents. Under the proposed formulation, predictor networks in SDMI and\nstatistical regularizers in JMI emerge as tractable surrogates for the MI\nobjective. We show that many existing SSRL methods are specific instances or\napproximations of these two paradigms. This paper provides a theoretical\nexplanation behind the choices of different architectural components of\nexisting SSRL methods, beyond heuristic conveniences.", "AI": {"tldr": "From variational MI lower bounds, the paper derives two SSRL paradigms (SDMI and JMI) explaining architectural choices (predictor nets, stop-gradient, regularizers) and shows many existing methods are instances of these frameworks.", "motivation": "Understand why SSRL architectures are effective by deriving them from first-principles, rather than treating components as heuristic additions.", "method": "Starting from a variational mutual information lower bound, derive two training paradigms: Self-Distillation MI (SDMI) and Joint MI (JMI). Analyze the optimization constraints they impose (alternating vs. joint optimization, stop-gradients, symmetry). Interpret predictor networks and statistical regularizers as MI surrogates and map existing SSRL methods to the two paradigms.", "result": "SDMI requires alternating optimization with stop-gradient operations; JMI enables joint optimization with symmetric architectures. The predictor nets and regularizers emerge as tractable surrogates for the MI objective. Many existing SSRL methods are instances or approximations of SDMI or JMI.", "conclusion": "A principled, MI-based explanation for SSRL architectural choices, showing that different optimization structures (alternating vs. joint) naturally lead to different components, and that a broad set of methods can be unified under these two paradigms."}}
{"id": "2510.01531", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01531", "abs": "https://arxiv.org/abs/2510.01531", "authors": ["Djengo Cyun-Jyun Fang", "Tsung-Wei Ke"], "title": "Information Seeking for Robust Decision Making under Partial Observability", "comment": "The project page is available at https://infoseekerllm.github.io", "summary": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io", "AI": {"tldr": "InfoSeeker is an LLM decision-making framework that couples task planning with information seeking to align internal dynamics with the real world under partial observability and uncertain dynamics. It uses active information gathering actions before planning and demonstrates strong gains on a new benchmark suite (74% absolute improvement) and strong generalization across LLMs, robotics, and web navigation.", "motivation": "Address the gap where existing LLM planners manage observational uncertainty but ignore discrepancies between the model's internal dynamics and the actual environment, leading to suboptimal decisions under partial observability and dynamic environments.", "method": "Integrate task-oriented planning with information-seeking as controllable actions. The LLM plans actions to validate understanding, detect environmental changes, or test hypotheses, then generates or revises task plans. Evaluation on a novel partially observable benchmark suite, plus established benchmarks, with cross-LLM generalization.", "result": "InfoSeeker achieves 74% absolute performance gain over prior methods with good sample efficiency, generalizes across LLMs, and outperforms baselines on robotic manipulation and web navigation benchmarks.", "conclusion": "Tightly integrating planning and information seeking yields robust behavior under partial observability and dynamic environments, suggesting a promising direction for LLM-based agents that align internal models with the real world."}}
{"id": "2510.01708", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01708", "abs": "https://arxiv.org/abs/2510.01708", "authors": ["Zixing Lei", "Zibo Zhou", "Sheng Yin", "Yueru Chen", "Qingyao Xu", "Weixin Li", "Yunhong Wang", "Bowei Tang", "Wei Jing", "Siheng Chen"], "title": "PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization", "comment": "8 pages, 5 figures", "summary": "Humanoid whole-body control (WBC) policies trained in simulation often suffer\nfrom the sim-to-real gap, which fundamentally arises from simulator inductive\nbias, the inherent assumptions and limitations of any single simulator. These\nbiases lead to nontrivial discrepancies both across simulators and between\nsimulation and the real world. To mitigate the effect of simulator inductive\nbias, the key idea is to train policies jointly across multiple simulators,\nencouraging the learned controller to capture dynamics that generalize beyond\nany single simulator's assumptions. We thus introduce PolySim, a WBC training\nplatform that integrates multiple heterogeneous simulators. PolySim can launch\nparallel environments from different engines simultaneously within a single\ntraining run, thereby realizing dynamics-level domain randomization.\nTheoretically, we show that PolySim yields a tighter upper bound on simulator\ninductive bias than single-simulator training. In experiments, PolySim\nsubstantially reduces motion-tracking error in sim-to-sim evaluations; for\nexample, on MuJoCo, it improves execution success by 52.8 over an IsaacSim\nbaseline. PolySim further enables zero-shot deployment on a real Unitree G1\nwithout additional fine-tuning, showing effective transfer from simulation to\nthe real world. We will release the PolySim code upon acceptance of this work.", "AI": {"tldr": "PolySim is a multi-simulator training platform for humanoid whole-body control (WBC) that trains across heterogeneous simulators to reduce simulator inductive bias and the sim-to-real gap via dynamics-level domain randomization; it yields tighter bias bounds than single-simulator training, improves sim-to-sim performance (e.g., ~52.8-point gain over IsaacSim), and enables zero-shot real-world deployment on a Unitree G1; code will be released.", "motivation": "The sim-to-real gap in humanoid WBC arises from simulator inductive biases and assumptions that differ across simulators and from reality. Training policies across a single simulator can overfit to its biases, limiting generalization. Multi-simulator training aiming to uncover dynamics that generalize beyond individual simulators can mitigate this gap.", "method": "PolySim integrates multiple heterogeneous simulators and launches parallel environments from different engines within a single training run. This enables dynamics-level domain randomization by exposing the policy to diverse simulator dynamics, and it provides a theoretical bound showing a tighter upper bound on simulator inductive bias than single-simulator training.", "result": "In experiments, PolySim substantially reduces motion-tracking error in sim-to-sim evaluations. On MuJoCo, it improves execution success by 52.8 over an IsaacSim baseline. It also enables zero-shot deployment on a real Unitree G1 without additional fine-tuning. The authors plan to release the code upon acceptance.", "conclusion": "PolySim demonstrates that training across multiple simulators can tighten bounds on simulator inductive bias and improve cross-simulator and real-world transfer for WBC policies, enabling more robust sim-to-real transfer without per-simulator tuning."}}
{"id": "2510.01623", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01623", "abs": "https://arxiv.org/abs/2510.01623", "authors": ["Angen Ye", "Zeyu Zhang", "Boyuan Wang", "Xiaofeng Wang", "Dapeng Zhang", "Zheng Zhu"], "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.", "AI": {"tldr": "VLA-R1 introduces a reasoning-enhanced vision-language-action model that uses RLVR with GRPO, plus a 13K chain-of-thought dataset (VLA-CoT-13K), to improve reasoning and execution for better generalization and real-world robotic performance; code and data will be released.", "motivation": "Current VLA models lack explicit, verifiable step-by-step reasoning and often rely on weak reward signals during fine-tuning, limiting generalization and alignment with affordances and geometric relations.", "method": "An RLVR-based post-training strategy that uses verifiable rewards for region alignment, trajectory consistency, and output formatting, combined with Group Relative Policy Optimization (GRPO); creation of VLA-CoT-13K dataset with chain-of-thought supervision aligned to affordance and trajectory annotations; extensive multi-domain evaluations on in-domain/out-of-domain, simulation, and real-robot platforms.", "result": "VLA-R1 demonstrates superior generalization and real-world performance compared to prior VLA methods across diverse settings.", "conclusion": "Combining verifiable rewards for reasoning and chain-of-thought supervision with the VLA framework enhances reasoning quality and execution accuracy; the dataset and code release will support future research and broader adoption."}}
{"id": "2510.01349", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01349", "abs": "https://arxiv.org/abs/2510.01349", "authors": ["Hannah Lawrence", "Elyssa Hofgard", "Vasco Portilheiro", "Yuxuan Chen", "Tess Smidt", "Robin Walters"], "title": "To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking", "comment": "A short version of this paper appeared at the ICLR AI4Mat workshop in\n  April 2025", "summary": "Symmetry-aware methods for machine learning, such as data augmentation and\nequivariant architectures, encourage correct model behavior on all\ntransformations (e.g. rotations or permutations) of the original dataset. These\nmethods can improve generalization and sample efficiency, under the assumption\nthat the transformed datapoints are highly probable, or \"important\", under the\ntest distribution. In this work, we develop a method for critically evaluating\nthis assumption. In particular, we propose a metric to quantify the amount of\nanisotropy, or symmetry-breaking, in a dataset, via a two-sample neural\nclassifier test that distinguishes between the original dataset and its\nrandomly augmented equivalent. We validate our metric on synthetic datasets,\nand then use it to uncover surprisingly high degrees of alignment in several\nbenchmark point cloud datasets. We show theoretically that distributional\nsymmetry-breaking can actually prevent invariant methods from performing\noptimally even when the underlying labels are truly invariant, as we show for\ninvariant ridge regression in the infinite feature limit. Empirically, we find\nthat the implication for symmetry-aware methods is dataset-dependent:\nequivariant methods still impart benefits on some anisotropic datasets, but not\nothers. Overall, these findings suggest that understanding equivariance -- both\nwhen it works, and why -- may require rethinking symmetry biases in the data.", "AI": {"tldr": "A two-sample neural classifier test quantifies symmetry-breaking in data, showing symmetry-aware methods are not universally beneficial and their utility is dataset-dependent, prompting a rethink of symmetry biases in data.", "motivation": "Symmetry-aware techniques (data augmentation, equivariant models) assume transformed data points remain likely under the test distribution; this work questions that assumption and seeks to diagnose when symmetry biases help or hurt.", "method": "Introduce a metric for anisotropy by training a neural classifier to distinguish the original dataset from its randomly augmented version. Validate on synthetic datasets and apply to benchmark point clouds; supplement with theoretical results on invariant ridge regression in the infinite-feature limit.", "result": "The metric reveals substantial symmetry-breaking (alignment) in several benchmarks; distributional symmetry-breaking can impair invariant methods even when labels are invariant; the benefits of equivariant methods are data-dependent, aiding some anisotropic datasets but not others.", "conclusion": "Understanding equivariance requires reevaluating symmetry biases in data; symmetry-aware methods may not always help and their value depends on dataset anisotropy."}}
{"id": "2510.01544", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01544", "abs": "https://arxiv.org/abs/2510.01544", "authors": ["Shaoan Xie", "Lingjing Kong", "Xiangchen Song", "Xinshuai Dong", "Guangyi Chen", "Eric P. Xing", "Kun Zhang"], "title": "Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models", "comment": null, "summary": "Diffusion language models (dLLMs) offer a promising, non-autoregressive\nparadigm for text generation, yet training them for complex reasoning remains a\nkey challenge. Current reinforcement learning approaches often rely on sparse,\noutcome-based rewards, which can reinforce flawed reasoning paths that lead to\ncoincidentally correct answers. We argue that this stems from a fundamental\nmismatch with the natural structure of reasoning. We first propose a\ntheoretical framework that formalizes complex problem solving as a hierarchical\nselection process, where an intractable global constraint is decomposed into a\nseries of simpler, localized logical steps. This framework provides a\nprincipled foundation for algorithm design, including theoretical insights into\nthe identifiability of this latent reasoning structure. Motivated by this\ntheory, we identify unstructured refinement -- a failure mode where a model's\niterative steps do not contribute meaningfully to the solution -- as a core\ndeficiency in existing methods. We then introduce Step-Aware Policy\nOptimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising\nprocess with the latent reasoning hierarchy. By using a process-based reward\nfunction that encourages incremental progress, SAPO guides the model to learn\nstructured, coherent reasoning paths. Our empirical results show that this\nprincipled approach significantly improves performance on challenging reasoning\nbenchmarks and enhances the interpretability of the generation process.", "AI": {"tldr": "Hierarchical reasoning framework for diffusion LMs with Step-Aware Policy Optimization (SAPO), aligning denoising steps to latent reasoning to improve complex reasoning and interpretability.", "motivation": "Current RL with sparse, outcome-based rewards can reinforce flawed, coincidental reasoning paths and fail to capture the natural multi-step structure of problem solving; a theory-backed approach is needed.", "method": "Formalize complex problem solving as a hierarchical selection process; identify unstructured refinement as a key failure mode; introduce SAPO with a process-based reward to guide incremental progress and align denoising with latent reasoning hierarchy; provide theoretical insights into identifiability.", "result": "Empirical results show significant performance gains on challenging reasoning benchmarks and improved interpretability of the generation process.", "conclusion": "A principled, hierarchy-aware training paradigm for dLLMs via SAPO yields more structured reasoning paths, mitigates flawed iterative steps, and enhances interpretability."}}
{"id": "2510.01711", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01711", "abs": "https://arxiv.org/abs/2510.01711", "authors": ["Taeyoung Kim", "Jimin Lee", "Myungkyu Koo", "Dongyoung Kim", "Kyungmin Lee", "Changyeon Kim", "Younggyo Seo", "Jinwoo Shin"], "title": "Contrastive Representation Regularization for Vision-Language-Action Models", "comment": "20 pages, 12 figures", "summary": "Vision-Language-Action (VLA) models have shown its capabilities in robot\nmanipulation by leveraging rich representations from pre-trained\nVision-Language Models (VLMs). However, their representations arguably remain\nsuboptimal, lacking sensitivity to robotic signals such as control actions and\nproprioceptive states. To address the issue, we introduce Robot State-aware\nContrastive Loss (RS-CL), a simple and effective representation regularization\nfor VLA models, designed to bridge the gap between VLM representations and\nrobotic signals. In particular, RS-CL aligns the representations more closely\nwith the robot's proprioceptive states, by using relative distances between the\nstates as soft supervision. Complementing the original action prediction\nobjective, RS-CL effectively enhances control-relevant representation learning,\nwhile being lightweight and fully compatible with standard VLA training\npipeline. Our empirical results demonstrate that RS-CL substantially improves\nthe manipulation performance of state-of-the-art VLA models; it pushes the\nprior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,\nthrough more accurate positioning during grasping and placing, and boosts\nsuccess rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.", "AI": {"tldr": "RS-CL is a lightweight state-aware contrastive loss that aligns VLA model representations with robot proprioceptive signals using relative state distances as soft supervision, improving manipulation tasks.", "motivation": "VLM-based representations in Vision-Language-Action (VLA) models often lack sensitivity to robotic signals; bridging this gap by coupling representations to proprioceptive states should improve control-relevant features.", "method": "Introduce Robot State-aware Contrastive Loss (RS-CL) as a representation regularizer; use relative distances between robot states as soft supervision; complement the original action prediction objective; lightweight and fully compatible with standard VLA training pipelines.", "result": "RS-CL substantially improves manipulation performance, e.g., RoboCasa-Kitchen pick-and-place accuracy improves from 30.8% to 41.5%; real-robot manipulation success rate rises from 45.0% to 58.3%.", "conclusion": "RS-CL bridges the gap between VLM representations and robotic signals, delivering a simple, effective, and lightweight regularization that enhances control-relevant representation learning within standard VLA pipelines."}}
{"id": "2510.01640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01640", "abs": "https://arxiv.org/abs/2510.01640", "authors": ["Yifan Zhao", "Liangchen Li", "Yuqi Zhou", "Kai Wang", "Yan Liang", "Juyong Zhang"], "title": "Joint Deblurring and 3D Reconstruction for Macrophotography", "comment": "Accepted to Pacific Graphics 2025. To be published in Computer\n  Graphics Forum", "summary": "Macro lens has the advantages of high resolution and large magnification, and\n3D modeling of small and detailed objects can provide richer information.\nHowever, defocus blur in macrophotography is a long-standing problem that\nheavily hinders the clear imaging of the captured objects and high-quality 3D\nreconstruction of them. Traditional image deblurring methods require a large\nnumber of images and annotations, and there is currently no multi-view 3D\nreconstruction method for macrophotography. In this work, we propose a joint\ndeblurring and 3D reconstruction method for macrophotography. Starting from\nmulti-view blurry images captured, we jointly optimize the clear 3D model of\nthe object and the defocus blur kernel of each pixel. The entire framework\nadopts a differentiable rendering method to self-supervise the optimization of\nthe 3D model and the defocus blur kernel. Extensive experiments show that from\na small number of multi-view images, our proposed method can not only achieve\nhigh-quality image deblurring but also recover high-fidelity 3D appearance.", "AI": {"tldr": "A joint deblurring and multi-view 3D reconstruction framework for macrophotography using per-pixel defocus kernels and differentiable rendering, achieving high-quality deblurring and faithful 3D appearance from a small number of views.", "motivation": "Defocus blur severely degrades macrophotography imaging and hampers 3D reconstruction. Existing deblurring methods require many images/annotations, and there is no established multi-view 3D reconstruction approach for macrophotography.", "method": "From several multi-view blurry images, jointly optimize a clear 3D object model and per-pixel defocus blur kernels. The framework uses differentiable rendering to self-supervise the optimization of both the 3D model and the blur kernels in an end-to-end manner.", "result": "Experiments on a small set of multi-view images show that the method can produce high-quality deblurred images and reconstruct high-fidelity 3D appearances.", "conclusion": "The proposed method demonstrates that joint deblurring and multi-view 3D reconstruction for macrophotography is feasible with differentiable rendering, reducing the data requirements while achieving accurate 3D appearance."}}
{"id": "2510.01365", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.01365", "abs": "https://arxiv.org/abs/2510.01365", "authors": ["Maedeh Saberi", "Amir Barati Farimani", "Safa Jamali"], "title": "RheOFormer: A generative transformer model for simulation of complex fluids and flows", "comment": "8 pages, 5 figures. Submitted to PNAS", "summary": "The ability to model mechanics of soft materials under flowing conditions is\nkey in designing and engineering processes and materials with targeted\nproperties. This generally requires solution of internal stress tensor, related\nto the deformation tensor through nonlinear and history-dependent constitutive\nmodels. Traditional numerical methods for non-Newtonian fluid dynamics often\nsuffer from prohibitive computational demands and poor scalability to new\nproblem instances. Developments in data-driven methods have mitigated some\nlimitations but still require retraining across varied physical conditions. In\nthis work, we introduce Rheological Operator Transformer (RheOFormer), a\ngenerative operator learning method leveraging self-attention to efficiently\nlearn different spatial interactions and features of complex fluid flows. We\nbenchmark RheOFormer across a range of different viscometric and\nnon-viscometric flows with different types of viscoelastic and\nelastoviscoplastic mechanics in complex domains against ground truth solutions.\nOur results demonstrate that RheOFormer can accurately learn both scalar and\ntensorial nonlinear mechanics of different complex fluids and predict the\nspatio-temporal evolution of their flows, even when trained on limited\ndatasets. Its strong generalization capabilities and computational efficiency\nestablish RheOFormer as a robust neural surrogate for accelerating predictive\ncomplex fluid simulations, advancing data-driven experimentation, and enabling\nreal-time process optimization across a wide range of applications.", "AI": {"tldr": "RheOFormer is a self-attention based generative operator learner that acts as a neural surrogate for complex non-Newtonian fluid mechanics, delivering accurate, data-efficient predictions and enabling real-time optimization.", "motivation": "Non-Newtonian/viscoelastic flows involve nonlinear, history-dependent constitutive models and high computational cost. There is a need for scalable, generalizable data-driven surrogates that can adapt to varied conditions without retraining.", "method": "Introduce Rheological Operator Transformer (RheOFormer), a self-attention-based generative operator learning model. It learns different spatial interactions and features of complex fluid flows. Trained on a range of viscometric and non-viscometric flows with viscoelastic and elastoviscoplastic mechanics in complex domains to predict both scalar and tensorial nonlinear mechanics and the spatiotemporal evolution of flows, even from limited data.", "result": "RheOFormer accurately learns nonlinear scalar and tensorial mechanics, and predicts the spatiotemporal evolution of complex fluid flows. It shows strong generalization and computational efficiency, functioning as an effective neural surrogate.", "conclusion": "RheOFormer offers a robust framework for accelerating predictive simulations of complex fluids, enabling data-driven experimentation and real-time process optimization across diverse applications."}}
{"id": "2510.01569", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01569", "abs": "https://arxiv.org/abs/2510.01569", "authors": ["Yubin Kim", "Taehan Kim", "Eugene Park", "Chunjong Park", "Cynthia Breazeal", "Daniel McDuff", "Hae Won Park"], "title": "InvThink: Towards AI Safety via Inverse Reasoning", "comment": null, "summary": "We present InvThink, a simple yet powerful approach that gives large language\nmodels (LLMs) the capability of inverse thinking: reasoning through failure\nmodes before generating responses. Unlike existing safety alignment methods\nthat optimize directly for safe response, InvThink instructs models to 1)\nenumerate potential harms, 2) analyze their consequences, and 3) generate safe\noutputs that proactively avoid these risks. Our method reveals three key\nfindings: (i) safety improvements show stronger scaling with model size\ncompared to existing safety methods. (ii) InvThink mitigates safety tax; by\ntraining models to systematically consider failure modes, it preserves general\nreasoning capabilities on standard benchmarks. (iii) beyond general safety\ntasks, InvThink excels in high-stakes domains including external-facing\n(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,\nachieving up to 15.7% reduction in harmful responses compared to baseline\nmethods like SafetyPrompt. We further implement InvThink via supervised\nfine-tuning, and reinforcement learning across three LLM families. These\nresults suggest that inverse reasoning provides a scalable and generalizable\npath toward safer, more capable language models.", "AI": {"tldr": "InvThink uses inverse thinking to prime LLMs to anticipate harms before answering: enumerate potential harms, analyze consequences, and generate safe outputs. Safety gains scale with model size, preserve general reasoning, and extend to high-stakes and agentic risk domains, with up to 15.7% fewer harmful responses vs SafetyPrompt. Implemented via supervised fine-tuning and reinforcement learning across three LLM families.", "motivation": "Address safety in LLMs without sacrificing general reasoning by training models to think about failure modes beforehand, overcoming limitations of direct safety optimization.", "method": "Inverse thinking: (1) enumerate potential harms, (2) analyze their consequences, (3) generate safe outputs that avoid risks; implemented through supervised fine-tuning and reinforcement learning across three LLM families; evaluated on standard benchmarks and high-stakes domains.", "result": "Safety improvements scale with model size more than existing safety methods; up to 15.7% reduction in harmful responses versus baselines like SafetyPrompt; preserves general reasoning; effective in high-stakes domains (medicine, finance, law) and agentic risk scenarios.", "conclusion": "Inverse reasoning provides a scalable, generalizable approach to safer, more capable language models."}}
{"id": "2510.01761", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01761", "abs": "https://arxiv.org/abs/2510.01761", "authors": ["Wendu Zhang", "Heng Wang", "Shuangyi Wang", "Yuanrui Huang"], "title": "Dual-Mode Magnetic Continuum Robot for Targeted Drug Delivery", "comment": "7 pages, 3 figures, under review of ICRA 2026", "summary": "Magnetic continuum robots (MCRs) enable minimally invasive navigation through\ntortuous anatomical channels, yet axially magnetized designs have largely been\nlimited to bending-only motion. To expand deformation capabilities, this paper\npresents a simple assembly that embeds permanent magnets radially within the\ncatheter wall, allowing a single externally steered permanent magnet to\nindependently induce either bending or torsion. A physics-based formulation\ntogether with finite-element analysis establishes the actuation principles, and\nbenchtop experiments validate decoupled mode control under practical fields.\nBuilding on this, a dual-layer blockage mechanism consisting of outer grooves\nand inner plates leverages torsional shear to achieve on-demand drug release.\nFinally, an in-phantom intervention experiment demonstrates end-to-end\noperation: lumen following by bending for target approach, followed by\ntwist-activated release at the site. The resulting compact, cable-free platform\ncombines versatile deformation with precise payload delivery, indicating strong\npotential for next-generation, site-specific therapies.", "AI": {"tldr": "A compact magnetic continuum robot embeds magnets radially in the catheter wall to enable independent bending and torsion via a single external magnet, plus a dual-layer blockage for twist-activated drug release; validated by physics-based modeling, FE analysis, benchtop experiments, and an in-phantom end-to-end demonstration, indicating strong potential for site-specific therapies.", "motivation": "Extend the deformation repertoire of axially magnetized magnetic continuum robots beyond bending-only motion, enabling torsion control and integrated drug delivery for site-specific therapies while maintaining a compact, cable-free form factor.", "method": "Embed permanent magnets radially in the catheter wall; develop a physics-based actuation formulation; perform finite-element analysis to study decoupled bending and torsion; conduct benchtop experiments to validate mode decoupling under practical magnetic fields; implement a dual-layer blockage (outer grooves and inner plates) exploiting torsional shear for twist-activated drug release; test an in-phantom intervention with lumen following, bending to target, and twist-triggered release.", "result": "Demonstrated decoupled bending and torsion control with a single external magnet; achieved twist-activated drug release using the dual-layer blockage; verified end-to-end functionality in an in-phantom lumen, indicating practical feasibility for targeted therapies with a compact, cable-free platform.", "conclusion": "The proposed MCR platform offers versatile deformation and precise payload delivery, exhibiting strong potential for next-generation site-specific therapies and minimally invasive interventions, with a compact, cable-free design and integrated drug-release mechanism."}}
{"id": "2510.01641", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01641", "abs": "https://arxiv.org/abs/2510.01641", "authors": ["Xiaoyang Liu", "Zhengyan Zhou", "Zihang Xu", "Jiezhang Cao", "Zheng Chen", "Yulun Zhang"], "title": "FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring", "comment": null, "summary": "Recent advancements in image motion deblurring, driven by CNNs and\ntransformers, have made significant progress. Large-scale pre-trained diffusion\nmodels, which are rich in true-world modeling, have shown great promise for\nhigh-quality image restoration tasks such as deblurring, demonstrating stronger\ngenerative capabilities than CNN and transformer-based methods. However,\nchallenges such as unbearable inference time and compromised fidelity still\nlimit the full potential of the diffusion models. To address this, we introduce\nFideDiff, a novel single-step diffusion model designed for high-fidelity\ndeblurring. We reformulate motion deblurring as a diffusion-like process where\neach timestep represents a progressively blurred image, and we train a\nconsistency model that aligns all timesteps to the same clean image. By\nreconstructing training data with matched blur trajectories, the model learns\ntemporal consistency, enabling accurate one-step deblurring. We further enhance\nmodel performance by integrating Kernel ControlNet for blur kernel estimation\nand introducing adaptive timestep prediction. Our model achieves superior\nperformance on full-reference metrics, surpassing previous diffusion-based\nmethods and matching the performance of other state-of-the-art models. FideDiff\noffers a new direction for applying pre-trained diffusion models to\nhigh-fidelity image restoration tasks, establishing a robust baseline for\nfurther advancing diffusion models in real-world industrial applications. Our\ndataset and code will be available at https://github.com/xyLiu339/FideDiff.", "AI": {"tldr": "A single-step diffusion-based deblurring method, FideDiff, achieves high-fidelity restoration by enforcing temporal consistency across diffusion-like timesteps and leveraging kernel-controlled blur estimation, outperforming prior diffusion-based methods and matching state-of-the-art restoration models with efficient one-step inference.", "motivation": "Diffusion models excel at high-quality restoration but suffer from slow inference and fidelity-variance. There is a need for fast, high-fidelity deblurring suitable for real-world applications by leveraging pre-trained diffusion models.", "method": "Reformulate motion deblurring as a diffusion-like process where each timestep corresponds to a progressively blurred image. Train a consistency model that aligns all timesteps to a single clean image. Use matched blur trajectories for training data to enforce temporal consistency. Enhance with Kernel ControlNet for blur kernel estimation and implement adaptive timestep prediction for efficient inference.", "result": "Achieves superior full-reference deblurring metrics compared with previous diffusion-based methods and matches the performance of other state-of-the-art models, while enabling single-step (fast) deblurring. The work also provides dataset and code release.", "conclusion": "FideDiff demonstrates a robust approach to adapting large pre-trained diffusion models for high-fidelity image restoration, offering a practical baseline for real-world industrial applications and paving the way for efficient diffusion-based deblurring."}}
{"id": "2510.01378", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01378", "abs": "https://arxiv.org/abs/2510.01378", "authors": ["Kiwhan Song", "Jaeyeon Kim", "Sitan Chen", "Yilun Du", "Sham Kakade", "Vincent Sitzmann"], "title": "Selective Underfitting in Diffusion Models", "comment": null, "summary": "Diffusion models have emerged as the principal paradigm for generative\nmodeling across various domains. During training, they learn the score\nfunction, which in turn is used to generate samples at inference. They raise a\nbasic yet unsolved question: which score do they actually learn? In principle,\na diffusion model that matches the empirical score in the entire data space\nwould simply reproduce the training data, failing to generate novel samples.\nRecent work addresses this question by arguing that diffusion models underfit\nthe empirical score due to training-time inductive biases. In this work, we\nrefine this perspective, introducing the notion of selective underfitting:\ninstead of underfitting the score everywhere, better diffusion models more\naccurately approximate the score in certain regions of input space, while\nunderfitting it in others. We characterize these regions and design empirical\ninterventions to validate our perspective. Our results establish that selective\nunderfitting is essential for understanding diffusion models, yielding new,\ntestable insights into their generalization and generative performance.", "AI": {"tldr": "Selective underfitting: diffusion models approximate the score accurately in some input regions while underfitting in others, offering a region-dependent view that explains generalization and generation.", "motivation": "Clarify what score diffusion models learn and reconcile global underfitting with successful generation by introducing a region-specific bias in score approximation.", "method": "Define and formalize the notion of selective underfitting; characterize regions of input space where the score is well-fit versus underfit; design empirical interventions to test the hypothesis.", "result": "Evidence that selective underfitting captures diffusion models' behavior and provides new insights into their generalization and generative performance.", "conclusion": "Understanding diffusion models through the lens of selective underfitting yields testable predictions and advances a nuanced theoretical account of training biases in score-based generative modeling."}}
{"id": "2510.01586", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01586", "abs": "https://arxiv.org/abs/2510.01586", "authors": ["Zhenyu Pan", "Yiting Zhang", "Zhuo Liu", "Yolo Yunlong Tang", "Zeliang Zhang", "Haozheng Luo", "Yuwei Han", "Jianshu Zhang", "Dennis Wu", "Hong-Yu Chen", "Haoran Lu", "Haoyang Fang", "Manling Li", "Chenliang Xu", "Philip S. Yu", "Han Liu"], "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning", "comment": null, "summary": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead.", "AI": {"tldr": "AdvEvo-MARL is a co-evolutionary MARL framework that internalizes safety into task agents, using adversarial training to resist jailbreak prompts without external guards, achieving lower attack success rates and equal or improved task performance.", "motivation": "Current defenses either rely on self-verification (often ineffective for cross-agent risks) or external guard modules (adding overhead and single points of failure). Integrated safety within agents aims to reduce risk from cross-agent chains and delegation while maintaining efficiency.", "method": "Co-evolutionary reinforcement learning that jointly optimizes attackers (synthesizing evolving jailbreak prompts) and defenders (task agents trained to complete tasks and resist attacks) in adversarial environments. Introduces a public baseline for advantage estimation: a group-level mean-return baseline shared among agents in the same functional group to reduce variance and boost intra-group coordination.", "result": "Attack success rate (ASR) kept below 20% across scenarios (vs. up to 38.33% for baselines); task accuracy on reasoning tasks improved by up to 3.67%; no external guard modules required, reducing overhead while increasing safety and often performance.", "conclusion": "Integrated safety learning in task agents can simultaneously improve safety and utility in LLM-based multi-agent systems, avoiding external guards and extra system overhead."}}
{"id": "2510.01770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01770", "abs": "https://arxiv.org/abs/2510.01770", "authors": ["Christopher Leet", "Aidan Sciortino", "Sven Koenig"], "title": "An Anytime, Scalable and Complete Algorithm for Embedding a Manufacturing Procedure in a Smart Factory", "comment": null, "summary": "Modern automated factories increasingly run manufacturing procedures using a\nmatrix of programmable machines, such as 3D printers, interconnected by a\nprogrammable transport system, such as a fleet of tabletop robots. To embed a\nmanufacturing procedure into a smart factory, an operator must: (a) assign each\nof its processes to a machine and (b) specify how agents should transport parts\nbetween machines. The problem of embedding a manufacturing process into a smart\nfactory is termed the Smart Factory Embedding (SFE) problem. State-of-the-art\nSFE solvers can only scale to factories containing a couple dozen machines.\nModern smart factories, however, may contain hundreds of machines. We fill this\nhole by introducing the first highly scalable solution to the SFE, TS-ACES, the\nTraffic System based Anytime Cyclic Embedding Solver. We show that TS-ACES is\ncomplete and can scale to SFE instances based on real industrial scenarios with\nmore than a hundred machines.", "AI": {"tldr": "Introduces TS-ACES, a complete and scalable solver for the Smart Factory Embedding problem, enabling embedding in factories with over 100 machines.", "motivation": "Embedding manufacturing procedures in smart factories requires assigning each process to a machine and specifying part transport between machines; existing SFE solvers scale only to a few dozen machines, but modern factories may have hundreds.", "method": "Traffic System based Anytime Cyclic Embedding Solver (TS-ACES). It is a complete solver that uses a traffic-system viewpoint and Anytime/Cyclic embedding strategies to solve SFE at large scale.", "result": "TS-ACES is complete and scalable to SFE instances based on real industrial scenarios with more than a hundred machines.", "conclusion": "This work closes the scalability gap in SFE, enabling practical deployment of large-scale automated factories and broadening the applicability of smart factory embedding techniques."}}
{"id": "2510.01651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01651", "abs": "https://arxiv.org/abs/2510.01651", "authors": ["Rixin Zhou", "Peiqiang Qiu", "Qian Zhang", "Chuntao Li", "Xi Yang"], "title": "LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition", "comment": "18 pages, 7 figures, 2 Tables", "summary": "Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial\nstage of early Chinese writing and provide indispensable evidence for\narchaeological and historical studies. However, automatic BI recognition\nremains difficult due to severe visual degradation, multi-domain variability\nacross photographs, rubbings, and tracings, and an extremely long-tailed\ncharacter distribution. To address these challenges, we curate a large-scale BI\ndataset comprising 22454 full-page images and 198598 annotated characters\nspanning 6658 unique categories, enabling robust cross-domain evaluation.\nBuilding on this resource, we develop a two-stage detection-recognition\npipeline that first localizes inscriptions and then transcribes individual\ncharacters. To handle heterogeneous domains and rare classes, we equip the\npipeline with LadderMoE, which augments a pretrained CLIP encoder with\nladder-style MoE adapters, enabling dynamic expert specialization and stronger\nrobustness. Comprehensive experiments on single-character and full-page\nrecognition tasks demonstrate that our method substantially outperforms\nstate-of-the-art scene text recognition baselines, achieving superior accuracy\nacross head, mid, and tail categories as well as all acquisition modalities.\nThese results establish a strong foundation for bronze inscription recognition\nand downstream archaeological analysis.", "AI": {"tldr": "Introduces a large BI dataset (22k pages, ~199k characters, 6.7k classes) and a two-stage detection-recognition pipeline using LadderMoE on CLIP to achieve state-of-the-art recognition across domain variations and long-tail categories.", "motivation": "Bronze inscriptions are crucial for early Chinese writing and archaeology, but automatic recognition is hampered by severe degradation, cross-domain variability, and extreme long-tailed class distributions; a robust, cross-domain BI recognition system is needed.", "method": "A two-stage pipeline that first localizes inscriptions and then transcribes characters. Enhances a pretrained CLIP encoder with ladder-style MoE adapters (LadderMoE) to enable dynamic specialization and robustness across heterogeneous domains and rare classes.", "result": "Outperforms state-of-the-art scene text recognition baselines on both single-character and full-page tasks, with superior accuracy across head, mid, tail categories and all acquisition modalities.", "conclusion": "The dataset and LadderMoE-based approach establish a strong foundation for automatic bronze inscription recognition and support downstream archaeological analysis."}}
{"id": "2510.01384", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01384", "abs": "https://arxiv.org/abs/2510.01384", "authors": ["Jaeyeon Kim", "Seunggeun Kim", "Taekyun Lee", "David Z. Pan", "Hyeji Kim", "Sham Kakade", "Sitan Chen"], "title": "Fine-Tuning Masked Diffusion for Provable Self-Correction", "comment": null, "summary": "A natural desideratum for generative models is self-correction--detecting and\nrevising low-quality tokens at inference. While Masked Diffusion Models (MDMs)\nhave emerged as a promising approach for generative modeling in discrete\nspaces, their capacity for self-correction remains poorly understood. Prior\nattempts to incorporate self-correction into MDMs either require overhauling\nMDM architectures/training or rely on imprecise proxies for token quality,\nlimiting their applicability. Motivated by this, we introduce PRISM--Plug-in\nRemasking for Inference-time Self-correction of Masked Diffusions--a\nlightweight, model-agnostic approach that applies to any pretrained MDM.\nTheoretically, PRISM defines a self-correction loss that provably learns\nper-token quality scores, without RL or a verifier. These quality scores are\ncomputed in the same forward pass with MDM and used to detect low-quality\ntokens. Empirically, PRISM advances MDM inference across domains and scales:\nSudoku; unconditional text (170M); and code with LLaDA (8B).", "AI": {"tldr": "PRISM enables lightweight, inference-time self-correction for pretrained Masked Diffusion Models by learning per-token quality scores via a self-correction loss, improving generation on Sudoku, long text, and code.", "motivation": "Desideratum for generative models is self-correction. While Masked Diffusion Models are strong for discrete spaces, their capacity for self-correction is underexplored. Prior approaches either overhaul architectures/training or rely on imprecise proxies, limiting applicability. A lightweight, model-agnostic solution is needed.", "method": "Introduce PRISM (Plug-in Remasking for Inference-time Self-correction of Masked Diffusions). It defines a self-correction loss that provably learns per-token quality scores, without reinforcement learning or a verifier. The quality scores are computed in the same forward pass as the MDM and are used to detect low-quality tokens, without changing the MDM training or architecture. PRISM is plug-in and model-agnostic, applicable to any pretrained MDM.", "result": "Empirically, PRISM improves MDM inference across domains and scales: Sudoku; unconditional text (170M); and code with LLaDA (8B). The framework yields learned per-token quality scores and effective self-correction during inference.", "conclusion": "PRISM offers a lightweight, theoretically grounded, and empirically validated approach for inference-time self-correction in discrete-space diffusion models. It is model-agnostic and broadly applicable across domains, enabling improved generation without retraining."}}
{"id": "2510.01609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01609", "abs": "https://arxiv.org/abs/2510.01609", "authors": ["Bo Ma", "Hang Li", "ZeHua Hu", "XiaoFan Gui", "LuYao Liu", "Simon Lau"], "title": "AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence", "comment": null, "summary": "Interactive conversational recommender systems have gained significant\nattention for their ability to capture user preferences through natural\nlanguage interactions. However, existing approaches face substantial challenges\nin handling dynamic user preferences, maintaining conversation coherence, and\nbalancing multiple ranking objectives simultaneously. This paper introduces\nAgentRec, a next-generation LLM-powered multi-agent collaborative\nrecommendation framework that addresses these limitations through hierarchical\nagent networks with adaptive intelligence. Our approach employs specialized\nLLM-powered agents for conversation understanding, preference modeling, context\nawareness, and dynamic ranking, coordinated through an adaptive weighting\nmechanism that learns from interaction patterns. We propose a three-tier\nlearning strategy combining rapid response for simple queries, intelligent\nreasoning for complex preferences, and deep collaboration for challenging\nscenarios. Extensive experiments on three real-world datasets demonstrate that\nAgentRec achieves consistent improvements over state-of-the-art baselines, with\n2.8\\% enhancement in conversation success rate, 1.9\\% improvement in\nrecommendation accuracy (NDCG@10), and 3.2\\% better conversation efficiency\nwhile maintaining comparable computational costs through intelligent agent\ncoordination.", "AI": {"tldr": "AgentRec introduces a hierarchical, LLM-powered multi-agent framework for interactive conversational recommendation that uses adaptive weighting and a three-tier learning strategy to balance conversation quality and ranking objectives.", "motivation": "Existing interactive recommender systems struggle with dynamic user preferences, maintaining conversation coherence, and jointly optimizing multiple objectives; there is a need for coordinated, intelligent agent architectures.", "method": "AgentRec deploys hierarchical agent networks with specialized LLM agents for conversation understanding, preference modeling, context awareness, and dynamic ranking, coordinated by an adaptive weighting mechanism learned from interactions. It adopts a three-tier learning strategy: rapid response for simple queries, intelligent reasoning for complex preferences, and deep collaboration for challenging scenarios.", "result": "Experiments on three real-world datasets show improvements over baselines: 2.8% higher conversation success rate, 1.9% higher NDCG@10, and 3.2% better conversation efficiency, with comparable computational costs thanks to efficient agent coordination.", "conclusion": "AgentRec demonstrates consistent performance gains and effective coordination for next-generation interactive recommender systems, advancing the state of the art while maintaining practical efficiency."}}
{"id": "2510.01795", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01795", "abs": "https://arxiv.org/abs/2510.01795", "authors": ["Haibo Hu", "Lianming Huang", "Xinyu Wang", "Yufei Cui", "Nan Guan", "Chun Jason Xue"], "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving\nfor unified perception and reasoning, but high inference latency hinders\nreal-time deployment. Early-exit reduces latency by terminating inference at\nintermediate layers, yet its task-dependent nature limits generalization across\ndiverse scenarios. We observe that this limitation aligns with autonomous\ndriving: navigation systems can anticipate upcoming contexts (e.g.,\nintersections, traffic lights), indicating which tasks will be required. We\npropose Nav-EE, a navigation-guided early-exit framework that precomputes\ntask-specific exit layers offline and dynamically applies them online based on\nnavigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE\nachieves accuracy comparable to full inference while reducing latency by up to\n63.9%. Real-vehicle integration with Autoware Universe further demonstrates\nreduced inference latency (600ms to 300ms), supporting faster decision-making\nin complex scenarios. These results suggest that coupling navigation foresight\nwith early-exit offers a viable path toward efficient deployment of large\nmodels in autonomous systems. Code and data are available at our anonymous\nrepository: https://anonymous.4open.science/r/Nav-EE-BBC4", "AI": {"tldr": "Nav-EE precomputes task-specific early-exit layers for vision-language models in autonomous driving and selects them online using navigation priors, reducing latency while maintaining full-inference-like accuracy.", "motivation": "Vision-language models offer unified perception and reasoning but incur high latency; early-exit helps but is task-specific and lacks generalization. Autonomous driving can foresee upcoming contexts (intersections, traffic signals), suggesting a navigation-guided exit strategy.", "method": "Offline precomputation of task-specific exit layers; online dynamic application guided by navigation priors; evaluation on CODA, Waymo, BOSCH; real-vehicle integration with Autoware Universe.", "result": "Matches full-inference accuracy while reducing latency by up to 63.9%; in-vehicle latency drops from 600 ms to 300 ms in complex scenarios; demonstrates practicality of navigation-guided exits with large models.", "conclusion": "Coupling navigation foresight with early-exit provides a viable path to deploy large vision-language models in autonomous systems efficiently; code/data available at an anonymous repository."}}
{"id": "2510.01660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01660", "abs": "https://arxiv.org/abs/2510.01660", "authors": ["Duy Nguyen", "Dat Nguyen"], "title": "VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming", "comment": null, "summary": "Existing UDA pipelines fine-tune already well-trained backbone parameters for\nevery new source-and-target pair, resulting in the number of training\nparameters and storage memory growing linearly with each new pair, and also\npreventing the reuse of these well-trained backbone parameters.\n  Inspired by recent implications that existing backbones have textural biases,\nwe propose making use of domain-specific textural bias for domain adaptation\nvia visual reprogramming, namely VirDA.Instead of fine-tuning the full\nbackbone, VirDA prepends a domain-specific visual reprogramming layer to the\nbackbone. This layer produces visual prompts that act as an added textural bias\nto the input image, adapting its ``style'' to a target domain. To optimize\nthese visual reprogramming layers, we use multiple objective functions that\noptimize the intra- and inter-domain distribution differences when\ndomain-adapting visual prompts are applied. This process does not require\nmodifying the backbone parameters, allowing the same backbone to be reused\nacross different domains.\n  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M\ntrainable parameters. VirDA surpasses PDA, the state-of-the-art\nparameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its\nparameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans\nand FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%\nof their trainable parameters. Relative to the strongest current methods\n(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only\n2.2% and 1.1% accuracy, respectively.", "AI": {"tldr": "VirDA introduces parameter-efficient unsupervised domain adaptation by adding domain-specific visual reprogramming prompts to a fixed backbone, avoiding backbone fine-tuning while achieving competitive accuracy on Office-31.", "motivation": "Current UDA pipelines fine-tune backbones for each source-target pair, causing linear growth in trainable parameters and memory and hindering reuse. The idea that backbones carry textural biases motivates using domain-specific prompts to bias inputs rather than modifying the model.", "method": "Attach a domain-specific visual reprogramming layer to the backbone that generates visual prompts to bias input texture; train these prompts with multiple objectives to align intra- and inter-domain distributions; backbone remains fixed and reusable across domains; prompts adapt to target domains.", "result": "On Office-31, VirDA achieves 92.8% mean accuracy with 1.5M trainable parameters; surpasses PDA by 1.6% using 46% of PDA's parameters; outperforms CDTrans and FixBi by 0.2% and 1.4% while using 1.7% and 2.8% of their trainable parameters; relative to PMTrans and TVT, VirDA uses ~1.7% of their parameters with accuracy trade-offs of \u22122.2% and \u22121.1%.", "conclusion": "VirDA demonstrates a viable, parameter-efficient UDA approach by reusing backbones through visual reprogramming, achieving competitive accuracy with substantially reduced trainable parameters and memory."}}
{"id": "2510.01394", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01394", "abs": "https://arxiv.org/abs/2510.01394", "authors": ["Yusuf Kalayci", "Vinod Raman", "Shaddin Dughmi"], "title": "Optimal Stopping vs Best-of-$N$ for Inference Time Optimization", "comment": "24 pages", "summary": "Large language model (LLM) generation often requires balancing output quality\nagainst inference cost, especially when using multiple generations. We\nintroduce a new framework for inference-time optimization based on the\nclassical Pandora's Box problem. Viewing each generation as opening a costly\n\"box\" with random reward, we develop algorithms that decide when to stop\ngenerating without knowing the underlying reward distribution. Our first\ncontribution is a UCB-style Pandora's Box algorithm, which achieves performance\nthat is provably close to Weitzman's algorithm, the optimal strategy when the\ndistribution is known. We further adapt this method to practical LLM settings\nby addressing reward scaling across prompts via a Bradley-Terry inspired\ntransformation. This leads to an adaptive inference-time optimization method\nthat normalizes rewards and learns stopping thresholds on the fly. Experiments\non the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,\nshow that our adaptive strategy can obtain the same performance as non-adaptive\nBest-of-N sampling while requiring 15-35 percent fewer generations on average.\nOur results establish a principled bridge between optimal stopping theory and\ninference-time scaling, providing both theoretical performance bounds and\npractical efficiency gains for LLM deployment.", "AI": {"tldr": "Adaptive Pandora's Box framework for LLM inference that learns when to stop generating; achieves near-optimal performance with 15\u201335% fewer generations than Best-of-N.", "motivation": "LLMs often balance output quality and inference cost, especially when multiple generations are used. Existing approaches like Best-of-N assume known reward distributions or incur fixed costs; there is a need for online, distribution-free stopping decisions that adapt during deployment.", "method": "Model each generation as opening a costly box with a random reward. Develop a UCB-style Pandora's Box algorithm that performs near the optimal Weitzman policy when the distribution is known. Extend with a Bradley-Terry\u2013inspired reward transformation to normalize rewards across prompts, enabling online learning of stopping thresholds. Implement adaptive inference-time optimization that normalizes rewards and updates thresholds on the fly.", "result": "The adaptive strategy matches Best-of-N performance while requiring 15\u201335% fewer generations on average across AlpacaFarm and HH-RLHF datasets, for multiple LLM\u2013reward model pairs. The approach provides theoretical bounds close to the optimal policy and demonstrates practical efficiency gains in LLM deployment.", "conclusion": "This work bridges optimal stopping theory and inference-time scaling, offering both theoretical performance guarantees and tangible efficiency improvements for deploying large language models."}}
{"id": "2510.01611", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01611", "abs": "https://arxiv.org/abs/2510.01611", "authors": ["Min Zeng"], "title": "PychoBench: Evaluating the Psychology Intelligence of Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of industries, primarily due to their impressive generative\nabilities. Yet, their potential in applications requiring cognitive abilities,\nsuch as psychological counseling, remains largely untapped. This paper\ninvestigates the key question: Can LLMs be effectively applied to psychological\ncounseling? To determine whether an LLM can effectively take on the role of a\npsychological counselor, the first step is to assess whether it meets the\nqualifications required for such a role, namely the ability to pass the U.S.\nNational Counselor Certification Exam (NCE). This is because, just as a human\ncounselor must pass a certification exam to practice, an LLM must demonstrate\nsufficient psychological knowledge to meet the standards required for such a\nrole. To address this, we introduce PsychoBench, a benchmark grounded in\nU.S.national counselor examinations, a licensure test for professional\ncounselors that requires about 70% accuracy to pass. PsychoBench comprises\napproximately 2,252 carefully curated single-choice questions, crafted to\nrequire deep understanding and broad enough to cover various sub-disciplines of\npsychology. This benchmark provides a comprehensive assessment of an LLM's\nability to function as a counselor. Our evaluation shows that advanced models\nsuch as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing\nthreshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)\nremain far below it. These results suggest that only frontier LLMs are\ncurrently capable of meeting counseling exam standards, highlighting both the\npromise and the challenges of developing psychology-oriented LLMs.", "AI": {"tldr": "PsychoBench evaluates whether LLMs can meet licensing standards for psychological counseling by testing them on a national exam-style benchmark; frontier models surpass the passing threshold while smaller open models lag, signaling potential but notable challenges for psychology-oriented LLMs.", "motivation": "Assess if LLMs can function as licensed counselors by comparing their knowledge against the requirements of a U.S. counselor licensure exam.", "method": "Build PsychoBench with ~2,252 single-choice questions derived from U.S. National Counselor Examination content; evaluate multiple models including GPT-4o, Llama3-70B, Gemma3-27B, Qwen2.5-7B, Mistral-7B to measure passing chances.", "result": "Advanced, frontier models achieve scores well above the 70% passing threshold; smaller open models remain far below it, indicating that only cutting-edge LLMs currently meet counseling exam standards.", "conclusion": "Shows promise for LLM-based counseling with frontier models, but highlights substantial challenges and the need for further work on safety, reliability, and real-world applicability beyond exam performance."}}
{"id": "2510.01830", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01830", "abs": "https://arxiv.org/abs/2510.01830", "authors": ["Hongze Wang", "Boyang Sun", "Jiaxu Xing", "Fan Yang", "Marco Hutter", "Dhruv Shah", "Davide Scaramuzza", "Marc Pollefeys"], "title": "What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework", "comment": null, "summary": "Object-Goal Navigation (ObjectNav) is a critical component toward deploying\nmobile robots in everyday, uncontrolled environments such as homes, schools,\nand workplaces. In this context, a robot must locate target objects in\npreviously unseen environments using only its onboard perception. Success\nrequires the integration of semantic understanding, spatial reasoning, and\nlong-horizon planning, which is a combination that remains extremely\nchallenging. While reinforcement learning (RL) has become the dominant\nparadigm, progress has spanned a wide range of design choices, yet the field\nstill lacks a unifying analysis to determine which components truly drive\nperformance. In this work, we conduct a large-scale empirical study of modular\nRL-based ObjectNav systems, decomposing them into three key components:\nperception, policy, and test-time enhancement. Through extensive controlled\nexperiments, we isolate the contribution of each and uncover clear trends:\nperception quality and test-time strategies are decisive drivers of\nperformance, whereas policy improvements with current methods yield only\nmarginal gains. Building on these insights, we propose practical design\nguidelines and demonstrate an enhanced modular system that surpasses\nState-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We\nalso introduce a human baseline under identical conditions, where experts\nachieve an average 98% success, underscoring the gap between RL agents and\nhuman-level navigation. Our study not only sets the SotA performance but also\nprovides principled guidance for future ObjectNav development and evaluation.", "AI": {"tldr": "Large-scale empirical study shows perception quality and test-time strategies, not policy, primarily drive ObjectNav performance; proposes practical guidelines and a modular system that surpasses SotA; highlights a notable gap to human-level navigation.", "motivation": "ObjectNav requires integrating semantic understanding, spatial reasoning, and long-horizon planning in unseen environments. There is no unifying analysis identifying which components truly impact performance, despite many RL design choices.", "method": "Decompose modular RL ObjectNav systems into three components\u2014perception, policy, and test-time enhancements\u2014and conduct extensive controlled experiments with ablations to isolate each component's contribution. Compare against state-of-the-art methods and include a human baseline under identical conditions.", "result": "Perception quality and test-time strategies are decisive drivers of performance; policy improvements with current methods yield only marginal gains. An enhanced modular system surpasses the state-of-the-art by 6.6% on SPL and by 2.7% in success rate. A human baseline under identical conditions achieves ~98% success, highlighting the gap between RL agents and humans.", "conclusion": "The study provides principled design guidelines for ObjectNav, guiding future development and evaluation, and emphasizes prioritizing perceptual reliability and effective test-time strategies over marginal policy optimization."}}
{"id": "2510.01662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01662", "abs": "https://arxiv.org/abs/2510.01662", "authors": ["Minh Tran", "Maksim Siniukov", "Zhangyu Jin", "Mohammad Soleymani"], "title": "Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery", "comment": null, "summary": "Facial expression analysis is central to understanding human behavior, yet\nexisting coding systems such as the Facial Action Coding System (FACS) are\nconstrained by limited coverage and costly manual annotation. In this work, we\nintroduce Discrete Facial Encoding (DFE), an unsupervised, data-driven\nalternative of compact and interpretable dictionary of facial expressions from\n3D mesh sequences learned through a Residual Vector Quantized Variational\nAutoencoder (RVQ-VAE). Our approach first extracts identity-invariant\nexpression features from images using a 3D Morphable Model (3DMM), effectively\ndisentangling factors such as head pose and facial geometry. We then encode\nthese features using an RVQ-VAE, producing a sequence of discrete tokens from a\nshared codebook, where each token captures a specific, reusable facial\ndeformation pattern that contributes to the overall expression. Through\nextensive experiments, we demonstrate that Discrete Facial Encoding captures\nmore precise facial behaviors than FACS and other facial encoding alternatives.\nWe evaluate the utility of our representation across three high-level\npsychological tasks: stress detection, personality prediction, and depression\ndetection. Using a simple Bag-of-Words model built on top of the learned\ntokens, our system consistently outperforms both FACS-based pipelines and\nstrong image and video representation learning models such as Masked\nAutoencoders. Further analysis reveals that our representation covers a wider\nvariety of facial displays, highlighting its potential as a scalable and\neffective alternative to FACS for psychological and affective computing\napplications.", "AI": {"tldr": "Unsupervised, data-driven discrete facial encoding (DFE) using RVQ-VAE on 3DMM-extracted expressions; outperforms FACS and MAE baselines on stress, personality, and depression tasks.", "motivation": "Overcome the limitations of the Facial Action Coding System (FACS): limited coverage and costly manual annotation; need scalable, interpretable, and expressive facial representations for psychology and affective computing.", "method": "Extract identity-invariant expression features via a 3D Morphable Model (3DMM); encode with a Residual Vector Quantized Variational Autoencoder (RVQ-VAE) to produce a sequence of discrete tokens from a shared codebook; use Bag-of-Words on tokens for downstream tasks; evaluate on stress detection, personality prediction, and depression detection.", "result": "DFE captures more precise facial behaviors than FACS and other encoders; Bag-of-Words with learned tokens outperforms FACS pipelines and strong baselines (e.g., MAE) across tasks; broader coverage of facial displays.", "conclusion": "Discrete Facial Encoding offers a scalable, interpretable, and effective alternative to FACS for psychological and affective computing applications, with potential for broad adoption and further exploration."}}
{"id": "2510.01396", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.01396", "abs": "https://arxiv.org/abs/2510.01396", "authors": ["Wasut Pornpatcharapong"], "title": "Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems", "comment": "6 pages, 4 figures. This work has already been accepted for\n  presentation in The 29th International Computer Science and Engineering\n  Conference (ICSEC) 2025, Chiang Mai, Thailand, and will be published in IEEE\n  Xplore", "summary": "Free energy reconstruction methods such as Gaussian Process Regression (GPR)\nrequire Jacobians of the collective variables (CVs), a bottleneck that\nrestricts the use of complex or machine-learned CVs. We introduce a neural\nnetwork surrogate framework that learns CVs directly from Cartesian coordinates\nand uses automatic differentiation to provide Jacobians, bypassing analytical\nforms. On an MgCl2 ion-pairing system, our method achieved high accuracy for\nboth a simple distance CV and a complex coordination-number CV. Moreover,\nJacobian errors also followed a near-Gaussian distribution, making them\nsuitable for GPR pipelines. This framework enables gradient-based free energy\nmethods to incorporate complex and machine-learned CVs, broadening the scope of\nbiochemistry and materials simulations.", "AI": {"tldr": "Neural-network surrogate learns CVs directly from Cartesian coordinates and provides Jacobians via automatic differentiation, replacing the need for analytical Jacobians and enabling gradient-based free energy methods with complex ML CVs.", "motivation": "Free energy reconstruction methods like Gaussian Process Regression require analytic Jacobians of CVs, which bottlenecks the use of complex or machine-learned CVs. A learnable, differentiable CV representation can bypass this limitation.", "method": "Train a neural network to map Cartesian coordinates to CV values and use automatic differentiation to compute Jacobians. Validate on an MgCl2 ion-pairing system with a simple distance CV and a complex coordination-number CV, assessing CV accuracy and the distribution of Jacobian errors.", "result": "The method achieves high accuracy for both CVs; Jacobian errors follow a near-Gaussian distribution, making them suitable for integration into GPR pipelines. The framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs.", "conclusion": "This neural-network surrogate framework broadens the applicability of gradient-based free energy simulations to complex and machine-learned CVs in biochemistry and materials simulations by providing accurate CVs and differentiable Jacobians without analytic forms."}}
{"id": "2510.01620", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01620", "abs": "https://arxiv.org/abs/2510.01620", "authors": ["Peidong Liu", "Junjiang Lin", "Shaowen Wang", "Yao Xu", "Haiqing Li", "Xuhao Xie", "Siyi Wu", "Hao Li"], "title": "Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs", "comment": null, "summary": "Contextual Markov Decision Processes (CMDPs) offer a framework for sequential\ndecision-making under external signals, but existing methods often fail to\ngeneralize in high-dimensional or unstructured contexts, resulting in excessive\ncomputation and unstable performance. We propose an information-theoretic\nsummarization approach that uses large language models (LLMs) to compress\ncontextual inputs into low-dimensional, semantically rich summaries. These\nsummaries augment states by preserving decision-critical cues while reducing\nredundancy. Building on the notion of approximate context sufficiency, we\nprovide, to our knowledge, the first regret bounds and a latency-entropy\ntrade-off characterization for CMDPs. Our analysis clarifies how\ninformativeness impacts computational cost. Experiments across discrete,\ncontinuous, visual, and recommendation benchmarks show that our method\noutperforms raw-context and non-context baselines, improving reward, success\nrate, and sample efficiency, while reducing latency and memory usage. These\nfindings demonstrate that LLM-based summarization offers a scalable and\ninterpretable solution for efficient decision-making in context-rich,\nresource-constrained environments.", "AI": {"tldr": "LLM-based summarization of contextual inputs for CMDPs yields low-dimensional, semantically rich summaries that preserve decision-critical cues, enabling regret bounds, a latency-entropy trade-off, and practical gains in reward, success rate, and efficiency.", "motivation": "CMDPs struggle to generalize and scale in high-dimensional or unstructured contexts due to computation and instability; a principled, scalable representation of context is needed to maintain performance while reducing cost.", "method": "Compress contextual inputs with large language models into low-dimensional summaries that augment decision states (approximate context sufficiency). Derive regret bounds and a latency-entropy trade-off for CMDPs, connecting informativeness to computational cost. Validate across discrete, continuous, visual, and recommendation benchmarks.", "result": "Across domains, the approach outperforms raw-context and non-context baselines, improving reward, success rate, and sample efficiency, while reducing latency and memory usage.", "conclusion": "LLM-based contextual summarization offers a scalable, interpretable solution for decision-making in context-rich, resource-constrained CMDPs, balancing informativeness with computational efficiency."}}
{"id": "2510.01843", "categories": ["cs.RO", "I.2.9; I.2.8; G.1.6"], "pdf": "https://arxiv.org/pdf/2510.01843", "abs": "https://arxiv.org/abs/2510.01843", "authors": ["Wanyue Li", "Ji Ma", "Minghao Lu", "Peng Lu"], "title": "Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots", "comment": "8 pages, 8 figures, conference paper", "summary": "Humanoid robot soccer presents several challenges, particularly in\nmaintaining system stability during aggressive kicking motions while achieving\nprecise ball trajectory control. Current solutions, whether traditional\nposition-based control methods or reinforcement learning (RL) approaches,\nexhibit significant limitations. Model predictive control (MPC) is a prevalent\napproach for ordinary quadruped and biped robots. While MPC has demonstrated\nadvantages in legged robots, existing studies often oversimplify the leg swing\nprogress, relying merely on simple trajectory interpolation methods. This\nseverely constrains the foot's environmental interaction capability, hindering\ntasks such as ball kicking. This study innovatively adapts the spatial-temporal\ntrajectory planning method, which has been successful in drone applications, to\nbipedal robotic systems. The proposed approach autonomously generates foot\ntrajectories that satisfy constraints on target kicking position, velocity, and\nacceleration while simultaneously optimizing swing phase duration. Experimental\nresults demonstrate that the optimized trajectories closely mimic human kicking\nbehavior, featuring a backswing motion. Simulation and hardware experiments\nconfirm the algorithm's efficiency, with trajectory planning times under 1 ms,\nand its reliability, achieving nearly 100 % task completion accuracy when the\nsoccer goal is within the range of -90{\\deg} to 90{\\deg}.", "AI": {"tldr": "A drone-inspired spatial-temporal trajectory planner is adapted for a bipedal humanoid soccer robot to autonomously generate constrained foot trajectories for kicking, optimizing swing duration and achieving fast real-time planning.", "motivation": "Current kicking control for humanoid robots struggles with stability during aggressive kicks and precise ball trajectory control. Existing MPC and RL approaches are limited, and simple leg-swing interpolation constrains environmental interaction. There is a need for fast, constrained, real-time foot trajectory planning that can mimic human kicking dynamics.", "method": "Adapt spatial-temporal trajectory planning techniques from drone applications to a bipedal robot. The planner autonomously generates foot trajectories that meet target kicking position, velocity, and acceleration constraints while simultaneously optimizing the swing phase duration. Demonstrates planning times under 1 ms in simulation and hardware experiments.", "result": "The optimized trajectories closely resemble human kicking with a backswing. The approach yields high reliability, achieving nearly 100% task completion when the soccer goal is within a -90\u00b0 to 90\u00b0 range, validating both efficiency and robustness.", "conclusion": "Drone-inspired spatial-temporal planning effectively enhances kicking performance and environmental interaction for humanoid soccer, providing fast, constrained, real-time foot trajectory generation that complements or surpasses simple interpolation strategies."}}
{"id": "2510.01665", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01665", "abs": "https://arxiv.org/abs/2510.01665", "authors": ["Yongbo Chen", "Yanhao Zhang", "Shaifali Parashar", "Liang Zhao", "Shoudong Huang"], "title": "Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale", "comment": null, "summary": "Non-rigid structure-from-motion (NRSfM), a promising technique for addressing\nthe mapping challenges in monocular visual deformable simultaneous localization\nand mapping (SLAM), has attracted growing attention. We introduce a novel\nmethod, called Con-NRSfM, for NRSfM under conformal deformations, encompassing\nisometric deformations as a subset. Our approach performs point-wise\nreconstruction using 2D selected image warps optimized through a graph-based\nframework. Unlike existing methods that rely on strict assumptions, such as\nlocally planar surfaces or locally linear deformations, and fail to recover the\nconformal scale, our method eliminates these constraints and accurately\ncomputes the local conformal scale. Additionally, our framework decouples\nconstraints on depth and conformal scale, which are inseparable in other\napproaches, enabling more precise depth estimation. To address the sensitivity\nof the formulated problem, we employ a parallel separable iterative\noptimization strategy. Furthermore, a self-supervised learning framework,\nutilizing an encoder-decoder network, is incorporated to generate dense 3D\npoint clouds with texture. Simulation and experimental results using both\nsynthetic and real datasets demonstrate that our method surpasses existing\napproaches in terms of reconstruction accuracy and robustness. The code for the\nproposed method will be made publicly available on the project website:\nhttps://sites.google.com/view/con-nrsfm.", "AI": {"tldr": "Con-NRSfM introduces a non-rigid structure-from-motion framework for monocular deformable SLAM under conformal deformations, achieving local conformal scale recovery and decoupled depth-scale estimation via graph-based optimization and self-supervised learning, outperforming prior methods in accuracy and robustness.", "motivation": "Existing NRSfM approaches often rely on strict local assumptions (e.g., local planarity or locally linear deformations) and cannot recover the conformal scale. In monocular deformable SLAM, depth and scale are typically entangled, limiting accuracy.", "method": "Propose Con-NRSfM that performs point-wise reconstruction using 2D selected image warps optimized through a graph-based framework. It eliminates constraints on conformal scale and depth, decouples depth and conformal scale, uses parallel separable iterative optimization to address sensitivity, and integrates a self-supervised encoder-decoder to generate dense textured 3D point clouds.", "result": "Simulation and real-data experiments show superior reconstruction accuracy and robustness compared with existing NRSfM approaches, with dense textured 3D outputs; code will be publicly available.", "conclusion": "Con-NRSfM broadens NRSfM to general conformal deformations (including isometric) by recovering the local conformal scale and decoupling depth from scale, yielding more accurate 3D reconstructions in monocular deformable SLAM; implementation will be released publicly."}}
{"id": "2510.01407", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01407", "abs": "https://arxiv.org/abs/2510.01407", "authors": ["Ethan G. Rogers", "Cheng Wang"], "title": "Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction", "comment": "5 pages, 4 figures, NeurIPS 2025 Workshop MLForSys", "summary": "Image compression and reconstruction are crucial for various digital\napplications. While contemporary neural compression methods achieve impressive\ncompression rates, the adoption of such technology has been largely hindered by\nthe complexity and large computational costs of the convolution-based decoders\nduring data reconstruction. To address the decoder bottleneck in neural\ncompression, we develop a new compression-reconstruction framework based on\nincorporating low-rank representation in an autoencoder with vector\nquantization. We demonstrated that performing a series of computationally\nefficient low-rank operations on the learned latent representation of images\ncan efficiently reconstruct the data with high quality. Our approach\ndramatically reduces the computational overhead in the decoding phase of neural\ncompression/reconstruction, essentially eliminating the decoder compute\nbottleneck while maintaining high fidelity of image outputs.", "AI": {"tldr": "Low-rank latent autoencoder with vector quantization to cut decoder compute in neural image compression while preserving quality.", "motivation": "Decoder-based neural image compression relies on expensive convolutional decoders, causing high computational cost and latency; reducing decoding complexity is a key bottleneck for practical deployment.", "method": "Integrate low-rank representations into the autoencoder's latent space and apply a sequence of efficient low-rank operations during reconstruction, using vector quantization to discretize latent codes.", "result": "If validated, the approach dramatically reduces decoding overhead while maintaining high image fidelity, effectively alleviating the decoder bottleneck in neural compression.", "conclusion": "The proposed framework offers a scalable path to efficient neural image compression with low decoding cost and preserved quality, potentially broadening practical adoption."}}
{"id": "2510.01639", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01639", "abs": "https://arxiv.org/abs/2510.01639", "authors": ["Thinh Hung Truong", "Jey Han Lau", "Jianzhong Qi"], "title": "Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective", "comment": null, "summary": "We explore the geospatial reasoning capabilities of Large Language Models\n(LLMs), specifically, whether LLMs can read road network maps and perform\nnavigation. We frame trajectory recovery as a proxy task, which requires models\nto reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with\nover 4,000 real-world trajectories across diverse regions and transportation\nmodes. Using road network as context, our prompting framework enables LLMs to\ngenerate valid paths without accessing any external navigation tools.\nExperiments show that LLMs outperform off-the-shelf baselines and specialized\ntrajectory recovery models, with strong zero-shot generalization. Fine-grained\nanalysis shows that LLMs have strong comprehension of the road network and\ncoordinate systems, but also pose systematic biases with respect to regions and\ntransportation modes. Finally, we demonstrate how LLMs can enhance navigation\nexperiences by reasoning over maps in flexible ways to incorporate user\npreferences.", "AI": {"tldr": "LLMs can read road networks and perform navigation-like trajectory recovery without external tools, using GLOBALTRACE (~4k real-world trajectories) to outperform baselines and show strong zero-shot generalization, though they exhibit regional and mode biases; they can be used to enhance map-based navigation with flexible user preferences.", "motivation": "Assess whether LLMs can perform geospatial reasoning and map-based navigation by reading road networks, bridging natural language models with navigation tasks.", "method": "Introduce GLOBALTRACE, a dataset of over 4,000 real-world trajectories across diverse regions and transportation modes. Propose a prompting framework that uses the road network as context to guide LLMs in generating valid paths, without accessing external navigation tools. Evaluate on trajectory recovery (masked GPS traces) and analyze zero-shot generalization and biases.", "result": "LLMs outperform off-the-shelf baselines and specialized trajectory recovery models, with strong zero-shot generalization. Fine-grained analysis shows strong comprehension of road networks and coordinate systems but reveals systematic biases with respect to regions and transportation modes.", "conclusion": "LLMs can enhance navigation experiences by reasoning over maps and accommodating user preferences, though biases exist; suggests promising integration of map-based reasoning into navigation systems and directions for mitigating biases."}}
{"id": "2510.01848", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01848", "abs": "https://arxiv.org/abs/2510.01848", "authors": ["Diram Tabaa", "Gianni Di Caro"], "title": "GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics", "comment": null, "summary": "Simulating greenhouse environments is critical for developing and evaluating\nrobotic systems for agriculture, yet existing approaches rely on simplistic or\nsynthetic assets that limit simulation-to-real transfer. Recent advances in\nradiance field methods, such as Gaussian splatting, enable photorealistic\nreconstruction but have so far been restricted to individual plants or\ncontrolled laboratory conditions. In this work, we introduce GreenhouseSplat, a\nframework and dataset for generating photorealistic greenhouse assets directly\nfrom inexpensive RGB images. The resulting assets are integrated into a\nROS-based simulation with support for camera and LiDAR rendering, enabling\ntasks such as localization with fiducial markers. We provide a dataset of 82\ncucumber plants across multiple row configurations and demonstrate its utility\nfor robotics evaluation. GreenhouseSplat represents the first step toward\ngreenhouse-scale radiance-field simulation and offers a foundation for future\nresearch in agricultural robotics.", "AI": {"tldr": "GreenhouseSplat presents a pipeline and dataset for photorealistic greenhouse assets using radiance-field methods (Gaussian splatting) built from cheap RGB images, integrated into a ROS-based simulator with camera and LiDAR, and demonstrated on 82 cucumber plants to support robotics evaluation, marking a step toward greenhouse-scale radiance-field simulation.", "motivation": "Accurate, photorealistic greenhouse simulators are needed to bridge sim-to-real gaps in agricultural robotics. Existing assets are simplistic and limited in scale; radiance-field approaches show promise but have been confined to single plants or controlled conditions. A greenhouse-scale, photorealistic pipeline would enable robust evaluation and development.", "method": "Capture inexpensive RGB images of cucumber plants in greenhouse rows; reconstruct radiance-field assets using Gaussian splatting; assemble assets into a ROS-based simulation environment with camera and LiDAR rendering; create a dataset of 82 plants across multiple row configurations; demonstrate tasks such as localization with fiducial markers.", "result": "Photorealistic greenhouse assets and a ROS-integrated simulator derived from inexpensive RGB imagery; a dataset of 82 cucumber plants across several row configurations; demonstration of localization and robotics evaluation tasks in the simulated greenhouse.", "conclusion": "GreenhouseSplat is the first step toward scalable, radiance-field-based greenhouse simulation, providing a foundation for future research in agricultural robotics and improving sim-to-real transfer."}}
{"id": "2510.01669", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01669", "abs": "https://arxiv.org/abs/2510.01669", "authors": ["Jin Cao", "Hongrui Wu", "Ziyong Feng", "Hujun Bao", "Xiaowei Zhou", "Sida Peng"], "title": "UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction", "comment": "page: https://jin-cao-tma.github.io/UniVerse.github.io/ code:\n  https://github.com/zju3dv/UniVerse", "summary": "This paper tackles the challenge of robust reconstruction, i.e., the task of\nreconstructing a 3D scene from a set of inconsistent multi-view images. Some\nrecent works have attempted to simultaneously remove image inconsistencies and\nperform reconstruction by integrating image degradation modeling into neural 3D\nscene representations.However, these methods rely heavily on dense observations\nfor robustly optimizing model parameters.To address this issue, we propose to\ndecouple robust reconstruction into two subtasks: restoration and\nreconstruction, which naturally simplifies the optimization process.To this\nend, we introduce UniVerse, a unified framework for robust reconstruction based\non a video diffusion model. Specifically, UniVerse first converts inconsistent\nimages into initial videos, then uses a specially designed video diffusion\nmodel to restore them into consistent images, and finally reconstructs the 3D\nscenes from these restored images.Compared with case-by-case per-view\ndegradation modeling, the diffusion model learns a general scene prior from\nlarge-scale data, making it applicable to diverse image\ninconsistencies.Extensive experiments on both synthetic and real-world datasets\ndemonstrate the strong generalization capability and superior performance of\nour method in robust reconstruction. Moreover, UniVerse can control the style\nof the reconstructed 3D scene. Project page:\nhttps://jin-cao-tma.github.io/UniVerse.github.io/", "AI": {"tldr": "A decoupled two-stage pipeline using a video diffusion model (UniVerse) to restore inconsistent multi-view images, then reconstruct 3D scenes, achieving strong generalization and style-controllable results.", "motivation": "Current robust 3D reconstruction methods rely on dense multi-view observations and per-view degradation modeling, making robustness to diverse inconsistencies difficult. A decoupled approach leverages global scene priors learned from large-scale data to handle varied degradations with less dependence on view density.", "method": "Introduce UniVerse, a unified framework based on a video diffusion model. Steps: 1) convert inconsistent images into initial videos; 2) restore them into consistent images using a specially designed video diffusion model; 3) reconstruct 3D scenes from the restored images. The diffusion model captures a general scene prior and enables style control over the reconstructed 3D scene.", "result": "Extensive experiments on synthetic and real-world datasets demonstrate strong generalization and superior performance in robust reconstruction, validating the effectiveness of diffusion-based restoration for handling diverse image inconsistencies.", "conclusion": "Diffusion-based restoration enables generalizable robust 3D reconstruction and simplifies optimization by decoupling restoration from reconstruction, with added capability to control the style of the reconstructed scene."}}
{"id": "2510.01439", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01439", "abs": "https://arxiv.org/abs/2510.01439", "authors": ["Mohamad Abou Ali", "Fadi Dornaika"], "title": "Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons", "comment": null, "summary": "Edge Artificial Intelligence (Edge AI) embeds intelligence directly into\ndevices at the network edge, enabling real-time processing with improved\nprivacy and reduced latency by processing data close to its source. This review\nsystematically examines the evolution, current landscape, and future directions\nof Edge AI through a multi-dimensional taxonomy including deployment location,\nprocessing capabilities such as TinyML and federated learning, application\ndomains, and hardware types. Following PRISMA guidelines, the analysis traces\nthe field from early content delivery networks and fog computing to modern\non-device intelligence. Core enabling technologies such as specialized hardware\naccelerators, optimized software, and communication protocols are explored.\nChallenges including resource limitations, security, model management, power\nconsumption, and connectivity are critically assessed. Emerging opportunities\nin neuromorphic hardware, continual learning algorithms, edge-cloud\ncollaboration, and trustworthiness integration are highlighted, providing a\ncomprehensive framework for researchers and practitioners.", "AI": {"tldr": "A comprehensive, taxonomy-driven review of Edge AI's evolution, current landscape, and future directions.", "motivation": "To synthesize Edge AI research to guide researchers and practitioners by clarifying deployment locations, processing capabilities (e.g., TinyML, federated learning), application domains, and hardware, while addressing challenges and outlining future opportunities.", "method": "Systematic literature review guided by PRISMA, developing a multi-dimensional taxonomy (deployment, processing capabilities, domains, hardware), tracing the field from content delivery networks and fog computing to on-device intelligence, and analyzing enabling technologies (hardware accelerators, software optimizations, communication protocols) and challenges.", "result": "Proposes a comprehensive framework for Edge AI incorporating evolution, landscape, and future directions; highlights enabling technologies; enumerates challenges (resource constraints, security, model management, power, connectivity) and opportunities (neuromorphic hardware, continual learning, edge-cloud collaboration, trustworthiness).", "conclusion": "Edge AI is set for sustained growth with advances in specialized hardware, robust learning algorithms, secure edge-cloud ecosystems, and trust-centric AI, with the framework serving as guidance for researchers and practitioners."}}
{"id": "2510.01664", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01664", "abs": "https://arxiv.org/abs/2510.01664", "authors": ["Yejin Kim", "Youngbin Lee", "Juhyeong Kim", "Yongjae Lee"], "title": "GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents", "comment": "7 Pages, 2 figures", "summary": "This study demonstrates that GuruAgents, prompt-guided AI agents, can\nsystematically operationalize the strategies of legendary investment gurus. We\ndevelop five distinct GuruAgents, each designed to emulate an iconic investor,\nby encoding their distinct philosophies into LLM prompts that integrate\nfinancial tools and a deterministic reasoning pipeline. In a backtest on\nNASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique\nbehaviors driven by their prompted personas. The Buffett GuruAgent achieves the\nhighest performance, delivering a 42.2\\% CAGR that significantly outperforms\nbenchmarks, while other agents show varied results. These findings confirm that\nprompt engineering can successfully translate the qualitative philosophies of\ninvestment gurus into reproducible, quantitative strategies, highlighting a\nnovel direction for automated systematic investing. The source code and data\nare available at https://github.com/yejining99/GuruAgents.", "AI": {"tldr": "GuruAgents show that encoding legendary investors' philosophies into LLM prompts can yield reproducible, guru-driven trading strategies; notably Buffett GuruAgent reaches 42.2% CAGR in NASDAQ-100 backtests.", "motivation": "To test whether qualitative investing wisdom can be translated into quantitative, backtestable strategies via prompt engineering and deterministic reasoning, enabling automated systematic investing.", "method": "Create five GuruAgents, each mimicking a famous investor through tailored prompts; integrate financial tools with a deterministic reasoning pipeline; backtest on NASDAQ-100 constituents from Q4 2023 to Q2 2025.", "result": "Buffett GuruAgent achieves the highest performance with a 42.2% CAGR, substantially outperforming benchmarks; other agents yield varied results, indicating persona-dependent behavior; overall evidence that prompt engineering can convert qualitative philosophies into quantitative strategies.", "conclusion": "Prompt-driven emulation of investment gurus can produce reproducible, quantitative strategies, suggesting a novel direction for automated systematic investing; source code and data are publicly available."}}
{"id": "2510.01869", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.01869", "abs": "https://arxiv.org/abs/2510.01869", "authors": ["Alessandro Nazzari", "Roberto Rubinacci", "Marco Lovera"], "title": "TACOS: Task Agnostic COordinator of a multi-drone System", "comment": "6 pages, 6 figures, accepted as poster at 2025 IEEE International\n  Symposium on Multi-Robot & Multi-Agent Systems", "summary": "When a single pilot is responsible for managing a multi-drone system, the\ntask demands varying levels of autonomy, from direct control of individual\nUAVs, to group-level coordination, to fully autonomous swarm behaviors for\naccomplishing high-level tasks. Enabling such flexible interaction requires a\nframework that supports multiple modes of shared autonomy. As language models\ncontinue to improve in reasoning and planning, they provide a natural\nfoundation for such systems, reducing pilot workload by enabling high-level\ntask delegation through intuitive, language-based interfaces. In this paper we\npresent TACOS (Task-Agnostic COordinator of a multi-drone System), a unified\nframework that enables high-level natural language control of multi-UAV systems\nthrough Large Language Models (LLMs). TACOS integrates three key capabilities\ninto a single architecture: a one-to-many natural language interface for\nintuitive user interaction, an intelligent coordinator for translating user\nintent into structured task plans, and an autonomous agent that executes plans\ninteracting with the real-world. TACOS allows a LLM to interact with a library\nof executable APIs, bridging semantic reasoning with real-time multi-robot\ncoordination. We demonstrate the system in real-world multi-drone system and\nconduct an ablation study to assess the contribution of each module.", "AI": {"tldr": "A language-model\u2013driven framework (TACOS) enables flexible, high-level control of multi-UAV systems via a unified three-component architecture (NL interface, coordinator, and execution agent) and is validated on real drones with an ablation study.", "motivation": "To support varying levels of autonomy in multi-drone systems and reduce pilot workload by letting a single pilot delegate tasks through natural language, bridging semantic reasoning with real-time coordination.", "method": "A three-part architecture: (1) a one-to-many natural language interface for user interaction; (2) an intelligent coordinator that converts user intent into structured task plans; (3) an autonomous agent that executes the plans by interfacing with a library of executable APIs. The system is demonstrated on real-world multi-drone scenarios and evaluated via ablation studies.", "result": "Real-world demonstration of TACOS with ablation study showing the contribution of each module and validating the feasibility of language-guided multi-UAV coordination.", "conclusion": "TACOS offers a practical, scalable framework for language-driven multi-UAV control, highlighting the role of LLMs in bridging human intent and autonomous robot actions; future work could address latency, safety, robustness, and broader API integration."}}
{"id": "2510.01678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01678", "abs": "https://arxiv.org/abs/2510.01678", "authors": ["Ke Jia", "Ji Zhou", "Hanxin Li", "Zhigan Zhou", "Haojie Chu", "Xiaojie Li"], "title": "An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution", "comment": "Published in Expert Systems with Applications", "summary": "In industrial inspection and component alignment tasks, template matching\nrequires efficient estimation of a target's position and geometric state\n(rotation and scaling) under complex backgrounds to support precise downstream\noperations. Traditional methods rely on exhaustive enumeration of angles and\nscales, leading to low efficiency under compound transformations. Meanwhile,\nmost deep learning-based approaches only estimate similarity scores without\nexplicitly modeling geometric pose, making them inadequate for real-world\ndeployment. To overcome these limitations, we propose a lightweight end-to-end\nframework that reformulates template matching as joint localization and\ngeometric regression, outputting the center coordinates, rotation angle, and\nindependent horizontal and vertical scales. A Template-Aware Dynamic\nConvolution Module (TDCM) dynamically injects template features at inference to\nguide generalizable matching. The compact network integrates depthwise\nseparable convolutions and pixel shuffle for efficient matching. To enable\ngeometric-annotation-free training, we introduce a rotation-shear-based\naugmentation strategy with structure-aware pseudo labels. A lightweight\nrefinement module further improves angle and scale precision via local\noptimization. Experiments show our 3.07M model achieves high precision and 14ms\ninference under compound transformations. It also demonstrates strong\nrobustness in small-template and multi-object scenarios, making it highly\nsuitable for deployment in real-time industrial applications. The code is\navailable at:https://github.com/ZhouJ6610/PoseMatch-TDCM.", "AI": {"tldr": "A lightweight end-to-end template matching framework that jointly localizes and regresses geometric pose (center, rotation, horizontal and vertical scales) using a Template-Aware Dynamic Convolution Module (TDCM) and rotation-shear augmentation, achieving real-time performance and robustness to small templates and multi-object scenes.", "motivation": "Industrial inspection demands efficient, explicit pose estimation in template matching under complex backgrounds. Traditional exhaustive angle/scale search is inefficient, and many deep models only output similarity scores without explicit geometry, limiting real-world deployment.", "method": "Formulates template matching as joint localization and geometric regression. Introduces Template-Aware Dynamic Convolution Module (TDCM) that injects template features at inference, uses depthwise separable convolutions and pixel shuffle for efficiency, and employs a rotation-shear-based augmentation strategy with structure-aware pseudo labels to enable geometric-annotation-free training. Adds a lightweight refinement module for local optimization of angle and scale.", "result": "The model, with 3.07M parameters, achieves high precision and 14 ms inference under compound transformations. Demonstrates robustness in small-template and multi-object scenarios, indicating suitability for real-time industrial applications.", "conclusion": "The approach delivers a geometry-aware, efficient template-matching solution that combines joint pose regression with dynamic template-guided matching, enabling real-time deployment and improved robustness in challenging scenes; code is available at the authors\u2019 GitHub."}}
{"id": "2510.01447", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01447", "abs": "https://arxiv.org/abs/2510.01447", "authors": ["Dorsa Soleymani", "Ali Dadsetan", "Frank Rudzicz"], "title": "SoftAdaClip: A Smooth Clipping Strategy for Fair and Private Model Training", "comment": null, "summary": "Differential privacy (DP) provides strong protection for sensitive data, but\noften reduces model performance and fairness, especially for underrepresented\ngroups. One major reason is gradient clipping in DP-SGD, which can\ndisproportionately suppress learning signals for minority subpopulations.\nAlthough adaptive clipping can enhance utility, it still relies on uniform hard\nclipping, which may restrict fairness. To address this, we introduce\nSoftAdaClip, a differentially private training method that replaces hard\nclipping with a smooth, tanh-based transformation to preserve relative gradient\nmagnitudes while bounding sensitivity. We evaluate SoftAdaClip on various\ndatasets, including MIMIC-III (clinical text), GOSSIS-eICU (structured\nhealthcare), and Adult Income (tabular data). Our results show that SoftAdaClip\nreduces subgroup disparities by up to 87% compared to DP-SGD and up to 48%\ncompared to Adaptive-DPSGD, and these reductions in subgroup disparities are\nstatistically significant. These findings underscore the importance of\nintegrating smooth transformations with adaptive mechanisms to achieve fair and\nprivate model training.", "AI": {"tldr": "SoftAdaClip replaces hard gradient clipping in DP-SGD with a smooth tanh transformation, yielding better fairness and utility under differential privacy, outperforming DP-SGD and Adaptive-DPSGD on healthcare and tabular datasets with significant reductions in subgroup disparities.", "motivation": "Hard clipping in DP-SGD can unevenly damp learning signals for minority subpopulations, harming fairness; existing adaptive clipping helps but remains uniform and hard, potentially limiting subgroup fairness. A smooth, adaptive approach may preserve relative gradient magnitudes and improve fairness under DP.", "method": "Introduce SoftAdaClip: a DP training method that uses a tanh-based smooth transformation to bound sensitivity instead of hard clipping, paired with adaptive mechanisms to adjust clipping behavior; evaluate on MIMIC-III, GOSSIS-eICU, and Adult Income.", "result": "SoftAdaClip reduces subgroup disparities by up to 87% vs DP-SGD and up to 48% vs Adaptive-DPSGD; improvements are statistically significant across the tested datasets.", "conclusion": "Integrating smooth gradient transformations with adaptive clipping can achieve fairer and privately trained models, suggesting that smooth clipping strategies can complement DP mechanisms to improve utility and fairness."}}
{"id": "2510.01670", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01670", "abs": "https://arxiv.org/abs/2510.01670", "authors": ["Erfan Shayegani", "Keegan Hines", "Yue Dong", "Nael Abu-Ghazaleh", "Roman Lutz", "Spencer Whitehead", "Vidhisha Balachandran", "Besmira Nushi", "Vibhav Vineet"], "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness", "comment": null, "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that\ntake actions on GUIs to accomplish user goals. In this paper, we show that CUAs\nconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals\nregardless of feasibility, safety, reliability, or context. We characterize\nthree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)\nassumptions and decisions under ambiguity, and (iii) contradictory or\ninfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these\nthree patterns. Built on OSWorld, BLIND-ACT provides realistic environments and\nemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreement\nwith human annotations. We use BLIND-ACT to evaluate nine frontier models,\nincluding Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing\nhigh average BGD rates (80.8%) across them. We show that BGD exposes subtle\nrisks that arise even when inputs are not directly harmful. While\nprompting-based interventions lower BGD levels, substantial risk persists,\nhighlighting the need for stronger training- or inference-time interventions.\nQualitative analysis reveals observed failure modes: execution-first bias\n(focusing on how to act over whether to act), thought-action disconnect\n(execution diverging from reasoning), and request-primacy (justifying actions\ndue to user request). Identifying BGD and introducing BLIND-ACT establishes a\nfoundation for future research on studying and mitigating this fundamental risk\nand ensuring safe CUA deployment.", "AI": {"tldr": "Introduces BLIND-ACT to study Blind Goal-Directedness (BGD) in Computer-Use Agents (CUAs); shows BGD is pervasive across frontier models, with high agreement (93.75%) between LLM judges and humans; prompting reduces but does not eliminate BGD; identifies failure modes and provides a benchmark for mitigations.", "motivation": "Address the pervasive bias of CUAs toward pursuing goals regardless of feasibility, safety, or context. Provide a realistic benchmark and evaluation framework to study BGD and inform mitigation strategies for safe CUA deployment.", "method": "Build BLIND-ACT benchmark comprising 90 tasks within OSWorld environments to capture three BGD patterns. Use LLM-based judges to assess agent behavior and validate judgments against human annotations (93.75% agreement). Evaluate nine frontier models (e.g., Claude Sonnet, Opus 4, GPT-5 variants) and analyze BGD rates (average ~80.8%). Conduct qualitative analysis to identify failure modes and test prompting-based interventions.", "result": "BLIND-ACT enables consistent measurement of BGD across models, revealing high prevalence of BGD (80.8% on average). LLM judges align closely with human judgments (93.75%). Prompting reduces BGD but does not eliminate it, indicating need for stronger training or inference-time interventions. Qualitative analysis uncovers execution-first bias, thought-action disconnect, and request-primacy as core failure modes.", "conclusion": "Establishes a foundation for future research on diagnosing and mitigating BGD in CUAs, emphasizing the necessity of stronger safeguards during training and at inference to ensure safe CUA deployment."}}
{"id": "2510.01984", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01984", "abs": "https://arxiv.org/abs/2510.01984", "authors": ["Yue Wang"], "title": "SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot", "comment": null, "summary": "We present SPARC, a compact, open-source 3-DoF sagittal-plane spine module\nthat combines revolute (pitch) and prismatic (axial) motion with programmable\ntask-space impedance for quadruped robots. The system integrates three\ntorque-controlled actuators, a custom 1 kHz control board, and a protected\npower unit in a 1.26 kg package, enabling closed-loop stiffness and damping\nshaping along x, z, and theta. We develop an RNEA-based computed-acceleration\ncontroller with smooth Stribeck friction compensation to render spring-damper\nbehavior without explicit inertia shaping. Bench experiments validate the\napproach. Quasi-static push-pull tests show linear force-displacement\ncharacteristics with commanded horizontal stiffness spanning 300-700 N/m and <=\n1.5% relative error (R^2 >= 0.992, narrow 95% CIs). Dynamic\ndisplace-and-release trials confirm mass-spring-damper responses over multiple\ndamping settings, with small, interpretable phase deviations due to\nconfiguration-dependent inertia and low-speed friction effects. A task-space PD\ncontroller produces roughly linear stiffness but with greater variability and\ncoupling sensitivity. SPARC provides a portable platform for systematic studies\nof spine compliance in legged locomotion and will be released with complete\nhardware and firmware resources.", "AI": {"tldr": "A compact, open-source 3-DoF spine module (SPARC) for quadrupeds enabling programmable task-space impedance; validated with RNEA-based acceleration control and friction compensation; demonstrates linear stiffness 300\u2013700 N/m with <1.5% error; suitable for spine compliance studies; hardware/firmware to be released.", "motivation": "To enable systematic exploration of spine compliance in legged locomotion by providing a portable, torque-controlled spine module with tunable impedance.", "method": "Hardware: 3 torque-controlled actuators, 1 kHz control board, protected power unit in 1.26 kg package; control algorithm: RNEA-based computed-acceleration controller with smooth Stribeck friction compensation to render spring-damper behavior without explicit inertia shaping; bench tests include quasi-static push-pull (stiffness 300\u2013700 N/m, error \u22641.5%, R^2 \u2265 0.992), dynamic displace-and-release showing mass-spring-damper responses across damping settings; task-space PD controller shows roughly linear stiffness with more variability and coupling; release with complete hardware/firmware resources.", "result": "Achieved linear force-displacement characteristics with commanded horizontal stiffness spanning 300\u2013700 N/m and relative error \u22641.5% (R^2 \u2265 0.992); dynamic tests confirm mass-spring-damper behavior with interpretable phase deviations due to inertia and low-speed friction; PD controller offers linear stiffness but with variability and coupling sensitivity.", "conclusion": "SPARC provides a portable, open-source platform for systematic studies of spine compliance in legged locomotion and will be released with complete hardware and firmware resources."}}
{"id": "2510.01681", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01681", "abs": "https://arxiv.org/abs/2510.01681", "authors": ["Xuchen Li", "Xuzhao Li", "Jiahui Gao", "Renjie Pi", "Shiyu Hu", "Wentao Zhang"], "title": "Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning", "comment": "Preprint, Under review", "summary": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet they\nfrequently struggle with tasks requiring precise understanding and handling of\nfine-grained visual elements. This is mainly due to information loss during\nimage encoding or insufficient attention to critical regions. Recent work has\nshown promise by incorporating pixel-level visual information into the\nreasoning process, enabling VLMs to access high-resolution visual details\nduring their thought process. However, this pixel-level information is often\noverused, leading to inefficiency and distraction from irrelevant visual\ndetails. To address these challenges, we propose the first framework for\nadaptive pixel reasoning that dynamically determines necessary pixel-level\noperations based on the input query. Specifically, we first apply\noperation-aware supervised fine-tuning to establish baseline competence in\ntextual reasoning and visual operations, then design a novel rollout-guided\nreinforcement learning framework relying on feedback of the model's own\nresponses, which enables the VLM to determine when pixel operations should be\ninvoked based on query difficulty. Experiments on extensive multimodal\nreasoning benchmarks show that our model achieves superior performance while\nsignificantly reducing unnecessary visual operations. Impressively, our model\nachieves 73.4\\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of\nonly 20.1\\%, improving accuracy and simultaneously reducing tool usage by\n66.5\\% compared to the previous methods.", "AI": {"tldr": "Introduces adaptive pixel reasoning for Vision-Language Models that selectively uses pixel-level information via operation-aware fine-tuning and rollout-guided reinforcement learning, achieving higher accuracy with reduced pixel operations.", "motivation": "VLMs often lose fine-grained visual information due to encoding constraints or imprecise attention. While pixel-level data can improve reasoning, overuse leads to inefficiency and distraction from irrelevant details.", "method": "1) Operation-aware supervised fine-tuning to establish baseline textual reasoning and visual operations. 2) Rollout-guided reinforcement learning using feedback from the model's own responses to decide when to invoke pixel operations based on query difficulty.", "result": "On extensive multimodal reasoning benchmarks, the approach yields superior accuracy. It achieves 73.4% accuracy on HR-Bench 4K with a tool usage ratio of 20.1%, reducing tool usage by 66.5% compared to prior methods.", "conclusion": "Adaptive pixel reasoning enables dynamic, query-driven use of pixel-level information, improving accuracy while significantly reducing unnecessary pixel operations."}}
{"id": "2510.01450", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01450", "abs": "https://arxiv.org/abs/2510.01450", "authors": ["Yifei Zuo", "Yutong Yin", "Zhichen Zeng", "Ang Li", "Banghua Zhu", "Zhaoran Wang"], "title": "Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression", "comment": null, "summary": "Transformer architectures have achieved remarkable success in various\ndomains. While efficient alternatives to Softmax Attention have been widely\nstudied, the search for more expressive mechanisms grounded in theoretical\ninsight-even at greater computational cost-has been relatively underexplored.\nIn this work, we bridge this gap by proposing Local Linear Attention (LLA), a\nnovel attention mechanism derived from nonparametric statistics through the\nlens of test-time regression. First, we show that LLA offers theoretical\nadvantages over Linear and Softmax Attention for associative memory via a\nbias-variance trade-off analysis. Next, we address its computational challenges\nand propose two memory-efficient primitives to tackle the $\\Theta(n^2 d)$ and\n$\\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient,\nblockwise algorithm that enables scalable and parallel computation on modern\naccelerators. In addition, we implement and profile a customized inference\nkernel that significantly reduces memory overheads. Finally, we empirically\nvalidate the advantages and limitations of LLA on test-time regression,\nin-context regression, associative recall and state tracking tasks. Experiment\nresults demonstrate that LLA effectively adapts to non-stationarity,\noutperforming strong baselines in test-time training and in-context learning,\nand exhibiting promising evidence for its scalability and applicability in\nlarge-scale models. Code is available at\nhttps://github.com/Yifei-Zuo/Flash-LLA.", "AI": {"tldr": "Proposes Local Linear Attention (LLA), derived from test-time regression; analyzes bias-variance advantages over Softmax/Linear attention; introduces memory-efficient primitives and FlashLLA for hardware acceleration; empirically validated on test-time regression, in-context regression, associative recall, and state tracking.", "motivation": "Existing efficient attention methods (Softmax, linear) trade expressivity for speed. There is a theoretical gap for expressive, test-time adaptable attention with nonparametric/statistical grounding. The work seeks a principled, scalable mechanism that handles non-stationarity and is hardware-friendly.", "method": "Derive LLA from nonparametric statistics via test-time regression; perform a bias-variance trade-off analysis versus Softmax and Linear attention for associative memory; develop two memory-efficient primitives to reduce \u0398(n^2 d) and \u0398(n d^2) costs; introduce FlashLLA, a blockwise hardware-friendly algorithm for parallel accelerators; implement a customized inference kernel to reduce memory overhead; empirically evaluate on test-time regression, in-context regression, associative recall, and state tracking.", "result": "LLA adapts to non-stationarity and outperforms strong baselines in test-time training and in-context learning; provides evidence for scalability and applicability in large-scale models.", "conclusion": "LLA offers theoretical advantages and practical pathways for more expressive, scalable attention. The approach shows potential for broad impact and motivates further exploration into test-time regression-based attention and hardware-efficient implementations."}}
{"id": "2510.01671", "categories": ["cs.AI", "cs.HC", "68T01", "J.3"], "pdf": "https://arxiv.org/pdf/2510.01671", "abs": "https://arxiv.org/abs/2510.01671", "authors": ["Motoki Sato", "Yuki Matsushita", "Hidekazu Takahashi", "Tomoaki Kakazu", "Sou Nagata", "Mizuho Ohnuma", "Atsushi Yoshikawa", "Masayuki Yamamura"], "title": "A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation", "comment": "32 pages, 4 figures, 10 tables 32 pages, 4 figures, 10 tables. This\n  paper is currently under review at ACM Transactions on Computing for\n  Healthcare. Reproducibility resources:\n  http://github.com/motokinaru/LENOHA-medical-dialogue", "summary": "Patients awaiting invasive procedures often have unanswered pre-procedural\nquestions; however, time-pressured workflows and privacy constraints limit\npersonalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave\nNo One Behind Architecture), a safety-first, local-first system that routes\ninputs with a high-precision sentence-transformer classifier and returns\nverbatim answers from a clinician-curated FAQ for clinical queries, eliminating\nfree-text generation in the clinical path. We evaluated two domains (tooth\nextraction and gastroscopy) using expert-reviewed validation sets\n(n=400/domain) for thresholding and independent test sets (n=200/domain). Among\nthe four encoders, E5-large-instruct (560M) achieved an overall accuracy of\n0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were\nstatistically indistinguishable from GPT-4o on this task; Gemini made no errors\non this test set. Energy logging shows that the non-generative clinical path\nconsumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local\n8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single\non-prem GPU. These results indicate that near-frontier discrimination and\ngeneration-induced errors are structurally avoided in the clinical path by\nreturning vetted FAQ answers verbatim, supporting privacy, sustainability, and\nequitable deployment in bandwidth-limited environments.", "AI": {"tldr": "A safety-first, local, non-generative clinical QA pipeline (LENOHA) routes patient queries to clinician-curated FAQs and returns verbatim answers, achieving near-GPT-4o performance while dramatically reducing energy use and latency on on-prem hardware.", "motivation": "Patients awaiting invasive procedures often have unanswered questions, but time pressure and privacy concerns limit personalized counseling. A safe, local-first system that avoids generation and preserves privacy is needed.", "method": "Two-domain evaluation (tooth extraction, gastroscopy). A high-precision sentence-transformer classifier routes inputs to a clinician-curated FAQ, which provides verbatim answers (no text generation). Compared four encoders across expert-validated thresholding sets (n=400/domain) and independent test sets (n=200/domain). Energetic and latency metrics measured on-prem.", "result": "E5-large-instruct (560M) achieved overall accuracy 0.983 (CI 0.964\u20130.991), AUC 0.996, seven errors; GPT-4o performance not statistically different; Gemini had zero errors on the test set. Non-generative path uses ~1.0 mWh per input vs ~168 mWh per small-talk reply from an 8B SLM (\u2248170\u00d7 savings) with ~0.10 s latency on a single on\u2011prem GPU.", "conclusion": "Verbatim, clinician-curated FAQ retrieval avoids generation-induced errors while enabling privacy, sustainability, and equitable deployment in bandwidth-limited settings; the approach supports near-frontier discrimination without generation, suitable for clinical paths."}}
{"id": "2510.01986", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY", "math.OC", "q-bio.NC", "I.6"], "pdf": "https://arxiv.org/pdf/2510.01986", "abs": "https://arxiv.org/abs/2510.01986", "authors": ["Varun Kotian", "Vishrut Jain", "Andrea Michelle Rios Lazcano", "Daan Marinus Pool", "Riender Happee", "Barys Shyrokau"], "title": "Reducing Discomfort in Driving Simulators: Motion Cueing for Motion Sickness Mitigation", "comment": null, "summary": "Driving simulators are increasingly used in research and development.\nHowever, simulators often cause motion sickness due to downscaled motion and\nunscaled veridical visuals. In this paper, a motion cueing algorithm is\nproposed that reduces motion sickness as predicted by the subjective vertical\nconflict (SVC) model using model predictive control (MPC). Both sensory\nconflict and specific force errors are penalised in the cost function, allowing\nthe algorithm to jointly optimise fidelity and comfort.\n  Human-in-the-loop experiments were conducted to compare four simulator motion\nsettings: two variations of our MPC-based algorithm, one focused on pure\nspecific force tracking and the second compromising specific force tracking and\nmotion sickness minimisation, as well as reference adaptive washout and no\nmotion cases. The experiments were performed on a hexapod driving simulator\nwith participants exposed to passive driving.\n  Experimental motion sickness results closely matched the sickness model\npredictions. As predicted by the model, the no motion condition yielded the\nlowest sickness levels. However, it was rated lowest in terms of fidelity. The\ncompromise solution reduced sickness by over 50% (average MISC level 3 to 1.5)\ncompared to adaptive washout and the algorithm focusing on specific force\ntracking, without any significant reduction in fidelity rating.\n  The proposed approach for developing MCA that takes into account both the\nsimulator dynamics and time evolution of motion sickness offers a significant\nadvancement in achieving an optimal control of motion sickness and specific\nforce recreation in driving simulators, supporting broader simulator use.", "AI": {"tldr": "MPC-based motion cueing reduces simulator-induced motion sickness by jointly optimizing specific force fidelity and sensory conflict, validated in human-in-the-loop hexapod driving experiments; sickness drops by ~50% without harming fidelity.", "motivation": "To mitigate motion sickness in driving simulators caused by mismatches between downscaled motion and veridical visuals, enabling broader simulator use.", "method": "Develop an MPC-based motion cueing algorithm that penalizes sensory conflict and specific force errors, jointly optimizing fidelity and comfort. Compare two MPC variants and baselines (adaptive washout, no motion) in a hexapod driving simulator with passive driving; evaluate sickness and fidelity.", "result": "Experimental sickness aligned with the SVC model predictions. No-motion yields lowest sickness but lowest fidelity. The compromise MPC reduces sickness by over 50% relative to adaptive washout and the force-tracking variant, with no significant fidelity loss.", "conclusion": "The proposed motion cueing approach advances the joint control of motion sickness and motion recreation, enabling broader and more comfortable use of driving simulators."}}
{"id": "2510.01683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01683", "abs": "https://arxiv.org/abs/2510.01683", "authors": ["Han-Jay Shu", "Wei-Ning Chiu", "Shun-Ting Chang", "Meng-Ping Huang", "Takeshi Tohyama", "Ahram Han", "Po-Chih Kuo"], "title": "Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring", "comment": "5 pages, 1 figures", "summary": "Deep learning models achieve strong performance in chest radiograph (CXR)\ninterpretation, yet fairness and reliability concerns persist. Models often\nshow uneven accuracy across patient subgroups, leading to hidden failures not\nreflected in aggregate metrics. Existing error detection approaches -- based on\nconfidence calibration or out-of-distribution (OOD) detection -- struggle with\nsubtle within-distribution errors, while image- and representation-level\nconsistency-based methods remain underexplored in medical imaging. We propose\nan augmentation-sensitivity risk scoring (ASRS) framework to identify\nerror-prone CXR cases. ASRS applies clinically plausible rotations ($\\pm\n15^\\circ$/$\\pm 30^\\circ$) and measures embedding shifts with the RAD-DINO\nencoder. Sensitivity scores stratify samples into stability quartiles, where\nhighly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)\ndespite high AUROC and confidence. ASRS provides a label-free means for\nselective prediction and clinician review, improving fairness and safety in\nmedical AI.", "AI": {"tldr": "ASRS offers an augmentation-sensitivity risk score for chest X-ray models by applying clinically plausible rotations and measuring embedding shifts with the RAD-DINO encoder to flag error-prone cases for selective review, improving fairness and safety.", "motivation": "To address reliability gaps in medical imaging AI where subgroup performance varies and hidden within-distribution errors are not captured by aggregate metrics; existing confidence calibration and OOD methods miss subtle errors, and consistency-based approaches remain underexplored in radiology.", "method": "Apply rotations of \u00b115\u00b0 and \u00b130\u00b0 to CXR inputs; compute embedding shifts using the RAD-DINO encoder; derive sensitivity scores and stratify samples into stability quartiles; identify high-sensitivity cases that exhibit low recall despite high AUROC and confidence; use scores for label-free selective prediction and clinician review.", "result": "High-sensitivity quartiles show substantial recall drop from model performance (-0.2 to -0.3) while AUROC and confidence remain high; ASRS provides a label-free mechanism to flag error-prone cases for review, aiding fairness and safety.", "conclusion": "ASRS is a practical, label-free framework for detecting error-prone CXR cases via augmentation-sensitivity analysis, enabling selective prediction and clinician triage to improve fairness and reliability in medical AI."}}
{"id": "2510.01456", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01456", "abs": "https://arxiv.org/abs/2510.01456", "authors": ["Brett Barkley", "Preston Culbertson", "David Fridovich-Keil"], "title": "SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion", "comment": null, "summary": "Out-of-distribution (OOD) detection is essential for reliable deployment of\nmachine learning systems in vision, robotics, reinforcement learning, and\nbeyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator\nfor Diffusion (SCOPED), a fast and general-purpose OOD detection method for\ndiffusion models that reduces the number of forward passes on the trained model\nby an order of magnitude compared to prior methods, outperforming most\ndiffusion-based baselines and closely approaching the accuracy of the strongest\nones. SCOPED is computed from a single diffusion model trained once on a\ndiverse dataset, and combines the Jacobian trace and squared norm of the\nmodel's score function into a single test statistic. Rather than thresholding\non a fixed value, we estimate the in-distribution density of SCOPED scores\nusing kernel density estimation, enabling a flexible, unsupervised test that,\nin the simplest case, only requires a single forward pass and one\nJacobian-vector product (JVP), made efficient by Hutchinson's trace estimator.\nOn four vision benchmarks, SCOPED achieves competitive or state-of-the-art\nprecision-recall scores despite its low computational cost. The same method\ngeneralizes to robotic control tasks with shared state and action spaces,\nidentifying distribution shifts across reward functions and training regimes.\nThese results position SCOPED as a practical foundation for fast and reliable\nOOD detection in real-world domains, including perceptual artifacts in vision,\noutlier detection in autoregressive models, exploration in reinforcement\nlearning, and dataset curation for unsupervised training.", "AI": {"tldr": "SCOPED is a fast, diffusion-model OOD detector that combines score curvature and Jacobian trace into a single statistic, enabling unsupervised, threshold-free detection with far fewer forward passes. It delivers competitive accuracy and generalizes across vision and robotics with a single trained model.", "motivation": "Reliable OOD detection is essential for deploying ML systems in vision, robotics, and RL. Diffusion-model-based detectors tend to be computationally expensive and domain-specific. The goal is a fast, general-purpose, unsupervised OOD method that works from a single trained diffusion model.", "method": "Compute SCOPED: a test statistic that combines the Jacobian trace and the squared norm of the score function. Use Hutchinson's trace estimator to compute the trace efficiently, and require only a single forward pass and one JVP in the simplest setting. Estimate the in-distribution density of SCOPED scores via kernel density estimation to enable unsupervised, threshold-free detection.", "result": "SCOPED achieves competitive or state-of-the-art precision-recall on four vision benchmarks while dramatically reducing forward passes (order of magnitude fewer than prior methods). It generalizes to robotic control tasks, identifying distribution shifts across reward functions and training regimes.", "conclusion": "SCOPED provides a practical, fast, and general-purpose OOD detection foundation for diffusion models, applicable to perceptual artifacts, outlier detection in autoregressive models, exploration in RL, and dataset curation for unsupervised training."}}
{"id": "2510.01687", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01687", "abs": "https://arxiv.org/abs/2510.01687", "authors": ["John Hawkins"], "title": "Improving AGI Evaluation: A Data Science Perspective", "comment": null, "summary": "Evaluation of potential AGI systems and methods is difficult due to the\nbreadth of the engineering goal. We have no methods for perfect evaluation of\nthe end state, and instead measure performance on small tests designed to\nprovide directional indication that we are approaching AGI. In this work we\nargue that AGI evaluation methods have been dominated by a design philosophy\nthat uses our intuitions of what intelligence is to create synthetic tasks,\nthat have performed poorly in the history of AI. Instead we argue for an\nalternative design philosophy focused on evaluating robust task execution that\nseeks to demonstrate AGI through competence. This perspective is developed from\ncommon practices in data science that are used to show that a system can be\nreliably deployed. We provide practical examples of what this would mean for\nAGI evaluation.", "AI": {"tldr": "Shift AGI evaluation from intuition-based synthetic tasks to robust, competence-focused task execution, inspired by data science deployment practices.", "motivation": "Current AGI benchmarks rely on intuitive, synthetic tasks that have historically performed poorly and poorly reflect real-world capabilities; a reliable evaluation framework is needed to credibly track progress toward AGI.", "method": "Advocate an alternative design philosophy focused on evaluating robust task execution and demonstrated competence, drawing on data science practices used to show reliable deployment; provide practical examples of what robust evaluation would entail for AGI.", "result": "Proposes a conceptual framework with practical examples for evaluating AGI via robust task competence; no empirical results are claimed in the abstract.", "conclusion": "Adopting a deployment-oriented, competence-focused evaluation approach may yield more meaningful progress signals and better reflect real-world capabilities than intuition-based benchmarks."}}
{"id": "2510.02080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02080", "abs": "https://arxiv.org/abs/2510.02080", "authors": ["Lingxiang Hu", "Naima Ait Oufroukh", "Fabien Bonardi", "Raymond Ghandour"], "title": "EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction", "comment": null, "summary": "The application of monocular dense Simultaneous Localization and Mapping\n(SLAM) is often hindered by high latency, large GPU memory consumption, and\nreliance on camera calibration. To relax this constraint, we propose EC3R-SLAM,\na novel calibration-free monocular dense SLAM framework that jointly achieves\nhigh localization and mapping accuracy, low latency, and low GPU memory\nconsumption. This enables the framework to achieve efficiency through the\ncoupling of a tracking module, which maintains a sparse map of feature points,\nand a mapping module based on a feed-forward 3D reconstruction model that\nsimultaneously estimates camera intrinsics. In addition, both local and global\nloop closures are incorporated to ensure mid-term and long-term data\nassociation, enforcing multi-view consistency and thereby enhancing the overall\naccuracy and robustness of the system. Experiments across multiple benchmarks\nshow that EC3R-SLAM achieves competitive performance compared to\nstate-of-the-art methods, while being faster and more memory-efficient.\nMoreover, it runs effectively even on resource-constrained platforms such as\nlaptops and Jetson Orin NX, highlighting its potential for real-world robotics\napplications.", "AI": {"tldr": "EC3R-SLAM offers calibration-free monocular dense SLAM with a two-branch architecture: a tracking module that maintains a sparse feature map and a mapping module using a feed-forward 3D reconstruction model that also estimates intrinsics. It includes local and global loop closures for multi-view consistency, achieving competitive accuracy with lower latency and memory usage, and runs on resource-constrained hardware like laptops and Jetson platforms.", "motivation": "To overcome the bottlenecks of monocular dense SLAM: high latency, large GPU memory consumption, and the need for camera calibration. The goal is a calibration-free, memory-efficient, low-latency framework that remains accurate in real-world robotics scenarios.", "method": "A coupling of two modules: a tracking component preserving a sparse map of features and a mapping component based on a feed-forward 3D reconstruction model that jointly estimates camera intrinsics. The system also integrates both local and global loop closures to enforce multi-view consistency and improve robustness.", "result": "Experimental results on multiple benchmarks show EC3R-SLAM is competitive with state-of-the-art methods while being faster and more memory-efficient. It also demonstrates viability on resource-constrained hardware (laptops and Jetson Orin NX), indicating potential for real-world robotics.", "conclusion": "EC3R-SLAM demonstrates that a calibration-free monocular dense SLAM framework with a tight coupling between tracking and feed-forward mapping can achieve accurate localization and mapping with reduced latency and memory usage, suitable for deployment on edge devices in real-world applications."}}
{"id": "2510.01686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01686", "abs": "https://arxiv.org/abs/2510.01686", "authors": ["Jiacong Xu", "Yiqun Mei", "Ke Zhang", "Vishal M. Patel"], "title": "FreeViS: Training-free Video Stylization with Inconsistent References", "comment": "Project Page: \\url{https://xujiacong.github.io/FreeViS/}", "summary": "Video stylization plays a key role in content creation, but it remains a\nchallenging problem. Na\\\"ively applying image stylization frame-by-frame hurts\ntemporal consistency and reduces style richness. Alternatively, training a\ndedicated video stylization model typically requires paired video data and is\ncomputationally expensive. In this paper, we propose FreeViS, a training-free\nvideo stylization framework that generates stylized videos with rich style\ndetails and strong temporal coherence. Our method integrates multiple stylized\nreferences to a pretrained image-to-video (I2V) model, effectively mitigating\nthe propagation errors observed in prior works, without introducing flickers\nand stutters. In addition, it leverages high-frequency compensation to\nconstrain the content layout and motion, together with flow-based motion cues\nto preserve style textures in low-saliency regions. Through extensive\nevaluations, FreeViS delivers higher stylization fidelity and superior temporal\nconsistency, outperforming recent baselines and achieving strong human\npreference. Our training-free pipeline offers a practical and economic solution\nfor high-quality, temporally coherent video stylization. The code and videos\ncan be accessed via https://xujiacong.github.io/FreeViS/", "AI": {"tldr": "A training-free framework (FreeViS) for video stylization that preserves temporal coherence and style richness by integrating multiple stylized references into a pretrained image-to-video model, using high-frequency compensation and flow-based motion cues; outperforms baselines with a practical, economical pipeline.", "motivation": "Frame-by-frame stylization harms temporal consistency; training video stylizers requires paired data and is expensive; need a method that yields high-quality, temporally coherent stylized video without training.", "method": "Combine multiple stylized references with a pretrained I2V model; mitigate propagation errors; apply high-frequency compensation to constrain content/layout and motion; use flow-based motion cues to preserve textures in low-saliency areas; training-free.", "result": "Higher stylization fidelity and better temporal consistency than recent baselines; strong human preference; practical and economical; code and videos available.", "conclusion": "Offers a practical, training-free pipeline for high-quality, temporally coherent video stylization; lowers cost and data requirements; ready for use in content creation."}}
{"id": "2510.01457", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01457", "abs": "https://arxiv.org/abs/2510.01457", "authors": ["Brett Barkley", "David Fridovich-Keil"], "title": "Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization", "comment": null, "summary": "Synthetic data is a core component of data-efficient Dyna-style model-based\nreinforcement learning, yet it can also degrade performance. We study when it\nhelps, where it fails, and why, and we show that addressing the resulting\nfailure modes enables policy improvement that was previously unattainable. We\nfocus on Model-Based Policy Optimization (MBPO), which performs actor and\ncritic updates using synthetic action counterfactuals. Despite reports of\nstrong and generalizable sample-efficiency gains in OpenAI Gym, recent work\nshows that MBPO often underperforms its model-free counterpart, Soft\nActor-Critic (SAC), in the DeepMind Control Suite (DMC). Although both suites\ninvolve continuous control with proprioceptive robots, this shift leads to\nsharp performance losses across seven challenging DMC tasks, with MBPO failing\nin cases where claims of generalization from Gym would imply success. This\nreveals how environment-specific assumptions can become implicitly encoded into\nalgorithm design when evaluation is limited. We identify two coupled issues\nbehind these failures: scale mismatches between dynamics and reward models that\ninduce critic underestimation and hinder policy improvement during model-policy\ncoevolution, and a poor choice of target representation that inflates model\nvariance and produces error-prone rollouts. Addressing these failure modes\nenables policy improvement where none was previously possible, allowing MBPO to\noutperform SAC in five of seven tasks while preserving the strong performance\npreviously reported in OpenAI Gym. Rather than aiming only for incremental\naverage gains, we hope our findings motivate the community to develop\ntaxonomies that tie MDP task- and environment-level structure to algorithmic\nfailure modes, pursue unified solutions where possible, and clarify how\nbenchmark choices ultimately shape the conditions under which algorithms\ngeneralize.", "AI": {"tldr": "MBPO's success in data-efficient RL hinges on addressing two failure modes when using synthetic data: (1) scale mismatches between dynamics and reward models causing critic underestimation during model-policy coevolution, and (2) a poor target representation that inflates variance and yields error-prone rollouts. By addressing these issues, MBPO can outperform SAC in most DMC tasks while retaining Gym-level gains, highlighting the importance of environment-aware algorithm design and benchmark choices.", "motivation": "The abstract seeks to understand when synthetic data helps in MBPO, why it sometimes hurts, and how fixing its failure modes enables policy improvement. It also questions generalization across benchmarks (Gym vs. DeepMind Control Suite) and invites taxonomy tying task structure to algorithm behavior.", "method": "Empirical investigation of MBPO on seven DeepMind Control Suite tasks, diagnosing two coupled failure modes (dynamics/reward scale mismatch causing critic underestimation; poor target representation increasing variance). Demonstrates that addressing these issues enables policy improvement and enables MBPO to outperform SAC in five of seven tasks, while preserving Gym performance.", "result": "After addressing the identified failure modes, MBPO achieves policy improvement over SAC in five of seven DMC tasks and maintains the strong performance previously reported in OpenAI Gym, bridging the gap between Gym results and DMC performance.", "conclusion": "Benchmarks and task structure shape algorithm generalization. The authors advocate for a taxonomy linking MDP/task-level structure to failure modes, pursue unified solutions where possible, and emphasize how benchmark choices influence where algorithms generalize."}}
{"id": "2510.01700", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01700", "abs": "https://arxiv.org/abs/2510.01700", "authors": ["Rohan Wadhawan", "Fabrice Y Harel-Canada", "Zi-Yi Dou", "Suhaila Shakiah", "Robinson Piramuthu", "Nanyun Peng"], "title": "VaPR -- Vision-language Preference alignment for Reasoning", "comment": null, "summary": "Preference finetuning methods like Direct Preference Optimization (DPO) with\nAI-generated feedback have shown promise in aligning Large Vision-Language\nModels (LVLMs) with human preferences. However, existing techniques overlook\nthe prevalence of noise in synthetic preference annotations in the form of\nstylistic and length biases. To this end, we introduce a hard-negative response\ngeneration framework based on LLM-guided response editing, that produces\nrejected responses with targeted errors, maintaining stylistic and length\nsimilarity to the accepted ones. Using this framework, we develop the VaPR\ndataset, comprising 30K high-quality samples, to finetune three LVLM families:\nLLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver\nsignificant performance improvements across ten benchmarks, achieving average\ngains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable\nimprovements on reasoning tasks. A scaling analysis shows that performance\nconsistently improves with data size, with LLaVA models benefiting even at\nsmaller scales. Moreover, VaPR reduces the tendency to answer \"Yes\" in binary\nquestions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we\nshow that the framework generalizes to open-source LLMs as editors, with models\ntrained on VaPR-OS achieving ~99% of the performance of models trained on\n\\name, which is synthesized using GPT-4o. Our data, models, and code can be\nfound on the project page https://vap-r.github.io", "AI": {"tldr": "Introduces VaPR, a hard-negative generation framework for LVLM alignment via LLM-guided response editing, creating a 30K sample dataset to improve preference-finetuning; shows consistent gains across LVLM families and open-source editors.", "motivation": "Address noise and biases in synthetic preference data (notably stylistic and length biases) that degrade alignment quality when finetuning with preference data.", "method": "Use LLM-guided editing to generate hard-negative responses that resemble accepted answers in style and length but contain targeted errors. Build VaPR, a 30K high-quality dataset, to fine-tune LVLMs (LLaVA-V1.5, Qwen2VL, Qwen2.5VL) with DPO/AI-feedback; evaluate across ten benchmarks; extend to open-source editors (VaPR-OS).", "result": "Consistent performance gains across models: +6.5% (LLaVA), +4.0% (Qwen2VL), +1.5% (Qwen2.5VL) on ten benchmarks, notably in reasoning tasks. Scaling: performance improves with more data, with LLaVA benefiting at smaller scales. VaPR reduces the tendency to answer \u201cYes\u201d in binary questions. VaPR-OS editors reach ~99% of the full VaPR results (vs GPT-4o-synthesized data). Public release of data, models, and code.", "conclusion": "Hard-negative generation effectively mitigates synthetic preference noise and yields robust gains across LVLM families and open-source editors. The VaPR dataset and framework offer scalable, transferable improvements for alignment via preference finetuning and reduce common failure modes, with strong evidence of generalization."}}
{"id": "2510.02104", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02104", "abs": "https://arxiv.org/abs/2510.02104", "authors": ["Yunhan Lin", "Wenqi Wu", "Zhijie Zhang", "Huasong Min"], "title": "LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions", "comment": "8 pages, 6 figures", "summary": "The existing language-driven grasping methods struggle to fully handle\nambiguous instructions containing implicit intents. To tackle this challenge,\nwe propose LangGrasp, a novel language-interactive robotic grasping framework.\nThe framework integrates fine-tuned large language models (LLMs) to leverage\ntheir robust commonsense understanding and environmental perception\ncapabilities, thereby deducing implicit intents from linguistic instructions\nand clarifying task requirements along with target manipulation objects.\nFurthermore, our designed point cloud localization module, guided by 2D part\nsegmentation, enables partial point cloud localization in scenes, thereby\nextending grasping operations from coarse-grained object-level to fine-grained\npart-level manipulation. Experimental results show that the LangGrasp framework\naccurately resolves implicit intents in ambiguous instructions, identifying\ncritical operations and target information that are unstated yet essential for\ntask completion. Additionally, it dynamically selects optimal grasping poses by\nintegrating environmental information. This enables high-precision grasping\nfrom object-level to part-level manipulation, significantly enhancing the\nadaptability and task execution efficiency of robots in unstructured\nenvironments. More information and code are available here:\nhttps://github.com/wu467/LangGrasp.", "AI": {"tldr": "LangGrasp is a language-interactive grasping framework that uses fine-tuned LLMs to infer implicit intents and a 2D-guided point-cloud localization to enable fine-grained part-level manipulation, enabling robust grasping in unstructured environments.", "motivation": "Existing language-driven grasping methods struggle with ambiguous instructions and implicit intents; there is a need to leverage large language models' commonsense understanding and perceptual abilities to clarify tasks and identify target objects or parts.", "method": "Fine-tune LLMs to deduce implicit intents and clarify task requirements from linguistic instructions; introduce a point cloud localization module guided by 2D segmentation to achieve partial (part-level) localization; dynamically select grasping poses using environmental information.", "result": "LangGrasp accurately resolves implicit intents, identifying critical unstated operations and target information, and selects optimal grasp poses for high-precision part-level manipulation, improving adaptability and task execution efficiency in unstructured environments.", "conclusion": "By integrating LLM-based reasoning with vision-guided localization, LangGrasp enables robust, fine-grained language-driven robotic grasping; code is available at the linked GitHub repository."}}
{"id": "2510.01691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01691", "abs": "https://arxiv.org/abs/2510.01691", "authors": ["Jiyao Liu", "Jinjie Wei", "Wanying Qu", "Chenglong Ma", "Junzhi Ning", "Yunheng Li", "Ying Chen", "Xinzhe Luo", "Pengcheng Chen", "Xin Gao", "Ming Hu", "Huihui Xu", "Xin Wang", "Shujian Gao", "Dingkang Yang", "Zhongying Deng", "Jin Ye", "Lihao Liu", "Junjun He", "Ningsheng Xu"], "title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs", "comment": "26 pages, 13 figures", "summary": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate\nfor clinical AI, yet existing approaches remain constrained by scalar,\nscore-based metrics and fail to reflect the descriptive, human-like reasoning\nprocess central to expert evaluation. To address this gap, we introduce\nMedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning\nparadigm for language-based evaluation of medical image quality with\nMulti-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary\ntasks: (1) MedQ-Perception, which probes low-level perceptual capability via\nhuman-curated questions on fundamental visual attributes; and (2)\nMedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,\naligning model evaluation with human-like reasoning on image quality. The\nbenchmark spans five imaging modalities and over forty quality attributes,\ntotaling 2,600 perceptual queries and 708 reasoning assessments, covering\ndiverse image sources including authentic clinical acquisitions, images with\nsimulated degradations via physics-based reconstructions, and AI-generated\nimages. To evaluate reasoning ability, we propose a multi-dimensional judging\nprotocol that assesses model outputs along four complementary axes. We further\nconduct rigorous human-AI alignment validation by comparing LLM-based judgement\nwith radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates\nthat models exhibit preliminary but unstable perceptual and reasoning skills,\nwith insufficient accuracy for reliable clinical use. These findings highlight\nthe need for targeted optimization of MLLMs in medical IQA. We hope that\nMedQ-Bench will catalyze further exploration and unlock the untapped potential\nof MLLMs for medical image quality evaluation.", "AI": {"tldr": "MedQ-Bench introduces a perception-reasoning benchmark for assessing medical image quality via multi-modal LLMs, featuring MedQ-Perception and MedQ-Reasoning tasks across five modalities, with 2,600 perceptual queries and 708 reasoning assessments; evaluates 14 MLLMs and finds preliminary, unstable perceptual/reasoning skills, signaling need for optimization.", "motivation": "Traditional scalar IQA metrics inadequately reflect human-like, expert-quality judgments in medical imaging. There is a need for a benchmark that evaluates both perceptual ability and reasoning aligned with radiologists to guide the development of reliable medical IQA systems using MLLMs.", "method": "MedQ-Bench defines two complementary tasks: MedQ-Perception (low-level perceptual questions on visual attributes) and MedQ-Reasoning (no-reference and comparison-based reasoning). It spans five imaging modalities and >40 quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, including authentic clinical images, physics-based degraded reconstructions, and AI-generated images. A multi-dimensional judging protocol with four axes evaluates model outputs, and radiologist alignment validates the human-AI agreement.", "result": "Evaluation of 14 state-of-the-art MLLMs shows preliminary but unstable perceptual and reasoning capabilities, with accuracy currently insufficient for reliable clinical deployment.", "conclusion": "MedQ-Bench is a valuable framework to drive targeted optimization of MLLMs for medical image quality assessment and to catalyze broader exploration into human-aligned, perception-based evaluation in medical imaging."}}
{"id": "2510.01458", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01458", "abs": "https://arxiv.org/abs/2510.01458", "authors": ["Shawn Im", "Yixuan Li"], "title": "How Well Can Preference Optimization Generalize Under Noisy Feedback?", "comment": null, "summary": "As large language models (LLMs) advance their capabilities, aligning these\nmodels with human preferences has become crucial. Preference optimization,\nwhich trains models to distinguish between preferred and non-preferred\nresponses based on human feedback, has become a crucial component for aligning\nLLMs. However, most existing works assume noise-free feedback, which is\nunrealistic due to the inherent errors and inconsistencies in human judgments.\nThis paper addresses the impact of noisy feedback on preference optimization,\nproviding generalization guarantees under these conditions. In particular, we\nconsider noise models that correspond to common real-world sources of noise,\nsuch as mislabeling and uncertainty. Unlike traditional analyses that assume\nconvergence, our work focuses on finite-step preference optimization, offering\nnew insights that are more aligned with practical LLM training. We describe how\ngeneralization decays with different types of noise across levels of noise\nrates based on the preference data distribution and number of samples. Our\nanalysis for noisy preference learning applies to a broad family of preference\noptimization losses such as DPO, IPO, SLiC, etc. Empirical validation on\ncontemporary LLMs confirms the practical relevance of our findings, offering\nvaluable insights for developing AI systems that align with human preferences.", "AI": {"tldr": "Noisy human feedback in preference optimization is analyzed, yielding finite-step generalization guarantees across common loss families (DPO, IPO, SLiC, etc.) under realistic noise models; the work includes empirical validation on contemporary LLMs.", "motivation": "Human feedback is inherently noisy in real-world settings. Understanding how mislabeling and uncertainty affect generalization and training efficiency is essential for reliable alignment of LLMs.", "method": "Develop a generalization framework for noisy preference learning with mislabeling and uncertainty. Derive finite-step generalization bounds for a broad family of preference losses (DPO, IPO, SLiC, etc.), showing how generalization decays with noise rate, data distribution, and sample size. Provide empirical validation on contemporary LLMs.", "result": "The analysis yields explicit generalization guarantees under noise, quantifies how generalization degrades with increasing noise, and demonstrates the applicability of the bounds across multiple preference-loss families; empirical results corroborate the theory on modern LLMs.", "conclusion": "The findings inform robust design of preference-based alignment under imperfect human feedback, highlighting the need to model and mitigate noise in practice and offering a practical framework for evaluating noisy preference learning."}}
{"id": "2510.01724", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01724", "abs": "https://arxiv.org/abs/2510.01724", "authors": ["Madina Bekbergenova", "Lucas Pradi", "Benjamin Navet", "Emma Tysinger", "Franck Michel", "Matthieu Feraud", "Yousouf Taghzouti", "Yan Zhou Chen", "Olivier Kirchhoffer", "Florence Mehl", "Martin Legrand", "Tao Jiang", "Marco Pagni", "Soha Hassoun", "Jean-Luc Wolfender", "Wout Bittremieux", "Fabien Gandon", "Louis-F\u00e9lix Nothias"], "title": "MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs", "comment": null, "summary": "Mass spectrometry metabolomics generates vast amounts of data requiring\nadvanced methods for interpretation. Knowledge graphs address these challenges\nby structuring mass spectrometry data, metabolite information, and their\nrelationships into a connected network (Gaudry et al. 2024). However, effective\nuse of a knowledge graph demands an in-depth understanding of its ontology and\nits query language syntax. To overcome this, we designed MetaboT, an AI system\nutilizing large language models (LLMs) to translate user questions into SPARQL\nsemantic query language for operating on knowledge graphs (Steve Harris 2013).\nWe demonstrate its effectiveness using the Experimental Natural Products\nKnowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural\nproducts (Gaudry et al. 2024).MetaboT employs specialized AI agents for\nhandling user queries and interacting with the knowledge graph by breaking down\ncomplex tasks into discrete components, each managed by a specialised agent\n(Fig. 1a). The multi-agent system is constructed using the LangChain and\nLangGraph libraries, which facilitate the integration of LLMs with external\ntools and information sources (LangChain, n.d.). The query generation process\nfollows a structured workflow. First, the Entry Agent determines if the\nquestion is new or a follow-up to previous interactions. New questions are\nforwarded to the Validator Agent, which verifies if the question is related to\nthe knowledge graph. Then, the valid question is sent to the Supervisor Agent,\nwhich identifies if the question requires chemical conversions or standardized\nidentifiers. In this case it delegates the question to the Knowledge Graph\nAgent, which can use tools to extract necessary details, such as URIs or\ntaxonomies of chemical names, from the user query. Finally, an agent\nresponsible for crafting the SPARQL queries equipped with the ontology of the\nknowledge graph uses the provided identifiers to generate the query. Then, the\nsystem executes the generated query against the metabolomics knowledge graph\nand returns structured results to the user (Fig. 1b). To assess the performance\nof MetaboT we have curated 50 metabolomics-related questions and their expected\nanswers. In addition to submitting these questions to MetaboT, we evaluated a\nbaseline by submitting them to a standard LLM (GPT-4o) with a prompt that\nincorporated the knowledge graph ontology but did not provide specific entity\nIDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,\nunderscoring the necessity of our multi-agent system for accurately retrieving\nentities and generating correct SPARQL queries. MetaboT demonstrates promising\nperformance as a conversational question-answering assistant, enabling\nresearchers to retrieve structured metabolomics data through natural language\nqueries. By automating the generation and execution of SPARQL queries, it\nremoves technical barriers that have traditionally hindered access to knowledge\ngraphs. Importantly, MetaboT leverages the capabilities of LLMs while\nmaintaining experimentally grounded query generation, ensuring that outputs\nremain aligned with domain-specific standards and data structures. This\napproach facilitates data-driven discoveries by bridging the gap between\ncomplex semantic technologies and user-friendly interaction. MetaboT is\naccessible at [https://metabot.holobiomicslab.eu/], and its source code is\navailable at [https://github.com/HolobiomicsLab/MetaboT].", "AI": {"tldr": "A multi-agent AI system (MetaboT) translates natural language questions about metabolomics into SPARQL queries on a large knowledge graph (ENPKG), achieving high accuracy and outperforming a baseline.", "motivation": "Mass spectrometry metabolomics data is vast; knowledge graphs structure data but require ontology expertise; to democratize querying, an AI assistant translating NL to SPARQL is needed.", "method": "A LangChain/LangGraph-based multi-agent architecture with Entry, Validator, Supervisor, Knowledge Graph, and SPARQL Query agents; converts NL questions into SPARQL using KG ontology and IDs; queries ENPKG; evaluation on 50 curated questions against GPT-4o baseline.", "result": "MetaboT achieved 83.67% accuracy on 50 questions, vs 8.16% for the baseline; demonstrates effective translation to SPARQL and retrieval of structured results.", "conclusion": "MetaboT lowers barriers to semantic querying in metabolomics, enabling researchers to obtain structured KG data via natural language, with potential to drive data-driven discoveries; source code and demo available."}}
{"id": "2510.02129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02129", "abs": "https://arxiv.org/abs/2510.02129", "authors": ["Philip Reichenberg", "Tim Laue"], "title": "Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control", "comment": null, "summary": "Stand-up motions are an indispensable part of humanoid robot soccer. A robot\nincapable of standing up by itself is removed from the game for some time. In\nthis paper, we present our stand-up motions for the NAO robot. Our approach\ndates back to 2019 and has been evaluated and slightly expanded over the past\nsix years. We claim that the main reason for failed stand-up attempts are large\nerrors in the executed joint positions. By addressing such problems by either\nexecuting special motions to free up stuck limbs such as the arms, or by\ncompensating large errors with other joints, we significantly increased the\noverall success rate of our stand-up routine. The motions presented in this\npaper are also used by several other teams in the Standard Platform League,\nwhich thereby achieve similar success rates, as shown in an analysis of videos\nfrom multiple tournaments.", "AI": {"tldr": "The paper presents NAO stand-up motions for humanoid soccer, developed since 2019, addressing large joint-position errors with freeing motions and joint compensation, leading to higher stand-up success rates and adoption by other teams in the league, as evidenced by tournament video analyses.", "motivation": "Ensure reliable stand-up in robot soccer to avoid being penalized or removed from play; improve game continuity by reducing fall-related failures.", "method": "Develop stand-up motions for the NAO robot. To mitigate failed attempts caused by large joint-position errors, incorporate specialized motions to free stuck limbs (e.g., arms) and use compensation across other joints. The approach has been iteratively evaluated and expanded from 2019 through six years, with validation via analysis of videos from multiple tournaments.", "result": "Significant increase in stand-up success rate; the motions are adopted by several other teams in the Standard Platform League, achieving similar success rates as evidenced by cross-tournament video analyses.", "conclusion": "Robust, transferable stand-up routines can markedly improve game continuity in humanoid robot soccer; the authors\u2019 approach appears generalizable within the league, given its adoption by multiple teams and corroborating video analyses."}}
{"id": "2510.01704", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01704", "abs": "https://arxiv.org/abs/2510.01704", "authors": ["Pierre Musacchio", "Hyunmin Lee", "Jaesik Park"], "title": "Holistic Order Prediction in Natural Scenes", "comment": "25 pages, 11 figures, 6 tables", "summary": "Even in controlled settings, understanding instance-wise geometries is a\nchallenging task for a wide range of visual models. Although specialized\nsystems exist, modern arts rely on expensive input formats (category labels,\nbinary segmentation masks) and inference costs (a quadratic amount of forward\npasses). We mitigate these limitations by proposing InstaFormer, a network\ncapable of holistic order prediction. That is, solely given an input RGB image,\nInstaFormer returns the full occlusion and depth orderings for all the\ninstances in the scene in a single forward pass. At its core, InstaFormer\nrelies on interactions between object queries and latent mask descriptors that\nsemantically represent the same objects while carrying complementary\ninformation. We comprehensively benchmark and ablate our approach to highlight\nits effectiveness. Our code and models are open-source and available at this\nURL: https://github.com/SNU-VGILab/InstaOrder.", "AI": {"tldr": "InstaFormer predicts full occlusion and depth ordering for all scene instances from a single RGB image in one forward pass, using interactions between object queries and latent mask descriptors to jointly encode objects; open-source code provided.", "motivation": "Understanding instance-wise 3D geometry from RGB is hard and current methods rely on expensive inputs (labels, masks) and inference (quadratic passes). A single RGB-based, efficient approach for holistic ordering is needed.", "method": "A transformer-like framework (InstaFormer) that models interactions between object queries and latent mask descriptors representing the same objects but carrying complementary information, enabling holistic occlusion and depth order prediction in one forward pass.", "result": "Comprehensive benchmarking and ablation studies demonstrate the effectiveness of the approach; the authors also provide open-source code and models.", "conclusion": "Proposes a novel, efficient RGB-only method for predicting full occlusion and depth orderings across all instances in a scene in a single pass, reducing reliance on expensive inputs and multiple inferences; code is publicly available."}}
{"id": "2510.01459", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01459", "abs": "https://arxiv.org/abs/2510.01459", "authors": ["Weizhe Chen", "Sven Koenig", "Bistra Dilkina"], "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning", "comment": null, "summary": "Since the release of Deepseek-R1, reinforcement learning with verifiable\nrewards (RLVR) has become a central approach for training large language models\n(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss\nfunctions to make RLVR more efficient and effective. In this paper, motivated\nby studies of overthinking in LLMs, we propose Length-aware Sampling for Policy\nOptimization (LSPO), a novel meta-RLVR algorithm that dynamically selects\ntraining data at each step based on the average response length. We evaluate\nLSPO across multiple base models and datasets, demonstrating that it\nconsistently improves learning effectiveness. In addition, we conduct a\ndetailed ablation study to examine alternative ways of incorporating length\nsignals into dynamic sampling, offering further insights and highlighting\npromising directions for future research.", "AI": {"tldr": "A length-aware meta-RLVR method (LSPO) that adaptively samples training data by average response length improves RL-based training of LLMs across models and datasets; with ablations and future directions.", "motivation": "To address overthinking and inefficiencies in reinforcement learning with verifiable rewards (RLVR) for large language models by leveraging response-length signals to guide dynamic data sampling.", "method": "Proposes LSPO, a meta-RLVR algorithm that dynamically selects training data at each step based on the average response length, integrating length signals into the data-sampling process.", "result": "LSPO consistently improves learning effectiveness across multiple base models and datasets; ablation studies explore alternative ways of incorporating length signals into dynamic sampling and offer insights for future work.", "conclusion": "Length-aware sampling is a promising direction for RLVR and warrants further exploration of length-based signals and sampling strategies to enhance LLM reasoning training."}}
{"id": "2510.01751", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01751", "abs": "https://arxiv.org/abs/2510.01751", "authors": ["Masike Malatji"], "title": "A cybersecurity AI agent selection and decision support framework", "comment": "6 figures, 6 tables, AI agents decision support framework", "summary": "This paper presents a novel, structured decision support framework that\nsystematically aligns diverse artificial intelligence (AI) agent architectures,\nreactive, cognitive, hybrid, and learning, with the comprehensive National\nInstitute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.\nBy integrating agent theory with industry guidelines, this framework provides a\ntransparent and stepwise methodology for selecting and deploying AI solutions\nto address contemporary cyber threats. Employing a granular decomposition of\nNIST CSF 2.0 functions into specific tasks, the study links essential AI agent\nproperties such as autonomy, adaptive learning, and real-time responsiveness to\neach subcategory's security requirements. In addition, it outlines graduated\nlevels of autonomy (assisted, augmented, and fully autonomous) to accommodate\norganisations at varying stages of cybersecurity maturity. This holistic\napproach transcends isolated AI applications, providing a unified detection,\nincident response, and governance strategy. Through conceptual validation, the\nframework demonstrates how tailored AI agent deployments can align with\nreal-world constraints and risk profiles, enhancing situational awareness,\naccelerating response times, and fortifying long-term resilience via adaptive\nrisk management. Ultimately, this research bridges the gap between theoretical\nAI constructs and operational cybersecurity demands, establishing a foundation\nfor robust, empirically validated multi-agent systems that adhere to industry\nstandards.", "AI": {"tldr": "A structured, multi-agent AI framework aligned with NIST CSF 2.0, featuring graduated autonomy and a holistic cyber defense approach, with conceptual validation.", "motivation": "To bridge AI agent theory with industry cybersecurity standards, enabling transparent, stepwise selection and deployment of AI solutions against cyber threats while accommodating varying maturity levels.", "method": "Granular decomposition of NIST CSF 2.0 into tasks; linking AI properties (autonomy, adaptive learning, real-time responsiveness) to security requirements; defining autonomy levels (assisted, augmented, fully autonomous); outlining a unified detection, incident response, and governance strategy; conceptual validation to test alignment with real-world constraints.", "result": "Conceptual validation demonstrates alignment with constraints and risk profiles, improvements in situational awareness and response speed, and enhanced adaptive risk management; establishes groundwork for empirically validated multi-agent systems.", "conclusion": "Bridges theory and practice, enabling robust, standards-compliant multi-agent systems for cybersecurity with a foundation for future empirical validation."}}
{"id": "2510.02164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02164", "abs": "https://arxiv.org/abs/2510.02164", "authors": ["Nathaniel Hanson", "Austin Allison", "Charles DiMarzio", "Ta\u015fk\u0131n Pad\u0131r", "Kristen L. Dorsey"], "title": "SCANS: A Soft Gripper with Curvature and Spectroscopy Sensors for In-Hand Material Differentiation", "comment": "Accepted to IEEE Robotics & Automation Letters Special Issue on\n  Interdisciplinarity and Widening Horizons in Soft Robotics", "summary": "We introduce the soft curvature and spectroscopy (SCANS) system: a versatile,\nelectronics-free, fluidically actuated soft manipulator capable of assessing\nthe spectral properties of objects either in hand or through pre-touch caging.\nThis platform offers a wider spectral sensing capability than previous soft\nrobotic counterparts. We perform a material analysis to explore optimal soft\nsubstrates for spectral sensing, and evaluate both pre-touch and in-hand\nperformance. Experiments demonstrate explainable, statistical separation across\ndiverse object classes and sizes (metal, wood, plastic, organic, paper, foam),\nwith large spectral angle differences between items. Through linear\ndiscriminant analysis, we show that sensitivity in the near-infrared\nwavelengths is critical to distinguishing visually similar objects. These\ncapabilities advance the potential of optics as a multi-functional sensory\nmodality for soft robots. The complete parts list, assembly guidelines, and\nprocessing code for the SCANS gripper are accessible at:\nhttps://parses-lab.github.io/scans/.", "AI": {"tldr": "Introduces SCANS, an electronics-free soft gripper that uses near-infrared spectral sensing for material discrimination during pre-touch and in-hand manipulation, with open hardware/resources.", "motivation": "To extend soft robotics with an integrated, explainable spectral sensing modality capable of separating a wide range of materials, addressing limitations of purely tactile or visual sensing.", "method": "Systematically studies soft substrate materials for spectral sensing; tests pre-touch and in-hand sensing on diverse object classes; applies linear discriminant analysis and spectral-angle metrics to evaluate separability; provides complete bill of materials, assembly guidelines, and processing code.", "result": "Achieves statistically explainable separation across metal, wood, plastic, organic, paper, and foam; observes large spectral-angle differences between items; near-infrared bands are critical for differentiating visually similar objects.", "conclusion": "SCANS broadens optics as a multifunctional sensor in soft robotics; open-source designs enable broader adoption and replication."}}
{"id": "2510.01715", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01715", "abs": "https://arxiv.org/abs/2510.01715", "authors": ["Raahul Krishna Durairaju", "K. Saruladha"], "title": "PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning", "comment": null, "summary": "Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based\nalgorithm, enabling AI-driven artistic image synthesis. However, existing CNN\nand transformer-based models struggle to scale efficiently to complex styles\nand high-resolution inputs. We introduce PyramidStyler, a transformer framework\nwith Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding\nthat captures both local details and global context while reducing\ncomputational load. We further incorporate reinforcement learning to\ndynamically optimize stylization, accelerating convergence. Trained on\nMicrosoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to\n2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s\ninference--and yields further improvements (content 2.03; style 0.75) with\nminimal speed penalty (1.40 s) when using RL. These results demonstrate\nreal-time, high-quality artistic rendering, with broad applications in media\nand design.", "AI": {"tldr": "PyramidStyler introduces a transformer-based neural style transfer method with pyramidal positional encoding and reinforcement learning to achieve scalable, high-quality stylization at near real-time speeds, demonstrated on COCO and WikiArt with notable reductions in content and style losses and fast inference.", "motivation": "Standard NST methods (CNN/transformers) struggle to scale to complex styles and high-resolution inputs, requiring efficient, multi-scale context integration.", "method": "A transformer framework (PyramidStyler) using Pyramidal Positional Encoding (PPE) to capture hierarchical, multi-scale local and global information, reducing computation. The approach incorporates reinforcement learning to dynamically optimize stylization. Trained on Microsoft COCO and WikiArt, with reported performance gains and inference times.", "result": "Content loss reduced by 62.6% to 2.07 after 4000 epochs; style loss reduced by 57.4% to 0.86 after 4000 epochs; inference time 1.39 s. With RL, content loss 2.03 and style loss 0.75 with inference time 1.40 s (minimal speed penalty).", "conclusion": "Demonstrates real-time, high-quality artistic rendering with scalable NST suitable for media and design applications; RL provides adaptive improvement with negligible speed trade-offs."}}
{"id": "2510.01460", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01460", "abs": "https://arxiv.org/abs/2510.01460", "authors": ["Lu Li", "Tianwei Ni", "Yihao Sun", "Pierre-Luc Bacon"], "title": "The Three Regimes of Offline-to-Online Reinforcement Learning", "comment": null, "summary": "Offline-to-online reinforcement learning (RL) has emerged as a practical\nparadigm that leverages offline datasets for pretraining and online\ninteractions for fine-tuning. However, its empirical behavior is highly\ninconsistent: design choices of online-fine tuning that work well in one\nsetting can fail completely in another. We propose a stability--plasticity\nprinciple that can explain this inconsistency: we should preserve the knowledge\nof pretrained policy or offline dataset during online fine-tuning, whichever is\nbetter, while maintaining sufficient plasticity. This perspective identifies\nthree regimes of online fine-tuning, each requiring distinct stability\nproperties. We validate this framework through a large-scale empirical study,\nfinding that the results strongly align with its predictions in 45 of 63 cases.\nThis work provides a principled framework for guiding design choices in\noffline-to-online RL based on the relative performance of the offline dataset\nand the pretrained policy.", "AI": {"tldr": "A stability\u2013plasticity principle for offline-to-online RL, identifying three online-finetuning regimes with distinct stability requirements; validated by a large-scale study that shows predictions align in 45 of 63 cases, offering guidance based on whether offline data or the pretrained policy performs better.", "motivation": "Offline-to-online RL often yields inconsistent results when online fine-tuning changes; there is a need for a unifying principle to decide what to preserve and how plastic to be during online adaptation.", "method": "Introduce a stability\u2013plasticity framework and classify online-finetuning into three regimes, each with different stability properties. Perform a large-scale empirical study over 63 settings to test whether the framework\u2019s predictions hold.", "result": "The framework\u2019s predictions align with 45 of 63 cases. The results support the idea that choosing to preserve the better source\u2014offline data or pretrained policy\u2014while maintaining sufficient plasticity can explain much of the variability and guide design choices.", "conclusion": "Provides a principled framework for designing offline-to-online RL; guidance depends on the relative performance of the offline dataset and the pretrained policy. The study offers partial but substantial empirical support and points to adaptive strategies that balance stability and plasticity."}}
{"id": "2510.01800", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01800", "abs": "https://arxiv.org/abs/2510.01800", "authors": ["Thanh Ma", "Tri-Tam La", "Lam-Thu Le Huu", "Minh-Nghi Nguyen", "Khanh-Van Pham Luu", "Huu-Hoa Nguyen"], "title": "REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing", "comment": null, "summary": "Academic regulation advising is essential for helping students interpret and\ncomply with institutional policies, yet building effective systems requires\ndomain specific regulatory resources. To address this challenge, we propose\nREBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval\nreasoning framework that integrates retrieval augmented generation with graph\nbased reasoning. CatRAG unifies dense retrieval and graph reasoning, supported\nby a hierarchical, category labeled knowledge graph enriched with semantic\nfeatures for domain alignment. A lightweight intent classifier routes queries\nto the appropriate retrieval modules, ensuring both factual accuracy and\ncontextual depth. We construct a regulation specific dataset and evaluate REBot\non classification and question answering tasks, achieving state of the art\nperformance with an F1 score of 98.89%. Finally, we implement a web application\nthat demonstrates the practical value of REBot in real world academic advising\nscenarios.", "AI": {"tldr": "REBot is an LLM-enabled advisory chatbot for academic regulation advising, powered by CatRAG, a hybrid retrieval+graph reasoning framework that fuses dense retrieval with a category-labeled knowledge graph. It achieves state-of-the-art results and is demonstrated via a web app.", "motivation": "Academic regulation advising needs domain-specific, policy-aligned regulatory resources to interpret institutional policies accurately. Pure LLMs risk factual errors and poor domain alignment without structured retrieval and reasoning.", "method": "Introduce CatRAG: a hybrid system combining retrieval augmented generation with graph-based reasoning. Build a hierarchical, category-labeled knowledge graph with semantic features for domain alignment. Use a lightweight intent classifier to route queries to relevant retrieval modules. Create a regulation-specific dataset and evaluate on classification and question answering tasks. Develop a web application to demonstrate real-world advising scenarios.", "result": "Achieves state-of-the-art performance with an F1 score of 98.89% on the evaluation tasks. Demonstrates practical value of REBot in real-world academic advising through a web interface.", "conclusion": "REBot effectively integrates retrieval and graph reasoning to deliver accurate, context-rich regulatory guidance for academic advising, with potential for real-world deployment."}}
{"id": "2510.02167", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.02167", "abs": "https://arxiv.org/abs/2510.02167", "authors": ["Sara Strakosova", "Petr Novak", "Petr Kadera"], "title": "Product Digital Twin Supporting End-of-life Phase of Electric Vehicle Batteries Utilizing Product-Process-Resource Asset Network", "comment": "This work has been submitted to the IEEE for possible publication. 6\n  pages, 4 figures", "summary": "In the context of the circular economy, products in their end-of-life phase\nshould be either remanufactured or recycled. Both of these processes are\ncrucial for sustainability and environmental conservation. However,\nmanufacturers often do not support these processes enough by not sharing\nrelevant data. This paper proposes use of a digital twin technology, which is\ncapable to help optimizing the disassembly processes to reduce ecological\nimpact and enhance sustainability. The proposed approach is demonstrated\nthrough a disassembly use-case of the product digital twin of an electric\nvehicle battery. By utilizing product digital twins, challenges associated with\nthe disassembly of electric vehicle batteries can be solved flexibly and\nefficiently for various battery types. As a backbone for the product digital\ntwin representation, the paper uses the paradigm of product-process-resource\nasset networks (PAN). Such networks enable to model relevant relationships\nacross products, production resources, manufacturing processes, and specific\nproduction operations that have to be done in the manufacturing phase of a\nproduct. This paper introduces a Bi-Flow Product-Process-Resource Asset Network\n(Bi-PAN) representation, which extends the PAN paradigm to cover not only the\nmanufacturing, but also the remanufacturing/recycling phase.", "AI": {"tldr": "A digital twin-based Bi-Flow PAN (Bi-PAN) framework for end-of-life optimization of products, demonstrated on EV battery disassembly to enable flexible remanufacturing/recycling across battery types.", "motivation": "End-of-life management requires efficient disassembly and data-sharing barriers hinder remanufacturing/recycling; a digital twin framework can model product-process-resource relationships across manufacturing and end-of-life phases to reduce ecological impact.", "method": "Extend PAN to Bi-PAN to cover remanufacturing/recycling; leverage product-process-resource asset networks; implement using product digital twins; apply to a disassembly use-case of EV battery; enable flexible handling of diverse battery types.", "result": "Feasibility demonstrated; the Bi-PAN framework can flexibly and efficiently solve disassembly challenges across battery types; shows potential to reduce ecological impact, though no quantitative metrics provided in abstract.", "conclusion": "Bi-PAN extends PAN to encompass end-of-life stages, enabling integrated modeling of remanufacturing/recycling in digital twins; supports sustainability in circular economy by improving data sharing and optimization of disassembly processes."}}
{"id": "2510.01767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01767", "abs": "https://arxiv.org/abs/2510.01767", "authors": ["Sheng-Hsiang Hung", "Ting-Yu Yen", "Wei-Fang Sun", "Simon See", "Shih-Hsuan Hung", "Hung-Kuo Chu"], "title": "LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has established itself as an efficient\nrepresentation for real-time, high-fidelity 3D scene reconstruction. However,\nscaling 3DGS to large and unbounded scenes such as city blocks remains\ndifficult. Existing divide-and-conquer methods alleviate memory pressure by\npartitioning the scene into blocks, but introduce new bottlenecks: (i)\npartitions suffer from severe load imbalance since uniform or heuristic splits\ndo not reflect actual computational demands, and (ii) coarse-to-fine pipelines\nfail to exploit the coarse stage efficiently, often reloading the entire model\nand incurring high overhead. In this work, we introduce LoBE-GS, a novel\nLoad-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers\nthe large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning\nmethod that reduces preprocessing from hours to minutes, an optimization-based\nstrategy that balances visible Gaussians -- a strong proxy for computational\nload -- across blocks, and two lightweight techniques, visibility cropping and\nselective densification, to further reduce training cost. Evaluations on\nlarge-scale urban and outdoor datasets show that LoBE-GS consistently achieves\nup to $2\\times$ faster end-to-end training time than state-of-the-art\nbaselines, while maintaining reconstruction quality and enabling scalability to\nscenes infeasible with vanilla 3DGS.", "AI": {"tldr": "LoBE-GS tackles scaling 3D Gaussian Splatting to large scenes by depth-aware partitioning, load balancing, and light-weight training tricks, achieving up to 2x faster training with maintained quality.", "motivation": "3D Gaussian Splatting (3DGS) offers real-time, high-fidelity 3D reconstruction but struggles with large-scale or unbounded scenes due to memory constraints and load imbalance. Existing divide-and-conquer methods introduce bottlenecks such as unbalanced partitions and inefficient coarse-to-fine pipelines that re-load the model.", "method": "Introduce depth-aware partitioning to reduce preprocessing time; an optimization-based strategy to balance visible Gaussians across blocks; and lightweight techniques\u2014visibility cropping and selective densification\u2014to lower training costs.", "result": "Evaluations on large-scale urban and outdoor datasets show LoBE-GS achieves up to 2\u00d7 faster end-to-end training than state-of-the-art baselines, while preserving reconstruction quality and enabling scalability to scenes infeasible for vanilla 3DGS.", "conclusion": "LoBE-GS provides a scalable, efficient large-scale 3DGS pipeline by addressing partitioning, load balancing, and training-efficiency bottlenecks."}}
{"id": "2510.01471", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01471", "abs": "https://arxiv.org/abs/2510.01471", "authors": ["Haotian Xiang", "Jinwen Xu", "Qin Lu"], "title": "Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation", "comment": null, "summary": "A plethora of applications entail solving black-box optimization problems\nwith high evaluation costs, including drug discovery, material design, as well\nas hyperparameter tuning. Toward finding the global optimum of such black-box\noptimization problems with sample efficiency, Bayesian optimization (BO) is a\ntheoretically elegant framework that relies on a probabilistic surrogate model\nso as to iteratively select the query point with well-balanced\nexploration-exploitation tradeoffs. The Gaussian process (GP), as the de-facto\nchoice for surrogate modeling, has achieved compelling performances for vanilla\nBO with low-dimensional continuous variables. However, GPs fall short in coping\nwith high-dimensional counterparts with {\\it irregular} variables (e.g.,\ncategorical, ordinal, etc.). To alleviate this, neural network-based surrogates\nhave been explored. Inspired by the powerful capabilities of LLMs, we adopt the\nLLM as the surrogate to model the mapping from the high-dimensional input\nvariables to the objective function. To adapt to the current problem, we\nleverage the low-rank adaptation (LoRA) to fine-tune the LLM parameters\ntogether with the posterior of a linear regression head via the variational\nBayesian last layer (VBLL) framework. The resulting LoRA-VBLL is not only\ncomputationally light compared to existing alternatives, but also admits\nrecursive updates. To automate the critical selection of the LoRA rank as well\nas other hyperparameters, a weighted ensemble (ENS) of LoRA-VBLL surrogates has\nbeen devised, which further accommodates continual update of the per-model\nweight and individual LoRA-VBLL parameters via recursive Bayes. Extensive\nexperimental results demonstrate the compelling performance of the proposed\n(ENS-)LoRA-VBLL approaches on various high-dimensional benchmarks and the\nreal-world molecular optimization tasks.", "AI": {"tldr": "A lightweight BO surrogate using LoRA-tuned LLMs with a variational Bayesian last layer (VBLL) and recursive weighted ensemble (ENS) to handle high-dimensional, irregular inputs, achieving strong sample efficiency on molecular optimization and other tasks.", "motivation": "Bayesian optimization with Gaussian processes struggles in high-dimensional, irregular spaces (categorical/ordinal variables). A scalable, flexible surrogate is needed. Leveraging large language models with efficient fine-tuning (LoRA) and Bayesian last-layer inference can provide a powerful, data-efficient mapping from inputs to objective values, while recursive Bayesian updates and ensemble weighting enable continual adaptation.", "method": "Fine-tune a large language model with Low-Rank Adaptation (LoRA); attach a Bayesian last-layer head trained via variational Bayesian last layer (VBLL); perform recursive Bayesian updates for the linear head; assemble a weighted ensemble (ENS) of multiple LoRA-VBLL surrogates with per-model weights and individual LoRA-VBLL parameters updated recursively; integrate the surrogate into a Bayesian optimization loop for querying next points.", "result": "Extensive experiments show strong, sample-efficient performance on high-dimensional benchmarks and real-world molecular optimization tasks, with the ensemble approach improving robustness and adaptation over single-model surrogates.", "conclusion": "LoRA-VBLL with ENS offers a lightweight, scalable surrogate for BO in high-dimensional irregular spaces, benefiting drug discovery and material design. The framework supports continual learning via recursive Bayesian updates; future work could automate hyperparameter decisions (e.g., LoRA rank, ensemble weights) and extend to broader problem classes."}}
{"id": "2510.01815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01815", "abs": "https://arxiv.org/abs/2510.01815", "authors": ["Clara Maathuis", "Kasper Cools"], "title": "Human-AI Teaming Co-Learning in Military Operations", "comment": "Submitted to Sensors + Imaging; presented on 18th of September\n  (Artificial Intelligence for Security and Defence Applications III)", "summary": "In a time of rapidly evolving military threats and increasingly complex\noperational environments, the integration of AI into military operations proves\nsignificant advantages. At the same time, this implies various challenges and\nrisks regarding building and deploying human-AI teaming systems in an effective\nand ethical manner. Currently, understanding and coping with them are often\ntackled from an external perspective considering the human-AI teaming system as\na collective agent. Nevertheless, zooming into the dynamics involved inside the\nsystem assures dealing with a broader palette of relevant multidimensional\nresponsibility, safety, and robustness aspects. To this end, this research\nproposes the design of a trustworthy co-learning model for human-AI teaming in\nmilitary operations that encompasses a continuous and bidirectional exchange of\ninsights between the human and AI agents as they jointly adapt to evolving\nbattlefield conditions. It does that by integrating four dimensions. First,\nadjustable autonomy for dynamically calibrating the autonomy levels of agents\ndepending on aspects like mission state, system confidence, and environmental\nuncertainty. Second, multi-layered control which accounts continuous oversight,\nmonitoring of activities, and accountability. Third, bidirectional feedback\nwith explicit and implicit feedback loops between the agents to assure a proper\ncommunication of reasoning, uncertainties, and learned adaptations that each of\nthe agents has. And fourth, collaborative decision-making which implies the\ngeneration, evaluation, and proposal of decisions associated with confidence\nlevels and rationale behind them. The model proposed is accompanied by concrete\nexemplifications and recommendations that contribute to further developing\nresponsible and trustworthy human-AI teaming systems in military operations.", "AI": {"tldr": "A conceptual design for a trustworthy co-learning model enabling bidirectional human-AI teaming in military operations, integrating adjustable autonomy, multi-layered control, bidirectional feedback, and collaborative decision-making, with exemplifications and practical recommendations.", "motivation": "Rapidly evolving military threats and complex environments necessitate effective, ethical AI integration in operations. Moving beyond external system-level views to internal dynamics of human-AI teams addresses multidimensional safety, responsibility, and robustness, enabling continuous adaptation to battlefield conditions.", "method": "Proposes a design framework\u2014the trustworthy co-learning model\u2014built around four dimensions: adjustable autonomy, multi-layered control, bidirectional feedback, and collaborative decision-making; features continuous exchanges of reasoning and uncertainties, dynamic calibration to mission state, system confidence, and environmental uncertainty; supported by concrete exemplifications and recommendations.", "result": "A concrete architectural/design framework for trustworthy human-AI teaming, including exemplifications and recommendations to advance responsible deployment in military operations.", "conclusion": "The framework offers a path toward more responsible, trustworthy, and robust human-AI collaboration in high-stakes military contexts by embedding co-learning and adaptable governance into the teaming model."}}
{"id": "2510.02178", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02178", "abs": "https://arxiv.org/abs/2510.02178", "authors": ["Jialin Gao", "Donghao Zhou", "Mingjian Liang", "Lihao Liu", "Chi-Wing Fu", "Xiaowei Hu", "Pheng-Ann Heng"], "title": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis", "comment": null, "summary": "3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets. While\nrecent LLM and VLM-based approaches offer improved semantic richness, they\noften lack robust and flexible refinement, resulting in suboptimal layouts. We\ndevelop DisCo-Layout, a novel framework that disentangles and coordinates\nphysical and semantic refinement. For independent refinement, our Semantic\nRefinement Tool (SRT) corrects abstract object relationships, while the\nPhysical Refinement Tool (PRT) resolves concrete spatial issues via a\ngrid-matching algorithm. For collaborative refinement, a multi-agent framework\nintelligently orchestrates these tools, featuring a planner for placement\nrules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout's state-of-the-art performance, generating\nrealistic, coherent, and generalizable 3D indoor layouts. Our code will be\npublicly available.", "AI": {"tldr": "DisCo-Layout presents a disentangled, collaborative refinement framework for 3D indoor layout synthesis, improving realism and generalization via separate semantic and physical refinement tools and a multi-agent planner.", "motivation": "Current 3D indoor layout methods struggle to generalize due to reliance on fixed datasets; LLM/VLM help with semantics but lack robust refinement; a system is needed to refine both semantic relationships and spatial feasibility.", "method": "Independent refinement: Semantic Refinement Tool (SRT) corrects abstract object relationships; Physical Refinement Tool (PRT) uses a grid-matching algorithm to fix concrete spatial issues. Collaborative refinement: multi-agent framework with a planner for placement rules, a designer for initial layouts, and an evaluator for assessment; orchestrates SRT and PRT.", "result": "Achieves state-of-the-art performance, producing realistic, coherent, and generalizable 3D indoor layouts; code will be released.", "conclusion": "DisCo-Layout demonstrates effective disentanglement and coordination of semantic and physical refinement to improve layout realism and generalization, via a scalable multi-agent approach."}}
{"id": "2510.01784", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01784", "abs": "https://arxiv.org/abs/2510.01784", "authors": ["Xiaofei Wu", "Guozhen Zhang", "Zhiyong Xu", "Yuan Zhou", "Qinglin Lu", "Xuming He"], "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation", "comment": null, "summary": "Long-form video generation presents a dual challenge: models must capture\nlong-range dependencies while preventing the error accumulation inherent in\nautoregressive decoding. To address these challenges, we make two\ncontributions. First, for dynamic context modeling, we propose MemoryPack, a\nlearnable context-retrieval mechanism that leverages both textual and image\ninformation as global guidance to jointly model short- and long-term\ndependencies, achieving minute-level temporal consistency. This design scales\ngracefully with video length, preserves computational efficiency, and maintains\nlinear complexity. Second, to mitigate error accumulation, we introduce Direct\nForcing, an efficient single-step approximating strategy that improves\ntraining-inference alignment and thereby curtails error propagation during\ninference. Together, MemoryPack and Direct Forcing substantially enhance the\ncontext consistency and reliability of long-form video generation, advancing\nthe practical usability of autoregressive video models.", "AI": {"tldr": "MemoryPack and Direct Forcing aim to improve long-form autoregressive video generation: MemoryPack for dynamic context via text+image guidance with linear complexity, and Direct Forcing for single-step training-inference alignment to reduce error propagation, yielding minute-level temporal consistency and more reliable long-form generation.", "motivation": "Long-form video generation requires capturing long-range dependencies while avoiding error accumulation; existing autoregressive methods struggle with efficiency and propagation errors as video length grows.", "method": "MemoryPack: learnable context-retrieval using textual and image cues as global guidance to jointly model short- and long-term dependencies with linear complexity, enabling minute-level coherence. Direct Forcing: a single-step approximating strategy that aligns training and inference to reduce error accumulation.", "result": "Improved context consistency and reliability for autoregressive video models; scalable to longer videos with linear complexity and minute-level temporal consistency.", "conclusion": "MemoryPack plus Direct Forcing substantially enhance long-form video generation practicality and reliability, addressing both dynamic context modeling and error accumulation."}}
{"id": "2510.01472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01472", "abs": "https://arxiv.org/abs/2510.01472", "authors": ["Hengyi Zhu", "Grace Li Zhang", "Shaoyi Huang"], "title": "PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search", "comment": null, "summary": "Hardware-Aware Neural Architecture Search (HW-NAS) requires joint\noptimization of accuracy and latency under device constraints. Traditional\nsupernet-based methods require multiple GPU days per dataset. Large Language\nModel (LLM)-driven approaches avoid training a large supernet and can provide\nquick feedback, but we observe an exploration bias: the LLM repeatedly proposes\nneural network designs within limited search space and fails to discover\narchitectures across different latency ranges in the entire search space. To\naddress this issue, we propose PEL-NAS: a search space Partitioned,\narchitecture prompt co-Evolutionary and LLM-driven Neural Architecture Search\nthat can generate neural networks with high accuracy and low latency with\nreduced search cost. Our proposed PEL-NAS has three key components: 1) a\ncomplexity-driven partitioning engine that divides the search space by\ncomplexity to enforce diversity and mitigate exploration bias; 2) an\nLLM-powered architecture prompt co-evolution operator, in which the LLM first\nupdates a knowledge base of design heuristics based on results from the\nprevious round, then performs a guided evolution algorithm on architectures\nwith prompts that incorporate this knowledge base. Prompts and designs improve\ntogether across rounds which avoids random guesswork and improve efficiency; 3)\na zero-cost predictor to avoid training a large number of candidates from\nscratch. Experimental results show that on HW-NAS-Bench, PEL-NAS can achieve\noverall higher HV, lower IGD, and up to 54% lower latency than baselines at\nsimilar accuracy. Meanwhile, the search cost drops from days to minutes\ncompared with traditional supernet baselines.", "AI": {"tldr": "PEL-NAS is a hardware-aware NAS framework that partitions the search space by complexity, uses LLM-guided prompt co-evolution, and a zero-cost predictor to achieve higher accuracy and lower latency with dramatically reduced search cost on HW-NAS-Bench.", "motivation": "To overcome exploration bias and high search cost in LLM-driven NAS for HW-NAS, and to enable efficient discovery of architectures across the latency spectrum.", "method": "Three components: (1) complexity-driven partitioning engine that divides the search space by complexity to promote diversity; (2) LLM-powered architecture prompt co-evolution where the LLM updates a knowledge base of design heuristics from prior rounds and then guides evolution with prompts incorporating this knowledge; (3) a zero-cost predictor to screen candidates without full training.", "result": "On HW-NAS-Bench, PEL-NAS achieves higher HV (Pareto hypervolume), lower IGD (distance to Pareto optimal set), and up to 54% lower latency at similar accuracy compared with baselines; search cost reduces from days to minutes relative to traditional supernet baselines.", "conclusion": "PEL-NAS effectively mitigates exploration bias, accelerates neural architecture search for hardware constraints, and delivers improved Pareto-optimal accuracy-latency trade-offs with substantially reduced search cost."}}
{"id": "2510.01833", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01833", "abs": "https://arxiv.org/abs/2510.01833", "authors": ["Zhihao Dou", "Qinjian Zhao", "Zhongwei Wan", "Dinggen Zhang", "Weida Wang", "Towsif Raiyan", "Benteng Chen", "Qingtao Pan", "Yang Ouyang", "Zhiqiang Gao", "Shufei Zhang", "Sumon Biswas"], "title": "Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning", "comment": "19 pages and 5 figures", "summary": "Large language models (LLMs) have demonstrated remarkable reasoning abilities\nin complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,\ndue to their autoregressive token-level generation, the reasoning process is\nlargely constrained to local decision-making and lacks global planning. This\nlimitation frequently results in redundant, incoherent, or inaccurate\nreasoning, which significantly degrades overall performance. Existing\napproaches, such as tree-based algorithms and reinforcement learning (RL),\nattempt to address this issue but suffer from high computational costs and\noften fail to produce optimal reasoning trajectories. To tackle this challenge,\nwe propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy\nOptimization PTA-GRPO, a two-stage framework designed to improve both\nhigh-level planning and fine-grained CoT reasoning. In the first stage, we\nleverage advanced LLMs to distill CoT into compact high-level guidance, which\nis then used for supervised fine-tuning (SFT). In the second stage, we\nintroduce a guidance-aware RL method that jointly optimizes the final output\nand the quality of high-level guidance, thereby enhancing reasoning\neffectiveness. We conduct extensive experiments on multiple mathematical\nreasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across\ndiverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and\nLLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently\nachieves stable and significant improvements across different models and tasks,\nvalidating its effectiveness and generalization.", "AI": {"tldr": "Two-stage Plan-Then-Action approach PTA-GRPO improves chain-of-thought reasoning by first distilling high-level guidance from LLMs for supervised fine-tuning, then using guidance-aware reinforcement learning to optimize both the final answer and the quality of the high-level guidance, yielding stable gains on math benchmarks across diverse models.", "motivation": "LLMs struggle with global planning due to autoregressive, token-level generation, leading to local, incoherent, or inaccurate reasoning. Existing tree/RL methods incur high cost and may not yield optimal reasoning trajectories.", "method": "Stage 1: Distill chain-of-thought (CoT) into compact high-level guidance with LLMs and fine-tune a model via supervised learning on that guidance. Stage 2: Apply guidance-aware reinforcement learning (Plan-Then-Action with Group Relative Policy Optimization) that jointly optimizes the final output and the quality of high-level guidance.", "result": "PTA-GRPO consistently delivers stable and significant improvements across multiple mathematical reasoning benchmarks (MATH, AIME2024, AIME2025, AMC) and across diverse base models (Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, LLaMA3.2-3B).", "conclusion": "The two-stage PTA-GRPO framework effectively enhances both high-level planning and fine-grained CoT reasoning, demonstrating strong generalization across models and tasks and addressing the limitations of purely autoregressive, locally-decided reasoning."}}
{"id": "2510.02248", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02248", "abs": "https://arxiv.org/abs/2510.02248", "authors": ["Yan Miao", "Ege Yuceel", "Georgios Fainekos", "Bardh Hoxha", "Hideki Okamoto", "Sayan Mitra"], "title": "Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0", "comment": null, "summary": "Visual policy design is crucial for aerial navigation. However,\nstate-of-the-art visual policies often overfit to a single track and their\nperformance degrades when track geometry changes. We develop FalconGym 2.0, a\nphotorealistic simulation framework built on Gaussian Splatting (GSplat) with\nan Edit API that programmatically generates diverse static and dynamic tracks\nin milliseconds. Leveraging FalconGym 2.0's editability, we propose a\nPerformance-Guided Refinement (PGR) algorithm, which concentrates visual\npolicy's training on challenging tracks while iteratively improving its\nperformance. Across two case studies (fixed-wing UAVs and quadrotors) with\ndistinct dynamics and environments, we show that a single visual policy trained\nwith PGR in FalconGym 2.0 outperforms state-of-the-art baselines in\ngeneralization and robustness: it generalizes to three unseen tracks with 100%\nsuccess without per-track retraining and maintains higher success rates under\ngate-pose perturbations. Finally, we demonstrate that the visual policy trained\nwith PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a\nquadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30\ntrials spanning two three-gate tracks and a moving-gate track.", "AI": {"tldr": "A single visual policy trained with Performance-Guided Refinement (PGR) in FalconGym 2.0 generalizes to unseen tracks and achieves strong sim-to-real transfer for aerial navigation.", "motivation": "Current visual policies overfit to track geometry and degrade with track changes; a photorealistic, editable simulator is needed to diversify training and improve generalization, robustness, and real-world transfer.", "method": "Develop FalconGym 2.0 (GSplat-based photorealistic simulation with an Edit API) to generate diverse static/dynamic tracks rapidly; apply Performance-Guided Refinement to focus training on challenging tracks and iteratively improve performance; evaluate on fixed-wing UAVs and quadrotors across varied dynamics/environments; compare to baselines; demonstrate zero-shot sim-to-real transfer to a quadrotor hardware.", "result": "A single policy trained with PGR outperforms baselines in generalization and robustness; generalizes to three unseen tracks with 100% success without per-track retraining; maintains higher success under gate-pose perturbations; achieves 98.6% real-world gate success (69/70) over 30 trials across two 3-gate tracks and a moving gate.", "conclusion": "PGR-enabled FalconGym 2.0 enables robust, generalizable visual policies for aerial navigation and practical sim-to-real transfer, reducing per-track retraining needs."}}
{"id": "2510.01829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01829", "abs": "https://arxiv.org/abs/2510.01829", "authors": ["Cornelius Schr\u00f6der", "Marius-Raphael Schl\u00fcter", "Markus Lienkamp"], "title": "Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving", "comment": null, "summary": "In autonomous systems, precise object detection and uncertainty estimation\nare critical for self-aware and safe operation. This work addresses confidence\ncalibration for the classification task of 3D object detectors. We argue that\nit is necessary to regard the calibration of the full predictive confidence\ndistribution over all classes and deduce a metric which captures the\ncalibration of dominant and secondary class predictions. We propose two\nauxiliary regularizing loss terms which introduce either calibration of the\ndominant prediction or the full prediction vector as a training goal. We\nevaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet\nand DSVT-Pillar and find that combining our loss term, which regularizes for\ncalibration of the full class prediction, and isotonic regression lead to the\nbest calibration of CenterPoint and PillarNet with respect to both dominant and\nsecondary class predictions. We further find that DSVT-Pillar can not be\njointly calibrated for dominant and secondary predictions using the same\nmethod.", "AI": {"tldr": "Calibrates the full predictive confidence distribution for 3D object detectors, not just the top class. Introduces two auxiliary losses to calibrate either the dominant class or the full prediction vector, evaluated with post-hoc methods and training-time losses across CenterPoint, PillarNet, and DSVT-Pillar. Best results come from combining full-vector calibration loss with isotonic regression for CenterPoint and PillarNet; DSVT-Pillar cannot be jointly calibrated for dominant and secondary predictions with the same method.", "motivation": "Autonomous systems require reliable confidence estimates for 3D object detection to support safe operation. Existing work often focuses on dominant-class calibration; this work argues for calibrating the entire predictive distribution and accounting for both dominant and secondary predictions.", "method": "Introduce two auxiliary regularizing loss terms that steer calibration of the dominant prediction and of the full prediction vector, respectively. Evaluate a range of post-hoc and training-time calibration methods on CenterPoint, PillarNet, and DSVT-Pillar. Combine full-vector calibration loss with isotonic regression for improved calibration.", "result": "Full-vector calibration loss plus isotonic regression yields the best calibration for CenterPoint and PillarNet on both dominant and secondary predictions. DSVT-Pillar cannot be jointly calibrated for dominant and secondary predictions with the same method.", "conclusion": "Calibrating the full predictive distribution is beneficial for at least some 3D detectors, especially when paired with isotonic regression. Model-specific calibration strategies may be required, as demonstrated by DSVT-Pillar's limitation in joint calibration using a single method."}}
{"id": "2510.01479", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01479", "abs": "https://arxiv.org/abs/2510.01479", "authors": ["Shriram Karpoora Sundara Pandian", "Ali Baheri"], "title": "Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets", "comment": null, "summary": "Offline reinforcement learning (RL) enables policy optimization from fixed\ndatasets, making it suitable for safety-critical applications where online\nexploration is infeasible. However, these datasets are often contaminated by\nadversarial poisoning, system errors, or low-quality samples, leading to\ndegraded policy performance in standard behavioral cloning (BC) and offline RL\nmethods. This paper introduces Density-Ratio Weighted Behavioral Cloning\n(Weighted BC), a robust imitation learning approach that uses a small, verified\nclean reference set to estimate trajectory-level density ratios via a binary\ndiscriminator. These ratios are clipped and used as weights in the BC objective\nto prioritize clean expert behavior while down-weighting or discarding\ncorrupted data, without requiring knowledge of the contamination mechanism. We\nestablish theoretical guarantees showing convergence to the clean expert policy\nwith finite-sample bounds that are independent of the contamination rate. A\ncomprehensive evaluation framework is established, which incorporates various\npoisoning protocols (reward, state, transition, and action) on continuous\ncontrol benchmarks. Experiments demonstrate that Weighted BC maintains\nnear-optimal performance even at high contamination ratios outperforming\nbaselines such as traditional BC, batch-constrained Q-learning (BCQ) and\nbehavior regularized actor-critic (BRAC).", "AI": {"tldr": "Density-Ratio Weighted Behavioral Cloning for robust offline imitation from contaminated data, using a small clean reference to estimate trajectory density ratios via a binary discriminator, weighting the BC loss; provides finite-sample guarantees independent of contamination rate; empirically robust against various poisoning protocols.", "motivation": "Offline RL datasets are frequently contaminated (adversarial poisoning, system errors, low-quality samples), which harms standard BC and offline RL methods. A robust imitation method is needed that can down-weight corrupted data using minimal clean supervision and without modeling the contamination process, which is essential for safety-critical applications.", "method": "Train a binary discriminator to distinguish clean reference trajectories from the full dataset to estimate trajectory-level density ratios. Clip these ratios and use them as weights in the BC objective to emphasize clean expert behavior while down-weighting or discarding corrupted data. The approach does not require knowledge of the contamination mechanism. Provide finite-sample theoretical guarantees showing convergence to the clean policy with bounds independent of contamination rate. Establish an evaluation framework with multiple poisoning protocols on continuous-control benchmarks.", "result": "Empirically, Weighted BC maintains near-optimal performance even at high contamination ratios and outperforms traditional BC, BCQ, and BRAC across various poisoning settings.", "conclusion": "Density-Ratio Weighted Behavioral Cloning offers a robust imitation learning framework for offline RL that leverages a small clean reference set to mitigate data contamination, with theoretical guarantees and strong empirical robustness across poisoning types."}}
{"id": "2510.01857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01857", "abs": "https://arxiv.org/abs/2510.01857", "authors": ["Claudio Fanconi", "Nicol\u00e1s Astorga", "Mihaela van der Schaar"], "title": "Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning", "comment": null, "summary": "We reframe and operationalise adversarial inverse reinforcement learning\n(IRL) to large language model reasoning, learning a dense, token-level reward\nmodel for process supervision directly from expert demonstrations rather than\nimitating style via supervised fine-tuning. The learned reasoning reward serves\ntwo complementary roles: (i) it provides step-level feedback to optimise a\nreasoning policy during training; and (ii) it functions at inference as a\ncritic to rerank sampled traces under fixed compute budgets. We demonstrate\nthat our approach prioritises correctness over surface form, yielding scores\nthat correlate with eventual answer validity and enabling interpretable\nlocalisation of errors within a trace. Empirically, on GSM8K with Llama3 and\nQwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a\nlearning signal to elicit reasoning, and (ii) predictive performance is\nimproved from reward-guided reranking (notably for Llama-based policies). By\nunifying training signals, inference-time selection, and token-level\ndiagnostics into a single reasoning reward, this work suggests reusable\nprocess-level rewards with broad potential to enhance multi-step reasoning in\nlanguage models.", "AI": {"tldr": "A dense, token-level adversarial IRL reward is learned for LLM reasoning, used both as training signal and as an inference-time critic to rerank traces, improving correctness and enabling error localisation. Demonstrated on GSM8K with Llama3 and Qwen2.5, showing reward-guided reranking boosts reasoning performance and provides interpretable diagnostics.", "motivation": "Current supervised fine-tuning emphasizes surface-form imitation and lacks explicit process-level supervision for multi-step reasoning. A reward that tracks reasoning steps could guide models toward correct, verifiable reasoning and enable interpretable error analysis.", "method": "Train a dense, token-level reward model via adversarial inverse reinforcement learning from expert demonstrations. Use the learned reward to (i) provide step-level feedback during training to optimize a reasoning policy, and (ii) act as a critic at inference-time to rerank candidate traces under fixed compute budgets. Evaluate on GSM8K with Llama3 and Qwen2.5 backbones, comparing reward-guided reranking to baseline approaches.", "result": "The dense reasoning rewards correlate with answer validity and enable localization of errors within traces. Reward-guided reranking improves predictive performance, especially for Llama-based policies, indicating the reward\u2019s effectiveness as a learning signal for reasoning.", "conclusion": "Unified reasoning rewards can serve as reusable process-level signals that enhance multi-step reasoning during both training and inference, with broad potential for improving accuracy and interpretability in LLM-based reasoning systems."}}
{"id": "2510.02252", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02252", "abs": "https://arxiv.org/abs/2510.02252", "authors": ["Joao Pedro Araujo", "Yanjie Ze", "Pei Xu", "Jiajun Wu", "C. Karen Liu"], "title": "Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking", "comment": null, "summary": "Humanoid motion tracking policies are central to building teleoperation\npipelines and hierarchical controllers, yet they face a fundamental challenge:\nthe embodiment gap between humans and humanoid robots. Current approaches\naddress this gap by retargeting human motion data to humanoid embodiments and\nthen training reinforcement learning (RL) policies to imitate these reference\ntrajectories. However, artifacts introduced during retargeting, such as foot\nsliding, self-penetration, and physically infeasible motion are often left in\nthe reference trajectories for the RL policy to correct. While prior work has\ndemonstrated motion tracking abilities, they often require extensive reward\nengineering and domain randomization to succeed. In this paper, we\nsystematically evaluate how retargeting quality affects policy performance when\nexcessive reward tuning is suppressed. To address issues that we identify with\nexisting retargeting methods, we propose a new retargeting method, General\nMotion Retargeting (GMR). We evaluate GMR alongside two open-source\nretargeters, PHC and ProtoMotions, as well as with a high-quality closed-source\ndataset from Unitree. Using BeyondMimic for policy training, we isolate\nretargeting effects without reward tuning. Our experiments on a diverse subset\nof the LAFAN1 dataset reveal that while most motions can be tracked, artifacts\nin retargeted data significantly reduce policy robustness, particularly for\ndynamic or long sequences. GMR consistently outperforms existing open-source\nmethods in both tracking performance and faithfulness to the source motion,\nachieving perceptual fidelity and policy success rates close to the\nclosed-source baseline. Website:\nhttps://jaraujo98.github.io/retargeting_matters. Code:\nhttps://github.com/YanjieZe/GMR.", "AI": {"tldr": "A new retargeting method, General Motion Retargeting (GMR), improves motion-tracking fidelity and policy robustness for humanoid RL, approaching closed-source performance without extensive reward tuning.", "motivation": "To bridge the embodiment gap between humans and humanoid robots, and to assess how retargeting quality impacts policy performance when reward tuning is minimized, addressing artifacts like foot sliding and self-penetration left in reference data.", "method": "Introduce General Motion Retargeting (GMR) and compare it with two open-source retargeters (PHC and ProtoMotions) and a high-quality closed-source Unitree dataset. Use BeyondMimic to train policies, isolating retargeting effects without reward tuning. Evaluate on a diverse subset of the LAFAN1 dataset.", "result": "GMR consistently outperforms open-source retargeters in tracking fidelity and faithfulness to source motion, achieving perceptual fidelity and policy success rates close to the closed-source baseline. Most motions can be tracked, but retargeting artifacts still reduce policy robustness for dynamic or long sequences.", "conclusion": "Retargeting quality significantly impacts policy performance and generalization. GMR mitigates artifacts more effectively than existing open-source tools, reducing reliance on reward engineering and bringing open-source retargeting closer to closed-source standards for humanoid motion imitation."}}
{"id": "2510.01841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01841", "abs": "https://arxiv.org/abs/2510.01841", "authors": ["Giyeol Kim", "Sooyoung Yang", "Jihyong Oh", "Myungjoo Kang", "Chanho Eom"], "title": "Leveraging Prior Knowledge of Diffusion Model for Person Search", "comment": null, "summary": "Person search aims to jointly perform person detection and re-identification\nby localizing and identifying a query person within a gallery of uncropped\nscene images. Existing methods predominantly utilize ImageNet pre-trained\nbackbones, which may be suboptimal for capturing the complex spatial context\nand fine-grained identity cues necessary for person search. Moreover, they rely\non a shared backbone feature for both person detection and re-identification,\nleading to suboptimal features due to conflicting optimization objectives. In\nthis paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a\nnovel framework that leverages a pre-trained diffusion model while eliminating\nthe optimization conflict between two sub-tasks. We analyze key properties of\ndiffusion priors and propose three specialized modules: (i) Diffusion-Guided\nRegion Proposal Network (DGRPN) for enhanced person localization, (ii)\nMulti-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and\n(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage\ntext-aligned diffusion features. DiffPS sets a new state-of-the-art on\nCUHK-SYSU and PRW.", "AI": {"tldr": "DiffPS uses a diffusion-model prior to address the two-task optimization conflict in person search, introducing three modules (DGRPN, MSFRN, SFAN) to improve localization, shape bias mitigation, and diffusion-feature integration, achieving state-of-the-art on CUHK-SYSU and PRW.", "motivation": "Current person search pipelines rely on ImageNet-pretrained backbones and share features for detection and re-id, which leads to suboptimal spatial context capture and conflicting optimization; there is a need to leverage richer priors and decouple tasks.", "method": "Introduce three modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. The framework leverages a pre-trained diffusion model to provide priors while avoiding optimization conflict between detection and re-id.", "result": "Sets a new state-of-the-art on two standard benchmarks (CUHK-SYSU and PRW).", "conclusion": "DiffPS demonstrates that diffusion priors can supply rich spatial and semantic cues for person search, enabling better localization and re-id. By decoupling the two sub-tasks and integrating diffusion features via specialized modules, it achieves superior performance, suggesting a promising direction for diffusion-based priors in vision-and-language guided or vision tasks. "}}
{"id": "2510.01494", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01494", "abs": "https://arxiv.org/abs/2510.01494", "authors": ["Isha Gupta", "Rylan Schaeffer", "Joshua Kazdan", "Ken Liu", "Sanmi Koyejo"], "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed", "comment": null, "summary": "The field of adversarial robustness has long established that adversarial\nexamples can successfully transfer between image classifiers and that text\njailbreaks can successfully transfer between language models (LMs). However, a\npair of recent studies reported being unable to successfully transfer image\njailbreaks between vision-language models (VLMs). To explain this striking\ndifference, we propose a fundamental distinction regarding the transferability\nof attacks against machine learning models: attacks in the input data-space can\ntransfer, whereas attacks in model representation space do not, at least not\nwithout geometric alignment of representations. We then provide theoretical and\nempirical evidence of this hypothesis in four different settings. First, we\nmathematically prove this distinction in a simple setting where two networks\ncompute the same input-output map but via different representations. Second, we\nconstruct representation-space attacks against image classifiers that are as\nsuccessful as well-known data-space attacks, but fail to transfer. Third, we\nconstruct representation-space attacks against LMs that successfully jailbreak\nthe attacked models but again fail to transfer. Fourth, we construct data-space\nattacks against VLMs that successfully transfer to new VLMs, and we show that\nrepresentation space attacks \\emph{can} transfer when VLMs' latent geometries\nare sufficiently aligned in post-projector space. Our work reveals that\nadversarial transfer is not an inherent property of all attacks but contingent\non their operational domain - the shared data-space versus models' unique\nrepresentation spaces - a critical insight for building more robust models.", "AI": {"tldr": "Adversarial transferability is domain-dependent: data-space attacks can transfer across models, but representation-space attacks generally do not unless latent geometries are aligned.", "motivation": "Explain why some adversarial attacks transfer between models and others do not, especially in vision-language models, and identify the role of attack space (data vs. representation) in transfer.", "method": "Provide a theoretical result in a simple setting where two networks share an input-output map but differ in representations; build representation-space attacks for image classifiers and LMs that fail to transfer; show data-space attacks on VLMs transfer and that representation-space transfer is possible only with aligned latent geometries.", "result": "Proves a fundamental distinction: input-data-space attacks can transfer across models, while representation-space attacks do not transfer unless representations are geometrically aligned. Representation-space attacks in classifiers and LMs fail to transfer; data-space attacks on VLMs transfer; transfer of representation-space attacks is possible when post-projector latent geometries are sufficiently aligned.", "conclusion": "Adversarial transfer is not universal; robustness must address both data-space and representation-space vulnerabilities, with attention to latent geometry alignment in model adapters and post-projectors."}}
{"id": "2510.01902", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01902", "abs": "https://arxiv.org/abs/2510.01902", "authors": ["Pawe\u0142 Parys", "Sairam Vaidya", "Taylor Berg-Kirkpatrick", "Loris D'Antoni"], "title": "Constrained Adaptive Rejection Sampling", "comment": null, "summary": "Language Models (LMs) are increasingly used in applications where generated\noutputs must satisfy strict semantic or syntactic constraints. Existing\napproaches to constrained generation fall along a spectrum: greedy constrained\ndecoding methods enforce validity during decoding but distort the LM's\ndistribution, while rejection sampling (RS) preserves fidelity but wastes\ncomputation by discarding invalid outputs. Both extremes are problematic in\ndomains such as program fuzzing, where both validity and diversity of samples\nare essential. We present Constrained Adaptive Rejection Sampling (CARS), an\napproach that strictly improves the sample-efficiency of RS without\ndistributional distortion. CARS begins with unconstrained LM sampling and\nadaptively rules out constraint-violating continuations by recording them in a\ntrie and subtracting their probability mass from future draws. This adaptive\npruning ensures that prefixes proven invalid are never revisited, acceptance\nrates improve monotonically, and the resulting samples exactly follow the\nconstrained distribution. In experiments on a variety of domains -- e.g.,\nprogram fuzzing and molecular generation -- CARS consistently achieves higher\nefficiency -- measured in the number of LM forward passes per valid sample --\nwhile also producing stronger sample diversity than both GCD and methods that\napproximate the LM's distribution.", "AI": {"tldr": "Constrained Adaptive Rejection Sampling (CARS) makes rejection sampling for constrained language generation more efficient and distribution-preserving by adaptively pruning invalid constraint prefixes with a trie, yielding better sample efficiency and diversity across domains.", "motivation": "There is a need to generate outputs that satisfy hard semantic/syntactic constraints (e.g., program fuzzing, molecular generation) without compromising the LM's distribution. Greedy constrained decoding distorts the distribution, while rejection sampling wastes computation by discarding invalid samples.", "method": "Start from unconstrained LM sampling, build and update a trie of constraint-violating prefixes, and subtract their probability mass from future draws. This adaptive pruning prevents revisiting invalid prefixes, monotonically increases acceptance rates, and yields samples that exactly follow the constrained distribution.", "result": "Across domains like program fuzzing and molecular generation, CARS achieves higher sample efficiency (fewer LM forward passes per valid sample) and stronger sample diversity than GCD and methods that approximate the LM distribution, while preserving the constrained distribution.", "conclusion": "CARS provides a distribution-preserving, more efficient approach to constrained generation, avoiding distortion of the LM distribution and delivering better efficiency and diversity in constrained sampling."}}
{"id": "2510.02268", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02268", "abs": "https://arxiv.org/abs/2510.02268", "authors": ["Tianchong Jiang", "Jingtian Ji", "Xiangshan Tan", "Jiading Fang", "Anand Bhattad", "Vitor Guizilini", "Matthew R. Walter"], "title": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning", "comment": "Code and project materials are available at\n  ripl.github.io/know_your_camera", "summary": "We study view-invariant imitation learning by explicitly conditioning\npolicies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we\nshow that conditioning on extrinsics significantly improves generalization\nacross viewpoints for standard behavior cloning policies, including ACT,\nDiffusion Policy, and SmolVLA. To evaluate policy robustness under realistic\nviewpoint shifts, we introduce six manipulation tasks in RoboSuite and\nManiSkill that pair \"fixed\" and \"randomized\" scene variants, decoupling\nbackground cues from camera pose. Our analysis reveals that policies without\nextrinsics often infer camera pose using visual cues from static backgrounds in\nfixed scenes; this shortcut collapses when workspace geometry or camera\nplacement shifts. Conditioning on extrinsics restores performance and yields\nrobust RGB-only control without depth. We release the tasks, demonstrations,\nand code at https://ripl.github.io/know_your_camera/ .", "AI": {"tldr": "Conditioning policies on camera extrinsics via Plucker embeddings improves view-invariant imitation learning and robustness to viewpoint shifts; introduces fixed vs randomized scene variants for evaluation; yields RGB-only robust control without depth; code released.", "motivation": "To achieve true view-invariant imitation learning, policies should not rely on background cues to infer camera pose; embedding extrinsic information into policies can align actions across viewpoints.", "method": "Encode per-pixel ray Plucker embeddings; condition policies such as ACT, Diffusion Policy, SmolVLA on extrinsics; create RoboSuite/ManiSkill tasks with fixed vs randomized scenes to test generalization; evaluate RGB-only control with and without depth.", "result": "Extrinsic conditioning significantly improves cross-view generalization; without extrinsics policies exploit static backgrounds to infer pose, failing when scenes vary; extrinsics restore performance and enable robust RGB-only control.", "conclusion": "Explicit extrinsic conditioning is a practical and effective strategy for view-invariant imitation learning; supports broader applicability and reduces reliance on background cues; release of tasks, demos, and code enhances reproducibility."}}
{"id": "2510.01912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01912", "abs": "https://arxiv.org/abs/2510.01912", "authors": ["Yi Ai", "Yuanhao Cai", "Yulun Zhang", "Xiaokang Yang"], "title": "Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction", "comment": null, "summary": "Hyperspectral imaging (HSI) provides rich spatial-spectral information but\nremains costly to acquire due to hardware limitations and the difficulty of\nreconstructing three-dimensional data from compressed measurements. Although\ncompressive sensing systems such as CASSI improve efficiency, accurate\nreconstruction is still challenged by severe degradation and loss of fine\nspectral details. We propose the Flow-Matching-guided Unfolding network (FMU),\nwhich, to our knowledge, is the first to integrate flow matching into HSI\nreconstruction by embedding its generative prior within a deep unfolding\nframework. To further strengthen the learned dynamics, we introduce a mean\nvelocity loss that enforces global consistency of the flow, leading to a more\nrobust and accurate reconstruction. This hybrid design leverages the\ninterpretability of optimization-based methods and the generative capacity of\nflow matching. Extensive experiments on both simulated and real datasets show\nthat FMU significantly outperforms existing approaches in reconstruction\nquality. Code and models will be available at https://github.com/YiAi03/FMU.", "AI": {"tldr": "Introduces FMU, a hybrid flow-matching-guided unfolding network for hyperspectral image reconstruction from compressed measurements, combining a flow-based generative prior with deep unfolding and a mean velocity loss to enforce global flow consistency; claims strong reconstruction gains and reproducibility through released code.", "motivation": "HSI provides rich spatial-spectral information but is costly to acquire and reconstruct from compressed measurements is challenging; traditional compressive sensing and optimization struggle with degradation and loss of spectral details. A hybrid approach that leverages a strong generative prior within an interpretable unfolding framework could improve reconstruction quality and robustness.", "method": "FMU embeds a flow-matching generative prior into a deep unfolding network for HSI reconstruction, guiding iterative updates via learned dynamics. A mean velocity loss enforces global consistency of the flow, enhancing robustness and accuracy of reconstructions.", "result": "Experiments on simulated and real datasets show FMU significantly outperforms existing methods in reconstruction quality; code and models will be released for reproducibility (GitHub link provided).", "conclusion": "Combining flow-matching priors with optimization-inspired unfolding yields robust, high-quality HSI reconstructions from compressed measurements, highlighting the value of integrating generative priors with interpretable frameworks."}}
{"id": "2510.01499", "categories": ["cs.LG", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.01499", "abs": "https://arxiv.org/abs/2510.01499", "authors": ["Rui Ai", "Yuqi Pan", "David Simchi-Levi", "Milind Tambe", "Haifeng Xu"], "title": "Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information", "comment": null, "summary": "With the rapid progress of multi-agent large language model (LLM) reasoning,\nhow to effectively aggregate answers from multiple LLMs has emerged as a\nfundamental challenge. Standard majority voting treats all answers equally,\nfailing to consider latent heterogeneity and correlation across models. In this\nwork, we design two new aggregation algorithms called Optimal Weight (OW) and\nInverse Surprising Popularity (ISP), leveraging both first-order and\nsecond-order information. Our theoretical analysis shows these methods provably\nmitigate inherent limitations of majority voting under mild assumptions,\nleading to more reliable collective decisions. We empirically validate our\nalgorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as\nUltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all\ncases, our methods consistently outperform majority voting, offering both\npractical performance gains and conceptual insights for the design of robust\nmulti-agent LLM pipelines.", "AI": {"tldr": "Proposes Optimal Weight (OW) and Inverse Surprising Popularity (ISP) as aggregation methods for multi-agent LLM reasoning, using first- and second-order information to improve over majority voting; shows theoretical guarantees and empirical gains across synthetic data, UltraFeedback, MMLU, and ARMMAN healthcare tasks.", "motivation": "Majority voting treats all model outputs equally and ignores latent heterogeneity and correlations among models, which can degrade the reliability of collective decisions in multi-agent LLM systems.", "method": "Introduce two aggregation algorithms, OW and ISP, that leverage first-order and second-order statistics to weight model outputs. Provide theoretical analysis under mild assumptions showing mitigation of majority voting limitations. Validate empirically on synthetic data, standard LLM fine-tuning benchmarks (UltraFeedback, MMLU), and a real-world healthcare setting (ARMMAN).", "result": "OW and ISP consistently outperform majority voting across diverse tasks, delivering practical performance gains and offering conceptual insights for designing robust multi-agent LLM pipelines.", "conclusion": "The work contributes robust aggregation strategies for ensemble LLM reasoning, combining provable guarantees with empirical validation, and offers design guidance for future multi-agent LLM systems."}}
{"id": "2510.01924", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.01924", "abs": "https://arxiv.org/abs/2510.01924", "authors": ["Crystal Qian", "Aaron Parisi", "Cl\u00e9mentine Bouleau", "Vivian Tsai", "Ma\u00ebl Lebreton", "Lucas Dixon"], "title": "To Mask or to Mirror: Human-AI Alignment in Collective Reasoning", "comment": null, "summary": "As large language models (LLMs) are increasingly used to model and augment\ncollective decision-making, it is critical to examine their alignment with\nhuman social reasoning. We present an empirical framework for assessing\ncollective alignment, in contrast to prior work on the individual level. Using\nthe Lost at Sea social psychology task, we conduct a large-scale online\nexperiment (N=748), randomly assigning groups to leader elections with either\nvisible demographic attributes (e.g. name, gender) or pseudonymous aliases. We\nthen simulate matched LLM groups conditioned on the human data, benchmarking\nGemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some\nmirror human biases; others mask these biases and attempt to compensate for\nthem. We empirically demonstrate that human-AI alignment in collective\nreasoning depends on context, cues, and model-specific inductive biases.\nUnderstanding how LLMs align with collective human behavior is critical to\nadvancing socially-aligned AI, and demands dynamic benchmarks that capture the\ncomplexities of collective reasoning.", "AI": {"tldr": "An empirical framework to evaluate how well LLMs align with collective human reasoning, using a large-scale social task to compare how different models reflect or mask human biases in group decision-making.", "motivation": "To extend alignment research from individuals to groups, addressing how LLMs participate in collective decision-making and whether they reproduce, mask, or compensate for human biases.", "method": "Conducted a large online experiment (N=748 groups) on the Lost at Sea task, randomly assigning groups to leader elections with either visible demographic attributes or pseudonymous aliases, then simulated matched LLM groups conditioned on the human data and benchmarked multiple models (Gemini 2.5, GPT-4.1, Claude Haiku 3.5, Gemma 3).", "result": "LLM behaviors diverge: some mirror human biases, others mask or compensate for biases. Alignment with collective human behavior is context- and model-dependent, influenced by cues and inductive biases.", "conclusion": "Dynamic, context-sensitive benchmarks are required to properly assess socially-aligned AI in collective reasoning; understanding collective alignment is essential as LLMs play a growing role in group decision-making."}}
{"id": "2510.02298", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02298", "abs": "https://arxiv.org/abs/2510.02298", "authors": ["Wenye Yu", "Jun Lv", "Zixi Ying", "Yang Jin", "Chuan Wen", "Cewu Lu"], "title": "ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation", "comment": null, "summary": "Imitation learning has shown promise in learning from large-scale real-world\ndatasets. However, pretrained policies usually perform poorly without\nsufficient in-domain data. Besides, human-collected demonstrations entail\nsubstantial labour and tend to encompass mixed-quality data and redundant\ninformation. As a workaround, human-in-the-loop systems gather domain-specific\ndata for policy post-training, and exploit closed-loop policy feedback to offer\ninformative guidance, but usually require full-time human surveillance during\npolicy rollout. In this work, we devise ARMADA, a multi-robot deployment and\nadaptation system with human-in-the-loop shared control, featuring an\nautonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA\nenables paralleled policy rollout and requests human intervention only when\nnecessary, significantly reducing reliance on human supervision. Hence, ARMADA\nenables efficient acquisition of in-domain data, and leads to more scalable\ndeployment and faster adaptation to new scenarios. We evaluate the performance\nof ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on\naverage, surpassing prior state-of-the-art failure detection approaches by over\n20%. Besides, ARMADA manifests more than 4$\\times$ increase in success rate and\ngreater than 2$\\times$ reduction in human intervention rate over multiple\nrounds of policy rollout and post-training, compared to previous\nhuman-in-the-loop learning methods.", "AI": {"tldr": "Proposes ARMADA, a multi-robot deployment framework with FLOAT for autonomous failure detection, enabling scalable, low-supervision imitation learning. It reduces human intervention while improving performance across tasks.", "motivation": "Imitation learning requires abundant in-domain data; collecting high-quality demonstrations is expensive and time-consuming. Human-in-the-loop systems currently need continuous supervision and struggle with data quality and scalability.", "method": "Introduce ARMADA, a multi-robot deployment and adaptation system with human-in-the-loop shared control and an autonomous online failure detector FLOAT. FLOAT allows parallel rollout and only requests human input when needed, enabling efficient data acquisition and rapid adaptation.", "result": "FLOAT achieves ~95% accuracy on average, >20% improvement over SOTA failure detection. ARMADA yields >4x higher success rate and >2x lower human intervention rate across multiple rollout/post-training rounds on four real-world tasks.", "conclusion": "ARMADA enables scalable deployment and faster adaptation to new scenarios by reducing reliance on continuous human supervision and facilitating efficient in-domain data collection."}}
{"id": "2510.01914", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP", "68T07, 68U10", "I.4.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2510.01914", "abs": "https://arxiv.org/abs/2510.01914", "authors": ["Wei-Lung Mao", "Chun-Chi Wang", "Po-Heng Chou", "Yen-Ting Liu"], "title": "Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models", "comment": "12 pages, 16 figures, 7 tables, and published in IEEE Sensors Journal", "summary": "Since the defect detection of conventional industry components is\ntime-consuming and labor-intensive, it leads to a significant burden on quality\ninspection personnel and makes it difficult to manage product quality. In this\npaper, we propose an automated defect detection system for the dual in-line\npackage (DIP) that is widely used in industry, using digital camera optics and\na deep learning (DL)-based model. The two most common defect categories of DIP\nare examined: (1) surface defects, and (2) pin-leg defects. However, the lack\nof defective component images leads to a challenge for detection tasks. To\nsolve this problem, the ConSinGAN is used to generate a suitable-sized dataset\nfor training and testing. Four varieties of the YOLO model are investigated\n(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.\nThe proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in\naccuracy of 95.50\\%, detection time of 285 ms, and is far superior to\nthreshold-based approaches. In addition, the supervisory control and data\nacquisition (SCADA) system is developed, and the associated sensor architecture\nis described. The proposed automated defect detection can be easily established\nwith numerous types of defects or insufficient defect data.", "AI": {"tldr": "Proposes an automated DIP defect detection system using YOLOv7 with ConSinGAN data augmentation; achieves 95.5% accuracy and 285 ms per detection, addressing defect-data scarcity and enabling SCADA-integrated deployment.", "motivation": "Defect detection in manufacturing is time-consuming and labor-intensive, and there is a lack of defective component images, making automated detection essential for scalable quality control of DIP components.", "method": "Develop a DL-based detection pipeline using digital camera optics; generate defect images with ConSinGAN to augment training data; evaluate four YOLO variants (v3, v4, v7, v9) with and without ConSinGAN; identify YOLOv7 with ConSinGAN as best; also design and describe a supervisory control and data acquisition (SCADA) system and sensor architecture.", "result": "YOLOv7 with ConSinGAN achieves 95.50% accuracy and 285 ms detection time, outperforming threshold-based approaches; effective for both surface and pin-leg DIP defects; data augmentation enables detection with limited real defect samples.", "conclusion": "Automated DIP defect detection is feasible and easily deployable across different defect types, particularly when using ConSinGAN-augmented data with YOLOv7, and can be integrated into SCADA systems for industrial use."}}
{"id": "2510.01508", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01508", "abs": "https://arxiv.org/abs/2510.01508", "authors": ["Will Y. Zou", "Jean Feng", "Alexandre Kalimouttou", "Jennifer Yuntong Zhang", "Christopher W. Seymour", "Romain Pirracchio"], "title": "Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control", "comment": "11 pages, 5 figures. Neurips 2025 Workshop Learning from Time Series\n  for Health", "summary": "Reinforcement learning (RL) applications in Clinical Decision Support Systems\n(CDSS) frequently encounter skepticism from practitioners regarding inoperable\ndosing decisions. We address this challenge with an end-to-end approach for\nlearning optimal drug dosing and control policies for dual vasopressor\nadministration in intensive care unit (ICU) patients with septic shock. For\nrealistic drug dosing, we apply action space design that accommodates discrete,\ncontinuous, and directional dosing strategies in a system that combines offline\nconservative Q-learning with a novel recurrent modeling in a replay buffer to\ncapture temporal dependencies in ICU time-series data. Our comparative analysis\nof norepinephrine dosing strategies across different action space formulations\nreveals that the designed action spaces improve interpretability and facilitate\nclinical adoption while preserving efficacy. Empirical results1 on eICU and\nMIMIC demonstrate that action space design profoundly influences learned\nbehavioral policies. The proposed methods achieve improved patient outcomes of\nover 15% in survival improvement probability, while aligning with established\nclinical protocols.", "AI": {"tldr": "Action-space design with offline conservative Q-learning and recurrent replay improves norepinephrine dosing policies for septic shock in ICU, yielding interpretability and >15% survival improvement in eICU/MIMIC data.", "motivation": "Reinforcement learning in clinical decision support often faces skepticism due to safety and interpretability concerns. This work aims to enhance adoption by crafting action spaces and safe learning methods for dual vasopressor dosing in septic shock.", "method": "An end-to-end offline RL approach using conservative Q-learning, a recurrent modeling component in the replay buffer to capture temporal ICU time-series dependencies, and action-space design that supports discrete, continuous, and directional dosing. Evaluation across norepinephrine dosing strategies on eICU and MIMIC datasets.", "result": "Found that action-space design profoundly shapes learned policies, improving interpretability and clinical adoption while preserving efficacy. Empirical results show >15% improvement in survival probability and alignment with established clinical protocols.", "conclusion": "Designing appropriate action spaces is critical for deployable RL-based CDSS dosing policies; the proposed offline RL framework yields safer, more interpretable, and more effective dosing strategies in septic shock care."}}
{"id": "2510.02027", "categories": ["cs.AI", "cs.ET", "68T27, 03B42, 91A20, 62H20", "I.2.4; H.5.3; J.7; K.4.3"], "pdf": "https://arxiv.org/pdf/2510.02027", "abs": "https://arxiv.org/abs/2510.02027", "authors": ["Khalid M. Saqr"], "title": "Zero-shot reasoning for simulating scholarly peer-review", "comment": null, "summary": "The scholarly publishing ecosystem faces a dual crisis of unmanageable\nsubmission volumes and unregulated AI, creating an urgent need for new\ngovernance models to safeguard scientific integrity. The traditional human-only\npeer review regime lacks a scalable, objective benchmark, making editorial\nprocesses opaque and difficult to audit. Here we investigate a deterministic\nsimulation framework that provides the first stable, evidence-based standard\nfor evaluating AI-generated peer review reports. Analyzing 352 peer-review\nsimulation reports, we identify consistent system state indicators that\ndemonstrate its reliability. First, the system is able to simulate calibrated\neditorial judgment, with 'Revise' decisions consistently forming the majority\noutcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt\nto field-specific norms, rising to 45% in Health Sciences. Second, it maintains\nunwavering procedural integrity, enforcing a stable 29% evidence-anchoring\ncompliance rate that remains invariant across diverse review tasks and\nscientific domains. These findings demonstrate a system that is predictably\nrule-bound, mitigating the stochasticity of generative AI. For the scientific\ncommunity, this provides a transparent tool to ensure fairness; for publishing\nstrategists, it offers a scalable instrument for auditing workflows, managing\nintegrity risks, and implementing evidence-based governance. The framework\nrepositions AI as an essential component of institutional accountability,\nproviding the critical infrastructure to maintain trust in scholarly\ncommunication.", "AI": {"tldr": "A deterministic simulation framework for evaluating AI-generated peer reviews yields stable, evidence-based governance indicators\u2014predictable editorial outcomes and a fixed compliance rate\u2014enabling scalable auditing of scholarly integrity.", "motivation": "Growing submission volumes and unregulated AI threaten the integrity and auditability of peer review; there is a need for an objective, scalable benchmark to evaluate AI-generated reviews.", "method": "A deterministic simulation framework analyzed 352 peer-review simulation reports to identify reliable system-state indicators. Findings include a majority 'Revise' rate (>50%) across disciplines, field-sensitive 'Reject' rates (up to ~45% in Health Sciences), and a stable 29% evidence-anchoring compliance rate invariant to task/domain.", "result": "The framework exhibits consistent, rule-bound behavior, reducing AI stochasticity and providing a transparent, scalable tool for fairness auditing and governance in scholarly publishing.", "conclusion": "The framework positions AI as part of institutional accountability, offering infrastructure to uphold trust in scholarly communication and to manage integrity risks across publishing workflows."}}
{"id": "2510.01934", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01934", "abs": "https://arxiv.org/abs/2510.01934", "authors": ["Guangyao Zhai", "Yue Zhou", "Xinyan Deng", "Lars Heckler", "Nassir Navab", "Benjamin Busam"], "title": "Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors", "comment": "23 pages, 13 figures. Code is available at\n  \\url{https://github.com/ymxlzgy/FoundAD}", "summary": "Few-shot anomaly detection streamlines and simplifies industrial safety\ninspection. However, limited samples make accurate differentiation between\nnormal and abnormal features challenging, and even more so under\ncategory-agnostic conditions. Large-scale pre-training of foundation visual\nencoders has advanced many fields, as the enormous quantity of data helps to\nlearn the general distribution of normal images. We observe that the anomaly\namount in an image directly correlates with the difference in the learnt\nembeddings and utilize this to design a few-shot anomaly detector termed\nFoundAD. This is done by learning a nonlinear projection operator onto the\nnatural image manifold. The simple operator acts as an effective tool for\nanomaly detection to characterize and identify out-of-distribution regions in\nan image. Extensive experiments show that our approach supports multi-class\ndetection and achieves competitive performance while using substantially fewer\nparameters than prior methods. Backed up by evaluations with multiple\nfoundation encoders, including fresh DINOv3, we believe this idea broadens the\nperspective on foundation features and advances the field of few-shot anomaly\ndetection.", "AI": {"tldr": "FoundAD learns a nonlinear projection onto the natural image manifold to detect anomalies from few examples, leveraging pre-trained foundation encoders. It enables multi-class anomaly detection with far fewer parameters and competitive performance.", "motivation": "Industrial safety inspection often has scarce labeled anomaly data, and category-agnostic settings make differentiation between normal and abnormal features tough. Large pre-trained encoders help model the distribution of normal images, motivating a parameter-efficient detector that can work with few shots.", "method": "A nonlinear projection operator is learned that maps embeddings onto the natural image manifold. The anomaly amount in an image is correlated with differences in the learned embeddings, and the projection operator identifies out-of-distribution regions.", "result": "The approach supports multi-class anomaly detection and achieves competitive performance while using substantially fewer parameters than prior methods. It is validated across multiple foundation encoders, including DINOv3.", "conclusion": "FoundAD broadens the use of foundation features in few-shot anomaly detection, offering an efficient and effective tool for detecting anomalies in industrial inspection with limited data."}}
{"id": "2510.01510", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01510", "abs": "https://arxiv.org/abs/2510.01510", "authors": ["Jinwoo Kim", "Xingyue Huang", "Krzysztof Olejniczak", "Kyungbin Min", "Michael Bronstein", "Seunghoon Hong", "\u0130smail \u0130lkan Ceylan"], "title": "Flock: A Knowledge Graph Foundation Model via Learning on Random Walks", "comment": null, "summary": "We study the problem of zero-shot link prediction on knowledge graphs (KGs),\nwhich requires models to generalize over novel entities and novel relations.\nKnowledge graph foundation models (KGFMs) address this task by enforcing\nequivariance over both nodes and relations, learning from structural properties\nof nodes and relations, which are then transferable to novel graphs with\nsimilar structural properties. However, the conventional notion of\ndeterministic equivariance imposes inherent limits on the expressive power of\nKGFMs, preventing them from distinguishing structurally similar but\nsemantically distinct relations. To overcome this limitation, we introduce\nprobabilistic node-relation equivariance, which preserves equivariance in\ndistribution while incorporating a principled randomization to break symmetries\nduring inference. Building on this principle, we present Flock, a KGFM that\niteratively samples random walks, encodes them into sequences via a recording\nprotocol, embeds them with a sequence model, and aggregates representations of\nnodes and relations via learned pooling. Crucially, Flock respects\nprobabilistic node-relation equivariance and is a universal approximator for\nisomorphism-invariant link-level functions over KGs. Empirically, Flock\nperfectly solves our new diagnostic dataset Petals where current KGFMs fail,\nand achieves state-of-the-art performances on entity- and relation prediction\ntasks on 54 KGs from diverse domains.", "AI": {"tldr": "Probabilistic node-relation (N-R) equivariance unlocks expressiveness in KG foundation models by preserving equivariance in distribution and breaking symmetries during inference. The paper introduces Flock, a KGFM that samples random walks, records sequences, encodes via a sequence model, and learns pooling to aggregate node/relation representations; it is a universal approximator for isomorphism-invariant link-level functions and achieves state-of-the-art on 54 knowledge graphs, including solving the Petals diagnostic dataset.", "motivation": "Deterministic equivariance in existing KG foundation models constrains expressiveness, preventing the model from distinguishing structurally similar but semantically distinct relations. A probabilistic approach aims to maintain transferable equivariance while enabling discrimination in inference, improving zero-shot link prediction across novel entities and relations.", "method": "Introduce probabilistic node-relation equivariance. Develop Flock: iteratively sample random walks on the KG, encode them into sequences using a recording protocol, embed sequences with a sequence model, and aggregate node/relation representations via learned pooling. Ensure the model respects probabilistic N-R equivariance and acts as a universal approximator for isomorphism-invariant link-level functions.", "result": "Flock serves as a universal approximator for isomorphism-invariant link-level functions on KGs and achieves state-of-the-art results on multiple benchmarks, including solving the Petals diagnostic dataset where prior KGFMs fail and surpassing existing methods on entity and relation prediction across 54 KGs.", "conclusion": "Probabilistic node-relation equivariance, coupled with the Flock architecture, enhances expressive power and generalization for zero-shot link prediction on KGs, enabling perfect diagnostic performance on Petals and strong, broad performance across diverse knowledge graphs."}}
{"id": "2510.02060", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02060", "abs": "https://arxiv.org/abs/2510.02060", "authors": ["Sanghyu Yoon", "Dongmin Kim", "Suhee Yoon", "Ye Seul Sim", "Seungdong Yoa", "Hye-Seung Cho", "Soonyoung Lee", "Hankook Lee", "Woohyung Lim"], "title": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection", "comment": "9 pages, 4 figures", "summary": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD.", "AI": {"tldr": "Restores textual semantics in tabular anomaly detection by enriching benchmarks with structured metadata and introducing a zero-shot LLM framework, showing semantic context improves detection and interpretability.", "motivation": "Current tabular anomaly detection benchmarks provide only raw data points and lack semantic context; anomaly definitions rely on domain knowledge. This limits research flexibility and the ability to leverage domain knowledge.", "method": "Curate 20 tabular datasets enriched with structured textual metadata describing features and domain context; implement state-of-the-art anomaly detection approaches including classical, deep learning, and LLM-based methods; introduce a zero-shot LLM framework that uses semantic context without task-specific training.", "result": "Semantic context improves detection performance and interpretability; ReTabAD provides a benchmark resource for systematic exploration of context-aware anomaly detection.", "conclusion": "Textual metadata are valuable for context-aware AD; the benchmark enables broader research on leveraging domain knowledge and paves the way for future work in this area."}}
{"id": "2510.01948", "categories": ["cs.CV", "68T45", "I.2.10"], "pdf": "https://arxiv.org/pdf/2510.01948", "abs": "https://arxiv.org/abs/2510.01948", "authors": ["Fabio Montello", "Ronja G\u00fcldenring", "Lazaros Nalpantidis"], "title": "ClustViT: Clustering-based Token Merging for Semantic Segmentation", "comment": "Submitted to IEEE", "summary": "Vision Transformers can achieve high accuracy and strong generalization\nacross various contexts, but their practical applicability on real-world\nrobotic systems is limited due to their quadratic attention complexity. Recent\nworks have focused on dynamically merging tokens according to the image\ncomplexity. Token merging works well for classification but is less suited to\ndense prediction. We propose ClustViT, where we expand upon the Vision\nTransformer (ViT) backbone and address semantic segmentation. Within our\narchitecture, a trainable Cluster module merges similar tokens along the\nnetwork guided by pseudo-clusters from segmentation masks. Subsequently, a\nRegenerator module restores fine details for downstream heads. Our approach\nachieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different\ndatasets, with comparable segmentation accuracy. Our code and models will be\nmade publicly available.", "AI": {"tldr": "ClustViT extends ViT for segmentation by a trainable Cluster module that merges similar tokens guided by pseudo-clusters from segmentation masks, followed by a Regenerator to restore details, achieving significant compute reductions with comparable accuracy.", "motivation": "Vision Transformers have high accuracy but quadratic attention complexity, hindering real-world robotic deployment. Token merging helps classification but is less suited for dense prediction; thus a segmentation-friendly, efficient ViT variant is needed.", "method": "Introduce a trainable Cluster module that merges tokens along the network guided by pseudo-clusters from segmentation masks, then use a Regenerator module to recover fine details for downstream segmentation heads, enabling efficient dense prediction.", "result": "On three datasets, ClustViT achieves up to 2.18x fewer GFLOPs and 1.64x faster inference with comparable segmentation accuracy.", "conclusion": "ClustViT demonstrates an efficient, deployable ViT-based segmentation approach that reduces computation while maintaining accuracy; code and models will be released."}}
{"id": "2510.01520", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01520", "abs": "https://arxiv.org/abs/2510.01520", "authors": ["Hossein Sholehrasa", "Xuan Xu", "Doina Caragea", "Jim E. Riviere", "Majid Jaberi-Douraki"], "title": "Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties", "comment": null, "summary": "The safe use of pharmaceuticals in food-producing animals is vital to protect\nanimal welfare and human food safety. Adverse events (AEs) may signal\nunexpected pharmacokinetic or toxicokinetic effects, increasing the risk of\nviolative residues in the food chain. This study introduces a predictive\nframework for classifying outcomes (Death vs. Recovery) using ~1.28 million\nreports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary\nMedicine. A preprocessing pipeline merged relational tables and standardized\nAEs through VeDDRA ontologies. Data were normalized, missing values imputed,\nand high-cardinality features reduced; physicochemical drug properties were\nintegrated to capture chemical-residue links. We evaluated supervised models,\nincluding Random Forest, CatBoost, XGBoost, ExcelFormer, and large language\nmodels (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as\nundersampling and oversampling, with a focus on prioritizing recall for fatal\noutcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,\nachieving precision, recall, and F1-scores of 0.95. Incorporating Average\nUncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved\nminority-class detection, particularly in ExcelFormer and XGBoost.\nInterpretability via SHAP identified biologically plausible predictors,\nincluding lung, heart, and bronchial disorders, animal demographics, and drug\nphysicochemical properties. These features were strongly linked to fatal\noutcomes. Overall, the framework shows that combining rigorous data\nengineering, advanced machine learning, and explainable AI enables accurate,\ninterpretable predictions of veterinary safety outcomes. The approach supports\nFARAD's mission by enabling early detection of high-risk drug-event profiles,\nstrengthening residue risk assessment, and informing regulatory and clinical\ndecision-making.", "AI": {"tldr": "A large-scale ML framework using OpenFDA CVM data to predict fatal vs recovery outcomes in veterinary drug-event reports, achieving high precision/recall with CatBoost and ensemble methods; enhanced minority detection with AUM pseudo-labeling; interpretable via SHAP.", "motivation": "Protect animal welfare and human food safety by detecting adverse drug-event signals early to prevent violative residues; address data sparsity, high-cardinality features, and class imbalance in veterinary safety data.", "method": "Data preprocessing: merge relational tables, standardize AEs with VeDDRA; normalize, impute missing values, reduce high-cardinality features; integrate physicochemical drug properties; train supervised models (RF, CatBoost, XGBoost, ExcelFormer, LLMs Gemma 3-27B, Phi 3-12B); address class imbalance with undersampling/oversampling; use ensemble methods (Voting, Stacking); apply AUM-based pseudo-labeling to uncertain/minority cases; interpret with SHAP.", "result": "CatBoost and ensemble methods achieved precision, recall, and F1 around 0.95; AUM-based pseudo-labeling improved minority-class detection, notably for ExcelFormer and XGBoost; SHAP highlighted lung/heart/bronchial disorders, demographics, and drug properties as key predictors for fatal outcomes.", "conclusion": "Demonstrates that robust data engineering, advanced ML, and explainable AI enable accurate, interpretable veterinary safety predictions; supports FARAD's mission for early high-risk profile detection, stronger residue risk assessment, and informed regulatory/clinical decision-making."}}
{"id": "2510.02091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02091", "abs": "https://arxiv.org/abs/2510.02091", "authors": ["Xinyuan Song", "Keyu Wang", "PengXiang Li", "Lu Yin", "Shiwei Liu"], "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning", "comment": "ICASSP 2025", "summary": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models.", "AI": {"tldr": "Systematic, multi-dimensional analysis of LLM depth usage shows depth effects are task-/metric-/model-dependent; early layers dominate in retrieval, while middle/deep layers support reasoning and coherence, especially under generation-based evaluation; most layers can be pruned under likelihood-based metrics; distillation can reshape deeper-layer roles.", "motivation": "Address inconsistent claims that deep layers add little value by evaluating across diverse protocols, tasks, and architectures to understand when and how depth matters.", "method": "Prune and analyze layers across diverse evaluation settings (likelihood-based vs generation-based), task categories, and architectures; quantify layer importance, examine distribution of knowledge/retrieval vs reasoning, and explore distillation to alter depth roles.", "result": "Under likelihood metrics without generation, removing most layers preserves performance; initial layers are critical. Under generation-based evaluation, middle/deeper layers are crucial for reasoning and long-range coherence. Knowledge/retrieval relies on shallow layers; reasoning relies on deeper layers, which can be reshaped via distillation.", "conclusion": "Depth usage in LLMs is heterogeneous and context-dependent; interpretation and compression should be task-, metric-, and model-aware."}}
{"id": "2510.01545", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01545", "abs": "https://arxiv.org/abs/2510.01545", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "title": "Predictive Preference Learning from Human Interventions", "comment": "NeurIPS 2025 Spotlight. Project page:\n  https://metadriverse.github.io/ppl", "summary": "Learning from human involvement aims to incorporate the human subject to\nmonitor and correct agent behavior errors. Although most interactive imitation\nlearning methods focus on correcting the agent's action at the current state,\nthey do not adjust its actions in future states, which may be potentially more\nhazardous. To address this, we introduce Predictive Preference Learning from\nHuman Interventions (PPL), which leverages the implicit preference signals\ncontained in human interventions to inform predictions of future rollouts. The\nkey idea of PPL is to bootstrap each human intervention into L future time\nsteps, called the preference horizon, with the assumption that the agent\nfollows the same action and the human makes the same intervention in the\npreference horizon. By applying preference optimization on these future states,\nexpert corrections are propagated into the safety-critical regions where the\nagent is expected to explore, significantly improving learning efficiency and\nreducing human demonstrations needed. We evaluate our approach with experiments\non both autonomous driving and robotic manipulation benchmarks and demonstrate\nits efficiency and generality. Our theoretical analysis further shows that\nselecting an appropriate preference horizon L balances coverage of risky states\nwith label correctness, thereby bounding the algorithmic optimality gap. Demo\nand code are available at: https://metadriverse.github.io/ppl", "AI": {"tldr": "Predictive Preference Learning from Human Interventions (PPL) propagates human corrections into a horizon of future steps to guide future rollouts, improving safety and data efficiency in interactive imitation learning by leveraging implicit preferences from interventions.", "motivation": "Current interactive imitation learning methods typically correct actions only at the current state and do not explicitly shape future actions, which can be hazardous. PPL uses human interventions to influence predicted future behavior, addressing safety in long-horizon rollout.", "method": "Bootstrap each human intervention into a horizon of L future time steps, assuming the agent repeats the same action and the human would intervene again within the horizon. Apply preference optimization on these future states to propagate expert corrections into safety-critical regions, enhancing learning efficiency and reducing the number of demonstrations.", "result": "Empirical validation on autonomous driving and robotic manipulation benchmarks shows improved learning efficiency and generality; interventions effectively influence future rollouts. Theoretical analysis indicates that an appropriate choice of horizon L balances state coverage and label correctness, bounding the algorithmic optimality gap.", "conclusion": "Selecting L is crucial to trade off coverage of risky states against label noise; PPL provides a general, efficient framework for incorporating human interventions into future-safe policies, with available demos and code."}}
{"id": "2510.01954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01954", "abs": "https://arxiv.org/abs/2510.01954", "authors": ["Yongyi Su", "Haojie Zhang", "Shijie Li", "Nanqing Liu", "Jingyi Liao", "Junyi Pan", "Yuan Liu", "Xiaofen Xing", "Chong Sun", "Chen Li", "Nancy F. Chen", "Shuicheng Yan", "Xulei Yang", "Xun Xu"], "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs", "comment": "24 pages, 12 figures and 9 tables", "summary": "Multimodal large language models (MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text for detection, which\nlimits performance and prevents dense prediction tasks like segmentation. To\novercome these challenges, we introduce Patch-as-Decodable Token (PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),\nderived from visual patch embeddings of query images and interleaved seamlessly\nwith LLM's output textual tokens. A lightweight decoder then transforms LLM's\noutputs into detection, segmentation, and grounding predictions. Unlike prior\nmethods, PaDT processes VRTs independently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy for PaDT by\nrandomly selecting VRTs for supervised fine-tuning and introducing a robust\nper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggest PaDT consistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.", "AI": {"tldr": "PaDT is a unified Patch-as-Decodable Token framework for MLLMs that directly generates both text and diverse visual outputs, using Visual Reference Tokens interleaved with LLM tokens, with a lightweight decoder for detection, segmentation, and grounding. It processes VRTs per forward pass and expands the embedding table dynamically, enabling better localization; trained with random VRT sampling and per-token cross-entropy; achieves state-of-the-art across four tasks, competitive with larger LLMs; code released.", "motivation": "To overcome reliance on indirect representations in vision tasks (e.g., predicting coordinates as text) that limit dense predictions like segmentation, by enabling direct, unified generation of textual and visual outputs within MLLMs.", "method": "Introduce Patch-as-Decodable Token (PaDT). Derived Visual Reference Tokens (VRTs) from visual patch embeddings of query images are interleaved with LLM output tokens. A lightweight decoder converts LLM outputs into detection, segmentation, and grounding predictions. VRTs are processed independently at each forward pass; embedding table expands dynamically. Training uses randomly selected VRTs for supervised fine-tuning and applies a per-token cross-entropy loss.", "result": "Empirical studies show PaDT achieves state-of-the-art performance across four visual perception and understanding tasks, outperforming baselines and even some much larger MLLMs.", "conclusion": "PaDT offers a unified and effective approach to enable dense vision tasks within MLLMs, bridging text and diverse visual outputs, with code available for reproducibility."}}
{"id": "2510.01521", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01521", "abs": "https://arxiv.org/abs/2510.01521", "authors": ["Diptyaroop Maji", "Kang Yang", "Prashant Shenoy", "Ramesh K Sitaraman", "Mani Srivastava"], "title": "CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models", "comment": null, "summary": "Computational decarbonization aims to reduce carbon emissions in computing\nand societal systems such as data centers, transportation, and built\nenvironments. This requires accurate, fine-grained carbon intensity forecasts,\nyet existing tools have several key limitations: (i) they require grid-specific\nelectricity mix data, restricting use where such information is unavailable;\n(ii) they depend on separate grid-specific models that make it challenging to\nprovide global coverage; and (iii) they provide forecasts without uncertainty\nestimates, limiting reliability for downstream carbon-aware applications.\n  In this paper, we present CarbonX, an open-source tool that leverages Time\nSeries Foundation Models (TSFMs) for a range of decarbonization tasks. CarbonX\nutilizes the versatility of TSFMs to provide strong performance across multiple\ntasks, such as carbon intensity forecasting and imputation, and across diverse\ngrids. Using only historical carbon intensity data and a single general model,\nour tool achieves a zero-shot forecasting Mean Absolute Percentage Error (MAPE)\nof 15.82% across 214 grids worldwide. Across 13 benchmark grids, CarbonX\nperformance is comparable with the current state-of-the-art, with an average\nMAPE of 9.59% and tail forecasting MAPE of 16.54%, while also providing\nprediction intervals with 95% coverage. CarbonX can provide forecasts for up to\n21 days with minimal accuracy degradation. Further, when fully fine-tuned,\nCarbonX outperforms the statistical baselines by 1.2--3.9X on the imputation\ntask. Overall, these results demonstrate that CarbonX can be used easily on any\ngrid with limited data and still deliver strong performance, making it a\npractical tool for global-scale decarbonization.", "AI": {"tldr": "CarbonX is an open-source time-series foundation model tool designed for global carbon-intensity forecasting and imputation with uncertainty estimates, using a single general model trained on historical data.", "motivation": "There is a need for accurate, fine-grained carbon-intensity forecasts for decarbonization across computing and societal systems, but existing tools require grid-specific data, have limited global coverage, and lack uncertainty estimates.", "method": "Leverages Time Series Foundation Models (TSFMs) trained on historical carbon-intensity data to perform zero-shot forecasting across 214 grids with a single general model, provides 21-day forecasts with 95% prediction intervals, and can be fully fine-tuned for improved imputation performance (1.2\u20133.9x over baselines).", "result": "Zero-shot forecasting MAPE of 15.82% across 214 grids; average MAPE of 9.59% on 13 benchmark grids; tail forecasting MAPE of 16.54%; 95% coverage of prediction intervals; forecasts up to 21 days with minimal accuracy degradation; after fine-tuning, imputation outperforms statistical baselines by 1.2\u20133.9x.", "conclusion": "CarbonX can be applied to any grid with limited data, delivering strong performance and uncertainty estimates at global scale, making it a practical tool for carbon-aware decision making."}}
{"id": "2510.02125", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02125", "abs": "https://arxiv.org/abs/2510.02125", "authors": ["Claas Beger", "Ryan Yi", "Shuhao Fu", "Arseny Moskvichev", "Sarah W. Tsai", "Sivasankaran Rajamanickam", "Melanie Mitchell"], "title": "Do AI Models Perform Human-like Abstract Reasoning Across Modalities?", "comment": "10 pages, 4 figures", "summary": "OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI\nbenchmark, but does that mean state-of-the-art models recognize and reason with\nthe abstractions that the task creators intended? We investigate models'\nabstraction abilities on ConceptARC. We evaluate models under settings that\nvary the input modality (textual vs. visual), whether the model is permitted to\nuse external Python tools, and, for reasoning models, the amount of reasoning\neffort. In addition to measuring output accuracy, we perform fine-grained\nevaluation of the natural-language rules that models generate to explain their\nsolutions. This dual evaluation lets us assess whether models solve tasks using\nthe abstractions ConceptARC was designed to elicit, rather than relying on\nsurface-level patterns. Our results show that, while some models using\ntext-based representations match human output accuracy, the best models' rules\nare often based on surface-level ``shortcuts'' and capture intended\nabstractions far less often than humans. Thus their capabilities for general\nabstract reasoning may be overestimated by evaluations based on accuracy alone.\nIn the visual modality, AI models' output accuracy drops sharply, yet our\nrule-level analysis reveals that models might be underestimated, as they still\nexhibit a substantial share of rules that capture intended abstractions, but\nare often unable to correctly apply these rules. In short, our results show\nthat models still lag humans in abstract reasoning, and that using accuracy\nalone to evaluate abstract reasoning on ARC-like tasks may overestimate\nabstract-reasoning capabilities in textual modalities and underestimate it in\nvisual modalities. We believe that our evaluation framework offers a more\nfaithful picture of multimodal models' abstract reasoning abilities and a more\nprincipled way to track progress toward human-like, abstraction-centered\nintelligence.", "AI": {"tldr": "Accuracy alone can misrepresent abstract reasoning in ARC-like tasks: text models may reach human accuracy but rely on shortcuts, while visual models underperform, yet still show partial abstraction; a rule-level framework is needed to assess true abstraction abilities.", "motivation": "To determine whether state-of-the-art models genuinely learn and apply the abstractions targeted by ConceptARC, across modalities (text vs. visual) and tool usage, beyond surface patterns.", "method": "Evaluate models on ConceptARC with varying input modalities (text vs. visual), ability to use external Python tools, and different reasoning effort. Measure output accuracy and perform fine-grained analysis of the natural-language rules models generate to explain solutions, comparing to human abstractions.", "result": "Text-based models can match human accuracy, but best models rely on surface shortcuts and capture abstractions less often than humans. In visuals, accuracy drops, yet rule-level analysis shows a sizable share of abstraction-related rules, though they are often misapplied. Overall, models lag humans in abstract reasoning, and accuracy-alone assessments overestimate abstraction in text and underestimate it in vision.", "conclusion": "The evaluation framework offers a more faithful, multimodal view of abstract reasoning and provides a principled way to track progress toward human-like, abstraction-centered intelligence."}}
{"id": "2510.01990", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.01990", "abs": "https://arxiv.org/abs/2510.01990", "authors": ["Jianfei Xie", "Ziyang Li"], "title": "TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading", "comment": null, "summary": "The 'trust deficit' in online fruit and vegetable e-commerce stems from the\ninability of digital transactions to provide direct sensory perception of\nproduct quality. This paper constructs a 'Trust Pyramid' model through\n'dual-source verification' of consumer trust. Experiments confirm that quality\nis the cornerstone of trust. The study reveals an 'impossible triangle' in\nagricultural product grading, comprising biological characteristics,\ntimeliness, and economic viability, highlighting the limitations of traditional\nabsolute grading standards. To quantitatively assess this trade-off, we propose\nthe 'Triangular Trust Index' (TTI). We redefine the role of algorithms from\n'decision-makers' to 'providers of transparent decision-making bases',\ndesigning the explainable AI framework--TriAlignXA. This framework supports\ntrustworthy online transactions within agricultural constraints through\nmulti-objective optimization. Its core relies on three engines: the\nBio-Adaptive Engine for granular quality description; the Timeliness\nOptimization Engine for processing efficiency; and the Economic Optimization\nEngine for cost control. Additionally, the \"Pre-Mapping Mechanism\" encodes\nprocess data into QR codes, transparently conveying quality information.\nExperiments on grading tasks demonstrate significantly higher accuracy than\nbaseline models. Empirical evidence and theoretical analysis verify the\nframework's balancing capability in addressing the \"impossible triangle\". This\nresearch provides comprehensive support--from theory to practice--for building\na trustworthy online produce ecosystem, establishing a critical pathway from\nalgorithmic decision-making to consumer trust.", "AI": {"tldr": "A new explainable AI framework TriAlignXA with Triangular Trust Index to address trust in online produce, balancing quality, timeliness, and economics via a Trust Pyramid and dual-source verification, including QR-encoded quality data.", "motivation": "In online fruit/vegetable e-commerce, lack of sensory cues creates trust deficits; need transparent, explainable decision-making to support consumer trust under agricultural constraints.", "method": "Proposes a Trust Pyramid and dual-source verification; introduces Triangular Trust Index (TTI); develops TriAlignXA with Bio-Adaptive, Timeliness, and Economic engines; Pre-Mapping Mechanism encodes data to QR codes; evaluate via grading experiments and theoretical analysis; multi-objective optimization.", "result": "Experiments show higher grading accuracy than baselines; empirical and theoretical evidence that the framework balances the 'impossible triangle' of biology, timeliness, and economics; improved trust potential in online produce transactions.", "conclusion": "Provides a practical pathway from algorithmic decision-making to consumer trust; supports building a trustworthy online produce ecosystem; extends explainable AI for supply-chain transparency."}}
{"id": "2510.01525", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.01525", "abs": "https://arxiv.org/abs/2510.01525", "authors": ["Woojin Kim", "James R. Luedtke"], "title": "On Integer Programming for the Binarized Neural Network Verification Problem", "comment": null, "summary": "Binarized neural networks (BNNs) are feedforward neural networks with binary\nweights and activation functions. In the context of using a BNN for\nclassification, the verification problem seeks to determine whether a small\nperturbation of a given input can lead it to be misclassified by the BNN, and\nthe robustness of the BNN can be measured by solving the verification problem\nover multiple inputs. The BNN verification problem can be formulated as an\ninteger programming (IP) problem. However, the natural IP formulation is often\nchallenging to solve due to a large integrality gap induced by big-$M$\nconstraints. We present two techniques to improve the IP formulation. First, we\nintroduce a new method for obtaining a linear objective for the multi-class\nsetting. Second, we introduce a new technique for generating valid inequalities\nfor the IP formulation that exploits the recursive structure of BNNs. We find\nthat our techniques enable verifying BNNs against a higher range of input\nperturbation than existing IP approaches within a limited time.", "AI": {"tldr": "Two IP formulation improvements for verifying binarized neural networks: a linearized multi-class objective and new valid inequalities that exploit the network's recursive structure, yielding better verification under time constraints.", "motivation": "BNNs are attractive for efficiency, but verifying robustness to input perturbations is hard due to large integrality gaps in big-M IP formulations; improving the IP can enable verification for larger perturbations.", "method": "1) derive a linear objective suitable for multi-class classification within the IP; 2) develop valid inequalities that exploit the recursive, layered structure of BNNs to tighten the IP formulation and reduce the gap; analyze and test on verification tasks.", "result": "The techniques enable verifying BNNs over a higher range of input perturbations within a limited time, outperforming existing IP approaches.", "conclusion": "The proposed IP enhancements effectively tighten the formulation and improve practical verifiability of BNN robustness under perturbations."}}
{"id": "2510.02133", "categories": ["cs.AI", "cs.LG", "I.2.7; I.2.10; I.4.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.02133", "abs": "https://arxiv.org/abs/2510.02133", "authors": ["Karan Dua", "Hitesh Laxmichand Patel", "Puneet Mittal", "Ranjeet Gupta", "Amit Agarwal", "Praneet Pabolu", "Srikant Panda", "Hansa Meghwani", "Graham Horwood", "Fahad Shah"], "title": "FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models", "comment": "Accepted at EMNLP 2025", "summary": "Developing document understanding models at enterprise scale requires large,\ndiverse, and well-annotated datasets spanning a wide range of document types.\nHowever, collecting such data is prohibitively expensive due to privacy\nconstraints, legal restrictions, and the sheer volume of manual annotation\nneeded - costs that can scale into millions of dollars. We introduce FlexDoc, a\nscalable synthetic data generation framework that combines Stochastic Schemas\nand Parameterized Sampling to produce realistic, multilingual semi-structured\ndocuments with rich annotations. By probabilistically modeling layout patterns,\nvisual structure, and content variability, FlexDoc enables the controlled\ngeneration of diverse document variants at scale. Experiments on Key\nInformation Extraction (KIE) tasks demonstrate that FlexDoc-generated data\nimproves the absolute F1 Score by up to 11% when used to augment real datasets,\nwhile reducing annotation effort by over 90% compared to traditional\nhard-template methods. The solution is in active deployment, where it has\naccelerated the development of enterprise-grade document understanding models\nwhile significantly reducing data acquisition and annotation costs.", "AI": {"tldr": "FlexDoc is a scalable synthetic data framework using stochastic schemas and parameterized sampling to generate multilingual semi-structured documents with rich annotations, boosting KIE performance and reducing annotation effort; deployed for enterprise document understanding.", "motivation": "To overcome the data bottleneck in enterprise document understanding caused by privacy, legal constraints, and costly manual annotation, hindering scaling of models.", "method": "Combines stochastic schemas and parameterized sampling to probabilistically model layout, visual structure, and content variability; generates diverse, realistic, multilingual semi-structured documents with annotations.", "result": "When used to augment real data in KIE tasks, absolute F1 improves by up to 11%; annotation effort reduces by >90% relative to hard-template methods; validated in active deployment.", "conclusion": "FlexDoc enables scalable synthetic data generation for enterprise document understanding, reducing data collection/annotation costs and accelerating model development at scale."}}
{"id": "2510.01991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01991", "abs": "https://arxiv.org/abs/2510.01991", "authors": ["Lei Liu", "Can Wang", "Zhenghao Chen", "Dong Xu"], "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing", "comment": null, "summary": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges\nwith view, temporal, and non-editing region consistency, as well as with\nhandling complex text instructions. To address these issues, we propose\n4DGS-Craft, a consistent and interactive 4DGS editing framework. We first\nintroduce a 4D-aware InstructPix2Pix model to ensure both view and temporal\nconsistency. This model incorporates 4D VGGT geometry features extracted from\nthe initial scene, enabling it to capture underlying 4D geometric structures\nduring editing. We further enhance this model with a multi-view grid module\nthat enforces consistency by iteratively refining multi-view input images while\njointly optimizing the underlying 4D scene. Furthermore, we preserve the\nconsistency of non-edited regions through a novel Gaussian selection mechanism,\nwhich identifies and optimizes only the Gaussians within the edited regions.\nBeyond consistency, facilitating user interaction is also crucial for effective\n4DGS editing. Therefore, we design an LLM-based module for user intent\nunderstanding. This module employs a user instruction template to define atomic\nediting operations and leverages an LLM for reasoning. As a result, our\nframework can interpret user intent and decompose complex instructions into a\nlogical sequence of atomic operations, enabling it to handle intricate user\ncommands and further enhance editing performance. Compared to related works,\nour approach enables more consistent and controllable 4D scene editing. Our\ncode will be made available upon acceptance.", "AI": {"tldr": "Proposes 4DGS-Craft, a consistent and interactive 4D Gaussian Splatting (4DGS) editing framework that uses 4D-aware conditioning, multi-view refinement, Gaussian-region selective optimization, and an LLM-driven intent module to improve view/temporal consistency and handle complex instructions.", "motivation": "Current 4DGS editing suffers from inconsistencies across viewpoints and time, drift in non-edited regions, and difficulty in following complex text instructions. There is a need for a controllable, interactive editing framework that preserves 4D geometry.", "method": "Introduce a 4D-aware InstructPix2Pix model that leverages 4D VGGT geometry features from the initial scene to preserve 4D structure during edits; add a multi-view grid module for iterative refinement of multi-view inputs while jointly optimizing the 4D scene; implement a Gaussian selection mechanism to preserve non-edited regions by optimizing only Gaussians within edited regions; incorporate an LLM-based module with an instruction template to interpret user intent and decompose complex commands into atomic editing operations.", "result": "Claims more consistent and controllable 4D scene editing than related works; the code will be released upon acceptance (no quantitative metrics reported in the abstract).", "conclusion": "The framework demonstrates that combining 4D geometry-aware editing, multi-view refinement, selective Gaussian optimization, and LLM-driven intent understanding can yield robust, interactive 4DGS editing with improved consistency and user control."}}
{"id": "2510.01527", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01527", "abs": "https://arxiv.org/abs/2510.01527", "authors": ["Lecheng Kong", "Xiyuan Wang", "Yixin Chen", "Muhan Zhang"], "title": "Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs", "comment": "19 pages", "summary": "Large Language Models (LLMs) are emerging as versatile foundation models for\ncomputational chemistry, handling bidirectional tasks like reaction prediction\nand retrosynthesis. However, these models often lack round-trip consistency.\nFor instance, a state-of-the-art chemical LLM may successfully caption a\nmolecule, yet be unable to accurately reconstruct the original structure from\nits own generated text. This inconsistency suggests that models are learning\nunidirectional memorization rather than flexible mastery. Indeed, recent work\nhas demonstrated a strong correlation between a model's round-trip consistency\nand its performance on the primary tasks. This strong correlation reframes\nconsistency into a direct target for model improvement. We therefore introduce\nRound-Trip Reinforcement Learning (RTRL), a novel framework that trains a model\nto improve its consistency by using the success of a round-trip transformation\nas a reward signal. We further propose an iterative variant where forward and\nreverse mappings alternately train each other in a self-improvement loop, a\nprocess that is highly data-efficient and notably effective with the massive\namount of unlabelled data common in chemistry. Experiments demonstrate that\nRTRL significantly \\textbf{boosts performance and consistency} over strong\nbaselines across supervised, self-supervised, and synthetic data regimes. This\nwork shows that round-trip consistency is not just a desirable property but a\ntrainable objective, offering a new path toward more robust and reliable\nfoundation models.", "AI": {"tldr": "Train LLM-based chemistry models to maximize round-trip consistency between forward and reverse mappings using Round-Trip Reinforcement Learning (RTRL) and an iterative self-improvement loop, improving both performance and consistency especially with unlabeled data.", "motivation": "Current chemical LLMs exhibit strong unidirectional memorization and round-trip inconsistency: they can describe a molecule but fail to reconstruct the original structure from generated text. Prior work shows a strong link between round-trip consistency and task performance, suggesting consistency should be a trainable objective.", "method": "Introduce Round-Trip Reinforcement Learning (RTRL) that uses the success of forward-reverse transformations as a reward signal to improve consistency. Propose an iterative variant where forward and reverse mappings train each other in a self-improvement loop, enabling data-efficient learning from large amounts of unlabeled chemistry data.", "result": "RTRL significantly boosts both performance and round-trip consistency compared with strong baselines across supervised, self-supervised, and synthetic data regimes in chemistry tasks.", "conclusion": "Round-trip consistency can be learned as a trainable objective, offering a new path to more robust and reliable foundation models for computational chemistry by leveraging bidirectional mappings and self-improvement loops."}}
{"id": "2510.02190", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02190", "abs": "https://arxiv.org/abs/2510.02190", "authors": ["Yang Yao", "Yixu Wang", "Yuxuan Zhang", "Yi Lu", "Tianle Gu", "Lingyu Li", "Dingyi Zhao", "Keming Wu", "Haozhe Wang", "Ping Nie", "Yan Teng", "Yingchun Wang"], "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports", "comment": null, "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.", "AI": {"tldr": "A benchmark and evaluation framework for Deep Research Agents (DRAs) with 214 expert questions across 10 domains; assesses long-form report outputs with semantic quality, topical focus, and retrieval trust; DRAs outperform web-search-augmented models but have room for improvement.", "motivation": "Current benchmarks for DRAs fail to capture evaluation dimensions, response formatting, and scoring necessary to assess complex, multi-source reasoning; a robust, multidimensional evaluation framework is needed to assess DRA capabilities.", "method": "Develop 214 expert-curated challenging queries across 10 broad domains; create manually constructed reference bundles for composite evaluation; design a multidimensional scoring system for long-form reports including semantic quality, topical focus, and retrieval trustworthiness; compare mainstream DRAs against web-search-tool-augmented reasoning models; perform extensive experiments.", "result": "Mainstream DRAs outperform web-search augmented models; however, substantial room for improvement remains; the benchmark provides a solid foundation for capability assessment and architectural refinement in DRA systems.", "conclusion": "The study offers a robust framework for capability assessment, architectural refinement, and paradigm advancement in DRA systems."}}
{"id": "2510.01997", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01997", "abs": "https://arxiv.org/abs/2510.01997", "authors": ["Junyu Wu", "Jie Tang", "Jie Liu", "Gangshan Wu"], "title": "Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution", "comment": null, "summary": "Image Super-Resolution (SR) aims to reconstruct high-resolution images from\nlow-resolution counterparts, but the computational complexity of deep\nlearning-based methods often hinders practical deployment. CAMixer is the\npioneering work to integrate the advantages of existing lightweight SR methods\nand proposes a content-aware mixer to route token mixers of varied complexities\naccording to the difficulty of content recovery. However, several limitations\nremain, such as poor adaptability, coarse-grained masking and spatial\ninflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking\nmechanism that identifies pure pixels and exempts them from expensive\ncomputations. PP utilizes fixed color center points to classify pixels into\ndistinct categories, enabling fine-grained, spatially flexible masking while\nmaintaining adaptive flexibility. Integrated into the state-of-the-art\nATD-light model, PP-ATD-light achieves superior SR performance with minimal\noverhead, outperforming CAMixer-ATD-light in reconstruction quality and\nparameter efficiency when saving a similar amount of computation.", "AI": {"tldr": "PP-ATD-light introduces pixel-level masking (Pure-Pass) to skip expensive computation on pure pixels, using fixed color center points to classify pixels for fine-grained, spatially flexible masking, integrated into ATD-light to outperform CAMixer-ATD-light with similar computation.", "motivation": "To reduce the computational burden of deep SR while preserving performance and adaptability. Prior CAMixer struggles with adaptability, coarse masking, and spatial rigidity; a finer, pixel-level, and flexible masking mechanism could improve efficiency and quality.", "method": "Propose Pure-Pass (PP) masking that identifies pure pixels at the pixel level and exempts them from heavy computation. Uses fixed color center points to classify pixels into categories, enabling fine-grained, spatially flexible masking with adaptive capability. Integrates PP into the state-of-the-art ATD-light model to form PP-ATD-light.", "result": "PP-ATD-light achieves superior SR performance with minimal overhead and outperforms CAMixer-ATD-light in reconstruction quality and parameter efficiency when saving a similar amount of computation.", "conclusion": "Pixel-level Pure-Pass masking with fixed color centers provides fine-grained, spatially flexible, and adaptive computation reduction in SR, yielding better quality-for-cost with ATD-light."}}
{"id": "2510.01529", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01529", "abs": "https://arxiv.org/abs/2510.01529", "authors": ["Jaiden Fairoze", "Sanjam Garg", "Keewoo Lee", "Mingyuan Wang"], "title": "Bypassing Prompt Guards in Production with Controlled-Release Prompting", "comment": null, "summary": "As large language models (LLMs) advance, ensuring AI safety and alignment is\nparamount. One popular approach is prompt guards, lightweight mechanisms\ndesigned to filter malicious queries while being easy to implement and update.\nIn this work, we introduce a new attack that circumvents such prompt guards,\nhighlighting their limitations. Our method consistently jailbreaks production\nmodels while maintaining response quality, even under the highly protected chat\ninterfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok\n(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry\nbetween the prompt guard and the main LLM, encoding a jailbreak prompt that\nlightweight guards cannot decode but the main model can. This reveals an attack\nsurface inherent to lightweight prompt guards in modern LLM architectures and\nunderscores the need to shift defenses from blocking malicious inputs to\npreventing malicious outputs. We additionally identify other critical alignment\nissues, such as copyrighted data extraction, training data extraction, and\nmalicious response leakage during thinking.", "AI": {"tldr": "A novel jailbreak attack circumvents lightweight prompt guards by encoding jailbreak prompts that guards cannot decode but the main LLM can; attack is effective across major production models; indicates limitations of prompt guards and suggests defenses should prevent outputs rather than inputs; identifies additional alignment issues such as data extraction and leakage.", "motivation": "To ensure AI safety and alignment as LLMs become more capable, highlighting the vulnerabilities of lightweight prompt guards and the need for stronger defenses that do not rely solely on input filtering.", "method": "Introduce a jailbreak technique that exploits a resource asymmetry between the prompt guard and the main LLM, encoding jailbreak prompts in a way that guards cannot decode but the main model can. Evaluate across production models (e.g., Google Gemini 2.5 Flash/Pro, DeepSeek Chat/DeepThink, Grok, Mistral Le Chat/Magistral) to demonstrate consistent bypass while preserving response quality.", "result": "The attack consistently jailbreaks production models while maintaining response quality, exposing a fundamental vulnerability in lightweight prompt guards and their limited ability to constrain model output. Demonstrated across multiple protected chat interfaces, revealing a practical attack surface in modern LLM architectures.", "conclusion": "Lightweight prompt guards are insufficient for robust LLM safety. Defenses should shift from blocking malicious inputs to preventing malicious outputs. The work also points to other alignment concerns, including copyrighted data extraction, training data extraction, and malicious response leakage during chain-of-thought."}}
{"id": "2510.02194", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02194", "abs": "https://arxiv.org/abs/2510.02194", "authors": ["Yuhao Sun", "Zhuoer Xu", "Shiwen Cui", "Kun Yang", "Lingyun Yu", "Yongdong Zhang", "Hongtao Xie"], "title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress across a wide\nrange of tasks, but remain vulnerable to safety risks such as harmful content\ngeneration and jailbreak attacks. Existing safety techniques -- including\nexternal guardrails, inference-time guidance, and post-training alignment --\neach face limitations in balancing safety, utility, and controllability. In\nthis work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM\nsafety through safety-aware upcycling. Our approach first identifies\nsafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)\nstructure, where the router acts as a soft guardrail that selectively activates\noriginal MLPs and added safety experts. We further introduce a two-stage SFT\nstrategy to strengthen safety discrimination while preserving general\ncapabilities. To enable flexible control at inference time, we introduce a\nsafety temperature mechanism, allowing dynamic adjustment of the trade-off\nbetween safety and utility. Experiments across multiple benchmarks, base model,\nand model scales demonstrate that UpSafe$^\\circ$C achieves robust safety\nimprovements against harmful and jailbreak inputs, while maintaining\ncompetitive performance on general tasks. Moreover, analysis shows that safety\ntemperature provides fine-grained inference-time control that achieves the\nPareto-optimal frontier between utility and safety. Our results highlight a new\ndirection for LLM safety: moving from static alignment toward dynamic, modular,\nand inference-aware control.", "AI": {"tldr": "UpSafe\u00b0C proposes safety-aware upcycling of LLMs by converting safety-critical layers into a sparse Mixture-of-Experts (MoE) with a router as a soft guardrail, plus a two-stage SFT and an inference-time safety temperature to dynamically trade safety and utility, achieving better safety with competitive performance.", "motivation": "Safety in LLMs is constrained by static guardrails and post-hoc alignments. There is a need for modular, controllable safety that can adapt at inference time without sacrificing utility.", "method": "Identify safety-critical layers, upcycle them into a sparse MoE; router selectively activates original MLPs and safety experts. Employ a two-stage supervised fine-tuning to improve safety discrimination while preserving capabilities. Introduce a safety temperature to adjust safety-utility trade-off during inference.", "result": "Empirical results show robust safety improvements against harmful and jailbreak inputs across multiple benchmarks, base models, and scales, with maintenance of competitive general task performance. Safety temperature enables fine-grained inference-time control and Pareto-optimal trade-off frontier.", "conclusion": "Shifts safety design from static alignment toward dynamic, modular, and inference-aware control via architectural upcycling, enabling safer LLMs with tunable trade-offs."}}
{"id": "2510.02001", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02001", "abs": "https://arxiv.org/abs/2510.02001", "authors": ["Nanaka Hosokawa", "Ryo Takahashi", "Tomoya Kitano", "Yukihiro Iida", "Chisako Muramatsu", "Tatsuro Hayashi", "Yuta Seino", "Xiangrong Zhou", "Takeshi Hara", "Akitoshi Katsumata", "Hiroshi Fujita"], "title": "Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework", "comment": "Intended for submission to Scientific Reports", "summary": "In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to\nautomatically generate jaw cyst findings on dental panoramic radiographs. To\nimprove accuracy, we constructed a Self-correction Loop with Structured Output\n(SLSO) framework and verified its effectiveness. A 10-step process was\nimplemented for 22 cases of jaw cysts, including image input and analysis,\nstructured data generation, tooth number extraction and consistency checking,\niterative regeneration when inconsistencies were detected, and finding\ngeneration with subsequent restructuring and consistency verification. A\ncomparative experiment was conducted using the conventional Chain-of-Thought\n(CoT) method across seven evaluation items: transparency, internal structure,\nborders, root resorption, tooth movement, relationships with other structures,\nand tooth number. The results showed that the proposed SLSO framework improved\noutput accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates\nfor tooth number, tooth movement, and root resorption, respectively. In the\nsuccessful cases, a consistently structured output was achieved after up to\nfive regenerations. Although statistical significance was not reached because\nof the small size of the dataset, the overall SLSO framework enforced negative\nfinding descriptions, suppressed hallucinations, and improved tooth number\nidentification accuracy. However, the accurate identification of extensive\nlesions spanning multiple teeth is limited. Nevertheless, further refinement is\nrequired to enhance overall performance and move toward a practical finding\ngeneration system.", "AI": {"tldr": "GPT-4o-based pipeline with a Self-correction Loop with Structured Output (SLSO) aims to generate jaw cyst findings from panoramic radiographs. It improves several findings with iterative regenerations, but is limited by small dataset and difficulties with extensive lesions.", "motivation": "To reduce hallucinations and improve accuracy in AI-generated radiology findings by enforcing structured outputs and consistency through iterative self-correction.", "method": "A 10-step process over 22 jaw cyst cases: input image analysis, generation of structured data, tooth-number extraction with consistency checks, regeneration when inconsistencies are detected, final finding generation with restructuring and verification; comparison against conventional Chain-of-Thought (CoT) across seven evaluation items.", "result": "SLSO improved output accuracy for tooth number (66.9%), tooth movement (33.3%), and root resorption (28.6%). Consistent structured outputs achieved after up to five regenerations. Negative findings and reduced hallucinations reported; no statistical significance due to small sample; accurate learning limited for extensive lesions spanning multiple teeth.", "conclusion": "SLSO shows promise toward a practical automatic finding generation system but requires further refinement and larger datasets to improve overall performance and handle extensive multi-tooth lesions."}}
{"id": "2510.01533", "categories": ["cs.LG", "68T05"], "pdf": "https://arxiv.org/pdf/2510.01533", "abs": "https://arxiv.org/abs/2510.01533", "authors": ["Kobi Cohen-Arazi", "Michael Roe", "Zhen Hu", "Rohan Chavan", "Anna Ptasznik", "Joanna Lin", "Joao Morais", "Joseph Boccuzzi", "Tommaso Balercia"], "title": "NVIDIA AI Aerial: AI-Native Wireless Communications", "comment": "7 pages, 7 figures", "summary": "6G brings a paradigm shift towards AI-native wireless systems, necessitating\nthe seamless integration of digital signal processing (DSP) and machine\nlearning (ML) within the software stacks of cellular networks. This\ntransformation brings the life cycle of modern networks closer to AI systems,\nwhere models and algorithms are iteratively trained, simulated, and deployed\nacross adjacent environments. In this work, we propose a robust framework that\ncompiles Python-based algorithms into GPU-runnable blobs. The result is a\nunified approach that ensures efficiency, flexibility, and the highest possible\nperformance on NVIDIA GPUs. As an example of the capabilities of the framework,\nwe demonstrate the efficacy of performing the channel estimation function in\nthe PUSCH receiver through a convolutional neural network (CNN) trained in\nPython. This is done in a digital twin first, and subsequently in a real-time\ntestbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform,\nlays the foundation for scalable integration of AI/ML models into\nnext-generation cellular systems, and is essential for realizing the vision of\nnatively intelligent 6G networks.", "AI": {"tldr": "A framework that compiles Python-based ML algorithms into GPU-executable blobs to enable AI-native 6G systems, demonstrated by CNN-based channel estimation in a PUSCH receiver, validated in a digital twin and real-time testbed on NVIDIA GPUs (NVIDIA AI Aerial).", "motivation": "6G aims for AI-native wireless systems with tight DSP/ML integration across software stacks and life-cycle stages. There is a need to bridge Python ML workflows with GPU-accelerated telecom hardware to achieve scalable, high-performance AI deployment in networks.", "method": "Introduce a framework that compiles Python-based algorithms into GPU-runnable blobs, leveraging the NVIDIA AI Aerial platform. Validate via a CNN-based channel estimation in the PUSCH receiver, first in a digital twin and then in a real-time testbed.", "result": "Demonstrates efficiency, flexibility, and high performance on NVIDIA GPUs, enabling real-time channel estimation via a Python-trained CNN and supporting the seamless AI/ML lifecycle for 6G development.", "conclusion": "The framework provides a scalable pathway to integrate AI/ML into next-generation cellular systems, underpinning the vision of natively intelligent 6G networks."}}
{"id": "2510.02230", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02230", "abs": "https://arxiv.org/abs/2510.02230", "authors": ["Phuc Minh Nguyen", "Chinh D. La", "Duy M. H. Nguyen", "Nitesh V. Chawla", "Binh T. Nguyen", "Khoa D. Doan"], "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models", "comment": "23 pages, 15 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.", "AI": {"tldr": "RLVR can shrink the reasoning boundary due to negative interference and winner-take-all dynamics, driven by on-policy sampling. A simple data-curation approach that emphasizes low-likelihood problems improves Pass@k, mitigating shrinkage.", "motivation": "To understand why RLVR shrinks the reasoning capacity of LLMs, diagnose the learning dynamics that cause failure, and identify remedies to preserve or expand the reasoning boundary.", "method": "Theoretical analysis combined with empirical experiments on multiple mathematical reasoning benchmarks. The study identifies negative interference (solving some problems lowers success on others) and a winner-take-all effect (high-likelihood problems are disproportionately reinforced). It attributes these phenomena to on-policy sampling in standard RL objectives and proposes a data-curation algorithm that targets low-likelihood problems and evaluates its impact on Pass@k.", "result": "Two main phenomena are observed: (1) negative interference reduces the probability of correct solutions for other problems, leading to a decline in Pass@k; (2) winner-take-all concentrates learning on high-likelihood, correct solutions under the base model, suppressing low-likelihood ones. On-policy sampling drives convergence toward narrow strategies. The proposed data-curation algorithm focusing on low-likelihood problems yields notable Pass@k improvements. Code is provided at the authors' repository.", "conclusion": "RLVR can be fragile due to on-policy dynamics that favor a narrow set of solutions. Mitigation via targeted data curation for low-likelihood problems can enhance reasoning performance, suggesting broader applicability of such balancing strategies for RL-based instruction tuning and advising caution when relying on standard RL objectives for broad reasoning capabilities."}}
{"id": "2510.02028", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02028", "abs": "https://arxiv.org/abs/2510.02028", "authors": ["Mario Resino", "Borja P\u00e9rez", "Jaime Godoy", "Abdulla Al-Kaff", "Fernando Garc\u00eda"], "title": "LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction", "comment": "7 pages, 3 figures, 7 tables, Submitted to ICRA", "summary": "This work proposed a 3D autoencoder architecture, named LiLa-Net, which\nencodes efficient features from real traffic environments, employing only the\nLiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,\nequipped with Velodyne LiDAR. The system leverage skip connections concept to\nimprove the performance without using extensive resources as the\nstate-of-the-art architectures. Key changes include reducing the number of\nencoder layers and simplifying the skip connections, while still producing an\nefficient and representative latent space which allows to accurately\nreconstruct the original point cloud. Furthermore, an effective balance has\nbeen achieved between the information carried by the skip connections and the\nlatent encoding, leading to improved reconstruction quality without\ncompromising performance. Finally, the model demonstrates strong generalization\ncapabilities, successfully reconstructing objects unrelated to the original\ntraffic environment.", "AI": {"tldr": "LiLa-Net: a lightweight 3D autoencoder for LiDAR point clouds that uses simplified skip connections and fewer encoder layers to reconstruct scenes from real traffic data, achieving efficient latent representations and good generalization to unseen objects.", "motivation": "There is a need for compact, efficient LiDAR representations for autonomous driving that enable accurate reconstruction without heavy architectures, enabling real-time performance and better generalization to diverse traffic environments.", "method": "A 3D autoencoder architecture named LiLa-Net processes LiDAR point clouds from a real semi-autonomous vehicle equipped with Velodyne. It reduces encoder depth and simplifies skip connections, aiming for a balanced latent encoding that preserves essential information while enabling accurate reconstruction of the original point cloud. Training emphasizes an efficient latent space and improved reconstruction quality with limited resources, and includes evaluation of generalization to objects outside the training traffic environment.", "result": "The model achieves accurate reconstruction of the original point cloud with an efficient latent space. It demonstrates strong generalization, reconstructing objects unrelated to the training traffic environment while maintaining performance with reduced architectural complexity.", "conclusion": "LiLa-Net offers a compact, resource-efficient 3D autoencoder for LiDAR data that maintains reconstruction quality and generalization while using fewer encoder layers and simplified skip connections, suggesting a favorable trade-off between model complexity and representation fidelity."}}
{"id": "2510.01538", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01538", "abs": "https://arxiv.org/abs/2510.01538", "authors": ["Haokun Zhao", "Xiang Zhang", "Jiaqi Wei", "Yiwei Xu", "Yuting He", "Siqi Sun", "Chenyu You"], "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis", "comment": null, "summary": "Time series forecasting is central to decision-making in domains as diverse\nas energy, finance, climate, and public health. In practice, forecasters face\nthousands of short, noisy series that vary in frequency, quality, and horizon,\nwhere the dominant cost lies not in model fitting, but in the labor-intensive\npreprocessing, validation, and ensembling required to obtain reliable\npredictions. Prevailing statistical and deep learning models are tailored to\nspecific datasets or domains and generalize poorly. A general, domain-agnostic\nframework that minimizes human intervention is urgently in demand. In this\npaper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic\nframework for general time series forecasting. The framework comprises four\nspecialized agents: Curator performs LLM-guided diagnostics augmented by\nexternal tools that reason over data statistics to choose targeted\npreprocessing; Planner narrows the hypothesis space of model choice by\nleveraging multi-modal diagnostics and self-planning over the input; Forecaster\nperforms model fitting and validation and, based on the results, adaptively\nselects the best model configuration as well as ensemble strategy to make final\npredictions; and Reporter synthesizes the whole process into a comprehensive,\ntransparent report. With transparent natural-language rationales and\ncomprehensive reports, TSci transforms the forecasting workflow into a\nwhite-box system that is both interpretable and extensible across tasks.\nEmpirical results on eight established benchmarks demonstrate that TSci\nconsistently outperforms both statistical and LLM-based baselines, reducing\nforecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci\nproduces a clear and rigorous report that makes the forecasting workflow more\ntransparent and interpretable.", "AI": {"tldr": "TimeSeriesScientist (TSci) is an LLM-driven agentic framework with four specialized agents that automate preprocessing, model selection, fitting/validation, and reporting for general time series forecasting, achieving improved accuracy and transparent, interpretable outputs across benchmarks.", "motivation": "There is a strong need for a domain-agnostic forecasting framework that minimizes labor-intensive preprocessing, validation, and ensembling, since existing methods are often tailored to specific datasets and generalize poorly, hindering scalable, interpretable forecasting.", "method": "Four agents: Curator (LLM-guided diagnostics + tools to target preprocessing), Planner (narrows model-space via multi-modal diagnostics and self-planning), Forecaster (fits/validates models and adaptively selects configurations and ensembles), Reporter (compiles a transparent, comprehensive report). The framework emphasizes white-box reasoning and interpretability, with cross-task extensibility.", "result": "Empirical evaluation on eight benchmarks shows TSci consistently outperforms statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2% respectively, and delivering clear, rigorous reports.", "conclusion": "TSci provides a general, interpretable, and extensible forecasting workflow that reduces human intervention and improves accuracy across diverse time-series tasks."}}
{"id": "2510.02250", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02250", "abs": "https://arxiv.org/abs/2510.02250", "authors": ["Gonzalo Gonzalez-Pumariega", "Vincent Tu", "Chih-Lun Lee", "Jiachen Yang", "Ang Li", "Xin Eric Wang"], "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use", "comment": "23 pages, 7 figures, 10 tables", "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.", "AI": {"tldr": "Behavior Best-of-N (bBoN) scales computer-use agents by generating multiple rollouts and selecting among them via behavior narratives, improving robustness and achieving state-of-the-art performance on OSWorld with strong generalization to other platforms.", "motivation": "CUAs are unreliable and highly variable on long-horizon, complex tasks; robust performance requires scalable, structured trajectory understanding and selection.", "method": "Generate multiple rollouts per task, describe each rollout with behavior narratives, and select among them; perform ablations; evaluate on OSWorld, WindowsAgentArena, and AndroidWorld.", "result": "Achieves SoTA performance of 69.9% on OSWorld and near-human 72%; strong generalization to WindowsAgentArena and AndroidWorld; ablations validate design choices; highlights effectiveness of scaling CUAs when trajectories are understood and selected.", "conclusion": "Structured trajectory understanding paired with principled selection (bBoN) is an effective framework for scaling CUAs, enabling robust performance and broad generalization."}}
{"id": "2510.02030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02030", "abs": "https://arxiv.org/abs/2510.02030", "authors": ["Jenna Kline", "Maksim Kholiavchenko", "Samuel Stevens", "Nina van Tiel", "Alison Zhong", "Namrata Banerji", "Alec Sheets", "Sowbaranika Balasubramaniam", "Isla Duporge", "Matthew Thompson", "Elizabeth Campolongo", "Jackson Miliko", "Neil Rosser", "Tanya Berger-Wolf", "Charles V. Stewart", "Daniel I. Rubenstein"], "title": "kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring", "comment": "31 pages", "summary": "A comprehensive understanding of animal behavior ecology depends on scalable\napproaches to quantify and interpret complex, multidimensional behavioral\npatterns. Traditional field observations are often limited in scope,\ntime-consuming, and labor-intensive, hindering the assessment of behavioral\nresponses across landscapes. To address this, we present kabr-tools (Kenyan\nAnimal Behavior Recognition Tools), an open-source package for automated\nmulti-species behavioral monitoring. This framework integrates drone-based\nvideo with machine learning systems to extract behavioral, social, and spatial\nmetrics from wildlife footage. Our pipeline leverages object detection,\ntracking, and behavioral classification systems to generate key metrics,\nincluding time budgets, behavioral transitions, social interactions, habitat\nassociations, and group composition dynamics. Compared to ground-based methods,\ndrone-based observations significantly improved behavioral granularity,\nreducing visibility loss by 15% and capturing more transitions with higher\naccuracy and continuity. We validate kabr-tools through three case studies,\nanalyzing 969 behavioral sequences, surpassing the capacity of traditional\nmethods for data capture and annotation. We found that, like Plains zebras,\nvigilance in Grevy's zebras decreases with herd size, but, unlike Plains\nzebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit\nstrong behavioral inertia, with rare transitions to alert behaviors and\nobserved spatial segregation between Grevy's zebras, Plains zebras, and\ngiraffes in mixed-species herds. By enabling automated behavioral monitoring at\nscale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing\nconservation, biodiversity research, and ecological monitoring.", "AI": {"tldr": "Open-source drone-based framework (kabr-tools) enables automated, multi-species behavioral monitoring at scale, improving granularity and reducing labor vs. ground methods.", "motivation": "Traditional field observations are time-consuming, limited in scope, and labor-intensive, hindering scalable assessment of complex, multidimensional animal behaviors across landscapes.", "method": "Drone-based video integrated with machine learning for detection, tracking, and behavioral classification to extract metrics like time budgets, transitions, social interactions, habitat associations, and group dynamics; validated with three case studies across 969 sequences using Plains and Grevy's zebras and mixed-species herds.", "result": "Significant improvement in behavioral granularity (15% reduction in visibility loss), higher accuracy and continuity in detecting transitions, and successful analysis of 969 sequences across three case studies, revealing species-specific patterns in vigilance, habitat effect, behavioral inertia, and inter-species spatial segregation.", "conclusion": "kabr-tools enables automated, scalable behavioral monitoring suitable for ecosystem-wide studies, with implications for conservation, biodiversity research, and ecological monitoring."}}
{"id": "2510.01539", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01539", "abs": "https://arxiv.org/abs/2510.01539", "authors": ["Aniket Vashishtha", "Qirun Dai", "Hongyuan Mei", "Amit Sharma", "Chenhao Tan", "Hao Peng"], "title": "Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code", "comment": null, "summary": "Counterfactual reasoning, a hallmark of intelligence, consists of three\nsteps: inferring latent variables from observations (abduction), constructing\nalternatives (interventions), and predicting their outcomes (prediction). This\nskill is essential for advancing LLMs' causal understanding and expanding their\napplications in high-stakes domains such as scientific research. However,\nexisting efforts in assessing LLM's counterfactual reasoning capabilities tend\nto skip the abduction step, effectively reducing to interventional reasoning\nand leading to overestimation of LLM performance. To address this, we introduce\nexecutable counterfactuals, a novel framework that operationalizes causal\nreasoning through code and math problems. Our framework explicitly requires all\nthree steps of counterfactual reasoning and enables scalable synthetic data\ncreation with varying difficulty, creating a frontier for evaluating and\nimproving LLM's reasoning. Our results reveal substantial drop in accuracy\n(25-40%) from interventional to counterfactual reasoning for SOTA models like\no4-mini and Claude-4-Sonnet. To address this gap, we construct a training set\ncomprising counterfactual code problems having if-else condition and test on\nout-of-domain code structures (e.g. having while-loop); we also test whether a\nmodel trained on code would generalize to counterfactual math word problems.\nWhile supervised finetuning on stronger models' reasoning traces improves\nin-domain performance of Qwen models, it leads to a decrease in accuracy on OOD\ntasks such as counterfactual math problems. In contrast, reinforcement learning\ninduces the core cognitive behaviors and generalizes to new domains, yielding\ngains over the base model on both code (improvement of 1.5x-2x) and math\nproblems. Analysis of the reasoning traces reinforces these findings and\nhighlights the promise of RL for improving LLMs' counterfactual reasoning.", "AI": {"tldr": "Executable counterfactuals framework reveals that current LLMs struggle with the full three-step counterfactual reasoning (abduction, intervention, and prediction). There is a substantial accuracy drop (25-40%) from interventional to counterfactual tasks for state-of-the-art models, and reinforcement learning improves generalization and performance on code and math tasks, while supervised fine-tuning helps in-domain but hurts out-of-domain (OOD) tasks.", "motivation": "Existing evaluations of LLM counterfactual reasoning often skip the abductive step, reducing the problem to interventional reasoning and overestimating capabilities. A scalable, executable framework that enforces all three steps is needed to properly assess and improve causal reasoning, especially for high-stakes domains.", "method": "Propose executable counterfactuals using code and math problems that require abductive inference, interventions, and predictions. Generate scalable synthetic data with varying difficulty. Evaluate different training strategies (supervised finetuning on reasoning traces vs reinforcement learning) and test generalization to out-of-domain structures (e.g., while loops) and math word problems.", "result": "Found substantial performance gaps: SOTA models exhibit a 25-40% drop from interventional to counterfactual reasoning. Supervised finetuning improves in-domain (code) performance for some models (e.g., Qwen) but reduces accuracy on OOD counterfactual math problems. In contrast, reinforcement learning yields core cognitive behaviors and generalizes to new domains, providing 1.5x-2x gains on code and math problems.", "conclusion": "Reinforcement learning shows strong promise for instilling robust counterfactual reasoning in LLMs, and the executable counterfactuals framework provides a scalable path to evaluating and improving LLM causal reasoning across domains, including high-stakes settings."}}
{"id": "2510.02263", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02263", "abs": "https://arxiv.org/abs/2510.02263", "authors": ["Yuxiao Qu", "Anikait Singh", "Yoonho Lee", "Amrith Setlur", "Ruslan Salakhutdinov", "Chelsea Finn", "Aviral Kumar"], "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems", "comment": null, "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.", "AI": {"tldr": "RLAD introduces a two-player RL framework that learns concise reasoning abstractions to guide solution generation, improving generalization to harder problems by focusing test-time compute on abstraction generation.", "motivation": "Reasoning with large models often relies on pattern matching and verbose exploration rather than reusable procedural knowledge. There is a need for concise abstractions that capture procedural and factual knowledge to steer deduction.", "method": "Propose multiple reasoning abstractions for a problem, then use RL to incentivize building a solution that leverages these abstractions. The framework, RLAD, jointly trains an abstraction generator and a solution generator in a two-player setup, enabling structured exploration and decoupled learning signals.", "result": "The approach improves generalization to harder problems and reveals that allocating more test-time compute to generating abstractions yields greater gains than expanding solution generation budgets at test time.", "conclusion": "Reasoning abstractions effectively guide exploration and enable more robust, scalable reasoning in RL-trained models; RLAD is a practical framework for leveraging such abstractions."}}
{"id": "2510.02034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02034", "abs": "https://arxiv.org/abs/2510.02034", "authors": ["Mengtian Li", "Yunshu Bai", "Yimin Chu", "Yijun Shen", "Zhongmei Li", "Weifeng Ge", "Zhifeng Xie", "Chaofeng Chen"], "title": "GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing", "comment": "Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/", "summary": "We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape\nand texture morphing from multi-view images. Previous approaches usually rely\non point clouds or require pre-defined homeomorphic mappings for untextured\ndata. Our method overcomes these limitations by leveraging mesh-guided 3D\nGaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.\nThe core of our framework is a unified deformation strategy that anchors\n3DGaussians to reconstructed mesh patches, ensuring geometrically consistent\ntransformations while preserving texture fidelity through topology-aware\nconstraints. In parallel, our framework establishes unsupervised semantic\ncorrespondence by using the mesh topology as a geometric prior and maintains\nstructural integrity via physically plausible point trajectories. This\nintegrated approach preserves both local detail and global semantic coherence\nthroughout the morphing process with out requiring labeled data. On our\nproposed TexMorph benchmark, GaussianMorphing substantially outperforms prior\n2D/3D methods, reducing color consistency error ($\\Delta E$) by 22.2% and EI by\n26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/", "AI": {"tldr": "A mesh-guided, unsupervised semantic morphing framework (GaussianMorphing) that leverages 3D Gaussian Splatting anchored to mesh patches to achieve geometry- and texture-consistent 3D morphing without labels, outperforming prior methods on TexMorph.", "motivation": "The field lacks effective semantic-aware 3D morphing methods that work on textured shapes without requiring pre-defined homeomorphic mappings or labeled data. There is a need to preserve local detail while maintaining global semantic coherence, across multi-view images, and to handle both geometry and texture simultaneously.", "method": "GaussianMorphing unifies a deformation strategy that anchors 3D Gaussians to reconstructed mesh patches to ensure geometrically consistent transformations. It uses mesh-guided 3D Gaussian Splatting for high-fidelity geometry and appearance, and imposes topology-aware constraints to preserve texture. It also establishes unsupervised semantic correspondence via the mesh topology as a geometric prior and enforces physically plausible point trajectories to maintain structural integrity. The approach is end-to-end and does not require labeled data.", "result": "On the TexMorph benchmark, GaussianMorphing substantially outperforms prior 2D/3D methods, reducing color consistency error (Delta E) by 22.2% and EI by 26.2%. Project page provided.", "conclusion": "The integrated approach yields strong local detail preservation and global semantic coherence throughout morphing without labeled data, leveraging mesh topology and 3D Gaussian Splatting to bridge geometry and texture in a unified framework."}}
{"id": "2510.02276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02276", "abs": "https://arxiv.org/abs/2510.02276", "authors": ["Chenqi Li", "Yu Liu", "Timothy Denison", "Tingting Zhu"], "title": "BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals", "comment": null, "summary": "Biosignals offer valuable insights into the physiological states of the human\nbody. Although biosignal modalities differ in functionality, signal fidelity,\nsensor comfort, and cost, they are often intercorrelated, reflecting the\nholistic and interconnected nature of human physiology. This opens up the\npossibility of performing the same tasks using alternative biosignal\nmodalities, thereby improving the accessibility, usability, and adaptability of\nhealth monitoring systems. However, the limited availability of large labeled\ndatasets presents challenges for training models tailored to specific tasks and\nmodalities of interest. Unsupervised cross-modal knowledge transfer offers a\npromising solution by leveraging knowledge from an existing modality to support\nmodel training for a new modality. Existing methods are typically based on\nknowledge distillation, which requires running a teacher model alongside\nstudent model training, resulting in high computational and memory overhead.\nThis challenge is further exacerbated by the recent development of foundation\nmodels that demonstrate superior performance and generalization across tasks at\nthe cost of large model sizes. To this end, we explore a new framework for\nunsupervised cross-modal knowledge transfer of biosignals by training a\nlightweight bridge network to align the intermediate representations and enable\ninformation flow between foundation models and across modalities. Specifically,\nwe introduce an efficient strategy for selecting alignment positions where the\nbridge should be constructed, along with a flexible prototype network as the\nbridge architecture. Extensive experiments across multiple biosignal\nmodalities, tasks, and datasets show that BioX-Bridge reduces the number of\ntrainable parameters by 88--99\\% while maintaining or even improving transfer\nperformance compared to state-of-the-art methods.", "AI": {"tldr": "A lightweight bridge-based framework (BioX-Bridge) enables unsupervised cross-modal knowledge transfer for biosignals by aligning intermediate representations between foundation models across modalities, reducing trainable parameters by 88\u201399% without sacrificing, and often improving, transfer performance.", "motivation": "Biosignals are interrelated, allowing tasks to be performed with different modalities. The scarcity of large labeled datasets for each modality hinders task-specific model training. Unsupervised cross-modal transfer offers a resource-efficient alternative, but existing methods rely on costly teacher-student distillation, which is impractical with large foundation models.", "method": "Introduce BioX-Bridge, a lightweight bridge network that aligns intermediate representations to enable information flow between foundation models and across biosignal modalities. Propose an efficient strategy to select alignment positions for the bridge and a flexible prototype network architecture for the bridge.", "result": "Across multiple biosignal modalities, tasks, and datasets, BioX-Bridge reduces trainable parameters by 88\u201399% while maintaining or improving transfer performance relative to state-of-the-art methods.", "conclusion": "Unsupervised cross-modal transfer can be made practical and scalable for biosignals by using a compact bridge that aligns representations between foundation models across modalities, enabling efficient knowledge transfer with far fewer trainable parameters."}}
{"id": "2510.02043", "categories": ["cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02043", "abs": "https://arxiv.org/abs/2510.02043", "authors": ["Sahil Bhandary Karnoor", "Romit Roy Choudhury"], "title": "Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers", "comment": null, "summary": "Pose estimation refers to tracking a human's full body posture, including\ntheir head, torso, arms, and legs. The problem is challenging in practical\nsettings where the number of body sensors are limited. Past work has shown\npromising results using conditional diffusion models, where the pose prediction\nis conditioned on both <location, rotation> measurements from the sensors.\nUnfortunately, nearly all these approaches generalize poorly across users,\nprimarly because location measurements are highly influenced by the body size\nof the user. In this paper, we formulate pose estimation as an inverse problem\nand design an algorithm capable of zero-shot generalization. Our idea utilizes\na pre-trained diffusion model and conditions it on rotational measurements\nalone; the priors from this model are then guided by a likelihood term, derived\nfrom the measured locations. Thus, given any user, our proposed InPose method\ngeneratively estimates the highly likely sequence of poses that best explains\nthe sparse on-body measurements.", "AI": {"tldr": "InPose uses a pre-trained diffusion model conditioned on rotational measurements, guided by a location-based likelihood, to achieve zero-shot pose estimation from sparse on-body sensors.", "motivation": "Existing diffusion-based pose estimation methods conditioned on both location and rotation generalize poorly across users due to body-size variation. A user-agnostic, zero-shot approach is desirable for sparse sensor setups.", "method": "Formulate pose estimation as an inverse problem. Use a pre-trained diffusion model conditioned on rotations alone; incorporate a likelihood term derived from measured locations to guide the sampling process, yielding pose sequences that explain the sparse data.", "result": "The method is reported to achieve zero-shot generalization across users and produce highly plausible pose sequences that reconcile sparse location measurements with rotational priors.", "conclusion": "Zero-shot, diffusion-prior-based pose estimation is achievable by decoupling body-size-sensitive location cues from rotation priors, enabling robust inference from sparse sensor data."}}
{"id": "2510.01549", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01549", "abs": "https://arxiv.org/abs/2510.01549", "authors": ["Kevin Zhai", "Utsav Singh", "Anirudh Thatipelli", "Souradip Chakraborty", "Anit Kumar Sahu", "Furong Huang", "Amrit Singh Bedi", "Mubarak Shah"], "title": "MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models", "comment": null, "summary": "Diffusion models excel at generating images conditioned on text prompts, but\nthe resulting images often do not satisfy user-specific criteria measured by\nscalar rewards such as Aesthetic Scores. This alignment typically requires\nfine-tuning, which is computationally demanding. Recently, inference-time\nalignment via noise optimization has emerged as an efficient alternative,\nmodifying initial input noise to steer the diffusion denoising process towards\ngenerating high-reward images. However, this approach suffers from reward\nhacking, where the model produces images that score highly, yet deviate\nsignificantly from the original prompt. We show that noise-space regularization\nis insufficient and that preventing reward hacking requires an explicit\nimage-space constraint. To this end, we propose MIRA (MItigating Reward\nhAcking), a training-free, inference-time alignment method. MIRA introduces an\nimage-space, score-based KL surrogate that regularizes the sampling trajectory\nwith a frozen backbone, constraining the output distribution so reward can\nincrease without off-distribution drift (reward hacking). We derive a tractable\napproximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple\nrewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,\nAnimal-Animal, HPDv2), MIRA achieves >60\\% win rate vs. strong baselines while\npreserving prompt adherence; mechanism plots show reward gains with near-zero\ndrift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,\nmapping preference optimization to inference time with a frozen backbone,\nextending MIRA to non-differentiable rewards without fine-tuning.", "AI": {"tldr": "MIRA is a training-free, inference-time alignment method for diffusion models that uses an image-space KL surrogate with a frozen backbone to prevent reward hacking, improving reward optimization without drifting from the prompt; extended via MIRA-DPO to handle non-differentiable rewards.", "motivation": "Reward-oriented prompts often fail because models optimize rewards that can be gamed (reward hacking); noise-space regularization alone is insufficient to preserve prompt fidelity during alignment. An explicit image-space constraint is needed.", "method": "Introduce MIRA: an inference-time, training-free approach that regularizes diffusion sampling with an image-space, score-based KL surrogate using a frozen backbone. Derive a tractable KL approximation via diffusion scores. Evaluate across SDv1.5/SDXL, multiple rewards (Aesthetic, HPSv2, PickScore), and datasets (e.g., Animal-Animal, HPDv2). Extend to non-differentiable rewards with MIRA-DPO.", "result": "MIRA achieves over 60% win rate vs strong baselines while preserving prompt adherence; mechanism plots show reward gains with near-zero drift, whereas traditional noise optimization drifts with compute. DNO shows drift as compute increases.", "conclusion": "MIRA provides a training-free, inference-time alignment that mitigates reward hacking through an image-space constraint with a frozen backbone, enabling reliable reward optimization without prompt drift. MIRA-DPO extends this to non-differentiable rewards."}}
{"id": "2510.02086", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02086", "abs": "https://arxiv.org/abs/2510.02086", "authors": ["Arman Behnam"], "title": "VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation", "comment": null, "summary": "Accurate detection and segmentation of brain tumors from magnetic resonance\nimaging (MRI) are essential for diagnosis, treatment planning, and clinical\nmonitoring. While convolutional architectures such as U-Net have long been the\nbackbone of medical image segmentation, their limited capacity to capture\nlong-range dependencies constrains performance on complex tumor structures.\nRecent advances in diffusion models have demonstrated strong potential for\ngenerating high-fidelity medical images and refining segmentation boundaries.\n  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor\nDetection and Segmentation framework, a transformer-driven diffusion framework\nfor brain tumor detection and segmentation. By embedding a vision transformer\nat the core of the diffusion process, the model leverages global contextual\nreasoning together with iterative denoising to enhance both volumetric accuracy\nand boundary precision. The transformer backbone enables more effective\nmodeling of spatial relationships across entire MRI volumes, while diffusion\nrefinement mitigates voxel-level errors and recovers fine-grained tumor\ndetails.\n  This hybrid design provides a pathway toward improved robustness and\nscalability in neuro-oncology, moving beyond conventional U-Net baselines.\nExperimental validation on MRI brain tumor datasets demonstrates consistent\ngains in Dice similarity and Hausdorff distance, underscoring the potential of\ntransformer-guided diffusion models to advance the state of the art in tumor\nsegmentation.", "AI": {"tldr": "A Vision-Guided Diffusion Model (VGDM) combines a Vision Transformer with a diffusion-based denoising process for brain tumor detection and segmentation, achieving improved boundary precision and volumetric accuracy over U-Net baselines.", "motivation": "Conventional CNNs like U-Net struggle to capture long-range dependencies in complex brain tumor structures; diffusion models can refine segmentation boundaries, and integrating global context via a transformer could enhance robustness and scalability for MRI volumes.", "method": "Embed a Vision Transformer at the core of a diffusion-based segmentation framework. The model uses global contextual reasoning across the entire MRI volume within the diffusion process, and iterative denoising refines voxel-level details to produce accurate tumor segmentation. This hybrid design aims to improve both volumetric accuracy and boundary precision.", "result": "Experimental validation on MRI brain tumor datasets shows consistent improvements in Dice similarity coefficient and Hausdorff distance compared with baseline methods.", "conclusion": "Transformer-guided diffusion models are a promising direction for brain tumor detection and segmentation, offering robustness and boundary precision improvements beyond conventional U-Net approaches."}}
{"id": "2510.01555", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01555", "abs": "https://arxiv.org/abs/2510.01555", "authors": ["Kezhao Liu", "Jason Klein Liu", "Mingtao Chen", "Yiming Liu"], "title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) leverages a\nKullback-Leibler (KL) divergence loss to stabilize training and prevent\noverfitting. However, in methods such as GRPO, its implementation may be guided\nby principles from numerical value estimation-a practice that overlooks the\nterm's functional role as an optimization loss. To analyze this issue, we\nestablish a unified framework that connects two seemingly distinct\nimplementation styles: using the mathematical term $k_n$ as a detached\ncoefficient for the policy's score function ('$k_n$ in reward') or as a direct\nloss function through which gradients are propagated ('$k_n$ as loss'). We show\nthat the latter can always be analyzed via an equivalent gradient coefficient\nin the former, unifying the two perspectives. Through this framework, we prove\nthat the conventional '$k_1$ in reward' (like in PPO) is the principled loss\nfor Reverse KL (RKL) regularization. We further establish a key finding: under\non-policy conditions, the '$k_2$ as loss' formulation is, in fact,\ngradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our\nwork, identifies both as the theoretically sound implementations of the RKL\nobjective. In contrast, we show that the recently adopted '$k_3$ as loss' (like\nin GRPO) is merely a first-order, biased approximation of the principled loss.\nFurthermore, we argue that common off-policy implementations of '$k_n$ as loss'\nmethods are biased due to neglected importance sampling, and we propose a\nprincipled correction. Our findings provide a comprehensive, gradient-based\nrationale for choosing and correctly implementing KL regularization, paving the\nway for more robust and effective RLHF systems.", "AI": {"tldr": "A unified gradient-based framework reconciles KL-regularized RLHF implementations, showing that the PPO-style k1-in-reward and k2-as-loss are on-policy gradient-equivalent principled reverse KL (RKL) losses; k3-as-loss is a first-order biased approximation; off-policy variants suffer from ignored importance sampling, requiring principled corrections.", "motivation": "Reinforcement Learning from Human Feedback commonly uses KL regularization, but different papers implement the KL term in conflicting ways (as a reward term vs. as a loss term). The aim is to clarify the principled objective and provide guidance on correct implementation to avoid bias.", "method": "Introduce a unified framework that treats the KL term coefficient k_n in two ways: (i) as a detached coefficient multiplying the policy score function (k_n in reward) and (ii) as a direct loss through which gradients propagate (k_n as loss). Prove that the second view can be analyzed via an equivalent gradient coefficient in the first. Under on-policy conditions, prove gradient-equivalence between k2-as-loss and k1-in-reward, and establish k1-as-loss/principled for RKL; show that k3-as-loss is only a first-order, biased approximation. Analyze off-policy implementations and identify biases due to neglected importance sampling, offering principled corrections.", "result": "Key findings include (1) k1-in-reward is the principled loss for Reverse KL regularization; (2) under on-policy conditions, k2-as-loss is gradient-equivalent to k1-in-reward, so both implement the same RKL objective; (3) k3-as-loss (as used in GRPO) is a biased approximation of the principled loss; (4) off-policy k_n-as-loss methods are biased unless importance sampling corrections are applied; (5) the work provides a gradient-based rationale for choosing and correctly implementing KL regularization in RLHF.", "conclusion": "The paper provides a unified, gradient-based understanding of KL regularization in RLHF, reconciling prior implementation choices and offering principled guidelines and corrections for robust, unbiased training across on-policy and off-policy settings."}}
{"id": "2510.02097", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02097", "abs": "https://arxiv.org/abs/2510.02097", "authors": ["Walid Rabehi", "Marion Le Texier", "R\u00e9mi Lemoy"], "title": "Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques", "comment": null, "summary": "Quantitative analysis of historical urban sprawl in France before the 1970s\nis hindered by the lack of nationwide digital urban footprint data. This study\nbridges this gap by developing a scalable deep learning pipeline to extract\nurban areas from the Scan Histo historical map series (1925-1950), which\nproduces the first open-access, national-scale urban footprint dataset for this\npivotal period. Our key innovation is a dual-pass U-Net approach designed to\nhandle the high radiometric and stylistic complexity of historical maps. The\nfirst pass, trained on an initial dataset, generates a preliminary map that\nidentifies areas of confusion, such as text and roads, to guide targeted data\naugmentation. The second pass uses a refined dataset and the binarized output\nof the first model to minimize radiometric noise, which significantly reduces\nfalse positives. Deployed on a high-performance computing cluster, our method\nprocesses 941 high-resolution tiles covering the entirety of metropolitan\nFrance. The final mosaic achieves an overall accuracy of 73%, effectively\ncapturing diverse urban patterns while overcoming common artifacts like labels\nand contour lines. We openly release the code, training datasets, and the\nresulting nationwide urban raster to support future research in long-term\nurbanization dynamics.", "AI": {"tldr": "First national-scale, open-access urban footprint for historic France (1925-1950) built via a dual-pass U-Net; 73% accuracy; code and data released.", "motivation": "To overcome the lack of nationwide digital urban footprint data for historical France pre-1970s, enabling long-term urbanization studies.", "method": "Dual-pass U-Net: first pass identifies confusion areas (text/roads) to guide targeted augmentation; second pass uses refined data and binarized first-pass output to reduce radiometric noise; HPC processing of 941 high-res tiles to produce a national raster; handles radiometric/style variation in historical maps.", "result": "Produces first open-access, national-scale urban footprint dataset for 1925-1950; overall accuracy of 73%; mitigates artifacts like labels/contour lines; supports long-term urbanization research.", "conclusion": "Openly releasing code, training data, and resulting raster will facilitate future research on urban dynamics in historical contexts."}}
{"id": "2510.01562", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01562", "abs": "https://arxiv.org/abs/2510.01562", "authors": ["Seong Woo Han", "Daniel Duy Vo", "Brielin C. Brown"], "title": "Large-Scale Bayesian Causal Discovery with Interventional Data", "comment": null, "summary": "Inferring the causal relationships among a set of variables in the form of a\ndirected acyclic graph (DAG) is an important but notoriously challenging\nproblem. Recently, advancements in high-throughput genomic perturbation screens\nhave inspired development of methods that leverage interventional data to\nimprove model identification. However, existing methods still suffer poor\nperformance on large-scale tasks and fail to quantify uncertainty. Here, we\npropose Interventional Bayesian Causal Discovery (IBCD), an empirical Bayesian\nframework for causal discovery with interventional data. Our approach models\nthe likelihood of the matrix of total causal effects, which can be approximated\nby a matrix normal distribution, rather than the full data matrix. We place a\nspike-and-slab horseshoe prior on the edges and separately learn data-driven\nweights for scale-free and Erd\\H{o}s-R\\'enyi structures from observational\ndata, treating each edge as a latent variable to enable uncertainty-aware\ninference. Through extensive simulation, we show that IBCD achieves superior\nstructure recovery compared to existing baselines. We apply IBCD to CRISPR\nperturbation (Perturb-seq) data on 521 genes, demonstrating that edge posterior\ninclusion probabilities enable identification of robust graph structures.", "AI": {"tldr": "IBCD introduces an empirical Bayesian framework for causal discovery from interventional data, modeling total causal effects with a matrix normal likelihood, using spike-and-slab horseshoe priors on edges and data-driven priors for scale-free and Erd\u0151s\u2013R\u00e9nyi graphs, enabling uncertainty-aware edge inference and scalable structure recovery. It shows superior performance in simulations and robust graph identification in Perturb-seq data (521 genes).", "motivation": "Inferring causal DAGs with interventional perturbations is crucial in genomics, but existing methods struggle with large-scale data and lack uncertainty quantification.", "method": "An empirical Bayesian approach that models the likelihood of the total causal effects matrix (approximated by a matrix normal). It uses spike-and-slab horseshoe priors on edges, and learns data-driven weights for scale-free and Erd\u0151s\u2013R\u00e9nyi graph structures from observational data. Each edge is treated as a latent variable to enable uncertainty-aware inference.", "result": "Simulation studies show superior structure recovery compared to baselines. Application to Perturb-seq data with 521 genes demonstrates that edge posterior inclusion probabilities can identify robust, uncertainty-aware graph structures.", "conclusion": "IBCD provides a scalable, uncertainty-aware framework for causal discovery with interventional data, capable of robustly identifying graph structures in large-scale genomic settings."}}
{"id": "2510.02100", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02100", "abs": "https://arxiv.org/abs/2510.02100", "authors": ["Woowon Jang", "Jiwon Im", "Juseung Choi", "Niki Rashidian", "Wesley De Neve", "Utku Ozbulak"], "title": "When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos", "comment": "Accepted for publication in the 28th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI) Workshop\n  on Collaborative Intelligence and Autonomy in Image-guided Surgery (COLAS),\n  2025", "summary": "Video object segmentation (VOS) models such as SAM2 offer promising zero-shot\ntracking capabilities for surgical videos using minimal user input. Among the\navailable input types, point-based tracking offers an efficient and low-cost\nalternative, yet its reliability and failure cases in complex surgical\nenvironments are not well understood. In this work, we systematically analyze\nthe failure modes of point-based tracking in laparoscopic cholecystectomy\nvideos. Focusing on three surgical targets, the gallbladder, grasper, and\nL-hook electrocautery, we compare the performance of point-based tracking with\nsegmentation mask initialization. Our results show that point-based tracking is\ncompetitive for surgical tools but consistently underperforms for anatomical\ntargets, where tissue similarity and ambiguous boundaries lead to failure.\nThrough qualitative analysis, we reveal key factors influencing tracking\noutcomes and provide several actionable recommendations for selecting and\nplacing tracking points to improve performance in surgical video analysis.", "AI": {"tldr": "Point-based tracking in surgical videos competes with mask-based methods for tools but underperforms on anatomical targets due to tissue similarity and ambiguous boundaries; provides actionable guidance on point placement.", "motivation": "To understand reliability and failure modes of point-based tracking in complex surgical environments and provide practical guidelines.", "method": "Systematic analysis on laparoscopic cholecystectomy videos focusing on three targets (gallbladder, grasper, L-hook electrocautery); compare point-based tracking against segmentation mask initialization; include qualitative analysis.", "result": "Point-based tracking is competitive for surgical tools but consistently underperforms for anatomical targets due to tissue similarity and ambiguous boundaries; qualitative factors influencing tracking outcomes identified; actionable recommendations for selecting and placing tracking points to improve performance.", "conclusion": "The study offers guidelines to improve point-based tracking in surgical video analysis and discusses trade-offs with segmentation mask initialization."}}
{"id": "2510.01565", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.01565", "abs": "https://arxiv.org/abs/2510.01565", "authors": ["Runyu Lu", "Shiqi He", "Wenxuan Tan", "Shenggui Li", "Ruofan Wu", "Jeff J. Ma", "Ang Chen", "Mosharaf Chowdhury"], "title": "TetriServe: Efficient DiT Serving for Heterogeneous Image Generation", "comment": null, "summary": "Diffusion Transformer (DiT) models excel at generating highquality images\nthrough iterative denoising steps, but serving them under strict Service Level\nObjectives (SLOs) is challenging due to their high computational cost,\nparticularly at large resolutions. Existing serving systems use fixed degree\nsequence parallelism, which is inefficient for heterogeneous workloads with\nmixed resolutions and deadlines, leading to poor GPU utilization and low SLO\nattainment.\n  In this paper, we propose step-level sequence parallelism to dynamically\nadjust the parallel degree of individual requests according to their deadlines.\nWe present TetriServe, a DiT serving system that implements this strategy for\nhighly efficient image generation. Specifically, TetriServe introduces a novel\nround-based scheduling mechanism that improves SLO attainment: (1) discretizing\ntime into fixed rounds to make deadline-aware scheduling tractable, (2)\nadapting parallelism at the step level and minimize GPU hour consumption, and\n(3) jointly packing requests to minimize late completions. Extensive evaluation\non state-of-the-art DiT models shows that TetriServe achieves up to 32% higher\nSLO attainment compared to existing solutions without degrading image quality.", "AI": {"tldr": "TetriServe uses step-level sequence parallelism and round-based scheduling to serve Diffusion Transformer (DiT) models under strict SLOs, achieving efficient GPU utilization and up to 32% better SLO attainment without harming image quality.", "motivation": "DiT generation is computationally intensive, and current serving systems with fixed parallelism underutilize GPUs and fail to meet diverse deadlines for heterogeneous workloads, especially at high resolutions.", "method": "Introduce step-level sequence parallelism that dynamically adjusts parallel degree per request based on deadlines; implement a round-based scheduling mechanism that discretizes time into rounds, adapts parallelism at the step level, and jointly packs requests to minimize late completions; TetriServe is the end-to-end system implementing this strategy for DiT serving.", "result": "Extensive evaluation on state-of-the-art DiT models shows TetriServe achieves up to 32% higher SLO attainment compared to existing solutions, without degrading image quality.", "conclusion": "Deadline-aware, dynamic parallelism with round-based scheduling can significantly improve SLO attainment and GPU efficiency for DiT serving, making high-resolution diffusion-based image generation more practical in production."}}
{"id": "2510.02114", "categories": ["cs.CV", "68T10"], "pdf": "https://arxiv.org/pdf/2510.02114", "abs": "https://arxiv.org/abs/2510.02114", "authors": ["Ding-Ruei Shen"], "title": "FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation", "comment": "Master Thesis", "summary": "Federeated Learning (FL) offers a privacy-preserving solution for Semantic\nSegmentation (SS) tasks to adapt to new domains, but faces significant\nchallenges from these domain shifts, particularly when client data is\nunlabeled. However, most existing FL methods unrealistically assume access to\nlabeled data on remote clients or fail to leverage the power of modern Vision\nFoundation Models (VFMs). Here, we propose a novel and challenging task,\nFFREEDG, in which a model is pretrained on a server's labeled source dataset\nand subsequently trained across clients using only their unlabeled data,\nwithout ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a\nframework that leverages the knowledge of a VFM by integrating vision and\nlanguage modalities. Our approach employs a Vision-Language decoder guided by\nCLIP-based text embeddings to improve semantic disambiguation and uses a\nweak-to-strong consistency learning strategy for robust local training on\npseudo-labels. Our experiments on synthetic-to-real and\nclear-to-adverse-weather benchmarks demonstrate that our framework effectively\ntackles this new task, achieving competitive performance against established\ndomain generalization and adaptation methods and setting a strong baseline for\nfuture research.", "AI": {"tldr": "Introduces FFREEDG, a federated learning task for semantic segmentation with unlabeled client data and no access to the server's source data, and presents FRIEREN, a Vision-Language model\u2013based framework that uses CLIP-based text guidance and weak-to-strong consistency for robust pseudo-label training.", "motivation": "Tackle domain shifts in federated semantic segmentation when client data are unlabeled and server data cannot be re-accessed; leverage powerful Vision Foundation Models to improve semantic disambiguation across domains.", "method": "Pretraining on a labeled source server, then federated training across clients using only unlabeled data. FRIEREN integrates a Vision-Language decoder guided by CLIP-based text embeddings, and employs weak-to-strong consistency learning for robust local training on pseudo-labels.", "result": "Experimental results on synthetic-to-real and clear-to-adverse-weather benchmarks show that FRIEREN effectively addresses FFREEDG, achieving competitive performance against established domain generalization and adaptation methods and establishing a strong baseline for future research.", "conclusion": "FFREEDG is a viable and challenging benchmark for federated semantic segmentation with unlabeled data and no source access, and FRIEREN demonstrates how Vision-Language collaboration via CLIP and consistency-based pseudo-labeling can enable strong performance under such constraints."}}
{"id": "2510.01571", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.01571", "abs": "https://arxiv.org/abs/2510.01571", "authors": ["Hanqun Cao", "Hongrui Zhang", "Junde Xu", "Zhou Zhang", "Lingdong Shen", "Minghao Sun", "Ge Liu", "Jinbo Xu", "Wu-Jun Li", "Jinren Ni", "Cesar de la Fuente-Nunez", "Tianfan Fu", "Yejin Choi", "Pheng-Ann Heng", "Fang Wu"], "title": "From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?", "comment": "24 pages, 7 figures, 4 tables", "summary": "Protein language models (PLMs) have advanced computational protein science\nthrough large-scale pretraining and scalable architectures. In parallel,\nreinforcement learning (RL) has broadened exploration and enabled precise\nmulti-objective optimization in protein design. Yet whether RL can push PLMs\nbeyond their pretraining priors to uncover latent sequence-structure-function\nrules remains unclear. We address this by pairing RL with PLMs across four\ndomains: antimicrobial peptide design, kinase variant optimization, antibody\nengineering, and inverse folding. Using diverse RL algorithms and model\nclasses, we ask if RL improves sampling efficiency and, more importantly, if it\nreveals capabilities not captured by supervised learning. Across benchmarks, RL\nconsistently boosts success rates and sample efficiency. Performance follows a\nthree-factor interaction: task headroom, reward fidelity, and policy capacity\njointly determine gains. When rewards are accurate and informative, policies\nhave sufficient capacity, and tasks leave room beyond supervised baselines,\nimprovements scale; when rewards are noisy or capacity is constrained, gains\nsaturate despite exploration. This view yields practical guidance for RL in\nprotein design: prioritize reward modeling and calibration before scaling\npolicy size, match algorithm and regularization strength to task difficulty,\nand allocate capacity where marginal gains are largest. Implementation is\navailable at https://github.com/chq1155/RL-PLM.", "AI": {"tldr": "RL with protein language models enhances sampling efficiency and uncovers latent sequence-structure-function rules in protein design; gains depend on reward fidelity, policy capacity, and task headroom; prioritize reward modeling/calibration before scaling policy size.", "motivation": "To determine whether reinforcement learning can push PLMs beyond pretraining priors to uncover latent rules governing sequence-structure-function in proteins, and to evaluate this across diverse design domains.", "method": "Pair reinforcement learning with protein language models across four design domains (antimicrobial peptides, kinase variants, antibodies, inverse folding) using diverse RL algorithms and model classes; assess sampling efficiency, success rates, and the interaction of task headroom, reward fidelity, and policy capacity; derive practical guidelines.", "result": "Across benchmarks, RL consistently boosts success rates and sampling efficiency. Gains follow a three-factor interaction: with accurate rewards, sufficient policy capacity, and task headroom beyond supervised baselines, improvements scale; if rewards are noisy or capacity is limited, gains saturate despite exploration.", "conclusion": "RL\u2013PLM approaches can meaningfully improve protein design when rewards are well-modeled and policy capacity matches task difficulty; prioritize reward modeling and calibration before scaling policy size, and allocate capacity where marginal gains are largest. A public implementation is provided."}}
{"id": "2510.02155", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02155", "abs": "https://arxiv.org/abs/2510.02155", "authors": ["Shu Zou", "Xinyu Tian", "Lukas Wesemann", "Fabian Waschkowski", "Zhaoyuan Yang", "Jing Zhang"], "title": "Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting", "comment": "14 pages, video anomaly detection", "summary": "Prompting has emerged as a practical way to adapt frozen vision-language\nmodels (VLMs) for video anomaly detection (VAD). Yet, existing prompts are\noften overly abstract, overlooking the fine-grained human-object interactions\nor action semantics that define complex anomalies in surveillance videos. We\npropose ASK-Hint, a structured prompting framework that leverages\naction-centric knowledge to elicit more accurate and interpretable reasoning\nfrom frozen VLMs. Our approach organizes prompts into semantically coherent\ngroups (e.g. violence, property crimes, public safety) and formulates\nfine-grained guiding questions that align model predictions with discriminative\nvisual cues. Extensive experiments on UCF-Crime and XD-Violence show that\nASK-Hint consistently improves AUC over prior baselines, achieving\nstate-of-the-art performance compared to both fine-tuned and training-free\nmethods. Beyond accuracy, our framework provides interpretable reasoning traces\ntowards anomaly and demonstrates strong generalization across datasets and VLM\nbackbones. These results highlight the critical role of prompt granularity and\nestablish ASK-Hint as a new training-free and generalizable solution for\nexplainable video anomaly detection.", "AI": {"tldr": "ASK-Hint introduces a structured, action-centric prompting framework for frozen vision-language models in video anomaly detection, yielding improved accuracy, interpretability, and cross-dataset/generalization without training.", "motivation": "Current prompts for VLM-based VAD are often overly abstract and fail to capture fine-grained human\u2013object interactions and action semantics essential for anomalies; there is a need for interpretable, domain-informed prompts and training-free approaches.", "method": "Organize prompts into semantically coherent groups (e.g., violence, property crimes, public safety) and craft fine-grained guiding questions that tie model predictions to discriminative visual cues; leverages action-centric knowledge with frozen VLMs, enabling training-free inference.", "result": "On UCF-Crime and XD-Violence, ASK-Hint yields consistent AUC gains over prior baselines, attaining state-of-the-art performance among both fine-tuned and training-free methods; demonstrates interpretable reasoning traces and strong generalization across datasets and VLM backbones.", "conclusion": "Prompt granularity and structured, action-centric prompting are crucial; ASK-Hint provides a generalizable, training-free pathway for explainable video anomaly detection."}}
{"id": "2510.01578", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01578", "abs": "https://arxiv.org/abs/2510.01578", "authors": ["Haochen You", "Baojing Liu"], "title": "Gradient Shaping Beyond Clipping: A Functional Perspective on Update Magnitude Control", "comment": "Accepted as a conference paper at ACM Multimedia Asia 2025", "summary": "Gradient clipping is widely used to stabilize deep network training, but its\nformulation as a hard, fixed threshold limits flexibility and ignores gradient\ndistribution dynamics. We propose SPAMP (Statistical Per-layer Adaptive\nModulation and Projection), a unified framework that generalizes clipping into\nsmooth, per-layer gradient shaping. SPAMP tracks local gradient statistics,\ndynamically estimates thresholds, and applies power-based transformations to\nmodulate update magnitudes in a differentiable manner. This perspective recasts\nclipping and warmup as dual mechanisms for controlling the effective update\nscale $\\eta_t \\|g_t\\|$, offering a principled alternative to rigid heuristics.\nExtensive experiments across image and language tasks demonstrate that SPAMP\nimproves stability, convergence, and robustness over existing methods.", "AI": {"tldr": "SPAMP is a differentiable, per-layer gradient shaping framework that generalizes gradient clipping into adaptive, statistics-driven modulation to improve stability and convergence.", "motivation": "Rigid hard gradient clipping thresholds ignore the evolving distribution of gradients during training; a per-layer, adaptive mechanism is needed to flexibly control update magnitudes.", "method": "SPAMP tracks local gradient statistics per layer, dynamically estimates thresholds, and applies power-based transformations to modulate update magnitudes in a differentiable manner. It unifies clipping and warmup as dual mechanisms for controlling the effective update scale \u03b7_t ||g_t||.", "result": "Empirical evidence on image and language tasks shows SPAMP improves stability, convergence speed, and robustness compared with existing clipping/warmup methods.", "conclusion": "SPAMP offers a principled alternative to rigid heuristics, providing smooth gradient shaping that adapts to gradient dynamics and generalizes clipping across tasks."}}
{"id": "2510.02186", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02186", "abs": "https://arxiv.org/abs/2510.02186", "authors": ["Weijia Dou", "Xu Zhang", "Yi Bin", "Jian Liu", "Bo Peng", "Guoqing Wang", "Yang Yang", "Heng Tao Shen"], "title": "GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation", "comment": null, "summary": "Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to\n3D semantic segmentation expose a persistent trade-off. Directly projecting 2D\nfeatures into 3D yields noisy and fragmented predictions, whereas enforcing\ngeometric coherence necessitates costly training pipelines and large-scale\nannotated 3D data. We argue that this limitation stems from the dominant\nsegmentation-and-matching paradigm, which fails to reconcile 2D semantics with\n3D geometric structure. The geometric cues are not eliminated during the\n2D-to-3D transfer but remain latent within the noisy and view-aggregated\nfeatures. To exploit this property, we propose GeoPurify that applies a small\nStudent Affinity Network to purify 2D VLM-generated 3D point features using\ngeometric priors distilled from a 3D self-supervised teacher model. During\ninference, we devise a Geometry-Guided Pooling module to further denoise the\npoint cloud and ensure the semantic and structural consistency. Benefiting from\nlatent geometric information and the learned affinity network, GeoPurify\neffectively mitigates the trade-off and achieves superior data efficiency.\nExtensive experiments on major 3D benchmarks demonstrate that GeoPurify\nachieves or surpasses state-of-the-art performance while utilizing only about\n1.5% of the training data. Our codes and checkpoints are available at\n[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).", "AI": {"tldr": "GeoPurify introduces a lightweight Student Affinity Network and geometry-guided pooling to purify 2D VLM-derived 3D features using geometric priors from a 3D self-supervised teacher, achieving data-efficient, state-of-the-art 3D semantic segmentation.", "motivation": "Transferring 2D Vision-Language Model features to 3D segmentation faces a persistent trade-off: directly projecting 2D features yields noisy, fragmented predictions, while enforcing geometric coherence requires costly training and large-scale 3D annotations. The limitation stems from treating segmentation and matching separately, failing to reconcile 2D semantics with 3D geometry.", "method": "GeoPurify uses a small Student Affinity Network to purify 2D VLM-generated 3D point features by leveraging geometric priors distilled from a 3D self-supervised teacher. At inference, a Geometry-Guided Pooling module denoises the point cloud to preserve semantic-structural consistency, exploiting latent geometric information for improved data efficiency.", "result": "On major 3D benchmarks, GeoPurify matches or surpasses state-of-the-art performance while using only about 1.5% of the training data, demonstrating superior data efficiency; code is available publicly.", "conclusion": "Exploiting latent geometric cues within 2D-to-3D transfers via a lightweight affinity network and geometry-guided pooling can overcome the traditional trade-off, enabling high-quality 3D semantic segmentation with substantially less annotated data."}}
{"id": "2510.01581", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01581", "abs": "https://arxiv.org/abs/2510.01581", "authors": ["Joykirat Singh", "Justin Chih-Yao Chen", "Archiki Prasad", "Elias Stengel-Eskin", "Akshay Nambi", "Mohit Bansal"], "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression", "comment": "Code: https://github.com/joykirat18/TRAAC", "summary": "Recent thinking models solve complex reasoning tasks by scaling test-time\ncompute, but this scaling must be allocated in line with task difficulty. On\none hand, short reasoning (underthinking) leads to errors on harder problems\nthat require extended reasoning steps; but, excessively long reasoning\n(overthinking) can be token-inefficient, generating unnecessary steps even\nafter reaching a correct intermediate solution. We refer to this as\nunder-adaptivity, where the model fails to modulate its response length\nappropriately given problems of varying difficulty. To address under-adaptivity\nand strike a balance between under- and overthinking, we propose TRAAC (Think\nRight with Adaptive, Attentive Compression), an online post-training RL method\nthat leverages the model's self-attention over a long reasoning trajectory to\nidentify important steps and prune redundant ones. TRAAC also estimates\ndifficulty and incorporates it into training rewards, thereby learning to\nallocate reasoning budget commensurate with example difficulty. Our approach\nimproves accuracy, reduces reasoning steps, and enables adaptive thinking\ncompared to base models and other RL baselines. Across a variety of tasks\n(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute\naccuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%\ncompared to the base model, and a 7.9% accuracy gain paired with a 29.4% length\ndrop compared to the best RL baseline. TRAAC also shows strong generalization:\nalthough our models are trained on math datasets, they show accuracy and\nefficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,\nand OptimalThinkingBench. Our analysis further verifies that TRAAC provides\nfine-grained adjustments to thinking budget based on difficulty and that a\ncombination of task-difficulty calibration and attention-based compression\nyields gains across diverse tasks.", "AI": {"tldr": "TRAAC is an online post-training RL method that uses self-attention over a long reasoning trajectory to prune redundant steps and estimates problem difficulty to allocate reasoning budget, improving accuracy and reducing reasoning length across math and non-math tasks.", "motivation": "Current reasoning models struggle with balancing reasoning length: underthinking on hard problems leads to errors, while overthinking wastes tokens. There is under-adaptivity where response length does not adapt to task difficulty.", "method": "TRAAC (Think Right with Adaptive, Attentive Compression) performs post-training RL using the model\u2019s self-attention to identify important steps along a long reasoning chain and prune unnecessary ones. It also estimates task difficulty and uses it in rewards to train the model to allocate reasoning budget proportionally to difficulty, achieving adaptive thinking.", "result": "TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a 36.8% reduction in reasoning length versus the base model, and a 7.9% accuracy gain with a 29.4% length drop versus the best RL baseline across tasks AIME, AMC, GPQA-D, BBEH. It generalizes to non-math datasets (GPQA-D, BBEH, OptimalThinkingBench) and shows fine-grained adjustments to thinking budget by difficulty.", "conclusion": "Adaptive, attention-guided compression paired with task-difficulty calibration yields improved accuracy and efficiency across diverse tasks, demonstrating robust generalization and the value of aligning reasoning length with problem difficulty."}}
{"id": "2510.02197", "categories": ["cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02197", "abs": "https://arxiv.org/abs/2510.02197", "authors": ["Emmanuel Nsengiyumvaa", "Leonard Niyitegekaa", "Eric Umuhoza"], "title": "Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications", "comment": "20 pages", "summary": "Accurate livestock identification is a cornerstone of modern farming: it\nsupports health monitoring, breeding programs, and productivity tracking.\nHowever, common pig identification methods, such as ear tags and microchips,\nare often unreliable, costly, target pure breeds, and thus impractical for\nsmall-scale farmers. To address this gap, we propose a noninvasive biometric\nidentification approach that leverages uniqueness of the auricular vein\npatterns. To this end, we have collected 800 ear images from 20 mixed-breed\npigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a\nstandard smartphone and simple back lighting. A multistage computer vision\npipeline was developed to enhance vein visibility, extract structural and\nspatial features, and generate biometric signatures. These features were then\nclassified using machine learning models. Support Vector Machines (SVM)\nachieved the highest accuracy: correctly identifying pigs with 98.12% precision\nacross mixed-breed populations. The entire process from image processing to\nclassification was completed in an average of 8.3 seconds, demonstrating\nfeasibility for real-time farm deployment. We believe that by replacing fragile\nphysical identifiers with permanent biological markers, this system provides\nfarmers with a cost-effective and stress-free method of animal identification.\nMore broadly, the findings confirm the practicality of auricular vein\nbiometrics for digitizing livestock management, reinforcing its potential to\nextend the benefits of precision farming to resource-constrained agricultural\ncommunities.", "AI": {"tldr": "Noninvasive auricular vein biometrics for pig identification using smartphone imagery; SVM achieves 98.12% precision on 800 ear images from 20 mixed-breed pigs; processing ~8.3 seconds per identification; feasible for real-time farm deployment.", "motivation": "Need for reliable, low-cost, noninvasive identification in small-scale farming. Ear tags and microchips are unreliable or costly and often breed-specific. Auricular vein biometrics offer a permanent, stress-free solution to digitize livestock management.", "method": "Dataset: 800 ear images from 20 mixed-breed pigs (Landrace cross Pietrain and Duroc cross Pietrain). Imaging with a standard smartphone and simple back lighting. A multistage computer vision pipeline enhances vein visibility, extracts structural and spatial features, and generates biometric signatures. Classifiers (with SVM achieving the best performance) are trained to identify individual pigs.", "result": "SVM-based model achieved 98.12% precision in identifying pigs across mixed-breed populations. Total processing time from imaging to classification averaged 8.3 seconds, indicating feasibility for real-time farm use.", "conclusion": "The study demonstrates the practicality of auricular vein biometrics for digitizing livestock management and supports broader adoption of precision farming techniques in resource-constrained communities. The approach offers a noninvasive, cost-effective alternative to traditional identifiers."}}
{"id": "2510.01588", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01588", "abs": "https://arxiv.org/abs/2510.01588", "authors": ["Ziming Tang", "Chengbin Hou", "Tianyu Zhang", "Bangxu Tian", "Jinbao Wang", "Hairong Lv"], "title": "Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation", "comment": null, "summary": "Parkinson's disease (PD) is one of the most common neurodegenerative\ndisorder. PD telemonitoring emerges as a novel assessment modality enabling\nself-administered at-home tests of Unified Parkinson's Disease Rating Scale\n(UPDRS) scores, enhancing accessibility for PD patients. However, three types\nof noise would occur during measurements: (1) patient-induced measurement\ninaccuracies, (2) environmental noise, and (3) data packet loss during\ntransmission, resulting in higher prediction errors. To address these\nchallenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,\nthe original speech features are grouped into ordered bins, based on the\ncontinuous values of a selected feature, to construct contrastive pairs.\nSecond, the contrastive pairs are employed to train a multilayer perceptron\nencoder for generating noise-robust features. Finally, these features are\nconcatenated with the original features as the augmented features, which are\nthen fed into the UPDRS prediction models. Notably, we further introduces a\nnovel evaluation approach with customizable noise injection module, and\nextensive experiments show that NoRo can successfully enhance the noise\nrobustness of UPDRS prediction across various downstream prediction models\nunder different noisy environments.", "AI": {"tldr": "NoRo\u2014a noise-robust UPDRS prediction framework\u2014uses contrastive learning on ordered feature bins to generate noise-robust embeddings, improving UPDRS predictions under various noise in at-home PD telemonitoring.", "motivation": "PD telemonitoring at home faces patient-induced inaccuracies, environmental noise, and data transmission losses, leading to higher prediction errors. A robust, accessible UPDRS assessment from speech features is needed.", "method": "Group original speech features into ordered bins to form contrastive pairs; train a multilayer perceptron encoder via contrastive learning to generate noise-robust features; concatenate these robust features with the original features as augmented features; feed the augmented features into existing UPDRS prediction models; introduce a customizable noise-injection evaluation module to assess robustness.", "result": "NoRo improves the noise robustness of UPDRS prediction across various downstream models under different noisy environments, demonstrating the effectiveness of the proposed contrastive-bin feature learning approach.", "conclusion": "The proposed noise-robust feature learning framework effectively enhances UPDRS prediction robustness in noisy at-home telemonitoring settings, with a novel evaluation module to simulate and assess diverse noise conditions."}}
{"id": "2510.02213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02213", "abs": "https://arxiv.org/abs/2510.02213", "authors": ["Villanelle O'Reilly", "Jonathan Cox", "Georgios Leontidis", "Marc Hanheide", "Petra Bosilj", "James Brown"], "title": "MMDEW: Multipurpose Multiclass Density Estimation in the Wild", "comment": "8+1 pages, 4 figures, 5 tables", "summary": "Density map estimation can be used to estimate object counts in dense and\noccluded scenes where discrete counting-by-detection methods fail. We propose a\nmulticategory counting framework that leverages a Twins pyramid\nvision-transformer backbone and a specialised multi-class counting head built\non a state-of-the-art multiscale decoding approach. A two-task design adds a\nsegmentation-based Category Focus Module, suppressing inter-category cross-talk\nat training time. Training and evaluation on the VisDrone and iSAID benchmarks\ndemonstrates superior performance versus prior multicategory crowd-counting\napproaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11\nunderscores the necessity of crowd counting methods in dense scenes. The\nmethod's regional loss opens up multi-class crowd counting to new domains,\ndemonstrated through the application to a biodiversity monitoring dataset,\nhighlighting its capacity to inform conservation efforts and enable scalable\necological insights.", "AI": {"tldr": "Multicategory crowd counting using a Twins pyramid vision-transformer backbone with a specialized multi-class counting head and a segmentation-based Category Focus Module. It achieves large MAE reductions on VisDrone/iSAID and demonstrates ecological applications, including biodiversity monitoring.", "motivation": "In dense or occluded scenes, detection-based counting struggles. There is a need for multicategory crowd counting that suppresses inter-category interference and extends counting approaches to ecological monitoring domains.", "method": "A Twins pyramid vision-transformer backbone feeds a multiscale decoding-based counting head for multicategory estimation. A two-task design introduces a segmentation-based Category Focus Module to suppress inter-category cross-talk during training. The model leverages a regional loss and is evaluated on VisDrone and iSAID, with demonstration on a biodiversity monitoring dataset.", "result": "The approach achieves substantial MAE reductions (33%, 43%, and 64% relative to prior multicategory methods). It also outperforms a detection-based baseline such as YOLOv11 in dense scenes, underscoring the need for crowd-counting approaches in such contexts.", "conclusion": "The regional loss enables robust multi-class crowd counting across new domains, including biodiversity monitoring, enabling scalable ecological insights and conservation-relevant analyses."}}
{"id": "2510.01598", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2510.01598", "abs": "https://arxiv.org/abs/2510.01598", "authors": ["Youwei Bao", "Shuhan Yang", "Hyunsoo Yang"], "title": "Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness", "comment": "4 figures", "summary": "Deterministic pseudo random number generators (PRNGs) used in generative\nartificial intelligence (GAI) models produce predictable patterns vulnerable to\nexploitation by attackers. Conventional defences against the vulnerabilities\noften come with significant energy and latency overhead. Here, we embed\nhardware-generated true random bits from spin-transfer torque magnetic tunnel\njunctions (STT-MTJs) to address the challenges. A highly parallel,\nFPGA-assisted prototype computing system delivers megabit-per-second true\nrandom numbers, passing NIST randomness tests after in-situ operations with\nminimal overhead. Integrating the hardware random bits into a generative\nadversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to\n18.6 times compared to the low-quality random number generators (RNG) baseline.\nWith nanosecond switching speed, high energy efficiency, and established\nscalability, our STT-MTJ-based system holds the potential to scale beyond 106\nparallel cells, achieving gigabit-per-second throughput suitable for large\nlanguage model sampling. This advancement highlights spintronic RNGs as\npractical security components for next-generation GAI systems.", "AI": {"tldr": "Hardware-based STT-MTJ true random bits enable high-throughput, energy-efficient randomness for GAI, improving security against vulnerable PRNGs; scalable and practical for future large models.", "motivation": "Deterministic PRNGs in generative AI create predictable patterns exploitable by attackers; conventional defenses incur high energy and latency overhead, necessitating fast, low-power true randomness.", "method": "Use spin-transfer torque MTJ devices to generate true random bits, integrated via a highly parallel FPGA-assisted prototype delivering megabit-per-second TRNG. Evaluate by running in-situ within a GAN trained on CIFAR-10, and assess randomness with NIST tests. Discuss scalability toward >10^6 parallel cells for gigabit-per-second throughput.", "result": "In-situ generation passes NIST randomness tests. Integrating hardware RNGs into the GAN reduces insecure outputs by up to 18.6\u00d7 compared to a low-quality RNG baseline. The approach promises nanosecond switching, energy efficiency, and scalability toward >10^6 parallel cells, enabling gigabit-per-second throughput for large-model sampling.", "conclusion": "STT-MTJ-based hardware RNGs are practical security components for next-generation GAI systems, offering high-speed, energy-efficient, scalable true randomness that can harden generative models against RNG-related vulnerabilities."}}
{"id": "2510.02226", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02226", "abs": "https://arxiv.org/abs/2510.02226", "authors": ["Shira Schiber", "Ofir Lindenbaum", "Idan Schwartz"], "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models", "comment": "Under Review", "summary": "Recent advances in generative video models have enabled the creation of\nhigh-quality videos based on natural language prompts. However, these models\nfrequently lack fine-grained temporal control, meaning they do not allow users\nto specify when particular visual elements should appear within a generated\nsequence. In this work, we introduce TempoControl, a method that allows for\ntemporal alignment of visual concepts during inference, without requiring\nretraining or additional supervision. TempoControl utilizes cross-attention\nmaps, a key component of text-to-video diffusion models, to guide the timing of\nconcepts through a novel optimization approach. Our method steers attention\nusing three complementary principles: aligning its temporal shape with a\ncontrol signal (via correlation), amplifying it where visibility is needed (via\nenergy), and maintaining spatial focus (via entropy). TempoControl allows\nprecise control over timing while ensuring high video quality and diversity. We\ndemonstrate its effectiveness across various video generation applications,\nincluding temporal reordering for single and multiple objects, as well as\naction and audio-aligned generation.", "AI": {"tldr": "TempoControl enables precise temporal alignment of visual concepts during inference in text-to-video diffusion by steering cross-attention with a triad of signals (correlation, energy, entropy), without retraining.", "motivation": "Generative video models currently lack fine-grained temporal control; users cannot specify when particular visual elements should appear in a sequence, limiting coherence and alignment with cues like actions or audio.", "method": "Optimize and steer cross-attention maps during inference using three complementary objectives: (1) correlate attention\u2019s temporal shape with a control signal, (2) boost attention where the element should be visible (energy), and (3) preserve spatial focus (entropy). No retraining or extra supervision is required.", "result": "Demonstrates improved temporal control across tasks, including temporal reordering for single and multiple objects and action/audio-aligned generation, while maintaining high video quality and diversity.", "conclusion": "TempoControl provides a practical, supervision-free means to temporally steer concept appearance in video generation, enabling precise timing adjustments and broader applicabilities without sacrificing quality."}}
{"id": "2510.01621", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01621", "abs": "https://arxiv.org/abs/2510.01621", "authors": ["Zhen Li", "Fan Zhang", "Zheng Zhang", "Yu Chen"], "title": "Posterior Collapse as a Phase Transition in Variational Autoencoders", "comment": "12 pages, 8 figures", "summary": "We investigate the phenomenon of posterior collapse in variational\nautoencoders (VAEs) from the perspective of statistical physics, and reveal\nthat it constitutes a phase transition governed jointly by data structure and\nmodel hyper-parameters. By analyzing the stability of the trivial solution\nassociated with posterior collapse, we identify a critical hyper-parameter\nthreshold. This critical boundary, separating meaningful latent inference from\ncollapse, is characterized by a discontinuity in the KL divergence between the\napproximate posterior and the prior distribution. We validate this critical\nbehavior on both synthetic and real-world datasets, confirming the existence of\na phase transition. Our results demonstrate that posterior collapse is not\nmerely an optimization failure, but rather an emerging phase transition arising\nfrom the interplay between data structure and variational constraints. This\nperspective offers new insights into the trainability and representational\ncapacity of deep generative models.", "AI": {"tldr": "Posterior collapse in VAEs is identified as a phase transition driven by data structure and hyperparameters; a critical boundary where KL divergence to the prior changes discontinuously.", "motivation": "To reinterpret posterior collapse through statistical physics, clarifying when latent inference is meaningful and how data/model interplay governs trainability.", "method": "Stability analysis of the trivial posterior solution; derive conditions for a critical hyperparameter threshold; analyze KL divergence; empirical validation on synthetic and real datasets.", "result": "There exists a phase boundary separating meaningful latent inference from collapse; the boundary shows a discontinuity in KL divergence; collapse is an emergent phenomenon rather than mere optimization failure.", "conclusion": "Understanding posterior collapse as a phase transition yields insights into representational capacity and training dynamics of VAEs, linking data structure with variational constraints, with implications for model design."}}
{"id": "2510.02240", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02240", "abs": "https://arxiv.org/abs/2510.02240", "authors": ["Sicheng Feng", "Kaiwen Tuo", "Song Wang", "Lingdong Kong", "Jianke Zhu", "Huan Wang"], "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning", "comment": null, "summary": "Fine-grained visual reasoning remains a core challenge for multimodal large\nlanguage models (MLLMs). The recently introduced ReasonMap highlights this gap\nby showing that even advanced MLLMs struggle with spatial reasoning in\nstructured and information-rich settings such as transit maps, a task of clear\npractical and scientific importance. However, standard reinforcement learning\n(RL) on such tasks is impeded by sparse rewards and unstable optimization. To\naddress this, we first construct ReasonMap-Plus, an extended dataset that\nintroduces dense reward signals through Visual Question Answering (VQA) tasks,\nenabling effective cold-start training of fine-grained visual understanding\nskills. Next, we propose RewardMap, a multi-stage RL framework designed to\nimprove both visual understanding and reasoning capabilities of MLLMs.\nRewardMap incorporates two key designs. First, we introduce a difficulty-aware\nreward design that incorporates detail rewards, directly tackling the sparse\nrewards while providing richer supervision. Second, we propose a multi-stage RL\nscheme that bootstraps training from simple perception to complex reasoning\ntasks, offering a more effective cold-start strategy than conventional\nSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus\ndemonstrate that each component of RewardMap contributes to consistent\nperformance gains, while their combination yields the best results. Moreover,\nmodels trained with RewardMap achieve an average improvement of 3.47% across 6\nbenchmarks spanning spatial reasoning, fine-grained visual reasoning, and\ngeneral tasks beyond transit maps, underscoring enhanced visual understanding\nand reasoning capabilities.", "AI": {"tldr": "RewardMap introduces a multi-stage RL framework with dense VQA-derived rewards and a difficulty-aware reward design to improve fine-grained visual reasoning in multimodal LLMs; achieves ~3.47% average improvement across 6 benchmarks on ReasonMap, ReasonMap-Plus, and beyond.", "motivation": "ReasonMap shows that multimodal LLMs struggle with spatial reasoning in structured, information-rich visuals like transit maps, and standard RL is hampered by sparse rewards; there is a need for better cold-start training and denser supervision to develop fine-grained visual understanding.", "method": "Build ReasonMap-Plus by adding dense rewards via Visual Question Answering tasks to enable effective cold-start training. Propose RewardMap with (i) a difficulty-aware reward design that provides detail rewards to alleviate sparse rewards, and (ii) a multi-stage RL scheme that boots training from simple perception to complex reasoning, offering a more effective alternative to standard SFT.", "result": "Experiments show that each component of RewardMap contributes to consistent performance gains, and their combination yields the best results. Models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, indicating enhanced visual understanding and reasoning capabilities.", "conclusion": "RewardMap effectively enhances visual understanding and reasoning in multimodal LLMs, offering a better cold-start strategy and denser supervision through the proposed dense reward signals and staged RL, with demonstrated generalization beyond transit-map domains."}}
{"id": "2510.01624", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01624", "abs": "https://arxiv.org/abs/2510.01624", "authors": ["Feiyang Kang", "Michael Kuchnik", "Karthik Padthe", "Marin Vlastelica", "Ruoxi Jia", "Carole-Jean Wu", "Newsha Ardalani"], "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead", "comment": "Preprint. Under Review", "summary": "In post-training for reasoning Large Language Models (LLMs), the current\nstate of practice trains LLMs in two independent stages: Supervised Fine-Tuning\n(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as\n``RL'' below). In this work, we challenge whether high SFT scores translate to\nimproved performance after RL. We provide extensive counter-examples where this\nis not true. We find high SFT scores can be biased toward simpler or more\nhomogeneous data and are not reliably predictive of subsequent RL gains or\nscaled-up post-training effectiveness. In some cases, RL training on models\nwith improved SFT performance could lead to substantially worse outcome\ncompared to RL on the base model without SFT. We study alternative metrics and\nidentify generalization loss on held-out reasoning examples and Pass@large k\nperformance to provide strong proxies for the RL outcome. We trained hundreds\nof models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive\nevaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU\nhours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple\nstate-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL\nperformance, prediction based on generalization loss and Pass@large k achieves\nsubstantial higher precision, improving $R^2$ coefficient and Spearman's rank\ncorrelation coefficient by up to 0.5 (2x). This provides strong utility for\nbroad use cases. For example, in most experiments, we find SFT training on\nunique examples for a one epoch underperforms training on half examples for two\nepochs, either after SFT or SFT-then-RL; With the same SFT budget, training\nonly on short examples may lead to better SFT performance, though, it often\nleads to worse outcome after RL compared to training on examples with varying\nlengths. Evaluation tool will be open-sourced.", "AI": {"tldr": "High SFT scores do not reliably predict RL post-training gains; generalization loss and Pass@large k are stronger predictors; RL on SFT-improved models can underperform base models; diverse, varied-length data may yield better post-RL outcomes.", "motivation": "Question the common assumption that better supervised fine-tuning (SFT) performance translates into superior post-training RL outcomes, and identify practical predictors for RL success to guide data and budget decisions.", "method": "Train hundreds of models (up to 12B params) with SFT and RLVR (GRPO) across multiple families (Llama3, Mistral-Nemo, Qwen3) and seven math benchmarks, with up to 256 repetitions and over $1M GPU-hours. Evaluate correlations between pre/post SFT metrics and RL performance. Compare baselines using different data, lengths, and budgets. Propose alternative metrics (generalization loss on held-out reasoning, Pass@large k) and validate their predictive power.", "result": "SFT performance is not reliably predictive of RL gains. Generalization loss and Pass@large k provide substantially better predictive power (up to a 0.5 gain in R^2 and Spearman correlations). In some cases, RL on SFT-boosted models underperforms RL on the base model. Short-only SFT data can be worse for RL, even with the same SFT budget; diverse-length data often yields better RL outcomes. An open-source evaluation tool will be released.", "conclusion": "Relying on SFT scores as a proxy for RL success is unreliable. Instead, use generalization loss and Pass@k as robust predictors for RL outcomes, and adopt data strategies that emphasize varied-length, diverse exemplars. The approach and tooling support broad adoption and benchmarking in post-training for reasoning LLMs."}}
{"id": "2510.02253", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02253", "abs": "https://arxiv.org/abs/2510.02253", "authors": ["Zihan Zhou", "Shilin Lu", "Shuli Leng", "Shaocong Zhang", "Zhuming Lian", "Xinlei Yu", "Adams Wai-Kin Kong"], "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing", "comment": "Preprint", "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.", "AI": {"tldr": "DragFlow introduces region-based drag editing for diffusion-based image editing using FLUX priors (DiT), outperforming point-based methods and previous region-based baselines; it uses affine-region supervision, subject personalization adapters, gradient-mask constraints, and multimodal LLMs, with ReD Bench to benchmark region-level dragging; achieves state-of-the-art results on DragBench-DR and ReD Bench.", "motivation": "Drag-based editing with earlier base models caused distortions due to projection gaps from Stable Diffusion. Stronger priors from DiT-based flow matching (FLUX) enable better editing but have not been effectively leveraged for drag-based editing. There's a need for region-aware supervision to exploit structured features in DiTs, plus mechanisms to preserve background and subject fidelity and resolve task ambiguity.", "method": "Proposes DragFlow: a region-based editing framework that uses affine transformations for region-level supervision within DiT/FLUX priors. Replaces point-based guidance with region-based cues; integrates IP-Adapter-like personalization adapters to maintain subject consistency; uses gradient mask-based hard constraints to preserve background fidelity; employs multimodal large language models (MLLMs) to disambiguate tasks. Introduces ReD Bench, a Region-based Dragging benchmark, and evaluates on DragBench-DR and ReD Bench, comparing against point-based and region-based baselines.", "result": "DragFlow achieves substantial gains over baselines, setting a new state-of-the-art in drag-based image editing on both DragBench-DR and ReD Bench. Region-based supervision with affine transformations provides more reliable guidance for DiT features than point-based methods; personalization adapters and gradient masks help maintain subject and background fidelity. Code and datasets will be released.", "conclusion": "Region-based editing with strong diffusion priors, subject personalization, and ambiguity-resolving tools significantly improves drag-based image editing, establishing a new performance baseline and providing benchmark resources for future work."}}
{"id": "2510.01631", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01631", "abs": "https://arxiv.org/abs/2510.01631", "authors": ["Feiyang Kang", "Newsha Ardalani", "Michael Kuchnik", "Youssef Emad", "Mostafa Elhoushi", "Shubhabrata Sengupta", "Shang-Wen Li", "Ramya Raghavendra", "Ruoxi Jia", "Carole-Jean Wu"], "title": "Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls", "comment": "Published as a Main Conference paper at EMNLP 2025", "summary": "Training data plays a crucial role in Large Language Models (LLM) scaling,\nyet high quality data is of limited supply. Synthetic data techniques offer a\npotential path toward sidestepping these limitations. We conduct a large-scale\nempirical investigation (>1000 LLMs with >100k GPU hours) using a unified\nprotocol and scaling laws, comparing natural web data, diverse synthetic types\n(rephrased text, generated textbooks), and mixtures of natural and synthetic\ndata. Specifically, we found pre-training on rephrased synthetic data\n\\textit{alone} is not faster than pre-training on natural web texts; while\npre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts\ncan speed up 5-10x (to reach the same validation loss) at larger data budgets.\nPre-training on textbook-style synthetic data \\textit{alone} results in notably\nhigher loss on many downstream domains especially at small data budgets. \"Good\"\nratios of synthetic data in training data mixtures depend on the model size and\ndata budget, empirically converging to ~30% for rephrased synthetic data.\nLarger generator models do not necessarily yield better pre-training data than\n~8B-param models. These results contribute mixed evidence on \"model collapse\"\nduring large-scale single-round (n=1) model training on synthetic\ndata--training on rephrased synthetic data shows no degradation in performance\nin foreseeable scales whereas training on mixtures of textbook-style\npure-generated synthetic data shows patterns predicted by \"model collapse\". Our\nwork demystifies synthetic data in pre-training, validates its conditional\nbenefits, and offers practical guidance.", "AI": {"tldr": "Synthetic data can aid LLM pre-training when mixed with natural data, but pure synthetic data effects vary by type: rephrased data offers conditional benefit and speedups with mixtures; textbook-style synthetic data can harm performance, especially at small budgets; optimal synthetic ratios depend on model size and budget, around ~30% for rephrased data; larger generator size does not guarantee gains; evidence on model collapse is mixed but rephrased data shows no degradation at foreseeable scales.", "motivation": "LLM scaling is data-limited; synthetic data promises a path to expand effective training data without collecting more natural data. The study rigorously evaluates how different synthetic data types and their mixtures with natural data influence pre-training performance at scale.", "method": "Large-scale empirical analysis across >1000 LLMs and >100k GPU hours using a unified protocol and scaling laws. Comparisons include natural web data, rephrased synthetic data, textbooks-style synthetic data, and mixtures. Variables include model size, data budget, and synthetic data fraction. Metrics focus on validation loss and downstream performance, with attention to potential model collapse in single-round training.", "result": "Pre-training on rephrased synthetic data alone is not faster than using natural web data. A mixture of 1/3 rephrased synthetic and 2/3 natural web achieves 5\u201310x speedup in reaching the same validation loss at larger data budgets. Textbook-style synthetic data alone leads to higher downstream loss, especially at small budgets. Best synthetic contribution for rephrased data converges around ~30% of the training data, depending on model size and budget. Larger generator models do not necessarily outperform ~8B-parameter models. Mixed results on model collapse: rephrased data shows no degradation in foreseeable scales, while mixtures of textbook-style generated data exhibit patterns aligned with model collapse.", "conclusion": "Synthetic data can offer conditional benefits in LLM pre-training, particularly when using rephrased data in well-chosen mixtures with natural data. The study provides practical guidance on synthetic data ratios and cautions against overreliance on larger generators or on using textbook-style data in isolation. It helps demystify the role of synthetic data in large-scale pre-training."}}
{"id": "2510.02262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02262", "abs": "https://arxiv.org/abs/2510.02262", "authors": ["Guangyu Sun", "Archit Singhal", "Burak Uzkent", "Mubarak Shah", "Chen Chen", "Garin Kessler"], "title": "From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding", "comment": null, "summary": "Video Large Language Models (VLMs) have achieved remarkable results on a\nvariety of vision language tasks, yet their practical use is limited by the\n\"needle in a haystack\" problem: the massive number of visual tokens produced\nfrom raw video frames exhausts the model's context window. Existing solutions\nalleviate this issue by selecting a sparse set of frames, thereby reducing\ntoken count, but such frame-wise selection discards essential temporal\ndynamics, leading to suboptimal reasoning about motion and event continuity. In\nthis work we systematically explore the impact of temporal information and\ndemonstrate that extending selection from isolated key frames to key clips,\nwhich are short, temporally coherent segments, improves video understanding. To\nmaintain a fixed computational budget while accommodating the larger token\nfootprint of clips, we propose an adaptive resolution strategy that dynamically\nbalances spatial resolution and clip length, ensuring a constant token count\nper video. Experiments on three long-form video benchmarks demonstrate that our\ntraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and\n10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These\nresults highlight the importance of preserving temporal coherence in frame\nselection and provide a practical pathway for scaling Video LLMs to real world\nvideo understanding applications. Project webpage is available at\nhttps://guangyusun.com/f2c .", "AI": {"tldr": "A training-free method F2C improves video LLM understanding by selecting temporally coherent key clips under a constant token budget, outperforming uniform sampling on three long-form video benchmarks.", "motivation": "Video LLMs face the needle-in-a-haystack problem: raw video frames generate too many tokens, and frame-wise frame selection loses crucial temporal dynamics needed to reason about motion and events. Preserving temporal coherence in selection should improve understanding of video sequences.", "method": "Instead of selecting isolated key frames, the approach selects short, temporally coherent key clips and uses an adaptive resolution strategy to keep a fixed token budget. This balances clip length and spatial resolution to maximize information within a constant token count. The method is training-free and referred to as F2C.", "result": "On three long-form video benchmarks (Video-MME, LongVideoBench, MLVU), F2C outperforms uniform sampling by 8.1%, 5.6%, and 10.3%, respectively.", "conclusion": "Preserving temporal coherence in frame selection is crucial for video understanding in VLMs. An adaptive, clip-based sampling with a fixed token budget enables scaling Video LLMs without retraining, offering practical gains for real-world video tasks."}}
{"id": "2510.01634", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01634", "abs": "https://arxiv.org/abs/2510.01634", "authors": ["Ryan Y. Lin", "Siddhartha Ojha", "Nicholas Bai"], "title": "CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning", "comment": null, "summary": "Transformers achieve strong performance across diverse domains but implicitly\nassume Euclidean geometry in their attention mechanisms, limiting their\neffectiveness on data with non-Euclidean structure. While recent extensions to\nhyperbolic and spherical spaces show promise for hierarchical and cyclical\npatterns, respectively, they require committing to a single geometry a priori,\nreducing flexibility when data exhibits mixed geometric properties. We\nintroduce the Curvature-Adaptive Transformer (CAT), a novel architecture that\ndynamically learns per-token routing across three geometric attention branches\nthrough a lightweight, differentiable gating mechanism. Unlike fixed-geometry\napproaches, CAT enables adaptive geometric specialization, routing tokens to\nthe appropriate curvature based on their local relational structure. The\nrouting network provides interpretable curvature preferences while each branch\nemploys geometry-specific operations optimized for its respective manifold. On\nknowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves\napproximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines\nwith minimal overhead (5% parameter increase, comparable inference time). These\nresults demonstrate that learned geometric adaptation outperforms any single\nfixed geometry for complex relational reasoning, establishing CAT as a scalable\nand interpretable foundation for mixture-of-geometry architectures across\nlanguage, vision, and multimodal domains.", "AI": {"tldr": "The Curvature-Adaptive Transformer (CAT) dynamically routes tokens among three geometry-specific attention branches (Euclidean, hyperbolic, spherical) via a differentiable gate, enabling per-token geometric specialization. It outperforms fixed-geometry baselines on knowledge graph completion with minimal overhead, demonstrating the value of a mixture-of-geometry approach.", "motivation": "Transformers implicitly rely on Euclidean geometry, which limits performance on data with non-Euclidean structure. While existing hyperbolic/spherical extensions capture certain patterns, they fix geometry a priori. A model that adaptively selects geometry per token can better handle mixed geometric properties in data.", "method": "CAT introduces a per-token routing mechanism that selects among three geometric attention branches (Euclidean, hyperbolic, spherical) using a lightweight differentiable gate. Each branch uses operations tailored to its manifold; the routing network yields interpretable curvature preferences. The approach results in a mixture-of-geometry architecture with minimal parameter overhead.", "result": "On knowledge graph completion benchmarks FB15k-237 and WN18RR, CAT achieves about 10% gains in MRR and Hits@10 over fixed-geometry baselines, with only ~5% more parameters and comparable inference time, indicating effective geometric adaptation for relational reasoning.", "conclusion": "Dynamic, per-token geometry adaptation enables a scalable, interpretable mixture-of-geometry Transformer, outperforming single-geometry models and potentially extending benefits to language, vision, and multimodal tasks."}}
{"id": "2510.02264", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02264", "abs": "https://arxiv.org/abs/2510.02264", "authors": ["Mario Medrano-Paredes", "Carmen Fern\u00e1ndez-Gonz\u00e1lez", "Francisco-Javier D\u00edaz-Pernas", "Hichem Saoudi", "Javier Gonz\u00e1lez-Alonso", "Mario Mart\u00ednez-Zarzuela"], "title": "Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities", "comment": "All tables, graphs and figures generated can be obtained in the\n  Zenodo repository complementary to this work:\n  https://doi.org/10.5281/zenodo.15088423", "summary": "Advances in machine learning and wearable sensors offer new opportunities for\ncapturing and analyzing human movement outside specialized laboratories.\nAccurate assessment of human movement under real-world conditions is essential\nfor telemedicine, sports science, and rehabilitation. This preclinical\nbenchmark compares monocular video-based 3D human pose estimation models with\ninertial measurement units (IMUs), leveraging the VIDIMU dataset containing a\ntotal of 13 clinically relevant daily activities which were captured using both\ncommodity video cameras and five IMUs. During this initial study only healthy\nsubjects were recorded, so results cannot be generalized to pathological\ncohorts. Joint angles derived from state-of-the-art deep learning frameworks\n(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA\nBodyTrack) were evaluated against joint angles computed from IMU data using\nOpenSim inverse kinematics following the Human3.6M dataset format with 17\nkeypoints. Among them, MotionAGFormer demonstrated superior performance,\nachieving the lowest overall RMSE ($9.27\\deg \\pm 4.80\\deg$) and MAE ($7.86\\deg\n\\pm 4.18\\deg$), as well as the highest Pearson correlation ($0.86 \\pm 0.15$)\nand the highest coefficient of determination $R^{2}$ ($0.67 \\pm 0.28$). The\nresults reveal that both technologies are viable for out-of-the-lab kinematic\nassessment. However, they also highlight key trade-offs between video- and\nsensor-based approaches including costs, accessibility, and precision. This\nstudy clarifies where off-the-shelf video models already provide clinically\npromising kinematics in healthy adults and where they lag behind IMU-based\nestimates while establishing valuable guidelines for researchers and clinicians\nseeking to develop robust, cost-effective, and user-friendly solutions for\ntelehealth and remote patient monitoring.", "AI": {"tldr": "Monocular video 3D pose estimation can approach IMU accuracy in healthy adults for 13 daily activities but shows trade-offs in precision and generalizability.", "motivation": "Need for accurate, real-world motion capture for telemedicine, sports science, and rehabilitation; benchmark comparing video-based pose estimation to IMUs using the VIDIMU dataset.", "method": "Evaluate state-of-the-art DL-based 3D pose pipelines (MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, NVIDIA BodyTrack) against IMU-derived joint angles computed via OpenSim inverse kinematics (Human3.6M-like 17-keypoint format) on VIDIMU\u2019s 13 activities; healthy subjects only.", "result": "MotionAGFormer achieves best metrics: RMSE 9.27\u00b0 \u00b1 4.80\u00b0, MAE 7.86\u00b0 \u00b1 4.18\u00b0, Pearson r 0.86 \u00b1 0.15, R\u00b2 0.67 \u00b1 0.28. Both video and IMU approaches are viable for out-of-lab kinematics; video models offer clinical promise but exhibit trade-offs in cost, accessibility, and precision.", "conclusion": "The study clarifies where off-the-shelf video models can support clinical kinematics in healthy adults and where IMU-based estimates remain superior, providing guidelines for researchers/clinicians to develop robust, cost-effective telehealth and remote monitoring solutions."}}
{"id": "2510.01637", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01637", "abs": "https://arxiv.org/abs/2510.01637", "authors": ["Liyan Xie", "Muhammad Siddeek", "Mohamed Seif", "Andrea J. Goldsmith", "Mengdi Wang"], "title": "Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking", "comment": null, "summary": "Watermarking has become a key technique for proprietary language models,\nenabling the distinction between AI-generated and human-written text. However,\nin many real-world scenarios, LLM-generated content may undergo post-generation\nedits, such as human revisions or even spoofing attacks, making it critical to\ndetect and localize such modifications. In this work, we introduce a new task:\ndetecting post-generation edits locally made to watermarked LLM outputs. To\nthis end, we propose a combinatorial pattern-based watermarking framework,\nwhich partitions the vocabulary into disjoint subsets and embeds the watermark\nby enforcing a deterministic combinatorial pattern over these subsets during\ngeneration. We accompany the combinatorial watermark with a global statistic\nthat can be used to detect the watermark. Furthermore, we design lightweight\nlocal statistics to flag and localize potential edits. We introduce two\ntask-specific evaluation metrics, Type-I error rate and detection accuracy, and\nevaluate our method on open-source LLMs across a variety of editing scenarios,\ndemonstrating strong empirical performance in edit localization.", "AI": {"tldr": "Introduces a combinatorial watermarking framework to detect and localize post-generation edits on watermarked LLM outputs, using vocabulary partitioning, deterministic patterns, a global watermark statistic, and lightweight local statistics; proposes Type-I error rate and detection accuracy as evaluation metrics and shows strong localization across edit scenarios.", "motivation": "In real-world use, LLM outputs may be edited after generation or spoofed; detecting and localizing such edits on watermarked text is critical for provenance and authenticity.", "method": "Partition vocabulary into disjoint subsets; embed watermark via a deterministic combinatorial pattern during generation; use a global statistic for watermark detection; develop lightweight local statistics to flag edits; evaluate with metrics Type-I error rate and detection accuracy.", "result": "Empirical evaluation on open-source LLMs across diverse editing scenarios shows strong performance in edit localization; the framework effectively detects and localizes post-generation edits.", "conclusion": "A combinatorial watermark with global and local statistics is effective for detecting and localizing edits to watermarked LLM outputs, with robust evaluation metrics for edit detection and localization."}}
{"id": "2510.02266", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.02266", "abs": "https://arxiv.org/abs/2510.02266", "authors": ["Shiyi Zhang", "Dong Liang", "Yihang Zhou"], "title": "NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes", "comment": null, "summary": "Reconstructing visual information from brain activity via computer vision\ntechnology provides an intuitive understanding of visual neural mechanisms.\nDespite progress in decoding fMRI data with generative models, achieving\naccurate cross-subject reconstruction of visual stimuli remains challenging and\ncomputationally demanding. This difficulty arises from inter-subject\nvariability in neural representations and the brain's abstract encoding of core\nsemantic features in complex visual inputs. To address these challenges, we\npropose NeuroSwift, which integrates complementary adapters via diffusion:\nAutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter\nis trained on Stable Diffusion generated images paired with COCO captions to\nemulate higher visual cortex encoding. For cross-subject generalization, we\npretrain on one subject and then fine-tune only 17 percent of parameters (fully\nconnected layers) for new subjects, while freezing other components. This\nenables state-of-the-art performance with only one hour of training per subject\non lightweight GPUs (three RTX 4090), and it outperforms existing methods.", "AI": {"tldr": "NeuroSwift enables cross-subject visual reconstruction from fMRI using diffusion-based adapters, achieving state-of-the-art results with minimal subject-specific training by fine-tuning only 17% of parameters on lightweight GPUs.", "motivation": "Cross-subject decoding of visual stimuli from fMRI is hampered by substantial inter-subject variability and the brain's abstract encoding of core semantic features, making scalable and accurate reconstruction challenging.", "method": "The approach combines AutoKL for low-level features and a CLIP adapter for semantics within a diffusion framework. The CLIP adapter is trained on Stable Diffusion-generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, the model is pretrained on one subject and then fine-tuned by updating only 17% of parameters (fully connected layers) while freezing the rest, enabling ~1 hour of per-subject training on three RTX 4090 GPUs.", "result": "The method achieves state-of-the-art cross-subject reconstruction performance and outperforms existing methods while substantially reducing training time and computational requirements.", "conclusion": "NeuroSwift demonstrates efficient cross-subject brain decoding with modular diffusion-based adapters and minimal fine-tuning, offering a scalable path toward practical neural reconstruction across diverse subjects."}}
{"id": "2510.01643", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01643", "abs": "https://arxiv.org/abs/2510.01643", "authors": ["Maryam Aliakbarpour", "Vladimir Braverman", "Junze Yin", "Haochen Zhang"], "title": "Support Basis: Fast Attention Beyond Bounded Entries", "comment": null, "summary": "The quadratic complexity of softmax attention remains a central bottleneck in\nscaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a\nsub-quadratic attention approximation algorithm, but it works only under the\nrestrictive bounded-entry assumption. Since this assumption rarely holds in\npractice, its applicability to modern LLMs is limited.\n  In this paper, we introduce support-basis decomposition, a new framework for\nefficient attention approximation beyond bounded entries. We empirically\ndemonstrate that the entries of the query and key matrices exhibit sub-Gaussian\nbehavior. Our approach uses this property to split large and small entries,\nenabling exact computation on sparse components and polynomial approximation on\ndense components. We establish rigorous theoretical guarantees, proving a\nsub-quadratic runtime, and extend the method to a multi-threshold setting that\neliminates all distributional assumptions. Furthermore, we provide the first\ntheoretical justification for the empirical success of polynomial attention\n[Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be\nclosely approximated by a combination of multiple polynomial attentions with\nsketching.", "AI": {"tldr": "Proposes support-basis decomposition to achieve sub-quadratic softmax attention by splitting large and small entries based on sub-Gaussian behavior, enabling exact computation on sparse parts and polynomial approximation on dense parts; extends to multi-threshold settings and provides theory linking polynomial attention with sketching.", "motivation": "The quadratic cost of softmax attention limits scaling of LLMs. Prior sub-quadratic methods (e.g., bounded-entry approaches) are often invalid in practice, motivating a framework that works beyond such assumptions.", "method": "Empirically show sub-Gaussian behavior in query/key entries. Decompose attention into support (sparse, exactly computed) and basis (dense, approximated with polynomials). Prove sub-quadratic runtime. Extend to a multi-threshold scheme that removes distributional assumptions. Provide theoretical justification for polynomial attention using sketching, aligning with ICML 2024 work.", "result": "Sub-quadratic runtime guaranteed with rigorous analysis. The approach handles non-bounded entries, with exact processing of sparse components and polynomial approximations of dense components. Extends to multi-threshold settings that relax distribution assumptions. The paper also offers the first theoretical grounding for approximating softmax attention via a combination of polynomial attentions built with sketching.", "conclusion": "Support-basis decomposition broadens the scope of efficient attention beyond bounded-entry restrictions, delivering practical algorithms with strong theoretical guarantees and connecting polynomial-attention ideas to established sketching techniques."}}
{"id": "2510.02270", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02270", "abs": "https://arxiv.org/abs/2510.02270", "authors": ["Sathira Silva", "Eman Ali", "Chetan Arora", "Muhammad Haris Khan"], "title": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification", "comment": null, "summary": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for\nfine-grained image classification requires sensitivity to microscopic local\ncues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse\nglobal features restricts its performance on fine-grained classification tasks.\nPrior efforts inject fine-grained knowledge by aligning large language model\n(LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach\noverlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training\nframework that jointly refines CLIP's visual and textual representations using\nfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)\nwithin a lightweight TokenFusion module, which builds a saliency-guided\n$\\texttt{[FG]}$ token from patch embeddings and fuses it with the global\n$\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we\nintroduce a two-headed LLM-derived classifier: a frozen classifier that, via\nmulti-view alignment, provides a stable text-based prior for pseudo-labeling,\nand a learnable classifier initialized from LLM descriptions and fine-tuned\nwith TokenFusion. We further develop Dynamic Knowledge Aggregation, which\nconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to\niteratively refine pseudo-labels. Together, these components uncover latent\nfine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy\ngain across 13 fine-grained benchmarks while requiring only light adaptation.\nOur code is available at https://github.com/sathiiii/microCLIP.", "AI": {"tldr": "microCLIP is a self-training framework that enhances CLIP for fine-grained image classification by introducing a saliency-guided FG token via SOAP and a two-headed, LLM-informed classifier, plus Dynamic Knowledge Aggregation, achieving ~2.9% average accuracy gain across 13 benchmarks with light adaptation.", "motivation": "Fine-grained classification requires local, spatially precise cues; while CLIP transfers well globally, its global [CLS] features miss subcategory cues. Prior LLM-alignment methods align descriptions to [CLS] but ignore spatial precision. A lightweight, self-training approach can exploit fine-grained signals while keeping adaptation light.", "method": "Introduce SOAP within a TokenFusion module to build a saliency-guided [FG] token from patch embeddings and fuse with the [CLS] token to align coarse and fine features. Use a two-headed LLM-derived classifier: a frozen classifier yields a stable text-based prior for pseudo-labeling via multi-view alignment; a learnable classifier initialized from LLM descriptions and refined with TokenFusion. Propose Dynamic Knowledge Aggregation to convexly combine fixed LLM/CLIP priors with evolving TokenFusion logits to iteratively refine pseudo-labels.", "result": "Yields a 2.90 percentage point average accuracy gain across 13 fine-grained benchmarks with light adaptation; demonstrates consistent improvement. Code available at GitHub.", "conclusion": "The framework reveals latent fine-grained signals in CLIP, enabling effective unsupervised adaptation for fine-grained image classification with modest computational overhead."}}
{"id": "2510.01649", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01649", "abs": "https://arxiv.org/abs/2510.01649", "authors": ["Muhammad Tanzil Furqon", "Mahardhika Pratama", "Igor \u0160krjanc", "Lin Liu", "Habibullah Habibullah", "Kutluyil Dogancay"], "title": "Source-Free Cross-Domain Continual Learning", "comment": null, "summary": "Although existing cross-domain continual learning approaches successfully\naddress many streaming tasks having domain shifts, they call for a fully\nlabeled source domain hindering their feasibility in the privacy constrained\nenvironments. This paper goes one step ahead with the problem of source-free\ncross-domain continual learning where the use of source-domain samples are\ncompletely prohibited. We propose the idea of rehearsal-free frequency-aware\ndynamic prompt collaborations (REFEREE) to cope with the absence of labeled\nsource-domain samples in realm of cross-domain continual learning. REFEREE is\nbuilt upon a synergy between a source-pre-trained model and a large-scale\nvision-language model, thus overcoming the problem of sub-optimal\ngeneralizations when relying only on a source pre-trained model. The domain\nshift problem between the source domain and the target domain is handled by a\nfrequency-aware prompting technique encouraging low-frequency components while\nsuppressing high-frequency components. This strategy generates frequency-aware\naugmented samples, robust against noisy pseudo labels. The noisy pseudo-label\nproblem is further addressed with the uncertainty-aware weighting strategy\nwhere the mean and covariance matrix are weighted by prediction uncertainties,\nthus mitigating the adverse effects of the noisy pseudo label. Besides, the\nissue of catastrophic forgetting (CF) is overcome by kernel linear discriminant\nanalysis (KLDA) where the backbone network is frozen while the classification\nis performed using the linear discriminant analysis approach guided by the\nrandom kernel method. Our rigorous numerical studies confirm the advantage of\nour approach where it beats prior arts having access to source domain samples\nwith significant margins.", "AI": {"tldr": "Proposes REFEREE, a rehearsal-free, source-free cross-domain continual learning framework that pairs a source-pretrained model with a vision-language model, using frequency-aware prompts to produce robust low-frequency augmentations, uncertainty-weighted pseudo-labels, and kernel-LDA classification to combat forgetting, achieving strong results without source data.", "motivation": "Privacy constraints and practical deployment scenarios often forbid access to labeled source-domain data for continual learning. Existing cross-domain methods assume a source domain available for training, which is unrealistic in many settings. The paper aims to enable effective cross-domain continual learning without source samples, handling domain shift, noisy pseudo labels, and catastrophic forgetting.", "method": "Leverage synergy between a source-pretrained model and a large-scale vision-language model. Introduce a frequency-aware prompting scheme that emphasizes low-frequency components and suppresses high-frequency ones to produce frequency-aware augmented samples robust to noisy pseudo labels. Apply an uncertainty-aware weighting strategy where the mean and covariance of predictions are weighted by uncertainty to mitigate noisy pseudo labels. Address catastrophic forgetting by using kernel linear discriminant analysis (KLDA) with a frozen backbone and classification via linear discriminant analysis guided by a random kernel method.", "result": "Numerical studies show REFEREE outperforms prior methods that have access to source-domain data by significant margins, despite not using source samples.", "conclusion": "REFEREE effectively tackles source-free cross-domain continual learning by combining a source-pretrained model, a vision-language model, frequency-aware prompting, uncertainty-weighted pseudo-labels, and KLDA-based classification, offering strong performance gains under privacy constraints."}}
{"id": "2510.02282", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02282", "abs": "https://arxiv.org/abs/2510.02282", "authors": ["Kyoungjun Park", "Yifan Yang", "Juheon Yi", "Shicheng Zheng", "Yifei Shen", "Dongqi Han", "Caihua Shan", "Muhammad Muaz", "Lili Qiu"], "title": "VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL", "comment": null, "summary": "With the rapid advancement of AI-generated videos, there is an urgent need\nfor effective detection tools to mitigate societal risks such as misinformation\nand reputational harm. In addition to accurate classification, it is essential\nthat detection models provide interpretable explanations to ensure transparency\nfor regulators and end users. To address these challenges, we introduce\nVidGuard-R1, the first video authenticity detector that fine-tunes a\nmulti-modal large language model (MLLM) using group relative policy\noptimization (GRPO). Our model delivers both highly accurate judgments and\ninsightful reasoning. We curate a challenging dataset of 140k real and\nAI-generated videos produced by state-of-the-art generation models, carefully\ndesigning the generation process to maximize discrimination difficulty. We then\nfine-tune Qwen-VL using GRPO with two specialized reward models that target\ntemporal artifacts and generation complexity. Extensive experiments demonstrate\nthat VidGuard-R1 achieves state-of-the-art zero-shot performance on existing\nbenchmarks, with additional training pushing accuracy above 95%. Case studies\nfurther show that VidGuard-R1 produces precise and interpretable rationales\nbehind its predictions. The code is publicly available at\nhttps://VidGuard-R1.github.io.", "AI": {"tldr": "VidGuard-R1 is a video authenticity detector that fine-tunes a multi-modal LLM (Qwen-VL) with group relative policy optimization, achieving state-of-the-art results and offering interpretable explanations on a 140k-video dataset; code released.", "motivation": "To counter AI-generated video threats (misinformation and reputational harm) and to provide transparent, interpretable detections for regulators and end users.", "method": "Curated 140k real/AI-generated videos with hard-discrimination design; fine-tuned Qwen-VL using GRPO and two reward models targeting temporal artifacts and generation complexity; evaluated in zero-shot and with additional training to reach ~95% accuracy; case studies show rationales.", "result": "Achieves state-of-the-art zero-shot performance on benchmarks; training pushes accuracy above 95%; interpretable rationales produced; code publicly available.", "conclusion": "Demonstrates effective combination of GRPO-based fine-tuning and MLLMs for accurate, interpretable video authenticity detection; contributes dataset and methodology, advancing transparency in AI-generated content detection."}}
{"id": "2510.01650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01650", "abs": "https://arxiv.org/abs/2510.01650", "authors": ["Kwanhee Lee", "Hyeondo Jang", "Dongyeop Lee", "Dan Alistarh", "Namhoon Lee"], "title": "The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM", "comment": "Preprint", "summary": "Neural network pruning is a promising technique to mitigate the excessive\ncomputational and memory requirements of large language models (LLMs). Despite\nits promise, however, progress in this area has diminished, as conventional\nmethods are seemingly unable to surpass moderate sparsity levels (50-60%)\nwithout severely degrading model accuracy. This work breaks through the current\nimpasse, presenting a principled and effective method called $\\texttt{Elsa}$,\nwhich achieves extreme sparsity levels of up to 90% while retaining high model\nfidelity. This is done by identifying several limitations in current practice,\nall of which can be traced back to their reliance on a surrogate objective\nformulation. $\\texttt{Elsa}$ tackles this issue directly and effectively via\nstandard and well-established constrained optimization techniques based on\nADMM. Our extensive experiments across a wide range of models and scales show\nthat $\\texttt{Elsa}$ achieves substantial improvements over existing methods;\ne.g., it achieves 7.8$\\times$ less perplexity than the best existing method on\nLLaMA-2-7B at 90% sparsity. Furthermore, we present\n$\\texttt{Elsa}_{\\text{-L}}$, a quantized variant that scales to extremely large\nmodels (27B), and establish its theoretical convergence guarantees. These\nresults highlight meaningful progress in advancing the frontier of LLM\nsparsity, while promising that significant opportunities for further\nadvancement may remain in directions that have so far attracted limited\nexploration.", "AI": {"tldr": "Elsa, an ADMM-based constrained pruning method, achieves up to 90% sparsity in LLMs with high fidelity, outperforming prior surrogates; Elsa_L extends to 27B with convergence guarantees.", "motivation": "Current pruning methods rely on surrogate objectives and struggle to surpass ~60% sparsity without accuracy loss, hindering practical efficiency gains for LLMs.", "method": "Formulates pruning as a constrained optimization problem solved with ADMM, identifies surrogate objective limitations, introduces Elsa and Elsa_L (quantized) with convergence guarantees.", "result": "Attains up to 90% sparsity across models; 7.8x lower perplexity than best prior method on LLaMA-2-7B at 90% sparsity; Elsa_L scales to 27B with convergence guarantees.", "conclusion": "Breaks through the sparsity barrier, advancing LLM sparsity; suggests substantial opportunities for further exploration in this direction."}}
{"id": "2510.02283", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02283", "abs": "https://arxiv.org/abs/2510.02283", "authors": ["Justin Cui", "Jie Wu", "Ming Li", "Tao Yang", "Xiaojie Li", "Rui Wang", "Andrew Bai", "Yuanhao Ban", "Cho-Jui Hsieh"], "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation", "comment": "preprint", "summary": "Diffusion models have revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronounced quality degradation, arising from\nthe compounding of errors within the continuous latent space. In this paper, we\npropose a simple yet effective approach to mitigate quality degradation in\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintains temporal consistency while scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model's position embedding and more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/", "AI": {"tldr": "Introduces a self-guided, teacher-assisted approach to long-horizon video generation that uses samples from self-generated sequences to steer a student model, achieving up to 20x horizon extension and 4m15s video length with improved fidelity and temporal consistency, without long-video teachers or retraining.", "motivation": "To address error accumulation and quality degradation when extrapolating diffusion-based video generation beyond short-horizon teacher capabilities, while reducing computational costs of transformer-based models and avoiding the need for long-video supervision.", "method": "Leverages teacher models' rich knowledge to guide the student via sampled segments from self-generated long videos; maintains temporal consistency; avoids recomputing overlapping frames; scales length up to 20x beyond teacher; relies on the base model's position embedding capacity; uses an improved benchmarking dataset; builds improved benchmark.", "result": "Outperforms baselines in fidelity and consistency; achieves up to 4:15 duration (99.9% of base model's position-embedding span) and >50x longer than baseline; demonstrates strong gains on standard benchmarks and proposed improved benchmark.", "conclusion": "The proposed self-forcing guidance effectively mitigates long-horizon degradation, enabling practical long video generation with preserved quality without needing long-video teachers or retraining, and the approach is scalable to very long video horizons."}}
{"id": "2510.01656", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01656", "abs": "https://arxiv.org/abs/2510.01656", "authors": ["Jiashun Liu", "Johan Obando-Ceron", "Han Lu", "Yancheng He", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Pablo Samuel Castro", "Aaron Courville", "Ling Pan"], "title": "Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning", "comment": null, "summary": "Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing\nthem with average advantage baselines. This shift is largely pragmatic:\nconventional value functions are computationally expensive to train at LLM\nscale and often fail under sparse rewards and long reasoning horizons. We\nrevisit this bottleneck from an architectural perspective and introduce\nAsymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable\nframework that restores the critics role while remaining efficient in\nlarge-model settings. AsyPPO employs a set of lightweight mini-critics, each\ntrained on disjoint prompt shards. This design encourages diversity while\npreserving calibration, reducing value-estimation bias. Beyond robust\nestimation, AsyPPO leverages inter-critic uncertainty to refine the policy\nupdate: (i) masking advantages in states where critics agree and gradients add\nlittle learning signal, and (ii) filtering high-divergence states from entropy\nregularization, suppressing spurious exploration. After training on open-source\ndata with only 5,000 samples, AsyPPO consistently improves learning stability\nand performance across multiple benchmarks over strong baselines, such as GRPO,\nachieving performance gains of more than six percent on Qwen3-4b-Base and about\nthree percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without\nadditional tricks. These results highlight the importance of architectural\ninnovations for scalable, efficient algorithms.", "AI": {"tldr": "AsyPPO introduces lightweight, diverse mini-critics for RL with LLMs, restoring the critic role efficiently, and uses inter-critic uncertainty to refine updates, achieving solid gains over PPO with minimal data.", "motivation": "RL4LLMs rely on average baselines; traditional value functions are costly and error-prone with sparse rewards and long horizons. An architectural approach to restore critics could improve stability and performance at scale.", "method": "Train multiple small critics on disjoint prompt shards; use inter-critic uncertainty to (a) mask advantages where critics agree and gradients are small, and (b) filter high-divergence states from entropy regularization; evaluate on open-source data with ~5k samples; compare to PPO/GRPO.", "result": "AsyPPO yields stability and performance gains across benchmarks, e.g., >6% on Qwen3-4b-Base and ~3% on Qwen3-8b-Base/14b-Base over classic PPO, without extra tricks.", "conclusion": "Architectural design enabling scalable, efficient RL for LLMs by distributing critics and leveraging their uncertainty can outperform traditional PPO baselines; importance of critic-enabled architectures."}}
{"id": "2510.02284", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02284", "abs": "https://arxiv.org/abs/2510.02284", "authors": ["David Romero", "Ariana Bermudez", "Hao Li", "Fabio Pizzati", "Ivan Laptev"], "title": "Learning to Generate Object Interactions with Physics-Guided Video Diffusion", "comment": null, "summary": "Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available.", "AI": {"tldr": "KineMask enables physics-guided video generation by conditioning on object velocity and a two-stage training strategy that gradually removes future motion supervision, improving realistic rigid-body interactions and enabling multi-level conditioning; trained on synthetic scenes and validated on real scenes; will release code, model, and data.", "motivation": "Current video generation models struggle with physically plausible object interactions and physics-based control, limiting applicability to robotics and simulation.", "method": "Two-stage training that reduces reliance on future motion supervision via object masks; train video diffusion models on synthetic scenes of simple interactions; integrate low-level motion control with high-level textual conditioning through predictive scene descriptions.", "result": "Significant improvements in object interactions on real scenes; ablations show complementary roles of low- and high-level conditioning; outperform recent models of similar size; results validated through extensive experiments.", "conclusion": "Physics-guided video generation is feasible with the proposed KineMask framework, enabling realistic dynamics and controllable synthesis; the approach bridges low- and high-level conditioning and offers practical resources (code/model/data)."}}
{"id": "2510.01658", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01658", "abs": "https://arxiv.org/abs/2510.01658", "authors": ["Amin Jalali", "Milad Soltany", "Michael Greenspan", "Ali Etemad"], "title": "Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing", "comment": "Accepted in Transactions on Machine Learning Research", "summary": "We propose TimeHUT, a novel method for learning time-series representations\nby hierarchical uniformity-tolerance balancing of contrastive representations.\nOur method uses two distinct losses to learn strong representations with the\naim of striking an effective balance between uniformity and tolerance in the\nembedding space. First, TimeHUT uses a hierarchical setup to learn both\ninstance-wise and temporal information from input time-series. Next, we\nintegrate a temperature scheduler within the vanilla contrastive loss to\nbalance the uniformity and tolerance characteristics of the embeddings.\nAdditionally, a hierarchical angular margin loss enforces instance-wise and\ntemporal contrast losses, creating geometric margins between positive and\nnegative pairs of temporal sequences. This approach improves the coherence of\npositive pairs and their separation from the negatives, enhancing the capture\nof temporal dependencies within a time-series sample. We evaluate our approach\non a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and\nmultivariate classification, as well as Yahoo and KPI datasets for anomaly\ndetection. The results demonstrate that TimeHUT outperforms prior methods by\nconsiderable margins on classification, while obtaining competitive results for\nanomaly detection. Finally, detailed sensitivity and ablation studies are\nperformed to evaluate different components and hyperparameters of our method.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.02287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02287", "abs": "https://arxiv.org/abs/2510.02287", "authors": ["Yichen Li", "Antonio Torralba"], "title": "MultiModal Action Conditioned Video Generation", "comment": null, "summary": "Current video models fail as world model as they lack fine-graiend control.\nGeneral-purpose household robots require real-time fine motor control to handle\ndelicate tasks and urgent situations. In this work, we introduce fine-grained\nmultimodal actions to capture such precise control. We consider senses of\nproprioception, kinesthesia, force haptics, and muscle activation. Such\nmultimodal senses naturally enables fine-grained interactions that are\ndifficult to simulate with text-conditioned generative models. To effectively\nsimulate fine-grained multisensory actions, we develop a feature learning\nparadigm that aligns these modalities while preserving the unique information\neach modality provides. We further propose a regularization scheme to enhance\ncausality of the action trajectory features in representing intricate\ninteraction dynamics. Experiments show that incorporating multimodal senses\nimproves simulation accuracy and reduces temporal drift. Extensive ablation\nstudies and downstream applications demonstrate the effectiveness and\npracticality of our work.", "AI": {"tldr": "A multimodal action framework for fine-grained control using proprioception, kinesthesia, force haptics, and muscle activation; aligns modalities with a learnable feature space and introduces causality regularization, improving simulation accuracy and reducing drift.", "motivation": "Current video models lack robust world models suitable for real-time, fine-grained motor control in household robots, making delicate tasks and urgent situations difficult to handle.", "method": "A feature learning paradigm that aligns multiple modalities while preserving the unique information each modality provides, coupled with a regularization scheme that enhances the causality of action trajectory features to capture intricate interaction dynamics.", "result": "Incorporating multimodal senses improves simulation accuracy and reduces temporal drift; extensive ablations and downstream applications demonstrate effectiveness and practicality.", "conclusion": "Multimodal sensing enables fine-grained control and practical benefits for real-time robotics and simulation, offering improvements over text-conditioned models in capturing nuanced interactions."}}
{"id": "2510.01663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01663", "abs": "https://arxiv.org/abs/2510.01663", "authors": ["Wangxuan Fan", "Ching Wang", "Siqi Li", "Nan Liu"], "title": "Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value", "comment": "15 pages, 6 figures, 9 tables", "summary": "For many real-world applications, understanding feature-outcome relationships\nis as crucial as achieving high predictive accuracy. While traditional neural\nnetworks excel at prediction, their black-box nature obscures underlying\nfunctional relationships. Kolmogorov--Arnold Networks (KANs) address this by\nemploying learnable spline-based activation functions on edges, enabling\nrecovery of symbolic representations while maintaining competitive performance.\nHowever, KAN's architecture presents unique challenges for network pruning.\nConventional magnitude-based methods become unreliable due to sensitivity to\ninput coordinate shifts. We propose \\textbf{ShapKAN}, a pruning framework using\nShapley value attribution to assess node importance in a shift-invariant\nmanner. Unlike magnitude-based approaches, ShapKAN quantifies each node's\nactual contribution, ensuring consistent importance rankings regardless of\ninput parameterization. Extensive experiments on synthetic and real-world\ndatasets demonstrate that ShapKAN preserves true node importance while enabling\neffective network compression. Our approach improves KAN's interpretability\nadvantages, facilitating deployment in resource-constrained environments.", "AI": {"tldr": "ShapKAN uses Shapley-value-based pruning for Kolmogorov\u2013Arnold Networks to achieve shift-invariant, interpretable network compression without sacrificing practical performance.", "motivation": "To recover and preserve interpretable feature\u2013outcome relationships in KANs while achieving compression; magnitude-based pruning is unreliable due to sensitivity to input coordinate shifts.", "method": "Compute Shapley value contributions of each node to the network's outputs, select low-contribution nodes for pruning while maintaining shift-invariance, and evaluate on synthetic and real datasets to compare against magnitude-based pruning.", "result": "ShapKAN preserves actual node importance rankings across input parameterizations and enables effective compression, maintaining interpretability advantages of KANs in resource-constrained settings.", "conclusion": "ShapKAN provides a robust, shift-invariant, attribution-based pruning framework for KANs that enhances interpretability and enables deployment in environments with limited resources."}}
{"id": "2510.02295", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02295", "abs": "https://arxiv.org/abs/2510.02295", "authors": ["Enxin Song", "Wenhao Chai", "Shusheng Yang", "Ethan Armand", "Xiaojun Shan", "Haiyang Xu", "Jianwen Xie", "Zhuowen Tu"], "title": "VideoNSA: Native Sparse Attention Scales Video Understanding", "comment": "Project Page: https://enxinsong.com/VideoNSA-web/, Code:\n  https://github.com/Espere-1119-Song/VideoNSA", "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.", "AI": {"tldr": "VideoNSA extends Native Sparse Attention to video-language models, enabling long-context understanding with a hardware-aware hybrid attention strategy.", "motivation": "Long context length in video-language models leads to missed transition frames and coherence loss over extended time scales; scaling attention efficiently for long videos is a key challenge.", "method": "Introduce VideoNSA by adapting Native Sparse Attention to video-language modeling (Qwen2.5-VL). Train end-to-end on a 216K video-instruction dataset. Use a hardware-aware hybrid attention: dense attention for text tokens, sparse (NSA) attention for video tokens. Employ a learnable, global-local attention allocation under a fixed budget with multiple sparse attention branches, enabling dynamic attention sinks.", "result": "VideoNSA outperforms token-compression and training-free sparse baselines on long-video understanding, temporal reasoning, and spatial benchmarks. Ablations show: (1) reliable scaling to 128K tokens; (2) optimal global-local attention allocation at fixed budget; (3) task-dependent branch usage patterns; (4) learnable combined sparse attention induces dynamic attention sinks.", "conclusion": "VideoNSA demonstrates effective long-context video-language modeling via a hardware-aware sparse-attention approach, offering practical scaling to very long sequences and insights into how to allocate attention between global and local contexts, as well as how learnable sparse attention can adapt to tasks."}}
{"id": "2510.01677", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01677", "abs": "https://arxiv.org/abs/2510.01677", "authors": ["Han Wu", "Yanming Sun", "Yunhe Yang", "Derek F. Wong"], "title": "Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis", "comment": null, "summary": "Multimodal sentiment analysis (MSA) leverages information fusion from diverse\nmodalities (e.g., text, audio, visual) to enhance sentiment prediction.\nHowever, simple fusion techniques often fail to account for variations in\nmodality quality, such as those that are noisy, missing, or semantically\nconflicting. This oversight leads to suboptimal performance, especially in\ndiscerning subtle emotional nuances. To mitigate this limitation, we introduce\na simple yet efficient \\textbf{A}daptive \\textbf{G}ated \\textbf{F}usion\n\\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion\nmechanism based on information entropy and modality importance. This mechanism\nmitigates the influence of noisy modalities and prioritizes informative cues\nfollowing unimodal encoding and cross-modal interaction. Experiments on\nCMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong\nbaselines in accuracy, effectively discerning subtle emotions with robust\nperformance. Visualization analysis of feature representations demonstrates\nthat AGFN enhances generalization by learning from a broader feature\ndistribution, achieved by reducing the correlation between feature location and\nprediction error, thereby decreasing reliance on specific locations and\ncreating more robust multimodal feature representations.", "AI": {"tldr": "Introduces Adaptive Gated Fusion Network (AGFN) for multimodal sentiment analysis that uses dual gates based on information entropy and modality importance to adaptively weight modalities, improving robustness to noisy/missing/conflicting cues and enhancing discrimination of subtle emotions.", "motivation": "Multimodal sentiment analysis often suffers when modalities have varying quality (noise, missing data, semantic conflicts). Simple fusion fails to account for these variations, leading to suboptimal performance.", "method": "AGFN employs a dual gate fusion mechanism that uses information entropy and modality importance to adaptively weight features. Fusion occurs after unimodal encoding and cross-modal interaction, attenuating noise and prioritizing informative cues in a simple yet efficient architecture.", "result": "On CMU-MOSI and CMU-MOSEI, AGFN significantly outperforms strong baselines in accuracy, demonstrating robust performance and improved ability to discern subtle emotions. Visualization indicates enhanced generalization by learning a broader feature distribution and reducing the correlation between feature location and prediction error.", "conclusion": "AGFN provides a simple, efficient, and robust approach to adaptive multimodal fusion, improving accuracy and generalization in MSA by modulating modality contributions according to entropy and importance."}}
{"id": "2510.02307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02307", "abs": "https://arxiv.org/abs/2510.02307", "authors": ["Ruozhen He", "Moayed Haji-Ali", "Ziyan Yang", "Vicente Ordonez"], "title": "NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation", "comment": null, "summary": "Text-to-image diffusion models trained on a fixed set of resolutions often\nfail to generalize, even when asked to generate images at lower resolutions\nthan those seen during training. High-resolution text-to-image generators are\ncurrently unable to easily offer an out-of-the-box budget-efficient alternative\nto their users who might not need high-resolution images. We identify a key\ntechnical insight in diffusion models that when addressed can help tackle this\nlimitation: Noise schedulers have unequal perceptual effects across\nresolutions. The same level of noise removes disproportionately more signal\nfrom lower-resolution images than from high-resolution images, leading to a\ntrain-test mismatch. We propose NoiseShift, a training-free method that\nrecalibrates the noise level of the denoiser conditioned on resolution size.\nNoiseShift requires no changes to model architecture or sampling schedule and\nis compatible with existing models. When applied to Stable Diffusion 3, Stable\nDiffusion 3.5, and Flux-Dev, quality at low resolutions is significantly\nimproved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and\nFlux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by\n10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results\ndemonstrate the effectiveness of NoiseShift in mitigating resolution-dependent\nartifacts and enhancing the quality of low-resolution image generation.", "AI": {"tldr": "NoiseShift is a training-free noise-calibration method that conditions the diffusion denoiser on image resolution to improve low-resolution generation without changing architecture or sampling, showing notable FID gains across SD3, SD3.5, and Flux-Dev on LAION-COCO and CelebA.", "motivation": "Diffusion models trained at fixed resolutions struggle to generalize to lower resolutions, and the same noise level disproportionately harms low-resolution images, causing train-test mismatch. There is a need for a simple, budget-friendly way to improve low-res generation without retraining or changing architecture.", "method": "NoiseShift recalibrates the denoiser's noise conditioning as a function of target resolution. It is training-free, requires no architectural changes or sampling schedule changes, and is compatible with existing diffusion models like Stable Diffusion 3/3.5 and Flux-Dev.", "result": "On LAION-COCO, FID improves (lower is better) by 15.89% for SD3.5, 8.56% for SD3, and 2.44% for Flux-Dev on average. On CelebA, improvements are 10.36% (SD3.5), 5.19% (SD3), and 3.02% (Flux-Dev).", "conclusion": "NoiseShift effectively mitigates resolution-dependent artifacts in low-resolution image generation, offering a practical, training-free enhancement compatible with existing diffusion models and sampling workflows."}}
{"id": "2510.01693", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01693", "abs": "https://arxiv.org/abs/2510.01693", "authors": ["Juncheng Dong", "Weibin Mo", "Zhengling Qi", "Cong Shi", "Ethan X. Fang", "Vahid Tarokh"], "title": "PASTA: A Unified Framework for Offline Assortment Learning", "comment": null, "summary": "We study a broad class of assortment optimization problems in an offline and\ndata-driven setting. In such problems, a firm lacks prior knowledge of the\nunderlying choice model, and aims to determine an optimal assortment based on\nhistorical customer choice data. The combinatorial nature of assortment\noptimization often results in insufficient data coverage, posing a significant\nchallenge in designing provably effective solutions. To address this, we\nintroduce a novel Pessimistic Assortment Optimization (PASTA) framework that\nleverages the principle of pessimism to achieve optimal expected revenue under\ngeneral choice models. Notably, PASTA requires only that the offline data\ndistribution contains an optimal assortment, rather than providing the full\ncoverage of all feasible assortments. Theoretically, we establish the first\nfinite-sample regret bounds for offline assortment optimization across several\nwidely used choice models, including the multinomial logit and nested logit\nmodels. Additionally, we derive a minimax regret lower bound, proving that\nPASTA is minimax optimal in terms of sample and model complexity. Numerical\nexperiments further demonstrate that our method outperforms existing baseline\napproaches.", "AI": {"tldr": "PASTA framework for offline data-driven assortment optimization using pessimism; provides finite-sample regret bounds for MNL and nested logit; minimax-optimal; only requires that the offline data contain an optimal assortment.", "motivation": "In offline data-driven settings, firms lack prior knowledge of the choice model and face data coverage issues. The goal is to produce provably good assortments from historical data with limited coverage.", "method": "Introduce Pessimistic Assortment Optimization (PASTA) that applies a pessimism principle to optimize expected revenue under general choice models. Requires only that the offline data distribution contains an optimal assortment, not full coverage of all assortments. Theoretical results include finite-sample regret bounds for MNL and nested logit models and a minimax regret lower bound.", "result": "Finite-sample regret bounds for offline assortment optimization across several common choice models (MNL and nested logit) are established. A minimax lower bound is derived, showing PASTA is minimax optimal in sample and model complexity. Numerical experiments show the method outperforms baselines.", "conclusion": "PASTA provides a robust, data-driven solution for offline assortment optimization with provable guarantees; it relaxes data-coverage requirements and achieves minimax-optimal performance under multiple choice models; supported by empirical results."}}
{"id": "2510.02311", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02311", "abs": "https://arxiv.org/abs/2510.02311", "authors": ["Guanqi Zhan", "Xianzheng Ma", "Weidi Xie", "Andrew Zisserman"], "title": "Inferring Dynamic Physical Properties from Video Foundation Models", "comment": null, "summary": "We study the task of predicting dynamic physical properties from videos. More\nspecifically, we consider physical properties that require temporal information\nto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,\nand dynamic friction of an object sliding on a surface. To this end, we make\nthe following contributions: (i) We collect a new video dataset for each\nphysical property, consisting of synthetic training and testing splits, as well\nas a real split for real world evaluation. (ii) We explore three ways to infer\nthe physical property from videos: (a) an oracle method where we supply the\nvisual cues that intrinsically reflect the property using classical computer\nvision techniques; (b) a simple read out mechanism using a visual prompt and\ntrainable prompt vector for cross-attention on pre-trained video generative and\nself-supervised models; and (c) prompt strategies for Multi-modal Large\nLanguage Models (MLLMs). (iii) We show that video foundation models trained in\na generative or self-supervised manner achieve a similar performance, though\nbehind that of the oracle, and MLLMs are currently inferior to the other\nmodels, though their performance can be improved through suitable prompting.", "AI": {"tldr": "The work studies predicting dynamic physical properties from videos (elasticity, viscosity, dynamic friction), introducing datasets and evaluating three inference approaches\u2014oracle CV cues, visual-prompt readouts on video foundation models, and prompting MLLMs\u2014with video foundation models nearing oracle performance while MLLMs lag but can be improved.", "motivation": "Temporal physical properties require time-based cues; inferring properties like elasticity, viscosity, and friction from video hinges on temporal dynamics, posing a challenge for conventional vision models. The study contributes new benchmarks to evaluate such reasoning, including synthetic and real-world splits.", "method": "Datasets: new video datasets for each property, with synthetic training/testing splits and a real-world evaluation split. Methods: (a) oracle using classical computer-vision cues tailored to each property; (b) a readout mechanism using a visual prompt and a trainable prompt vector for cross-attention on pretrained video generative/self-supervised models; (c) prompt-based strategies for multi-modal large language models (MLLMs).", "result": "Video foundation models trained in generative or self-supervised regimes achieve performance close to the oracle but still behind it; MLLMs underperform relative to the other models, though prompting can improve their results.", "conclusion": "Foundational video models can approximate physics cues from motion data, underscoring the value of temporal information for dynamic property inference. MLLMs show potential but require more effective prompting; the introduced datasets offer a benchmark to compare physical-property inference methods from video."}}
{"id": "2510.01706", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01706", "abs": "https://arxiv.org/abs/2510.01706", "authors": ["Shaan Shah", "Meenakshi Khosla"], "title": "Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport", "comment": null, "summary": "Standard representational similarity methods align each layer of a network to\nits best match in another independently, producing asymmetric results, lacking\na global alignment score, and struggling with networks of different depths.\nThese limitations arise from ignoring global activation structure and\nrestricting mappings to rigid one-to-one layer correspondences. We propose\nHierarchical Optimal Transport (HOT), a unified framework that jointly infers\nsoft, globally consistent layer-to-layer couplings and neuron-level transport\nplans. HOT allows source neurons to distribute mass across multiple target\nlayers while minimizing total transport cost under marginal constraints. This\nyields both a single alignment score for the entire network comparison and a\nsoft transport plan that naturally handles depth mismatches through mass\ndistribution. We evaluate HOT on vision models, large language models, and\nhuman visual cortex recordings. Across all domains, HOT matches or surpasses\nstandard pairwise matching in alignment quality. Moreover, it reveals smooth,\nfine-grained hierarchical correspondences: early layers map to early layers,\ndeeper layers maintain relative positions, and depth mismatches are resolved by\ndistributing representations across multiple layers. These structured patterns\nemerge naturally from global optimization without being imposed, yet are absent\nin greedy layer-wise methods. HOT thus enables richer, more interpretable\ncomparisons between representations, particularly when networks differ in\narchitecture or depth.", "AI": {"tldr": "HOT provides a unified hierarchical optimal transport framework to jointly infer soft, globally-consistent layer couplings and neuron-level transport plans, yielding a single network-wide alignment score and handling depth mismatches.", "motivation": "Standard representational similarity methods suffer from asymmetric, one-to-one layer mappings, lack a global score, and fail when networks differ in depth; these issues stem from ignoring global activation structure.", "method": "Formulate hierarchical optimal transport that jointly infers soft layer-to-layer couplings and neuron-level transport plans under marginal constraints, allowing source neurons to distribute mass across multiple target layers, yielding a global alignment score.", "result": "Across vision models, large language models, and human visual cortex data, HOT matches or surpasses pairwise methods, reveals smooth hierarchical correspondences (early layers map to early layers; deeper layers preserve relative order), and resolves depth mismatches via mass distribution.", "conclusion": "HOT enables richer, more interpretable network comparisons, robust to architectural and depth differences, derived from global optimization rather than greedy layer-wise mapping."}}
{"id": "2510.02313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02313", "abs": "https://arxiv.org/abs/2510.02313", "authors": ["Mengyu Yang", "Yiming Chen", "Haozheng Pei", "Siddhant Agarwal", "Arun Balajee Vasudevan", "James Hays"], "title": "Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions", "comment": "ICCV 2025. Project page: https://clink-chop-thud.github.io/", "summary": "Can a model distinguish between the sound of a spoon hitting a hardwood floor\nversus a carpeted one? Everyday object interactions produce sounds unique to\nthe objects involved. We introduce the sounding object detection task to\nevaluate a model's ability to link these sounds to the objects directly\ninvolved. Inspired by human perception, our multimodal object-aware framework\nlearns from in-the-wild egocentric videos. To encourage an object-centric\napproach, we first develop an automatic pipeline to compute segmentation masks\nof the objects involved to guide the model's focus during training towards the\nmost informative regions of the interaction. A slot attention visual encoder is\nused to further enforce an object prior. We demonstrate state of the art\nperformance on our new task along with existing multimodal action understanding\ntasks.", "AI": {"tldr": "Introduces sounding object detection, a multimodal, object-centric task that links sounds to the specific objects involved in everyday interactions, using segmentation-guided training and a slot-attention encoder to enforce object priors, achieving state-of-the-art results on the new task and related multimodal benchmarks.", "motivation": "Reflects human perception by tying sounds to the exact object involved in an interaction. The paper argues that everyday object sounds are object-specific and that models should learn to associate audio with the objects rather than general scene cues, prompting an object-centric multimodal learning framework evaluated on egocentric video.", "method": "Develops an automatic pipeline to compute segmentation masks to guide training toward informative regions of object interaction. Employs a slot-attention visual encoder to enforce an object-centric prior. Proposes the sounding object detection task and evaluates on this task and existing multimodal action understanding benchmarks using in-the-wild egocentric video data.", "result": "Achieves state-of-the-art performance on the new sounding object detection task and on existing multimodal action understanding tasks, demonstrating the effectiveness of object-centric, audio-visual learning.", "conclusion": "The work validates that incorporating object priors and segmentation-guided training enables models to link sounds to the corresponding objects, contributing to more interpretable and human-like multimodal understanding in real-world videos."}}
{"id": "2510.01712", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01712", "abs": "https://arxiv.org/abs/2510.01712", "authors": ["Aidan Acquah", "Shing Chan", "Aiden Doherty"], "title": "ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning", "comment": null, "summary": "The use of reliable and accurate human activity recognition (HAR) models on\npassively collected wrist-accelerometer data is essential in large-scale\nepidemiological studies that investigate the association between physical\nactivity and health outcomes. While the use of self-supervised learning has\ngenerated considerable excitement in improving HAR, it remains unknown the\nextent to which these models, coupled with hidden Markov models (HMMs), would\nmake a tangible improvement to classification performance, and the effect this\nmay have on the predicted daily activity intensity compositions. Using 151\nCAPTURE-24 participants' data, we trained the ActiNet model, a self-supervised,\n18-layer, modified ResNet-V2 model, followed by hidden Markov model (HMM)\nsmoothing to classify labels of activity intensity. The performance of this\nmodel, evaluated using 5-fold stratified group cross-validation, was then\ncompared to a baseline random forest (RF) + HMM, established in existing\nliterature. Differences in performance and classification outputs were compared\nwith different subgroups of age and sex within the Capture-24 population. The\nActiNet model was able to distinguish labels of activity intensity with a mean\nmacro F1 score of 0.82, and mean Cohen's kappa score of 0.86. This exceeded the\nperformance of the RF + HMM, trained and validated on the same dataset, with\nmean scores of 0.77 and 0.81, respectively. These findings were consistent\nacross subgroups of age and sex. These findings encourage the use of ActiNet\nfor the extraction of activity intensity labels from wrist-accelerometer data\nin future epidemiological studies.", "AI": {"tldr": "Self-supervised ActiNet with HMM smoothing outperforms RF+HMM for wrist-worn HAR in CAPTURE-24 data, with higher macro F1 (0.82) and Cohen's kappa (0.86) and consistent across age/sex subgroups; supports using ActiNet for epidemiological activity labeling.", "motivation": "To determine whether combining self-supervised learning with hidden Markov model smoothing can meaningfully improve activity-label classification and affect estimated activity intensity distributions in large-scale epidemiological studies using wrist accelerometer data.", "method": "Train ActiNet (18-layer modified ResNet-V2) in a self-supervised fashion on CAPTURE-24 data (n=151), followed by HMM smoothing. Compare against a baseline Random Forest + HMM using identical data and evaluation (5-fold stratified group cross-validation). Subgroup analyses by age and sex.", "result": "ActiNet achieved mean macro F1 of 0.82 and Cohen's kappa of 0.86, outperforming RF+HMM (0.77 F1, 0.81 kappa). Findings were consistent across age/sex subgroups.", "conclusion": "ActiNet + HMM is a promising approach for reliable activity intensity labeling in epidemiological studies, enabling more accurate extraction of activity patterns from wrist accelerometers and potentially improving downstream health association analyses."}}
{"id": "2510.02314", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02314", "abs": "https://arxiv.org/abs/2510.02314", "authors": ["Bo-Hsu Ke", "You-Zhe Xie", "Yu-Lun Liu", "Wei-Chen Chiu"], "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions", "comment": "ICCV 2025. Project page: https://hentci.github.io/stealthattack/", "summary": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/", "AI": {"tldr": "A density-guided poisoning attack on 3D Gaussian Splatting (3DGS) using KDE to insert low-density Gaussian points that create illusory objects visible from poisoned views, plus adaptive noise to break multi-view consistency, and a KDE-based evaluation protocol for benchmarking; results show state-of-the-art effectiveness.", "motivation": "As 3D scene representations like NeRF and 3DGS become widespread, their vulnerabilities warrant study. The abstract focuses on poisoning attacks against 3DGS, highlighting robustness concerns and the need for evaluation and defenses against image-level tampering.", "method": "Identify low-density regions via Kernel Density Estimation (KDE) and inject Gaussian points into those regions to embed viewpoint-dependent illusory objects that are clearly visible from poisoned views while minimally affecting benign views. Employ an adaptive noise strategy to disrupt multi-view consistency across views. Propose a KDE-based evaluation protocol to systematically assess attack difficulty and benchmark future work.", "result": "Extensive experiments show the proposed method achieves superior performance compared to state-of-the-art poisoning techniques, validating its effectiveness.", "conclusion": "The work presents a stealthy, density-guided poisoning approach for 3DGS along with an KDE-based evaluation framework, offering a new toolkit for evaluating vulnerabilities and guiding defenses in 3D scene representations."}}
{"id": "2510.01717", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01717", "abs": "https://arxiv.org/abs/2510.01717", "authors": ["Shaba Shaon", "Dinh C. Nguyen"], "title": "Latency-aware Multimodal Federated Learning over UAV Networks", "comment": "Accepted at IEEE Transactions on Network Science and Engineering", "summary": "This paper investigates federated multimodal learning (FML) assisted by\nunmanned aerial vehicles (UAVs) with a focus on minimizing system latency and\nproviding convergence analysis. In this framework, UAVs are distributed\nthroughout the network to collect data, participate in model training, and\ncollaborate with a base station (BS) to build a global model. By utilizing\nmultimodal sensing, the UAVs overcome the limitations of unimodal systems,\nenhancing model accuracy, generalization, and offering a more comprehensive\nunderstanding of the environment. The primary objective is to optimize FML\nsystem latency in UAV networks by jointly addressing UAV sensing scheduling,\npower control, trajectory planning, resource allocation, and BS resource\nmanagement. To address the computational complexity of our latency minimization\nproblem, we propose an efficient iterative optimization algorithm combining\nblock coordinate descent and successive convex approximation techniques, which\nprovides high-quality approximate solutions. We also present a theoretical\nconvergence analysis for the UAV-assisted FML framework under a non-convex loss\nfunction. Numerical experiments demonstrate that our FML framework outperforms\nexisting approaches in terms of system latency and model training performance\nunder different data settings.", "AI": {"tldr": "A UAV-enabled federated multimodal learning framework minimizes system latency via joint optimization (scheduling, power, trajectory, resource allocation) with a BCD-SCA algorithm, backed by convergence analysis, outperforming baselines in latency and training accuracy.", "motivation": "To achieve fast, accurate federated learning in UAV networks by leveraging multimodal sensing while addressing the nonconvex, combinatorial nature of the optimization problem.", "method": "Propose an iterative optimization framework combining Block Coordinate Descent (BCD) and Successive Convex Approximation (SCA) to jointly optimize UAV sensing scheduling, power control, trajectory planning, and base station resource management for FML; provide theoretical convergence analysis under a non-convex loss.", "result": "The framework yields high-quality approximate solutions and demonstrates reduced system latency and improved model training performance compared with existing approaches across various data settings in numerical experiments.", "conclusion": "UAV-assisted federated multimodal learning with coordinated resource management and multimodal sensing is effective for latency-constrained FL; the proposed BCD-SCA algorithm provides convergence guarantees and practical performance gains."}}
{"id": "2510.02315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02315", "abs": "https://arxiv.org/abs/2510.02315", "authors": ["Eric Tillmann Bill", "Enis Simsar", "Thomas Hofmann"], "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity", "comment": "Code: https://github.com/ericbill21/FOCUS/", "summary": "Text-to-image (T2I) models excel on single-entity prompts but struggle with\nmulti-subject descriptions, often showing attribute leakage, identity\nentanglement, and subject omissions. We introduce the first theoretical\nframework with a principled, optimizable objective for steering sampling\ndynamics toward multi-subject fidelity. Viewing flow matching (FM) through\nstochastic optimal control (SOC), we formulate subject disentanglement as\ncontrol over a trained FM sampler. This yields two architecture-agnostic\nalgorithms: (i) a training-free test-time controller that perturbs the base\nvelocity with a single-pass update, and (ii) Adjoint Matching, a lightweight\nfine-tuning rule that regresses a control network to a backward adjoint signal\nwhile preserving base-model capabilities. The same formulation unifies prior\nattention heuristics, extends to diffusion models via a flow-diffusion\ncorrespondence, and provides the first fine-tuning route explicitly designed\nfor multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and\nStable Diffusion XL, both algorithms consistently improve multi-subject\nalignment while maintaining base-model style. Test-time control runs\nefficiently on commodity GPUs, and fine-tuned controllers trained on limited\nprompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal\nControl for Unentangled Subjects), which achieves state-of-the-art\nmulti-subject fidelity across models.", "AI": {"tldr": "Introduces a principled, optimizable framework to improve multi-subject fidelity in text-to-image generation by treating sampling as a stochastic control problem and disentangling subjects via flow matching.", "motivation": "T2I models excel for single entities but fail on multi-subject prompts due to attribute leakage, identity entanglement, and omissions. The work seeks a theoretical objective to steer sampling dynamics toward faithful multi-subject rendering and to unify prior heuristics.", "method": "Formulates subject disentanglement as control over a trained flow-matching sampler via stochastic optimal control. Proposes two architecture-agnostic algorithms: (1) a training-free test-time controller that perturbs the base velocity with a single-pass update; (2) Adjoint Matching, a light fine-tuning rule regressing a control network to a backward adjoint signal while preserving base-model capabilities. Extends flow-matching to flow-diffusion and unifies attention heuristics.", "result": "On Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both methods improve multi-subject alignment while maintaining style. Test-time control is efficient on commodity GPUs and the learned controllers generalize to unseen prompts. FOCUS achieves state-of-the-art multi-subject fidelity across models.", "conclusion": "A unified, theory-backed pathway for multi-subject fidelity in diffusion-based T2I, offering practical, scalable tools (training-free control and Adjoint Matching) and a coherent link between attention cues and sampling dynamics, with broad applicability across diffusion-model families."}}
{"id": "2510.01718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01718", "abs": "https://arxiv.org/abs/2510.01718", "authors": ["Jialin Zhao"], "title": "Accelerating Attention with Basis Decomposition", "comment": null, "summary": "Attention is a core operation in large language models (LLMs) and\nvision-language models (VLMs). We present BD Attention (BDA), the first\nlossless algorithmic reformulation of attention. BDA is enabled by a simple\nmatrix identity from Basis Decomposition (BD), which restructures multi-head\nprojections into a compact form while preserving exact outputs. Unlike\nI/O-aware system optimizations such as FlashAttention, BDA provides a\nmathematically guaranteed acceleration that is architecture-agnostic. On\nDeepSeek-V2-Lite (16B, FP16), BDA requires only 4s of offline preparation with\nno retraining required and, on modern GPUs, achieves 32% faster key/value\nprojections and 25% smaller weights, while increasing end-to-end perplexity\n(PPL) by just 0.02% (FP16) or 0.0004% (FP32), a negligible effect on model\nperformance. These results position BDA as the first theoretically exact method\nfor lossless attention acceleration that is complementary to existing\nengineering-level optimizations. Our code is available at\nhttps://github.com/abcbdf/basis-decomposition-official.", "AI": {"tldr": "BD Attention provides a lossless, exact reformulation of attention using Basis Decomposition, enabling architecture-agnostic speedups with negligible impact on model performance.", "motivation": "Attention computations are a bottleneck in large language and vision-language models; the goal is a provably exact acceleration that does not degrade outputs or require retraining.", "method": "Basis Decomposition restructures multi-head projections into a compact form via a simple matrix identity, preserving exact outputs. This yields a lossless reformulation of attention and is architecture-agnostic. Offline preparation is required.", "result": "On DeepSeek-V2-Lite (16B, FP16): ~4 seconds offline preparation; on modern GPUs: ~32% faster key/value projections and ~25% smaller weights, with end-to-end perplexity increases of 0.02% (FP16) or 0.0004% (FP32).", "conclusion": "BD Attention is the first theoretically exact method for lossless attention acceleration, complementary to existing engineering optimizations and architecture-agnostic. The authors provide code."}}
{"id": "2510.01721", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2510.01721", "abs": "https://arxiv.org/abs/2510.01721", "authors": ["Saptarshi Mandal", "Yashaswini Murthy", "R. Srikant"], "title": "Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation", "comment": "Preprint. 32 Pages", "summary": "Distributionally robust reinforcement learning (DRRL) focuses on designing\npolicies that achieve good performance under model uncertainties. In\nparticular, we are interested in maximizing the worst-case long-term discounted\nreward, where the data for RL comes from a nominal model while the deployed\nenvironment can deviate from the nominal model within a prescribed uncertainty\nset. Existing convergence guarantees for robust temporal-difference (TD)\nlearning for policy evaluation are limited to tabular MDPs or are dependent on\nrestrictive discount-factor assumptions when function approximation is used. We\npresent the first robust TD learning with linear function approximation, where\nrobustness is measured with respect to the total-variation distance and\nWasserstein-l distance uncertainty set. Additionally, our algorithm is both\nmodel-free and does not require generative access to the MDP. Our algorithm\ncombines a two-time-scale stochastic-approximation update with an outer-loop\ntarget-network update. We establish an $\\tilde{O}(1/\\epsilon^2)$ sample\ncomplexity to obtain an $\\epsilon$-accurate value estimate. Our results close a\nkey gap between the empirical success of robust RL algorithms and the\nnon-asymptotic guarantees enjoyed by their non-robust counterparts. The key\nideas in the paper also extend in a relatively straightforward fashion to\nrobust Q-learning with function approximation.", "AI": {"tldr": "First robust TD learning with linear function approximation under distributional robustness; model-free, no generative access; two-time-scale updates with a target network; achieves tilde O(1/epsilon^2) sample complexity for epsilon-accurate value estimates; extends to robust Q-learning.", "motivation": "Robust reinforcement learning under model uncertainty lacks non-asymptotic guarantees when using function approximation. Existing results are limited to tabular MDPs or require restrictive discount-factor assumptions. This work aims to provide the first robust TD learning guarantee with linear function approximation under distributional uncertainty (TV and Wasserstein-l distance).", "method": "Propose a two-time-scale stochastic approximation algorithm with an outer-loop target-network update for robust TD learning with linear function approximation. Uncertainty is modeled via total-variation distance and Wasserstein-l distance sets. The algorithm is model-free and does not require generative access. The analysis yields convergence and non-asymptotic guarantees, with the outer loop stabilizing the target. Extends naturally to robust Q-learning.", "result": "Establishes an O~(1/\u03b5^2) sample complexity to obtain an \u03b5-accurate value estimate under robust TD with linear function approximation for TV and Wasserstein distance uncertainty sets. The algorithm is model-free and does not require access to the true MDP; shows robustness to model deviations within the specified sets.", "conclusion": "This work closes a key gap between practical robustness seen in RL applications and theoretical guarantees for robust RL with function approximation. The ideas generalize to robust Q-learning, indicating a viable path for provable robustness in modern RL settings."}}
{"id": "2510.01723", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01723", "abs": "https://arxiv.org/abs/2510.01723", "authors": ["Tanay Rastogi", "Anders Karlstr\u00f6m"], "title": "Workplace Location Choice Model based on Deep Neural Network", "comment": null, "summary": "Discrete choice models (DCMs) have long been used to analyze workplace\nlocation decisions, but they face challenges in accurately mirroring individual\ndecision-making processes. This paper presents a deep neural network (DNN)\nmethod for modeling workplace location choices, which aims to better understand\ncomplex decision patterns and provides better results than traditional discrete\nchoice models (DCMs). The study demonstrates that DNNs show significant\npotential as a robust alternative to DCMs in this domain. While both models\neffectively replicate the impact of job opportunities on workplace location\nchoices, the DNN outperforms the DCM in certain aspects. However, the DCM\nbetter aligns with data when assessing the influence of individual attributes\non workplace distance. Notably, DCMs excel at shorter distances, while DNNs\nperform comparably to both data and DCMs for longer distances. These findings\nunderscore the importance of selecting the appropriate model based on specific\napplication requirements in workplace location choice analysis.", "AI": {"tldr": "DNNs can model workplace location choices and may outperform traditional DCMs in some aspects, but each method has strengths depending on distance and attribute emphasis; model choice should align with analysis goals.", "motivation": "To address limitations of traditional discrete choice models in mirroring individual decision processes in workplace location, and to evaluate whether DNNs provide a robust alternative.", "method": "Empirical comparison between DNNs and traditional DCMs on workplace location data, examining impact of job opportunities, individual attributes, and geographic distance; assess short vs long distance performance.", "result": "Compared DNNs and DCMs; DNNs outperform DCMs in certain aspects and show strong generalization; both reproduce job opportunities' effects; DCMs better explain short-distance and individual-attribute effects; DNNs perform well for longer distances.", "conclusion": "Model choice should be tailored to application; DNNs are a promising robust alternative, with strengths for complex patterns and longer-distance decisions, while DCMs remain advantageous for short distances and attribute-focused analyses."}}
{"id": "2510.01744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01744", "abs": "https://arxiv.org/abs/2510.01744", "authors": ["Lea Demelius", "Dominik Kowald", "Simone Kopeinik", "Roman Kern", "Andreas Tr\u00fcgler"], "title": "Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD", "comment": null, "summary": "Differential privacy (DP) is a prominent method for protecting information\nabout individuals during data analysis. Training neural networks with\ndifferentially private stochastic gradient descent (DPSGD) influences the\nmodel's learning dynamics and, consequently, its output. This can affect the\nmodel's performance and fairness. While the majority of studies on the topic\nreport a negative impact on fairness, it has recently been suggested that\nfairness levels comparable to non-private models can be achieved by optimizing\nhyperparameters for performance directly on differentially private models\n(rather than re-using hyperparameters from non-private models, as is common\npractice). In this work, we analyze the generalizability of this claim by 1)\ncomparing the disparate impact of DPSGD on different performance metrics, and\n2) analyzing it over a wide range of hyperparameter settings. We highlight that\na disparate impact on one metric does not necessarily imply a disparate impact\non another. Most importantly, we show that while optimizing hyperparameters\ndirectly on differentially private models does not mitigate the disparate\nimpact of DPSGD reliably, it can still lead to improved utility-fairness\ntrade-offs compared to re-using hyperparameters from non-private models. We\nstress, however, that any form of hyperparameter tuning entails additional\nprivacy leakage, calling for careful considerations of how to balance privacy,\nutility and fairness. Finally, we extend our analyses to DPSGD-Global-Adapt, a\nvariant of DPSGD designed to mitigate the disparate impact on accuracy, and\nconclude that this alternative may not be a robust solution with respect to\nhyperparameter choice.", "AI": {"tldr": "Hyperparameter tuning directly on DP models does not reliably fix fairness gaps in DPSGD; it can improve utility-fairness trade-offs compared to re-using non-private hyperparameters, but adds privacy leakage and is not a robust fairness remedy.", "motivation": "To assess whether optimizing hyperparameters for DP models generalizes to fairness outcomes, and whether this can mitigate DPSGD's disparate impact across metrics.", "method": "Empirical analysis across a wide range of hyperparameters and multiple performance/fairness metrics, comparing the disparate impact of DPSGD on each metric; evaluating under-DPSGD hyperparameter optimization vs re-using non-private hyperparameters; extending evaluation to DPSGD-Global-Adapt variant.", "result": "Disparate impact varies by metric; improvement in one metric does not guarantee improvements in others. Directly optimizing hyperparameters on DP models does not reliably reduce DPSGD's disparate impact, but can improve utility-fairness trade-offs relative to re-using non-private hyperparameters. Hyperparameter tuning incurs extra privacy leakage. DPSGD-Global-Adapt may not be robust to hyperparameter choice.", "conclusion": "Hyperparameter tuning under differential privacy is not a robust solution for fairness; careful balancing of privacy, utility, and fairness is required, and alternative approaches beyond hyperparameter optimization are needed."}}
{"id": "2510.01755", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.01755", "abs": "https://arxiv.org/abs/2510.01755", "authors": ["Johannes Hertrich", "Hok Shing Wong", "Alexander Denker", "Stanislas Ducotterd", "Zhenghan Fang", "Markus Haltmeier", "\u017deljko Kereta", "Erich Kobler", "Oscar Leong", "Mohammad Sadegh Salehi", "Carola-Bibiane Sch\u00f6nlieb", "Johannes Schwab", "Zakhar Shumaylov", "Jeremias Sulam", "German Sh\u00e2ma Wache", "Martin Zach", "Yasi Zhang", "Matthias J. Ehrhardt", "Sebastian Neumayer"], "title": "Learning Regularization Functionals for Inverse Problems: A Comparative Study", "comment": null, "summary": "In recent years, a variety of learned regularization frameworks for solving\ninverse problems in imaging have emerged. These offer flexible modeling\ntogether with mathematical insights. The proposed methods differ in their\narchitectural design and training strategies, making direct comparison\nchallenging due to non-modular implementations. We address this gap by\ncollecting and unifying the available code into a common framework. This\nunified view allows us to systematically compare the approaches and highlight\ntheir strengths and limitations, providing valuable insights into their future\npotential. We also provide concise descriptions of each method, complemented by\npractical guidelines.", "AI": {"tldr": "A unified framework that collects and harmonizes code for learned regularization methods in imaging inverse problems, enabling systematic comparison and practical guidelines.", "motivation": "There are many learned regularization frameworks with diverse architectures and training strategies, but direct comparison is hindered by non-modular, disparate implementations. A unified view is needed to fairly assess strengths, limitations, and future potential.", "method": "Aggregate and unify the available code into a common framework; provide concise descriptions of each method; perform systematic comparative analysis; derive practical guidelines for practitioners.", "result": "Enables fair, systematic comparisons across methods; highlights design trade-offs, strengths, and limitations; yields insights into future potential and actionable guidelines.", "conclusion": "The unified view serves as a valuable resource for researchers and practitioners to evaluate, compare, and advance learned regularization approaches for imaging inverse problems."}}
{"id": "2510.01758", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01758", "abs": "https://arxiv.org/abs/2510.01758", "authors": ["Bruno Corcuera", "Carlos Eiras-Franco", "Brais Cancela"], "title": "Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks", "comment": null, "summary": "Latent representations are critical for the performance and robustness of\nmachine learning models, as they encode the essential features of data in a\ncompact and informative manner. However, in vision tasks, these representations\nare often affected by noisy or irrelevant features, which can degrade the\nmodel's performance and generalization capabilities. This paper presents a\nnovel approach for enhancing latent representations using unsupervised Dynamic\nFeature Selection (DFS). For each instance, the proposed method identifies and\nremoves misleading or redundant information in images, ensuring that only the\nmost relevant features contribute to the latent space. By leveraging an\nunsupervised framework, our approach avoids reliance on labeled data, making it\nbroadly applicable across various domains and datasets. Experiments conducted\non image datasets demonstrate that models equipped with unsupervised DFS\nachieve significant improvements in generalization performance across various\ntasks, including clustering and image generation, while incurring a minimal\nincrease in the computational cost.", "AI": {"tldr": "Unsupervised Dynamic Feature Selection (DFS) enhances latent representations by per-instance removal of noisy or redundant features, boosting generalization in clustering and image generation with minimal extra cost.", "motivation": "Latent representations are prone to contamination by noise and task-irrelevant features, which harms performance and generalization. An unsupervised DFS approach avoids reliance on labels and can generalize across datasets and domains.", "method": "An unsupervised framework that, for each instance, identifies and removes misleading or redundant information in images so that only the most relevant features contribute to the latent space, thereby improving the quality of latent representations without using labeled data.", "result": "Experiments on image datasets show significant improvements in generalization across tasks such as clustering and image generation, with only a slight increase in computational cost.", "conclusion": "Unsupervised DFS is effective for enhancing latent representations and generalization without labeled data, and is broadly applicable across domains."}}
{"id": "2510.01764", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01764", "abs": "https://arxiv.org/abs/2510.01764", "authors": ["Waris Radji", "Thomas Michel", "Hector Piteau"], "title": "Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX", "comment": null, "summary": "Reinforcement learning (RL) research requires diverse, challenging\nenvironments that are both tractable and scalable. While modern video games may\noffer rich dynamics, they are computationally expensive and poorly suited for\nlarge-scale experimentation due to their CPU-bound execution. We introduce\nOctax, a high-performance suite of classic arcade game environments implemented\nin JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely\nadopted as a benchmark in RL research. Octax provides the JAX community with a\nlong-awaited end-to-end GPU alternative to the Atari benchmark, offering\nimage-based environments, spanning puzzle, action, and strategy genres, all\nexecutable at massive scale on modern GPUs. Our JAX-based implementation\nachieves orders-of-magnitude speedups over traditional CPU emulators while\nmaintaining perfect fidelity to the original game mechanics. We demonstrate\nOctax's capabilities by training RL agents across multiple games, showing\nsignificant improvements in training speed and scalability compared to existing\nsolutions. The environment's modular design enables researchers to easily\nextend the suite with new games or generate novel environments using large\nlanguage models, making it an ideal platform for large-scale RL\nexperimentation.", "AI": {"tldr": "GPU-accelerated CHIP-8 arcade RL environments in JAX (Octax): fast, faithful, scalable benchmarks for RL on GPUs.", "motivation": "Need diverse, challenging RL benchmarks that are tractable and scalable. Classic CPU-bound emulators limit large-scale experimentation; a GPU-friendly, end-to-end JAX solution with faithful game mechanics is desirable.", "method": "Re-implement CHIP-8 arcade games as image-based environments in JAX (end-to-end on GPU); modular design; supports puzzle/action/strategy genres; scalable across multiple games; compare performance to CPU emulators; enable generation of new environments (potentially via LLMs).", "result": "Orders-of-magnitude speedups over CPU emulators while maintaining perfect fidelity to original mechanics; successful training of RL agents across multiple games; improved training speed and scalability; modular architecture enables easy extension with new games or LLM-generated environments.", "conclusion": "Octax delivers a high-performance, faithful, GPU-based suite for RL research that enables large-scale experimentation and easy extensibility for additional games or novel environments."}}
{"id": "2510.01788", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.01788", "abs": "https://arxiv.org/abs/2510.01788", "authors": ["Cl\u00e9mentine Court\u00e8s", "Emmanuel Franck", "Michael Kraus", "Laurent Navoret", "L\u00e9opold Tr\u00e9mant"], "title": "Neural non-canonical Hamiltonian dynamics for long-time simulations", "comment": null, "summary": "This work focuses on learning non-canonical Hamiltonian dynamics from data,\nwhere long-term predictions require the preservation of structure both in the\nlearned model and in numerical schemes. Previous research focused on either\nfacet, respectively with a potential-based architecture and with degenerate\nvariational integrators, but new issues arise when combining both. In\nexperiments, the learnt model is sometimes numerically unstable due to the\ngauge dependency of the scheme, rendering long-time simulations impossible. In\nthis paper, we identify this problem and propose two different training\nstrategies to address it, either by directly learning the vector field or by\nlearning a time-discrete dynamics through the scheme. Several numerical test\ncases assess the ability of the methods to learn complex physical dynamics,\nlike the guiding center from gyrokinetic plasma physics.", "AI": {"tldr": "This work studies learning non-canonical Hamiltonian dynamics with structure-preserving methods, identifies gauge-related instability when coupling potential-based models with degenerate variational integrators, and proposes two training strategies (direct vector-field learning and learning a time-discrete scheme) validated on complex dynamics such as gyrokinetic guiding-center dynamics.", "motivation": "Long-term predictions require preservation of geometric structure in both the learned model and the numerical integrator. Prior work focused on either a potential-based architecture or degenerate variational integrators; combining them introduces instabilities due to gauge dependencies.", "method": "Propose two training strategies: (i) directly learn the vector field, or (ii) learn the time-discrete dynamics through the numerical scheme (train the scheme itself). Analyze the gauge-dependency issue and evaluate the approaches on numerical test cases.", "result": "Identifies gauge-dependency\u2013driven instability when mixing potential-based models with degenerate variational integrators. Proposes two training strategies to address this problem. Numerical experiments across several test problems show the methods can learn and represent complex physical dynamics, including guiding-center dynamics from gyrokinetic plasma physics.", "conclusion": "Two practical training strategies are viable to stabilize learning and preserve geometric structure in non-canonical Hamiltonian dynamics, enabling more reliable long-term simulations; demonstrated on test cases including guiding-center dynamics."}}
{"id": "2510.01793", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01793", "abs": "https://arxiv.org/abs/2510.01793", "authors": ["Adil Koeken", "Alexander Ziller", "Moritz Knolle", "Daniel Rueckert"], "title": "Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation", "comment": null, "summary": "The generation of privacy-preserving synthetic datasets is a promising avenue\nfor overcoming data scarcity in medical AI research. Post-hoc privacy filtering\ntechniques, designed to remove samples containing personally identifiable\ninformation, have recently been proposed as a solution. However, their\neffectiveness remains largely unverified. This work presents a rigorous\nevaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary\nto claims from the original publications, our results demonstrate that current\nfilters exhibit limited specificity and consistency, achieving high sensitivity\nonly for real images while failing to reliably detect near-duplicates generated\nfrom training data. These results demonstrate a critical limitation of post-hoc\nfiltering: rather than effectively safeguarding patient privacy, these methods\nmay provide a false sense of security while leaving unacceptable levels of\npatient information exposed. We conclude that substantial advances in filter\ndesign are needed before these methods can be confidently deployed in sensitive\napplications.", "AI": {"tldr": "Post-hoc privacy filters for synthetic chest X-ray datasets show limited specificity and consistency, with high sensitivity only for real images and poor detection of near-duplicates from training data, implying insufficient protection of patient privacy.", "motivation": "To rigorously evaluate whether post-hoc privacy filtering can reliably protect patient privacy in privacy-preserving synthetic data pipelines for medical imaging.", "method": "A systematic evaluation of a filtering pipeline applied to chest X-ray synthesis, assessing the filters' ability to distinguish real images from synthetic or near-duplicates of training data, and measuring specificity, consistency, and sensitivity.", "result": "Filters exhibit limited specificity and consistency; high sensitivity only for real images but fail to reliably detect near-duplicates generated from training data, leading to potential privacy leakage and a false sense of security.", "conclusion": "Current post-hoc privacy filters are insufficient for robust privacy protection in synthetic medical image generation; substantial advances in filter design are needed before deployment in sensitive clinical applications."}}
{"id": "2510.01982", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01982", "abs": "https://arxiv.org/abs/2510.01982", "authors": ["Yujie Zhou", "Pengyang Ling", "Jiazi Bu", "Yibin Wang", "Yuhang Zang", "Jiaqi Wang", "Li Niu", "Guangtao Zhai"], "title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models", "comment": "Github Page: https://github.com/bcmi/Granular-GRPO", "summary": "The integration of online reinforcement learning (RL) into diffusion and flow\nmodels has recently emerged as a promising approach for aligning generative\nmodels with human preferences. Stochastic sampling via Stochastic Differential\nEquations (SDE) is employed during the denoising process to generate diverse\ndenoising directions for RL exploration. While existing methods effectively\nexplore potential high-value samples, they suffer from sub-optimal preference\nalignment due to sparse and narrow reward signals. To address these challenges,\nwe propose a novel Granular-GRPO ($\\text{G}^2$RPO ) framework that achieves\nprecise and comprehensive reward assessments of sampling directions in\nreinforcement learning of flow models. Specifically, a Singular Stochastic\nSampling strategy is introduced to support step-wise stochastic exploration\nwhile enforcing a high correlation between the reward and the injected noise,\nthereby facilitating a faithful reward for each SDE perturbation. Concurrently,\nto eliminate the bias inherent in fixed-granularity denoising, we introduce a\nMulti-Granularity Advantage Integration module that aggregates advantages\ncomputed at multiple diffusion scales, producing a more comprehensive and\nrobust evaluation of the sampling directions. Experiments conducted on various\nreward models, including both in-domain and out-of-domain evaluations,\ndemonstrate that our $\\text{G}^2$RPO significantly outperforms existing\nflow-based GRPO baselines,highlighting its effectiveness and robustness.", "AI": {"tldr": "Introduces G^2RPO (Granular-GRPO), a reinforcement-learning forflow models framework with granular, multi-scale reward evaluation and singular stochastic sampling to tightly align per-step rewards with SDE perturbations, yielding strong improvements over base GRPO methods.", "motivation": "Existing online RL for diffusion/flow models uses stochastic sampling to explore denoising directions but suffers from sparse/narrow rewards and misalignment between reward signals and the noise injected during SDE steps, leading to inefficient exploration and suboptimal alignment with human preferences.", "method": "Proposes two innovations: (1) Singular Stochastic Sampling to maintain a strong correlation between injected noise and the observed reward for each SDE perturbation, enabling faithful per-step reward signaling; (2) Multi-Granularity Advantage Integration to aggregate advantages computed at multiple diffusion scales, reducing fixed-granularity bias and yielding a richer evaluation of sampling directions. Both are integrated into the Granular-GRPO framework.", "result": "Empirical evaluations on multiple reward models (in-domain and out-of-domain) show that G^2RPO significantly outperforms existing flow-based GRPO baselines, demonstrating improved robustness and reward-alignment.", "conclusion": "G^2RPO provides precise, comprehensive reward assessments for sampling directions in online RL for flow models and effectively mitigates reward sparsity and perturbation-reward misalignment through singular stochastic sampling and multi-granularity integration, leading to stronger downstream performance."}}
{"id": "2510.01796", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01796", "abs": "https://arxiv.org/abs/2510.01796", "authors": ["Meng-Hsi Chen", "Yu-Ang Lee", "Feng-Ting Liao", "Da-shan Shiu"], "title": "Rethinking the shape convention of an MLP", "comment": null, "summary": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow\ndesign where skip connections operate at the input/output dimensions while\nprocessing occurs in expanded hidden spaces. We challenge this convention by\nproposing wide-narrow-wide (Hourglass) MLP blocks where skip connections\noperate at expanded dimensions while residual computation flows through narrow\nbottlenecks. This inversion leverages higher-dimensional spaces for incremental\nrefinement while maintaining computational efficiency through parameter-matched\ndesigns. Implementing Hourglass MLPs requires an initial projection to lift\ninput signals to expanded dimensions. We propose that this projection can\nremain fixed at random initialization throughout training, enabling efficient\ntraining and inference implementations. We evaluate both architectures on\ngenerative tasks over popular image datasets, characterizing\nperformance-parameter Pareto frontiers through systematic architectural search.\nResults show that Hourglass architectures consistently achieve superior Pareto\nfrontiers compared to conventional designs. As parameter budgets increase,\noptimal Hourglass configurations favor deeper networks with wider skip\nconnections and narrower bottlenecks-a scaling pattern distinct from\nconventional MLPs. Our findings suggest reconsidering skip connection placement\nin modern architectures, with potential applications extending to Transformers\nand other residual networks.", "AI": {"tldr": "Hourglass MLPs invert the standard skip-connection pattern by placing skip connections in expanded dimensions while bottleneck residuals operate in narrow spaces, enabling refinement in high-dimensional spaces with fixed random input projections. They outperform conventional MLP designs on image-generative tasks, with deeper, wider-skipped Hourglass configurations favored as parameter budgets grow.", "motivation": "Challenge the traditional narrow-wide-narrow MLP layout to exploit high-dimensional spaces for incremental refinement while maintaining efficiency; explore how skip-connection placement affects learning dynamics and Pareto efficiency; propose a design with potential applicability to other residual architectures such as Transformers.", "method": "Introduce Hourglass MLP blocks with skip connections at expanded widths and residual paths through narrow bottlenecks. Start with a fixed random projection lifting inputs to expanded dimensions, enabling efficient training and inference. Evaluate on generative image tasks across datasets, performing systematic architectural search to map performance-parameter Pareto frontiers and compare to conventional MLP designs.", "result": "Hourglass MLPs consistently achieve superior Pareto frontiers over conventional designs. With larger parameter budgets, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks, revealing a distinct scaling pattern from traditional MLPs. The approach suggests notable efficiency gains and broader applicability beyond MLPs (e.g., Transformers and other residual architectures).", "conclusion": "Skip-connection placement materially affects performance and efficiency. Hourglass (wide-narrow-wide) designs warrant reconsideration in modern architectures, offering a path to improved Pareto efficiency and potential cross-domain applicability."}}
{"id": "2510.01817", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01817", "abs": "https://arxiv.org/abs/2510.01817", "authors": ["Adam Filipek"], "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction", "comment": "18 pages, 6 figures, small-scale experiments", "summary": "The Transformer architecture, underpinned by the Multi-Head Attention (MHA)\nmechanism, has become the de facto standard for state-of-the-art models in\nartificial intelligence. However, the quadratic computational complexity of MHA\nwith respect to sequence length presents a significant barrier to scaling,\nparticularly for applications involving long contexts. Prevailing solutions,\nsuch as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have\neffectively addressed the memory bandwidth bottleneck that dominates\nautoregressive inference latency by sharing Key and Value projections. While\nhighly successful, these methods do not reduce the fundamental number of\nfloating-point operations (FLOPs) required for the attention score computation,\nwhich remains a critical bottleneck for training and full-sequence processing.\nThis paper introduces Sparse Query Attention (SQA), a novel attention\narchitecture that pursues an alternative and complementary optimization path.\nInstead of reducing Key/Value heads, SQA reduces the number of Query heads.\nThis architectural modification directly decreases the computational complexity\nof the attention mechanism by a factor proportional to the reduction in query\nheads, thereby lowering the overall FLOPs. This work presents the theoretical\nfoundation of SQA, its mathematical formulation, and a family of architectural\nvariants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate\nthat SQA can achieve significant throughput improvements of up to 3x in\ncomputation-bound scenarios such as model pre-training, fine-tuning, and\nencoder-based tasks, with only a minimal impact on model quality in preliminary\nsmallscale experiments. SQA was discovered serendipitously during the\ndevelopment of the upcoming Reactive Transformer architecture, suggesting its\npotential as a powerful tool for building more efficient and scalable models", "AI": {"tldr": "Sparse Query Attention (SQA) reduces Transformer FLOPs by cutting the number of Query heads, yielding up to 3x throughput improvements on long sequences with minimal quality loss, and complements existing MHA optimizations (MQA/GQA).", "motivation": "The standard Multi-Head Attention (MHA) has quadratic complexity in sequence length, creating a bottleneck for long-context modeling and training. While MQA and GQA alleviate memory bandwidth during autoregressive inference by sharing Key/Value projections, they do not reduce the core FLOPs for attention score computation.", "method": "Introduce Sparse Query Attention that reduces the number of Query heads rather than Key/Value heads. Provide theoretical foundation, a mathematical formulation, and a family of architectural variants. Assess performance on very long sequences (32k\u2013200k tokens).", "result": "Achieves significant throughput improvements of up to 3x in computation-bound scenarios such as pre-training, fine-tuning, and encoder-based tasks, with only minimal degradation in model quality in preliminary small-scale experiments.", "conclusion": "SQA offers a complementary optimization path to existing attention efficiency methods, reducing compute by decreasing query-head count. Discovered during Reactive Transformer development, it has potential to enable more efficient and scalable transformers and warrants further exploration."}}
{"id": "2510.01824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01824", "abs": "https://arxiv.org/abs/2510.01824", "authors": ["Olivier Goudet", "Quentin Suire", "Adrien Go\u00ebffon", "Fr\u00e9d\u00e9ric Saubion", "Sylvain Lamprier"], "title": "Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning", "comment": null, "summary": "We introduce an order-invariant reinforcement learning framework for\nblack-box combinatorial optimization. Classical estimation-of-distribution\nalgorithms (EDAs) often rely on learning explicit variable dependency graphs,\nwhich can be costly and fail to capture complex interactions efficiently. In\ncontrast, we parameterize a multivariate autoregressive generative model\ntrained without a fixed variable ordering. By sampling random generation orders\nduring training - a form of information-preserving dropout - the model is\nencouraged to be invariant to variable order, promoting search-space diversity\nand shaping the model to focus on the most relevant variable dependencies,\nimproving sample efficiency. We adapt Generalized Reinforcement Policy\nOptimization (GRPO) to this setting, providing stable policy-gradient updates\nfrom scale-invariant advantages. Across a wide range of benchmark algorithms\nand problem instances of varying sizes, our method frequently achieves the best\nperformance and consistently avoids catastrophic failures.", "AI": {"tldr": "An order-invariant, multivariate autoregressive model for black-box combinatorial optimization is trained with random generation orders (information-preserving dropout) to promote order-invariance and diversity; it uses GRPO for stable, scale-invariant policy gradients, achieving strong performance and robustness across benchmarks.", "motivation": "EDAs rely on explicit dependency graphs, which are costly and may fail to capture complex interactions. There is a need for an approach that is invariant to variable ordering, scalable, and sample-efficient for large-scale combinatorial optimization.", "method": "Train a multivariate autoregressive generative model without a fixed order by sampling random generation orders during training (order randomization). This acts as information-preserving dropout. Adapt Generalized Reinforcement Policy Optimization (GRPO) to provide stable policy-gradient updates using scale-invariant advantages.", "result": "The method frequently achieves the best performance across a wide range of benchmark algorithms and problem instances of varying sizes and consistently avoids catastrophic failures.", "conclusion": "Order-invariant RL with order randomization improves search-space diversity, focuses on relevant variable dependencies, and yields robust, sample-efficient optimization performance compared to traditional EDAs."}}
{"id": "2510.01842", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01842", "abs": "https://arxiv.org/abs/2510.01842", "authors": ["Yannis Belkhiter", "Seshu Tirupathi", "Giulio Zizzo", "Sachin Sharma", "John D. Kelleher"], "title": "Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets", "comment": "Oral Presentations ADAPT Annual Scientific Conference 2025", "summary": "The field of AutoML has made remarkable progress in post-hoc model selection,\nwith libraries capable of automatically identifying the most performing models\nfor a given dataset. Nevertheless, these methods often rely on exhaustive\nhyperparameter searches, where methods automatically train and test different\ntypes of models on the target dataset. Contrastingly, pre-hoc prediction\nemerges as a promising alternative, capable of bypassing exhaustive search\nthrough intelligent pre-selection of models. Despite its potential, pre-hoc\nprediction remains under-explored in the literature. This paper explores the\nintersection of AutoML and pre-hoc model selection by leveraging traditional\nmodels and Large Language Model (LLM) agents to reduce the search space of\nAutoML libraries. By relying on dataset descriptions and statistical\ninformation, we reduce the AutoML search space. Our methodology is applied to\nthe AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark\ncontaining 175 tabular classification datasets available on OpenML. The\nproposed approach offers a shift in AutoML workflows, significantly reducing\ncomputational overhead, while still selecting the best model for the given\ndataset.", "AI": {"tldr": "A pre-hoc AutoML approach using traditional models and LLM agents to intelligently pre-select models, reducing the search space and computational cost while still identifying the best model for each dataset.", "motivation": "AutoML often relies on exhaustive post-hoc hyperparameter searches, which are computationally expensive. A pre-hoc prediction strategy can cut search space by leveraging dataset descriptions and statistics, potentially via LLMs, to choose a smaller set of candidate models before training.", "method": "Combine traditional machine learning models with LLM-driven agents to interpret dataset descriptions and statistical metadata, generating a reduced candidate model subset. Apply this pre-selection to the AWS AutoGluon portfolio (175 tabular classification datasets from OpenML), then evaluate whether the selected subset can still yield the best-performing model with significantly less computation.", "result": "The approach significantly reduces computational overhead associated with AutoML searches while still selecting the best model for each dataset, demonstrating the viability of pre-hoc model selection for AutoML workflows.", "conclusion": "Pre-hoc predictions using traditional models and LLM-guided guidance can effectively prune the AutoML search space without sacrificing model quality. This shift can lead to faster AutoML workflows and lower resource usage, with potential for broader integration into AutoML pipelines."}}
{"id": "2510.01853", "categories": ["cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.01853", "abs": "https://arxiv.org/abs/2510.01853", "authors": ["Vladimir Krsmanovic", "Matthias Cosler", "Mohamed Ghanem", "Bernd Finkbeiner"], "title": "Learning Representations Through Contrastive Neural Model Checking", "comment": null, "summary": "Model checking is a key technique for verifying safety-critical systems\nagainst formal specifications, where recent applications of deep learning have\nshown promise. However, while ubiquitous for vision and language domains,\nrepresentation learning remains underexplored in formal verification. We\nintroduce Contrastive Neural Model Checking (CNML), a novel method that\nleverages the model checking task as a guiding signal for learning aligned\nrepresentations. CNML jointly embeds logical specifications and systems into a\nshared latent space through a self-supervised contrastive objective. On\nindustry-inspired retrieval tasks, CNML considerably outperforms both\nalgorithmic and neural baselines in cross-modal and intra-modal settings.We\nfurther show that the learned representations effectively transfer to\ndownstream tasks and generalize to more complex formulas. These findings\ndemonstrate that model checking can serve as an objective for learning\nrepresentations for formal languages.", "AI": {"tldr": "CNML uses model checking as supervision to learn aligned representations of systems and specifications via a self-supervised contrastive objective, improving retrieval tasks and transferring to downstream tasks.", "motivation": "Formal verification benefits from learned representations, but representation learning is underexplored in this domain. The paper aims to connect model checking with representation learning to enable better cross-modal understanding between specifications and systems.", "method": "Jointly embed logical specifications and system models into a shared latent space using a self-supervised contrastive objective, optimizing to align correct spec-system pairs while distinguishing negatives; evaluated on industry-inspired retrieval tasks in cross-modal and intra-modal settings.", "result": "CNML outperforms both algorithmic and neural baselines on industry-inspired retrieval tasks in cross-modal and intra-modal settings; learned representations transfer to downstream tasks and generalize to more complex formulas.", "conclusion": "Model checking can serve as an effective objective for learning representations for formal languages, enabling better alignment between specifications and systems and benefiting downstream tasks."}}
{"id": "2510.02291", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02291", "abs": "https://arxiv.org/abs/2510.02291", "authors": ["Litu Rout", "Andreas Lugmayr", "Yasamin Jafarian", "Srivatsan Varadharajan", "Constantine Caramanis", "Sanjay Shakkottai", "Ira Kemelmacher-Shlizerman"], "title": "Test-Time Anchoring for Discrete Diffusion Posterior Sampling", "comment": "Preprint", "summary": "We study the problem of posterior sampling using pretrained discrete\ndiffusion foundation models, aiming to recover images from noisy measurements\nwithout retraining task-specific models. While diffusion models have achieved\nremarkable success in generative modeling, most advances rely on continuous\nGaussian diffusion. In contrast, discrete diffusion offers a unified framework\nfor jointly modeling categorical data such as text and images. Beyond\nunification, discrete diffusion provides faster inference, finer control, and\nprincipled training-free Bayesian inference, making it particularly well-suited\nfor posterior sampling. However, existing approaches to discrete diffusion\nposterior sampling face severe challenges: derivative-free guidance yields\nsparse signals, continuous relaxations limit applicability, and split Gibbs\nsamplers suffer from the curse of dimensionality. To overcome these\nlimitations, we introduce Anchored Posterior Sampling (APS) for masked\ndiffusion foundation models, built on two key innovations -- quantized\nexpectation for gradient-like guidance in discrete embedding space, and\nanchored remasking for adaptive decoding. Our approach achieves\nstate-of-the-art performance among discrete diffusion samplers across linear\nand nonlinear inverse problems on the standard benchmarks. We further\ndemonstrate the benefits of our approach in training-free stylization and\ntext-guided editing.", "AI": {"tldr": "Anchored Posterior Sampling (APS) for masked discrete diffusion models enables training-free posterior sampling for image recovery from noisy measurements, achieving state-of-the-art performance among discrete diffusion samplers and enabling training-free stylization and text-guided editing.", "motivation": "Discrete diffusion offers a unified, faster, and training-free Bayesian framework for discrete data; however existing posterior samplers face derivative-free guidance sparsity, continuous relaxations limits, and high-dimensional sampling challenges.", "method": "Two innovations: (1) quantized expectation to provide gradient-like guidance directly in discrete embedding space; (2) anchored remasking to adaptively decode and stabilize sampling. Built on masked diffusion foundation models to enable posterior sampling without retraining.", "result": "Empirically achieves state-of-the-art performance among discrete diffusion samplers on linear and nonlinear inverse problems across standard benchmarks; demonstrates utility in training-free stylization and text-guided editing.", "conclusion": "APS advances training-free posterior sampling for discrete diffusion models, enabling effective image recovery and downstream tasks without task-specific retraining, with broad applicability."}}
{"id": "2510.01855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01855", "abs": "https://arxiv.org/abs/2510.01855", "authors": ["Lexiang Hu", "Yikang Li", "Zhouchen Lin"], "title": "Explicit Discovery of Nonlinear Symmetries from Dynamic Data", "comment": null, "summary": "Symmetry is widely applied in problems such as the design of equivariant\nnetworks and the discovery of governing equations, but in complex scenarios, it\nis not known in advance. Most previous symmetry discovery methods are limited\nto linear symmetries, and recent attempts to discover nonlinear symmetries fail\nto explicitly get the Lie algebra subspace. In this paper, we propose LieNLSD,\nwhich is, to our knowledge, the first method capable of determining the number\nof infinitesimal generators with nonlinear terms and their explicit\nexpressions. We specify a function library for the infinitesimal group action\nand aim to solve for its coefficient matrix, proving that its prolongation\nformula for differential equations, which governs dynamic data, is also linear\nwith respect to the coefficient matrix. By substituting the central differences\nof the data and the Jacobian matrix of the trained neural network into the\ninfinitesimal criterion, we get a system of linear equations for the\ncoefficient matrix, which can then be solved using SVD. On top quark tagging\nand a series of dynamic systems, LieNLSD shows qualitative advantages over\nexisting methods and improves the long rollout accuracy of neural PDE solvers\nby over 20% while applying to guide data augmentation. Code and data are\navailable at https://github.com/hulx2002/LieNLSD.", "AI": {"tldr": "LieNLSD is the first method to identify nonlinear Lie symmetry generators (count and explicit expressions) for differential equations by framing the prolongation criterion as a linear system in a coefficient matrix and solving via SVD, enabling explicit nonlinear symmetry discovery and improved neural PDE modeling.", "motivation": "Symmetry is central to equivariant networks and the discovery of governing equations, but existing methods largely handle only linear symmetries and fail to provide the Lie algebra subspace, hindering nonlinear symmetry discovery.", "method": "Specify a library of infinitesimal group actions (functions). Use the prolonged infinitesimal criterion, which is linear in the coefficient matrix. Build a linear system from data (central differences and the Jacobian of a trained NN) and solve it with SVD to recover the coefficient matrix, the number of generators, and their explicit expressions.", "result": "LieNLSD can determine the number of infinitesimal generators with nonlinear terms and their expressions. It shows qualitative advantages over existing methods on tasks like top quark tagging and various dynamic systems, and improves the long rollout accuracy of neural PDE solvers by over 20%; it also supports guiding data augmentation. Code and data are publicly available.", "conclusion": "LieNLSD provides a first-principles, scalable framework to discover nonlinear Lie algebra generators and their explicit forms, enabling more expressive symmetry discovery and aiding symmetry-aware learning and physics-guided modeling."}}
{"id": "2510.02296", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02296", "abs": "https://arxiv.org/abs/2510.02296", "authors": ["Yu-Chien Liao", "Jr-Jen Chen", "Chi-Pin Huang", "Ci-Siang Lin", "Meng-Lin Wu", "Yu-Chiang Frank Wang"], "title": "Continual Personalization for Diffusion Models", "comment": null, "summary": "Updating diffusion models in an incremental setting would be practical in\nreal-world applications yet computationally challenging. We present a novel\nlearning strategy of Concept Neuron Selection (CNS), a simple yet effective\napproach to perform personalization in a continual learning scheme. CNS\nuniquely identifies neurons in diffusion models that are closely related to the\ntarget concepts. In order to mitigate catastrophic forgetting problems while\npreserving zero-shot text-to-image generation ability, CNS finetunes concept\nneurons in an incremental manner and jointly preserves knowledge learned of\nprevious concepts. Evaluation of real-world datasets demonstrates that CNS\nachieves state-of-the-art performance with minimal parameter adjustments,\noutperforming previous methods in both single and multi-concept personalization\nworks. CNS also achieves fusion-free operation, reducing memory storage and\nprocessing time for continual personalization.", "AI": {"tldr": "CNS identifies concept-related neurons in diffusion models to enable incremental personalization with minimal forgetting and fusion-free operation, achieving state-of-the-art results with fewer parameter updates.", "motivation": "Real-world incremental updates of diffusion models are desirable but computationally expensive; there is a need for methods to personalize models while mitigating catastrophic forgetting and preserving zero-shot generation.", "method": "Identify and fine-tune a small set of concept neurons in diffusion models that encode the target concepts, integrate them in an incremental continual-learning framework, and jointly preserve knowledge of previous concepts, all while avoiding fusion-based memory expansions.", "result": "On real-world datasets, CNS achieves state-of-the-art performance for single- and multi-concept personalization with minimal parameter updates; reduces memory storage and processing time; preserves zero-shot text-to-image generation.", "conclusion": "CNS provides an effective, efficient continual-learning approach for diffusion-model personalization via targeted neuron selection, balancing plasticity and stability with low memory overhead and fusion-free operation."}}
{"id": "2510.01858", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.01858", "abs": "https://arxiv.org/abs/2510.01858", "authors": ["Jacob J. W. Bakermans", "Pablo Tano", "Reidar Riveland", "Charles Findling", "Alexandre Pouget"], "title": "Compositional meta-learning through probabilistic task inference", "comment": null, "summary": "To solve a new task from minimal experience, it is essential to effectively\nreuse knowledge from previous tasks, a problem known as meta-learning.\nCompositional solutions, where common elements of computation are flexibly\nrecombined into new configurations, are particularly well-suited for\nmeta-learning. Here, we propose a compositional meta-learning model that\nexplicitly represents tasks as structured combinations of reusable\ncomputations. We achieve this by learning a generative model that captures the\nunderlying components and their statistics shared across a family of tasks.\nThis approach transforms learning a new task into a probabilistic inference\nproblem, which allows for finding solutions without parameter updates through\nhighly constrained hypothesis testing. Our model successfully recovers ground\ntruth components and statistics in rule learning and motor learning tasks. We\nthen demonstrate its ability to quickly infer new solutions from just single\nexamples. Together, our framework joins the expressivity of neural networks\nwith the data-efficiency of probabilistic inference to achieve rapid\ncompositional meta-learning.", "AI": {"tldr": "A compositional meta-learning model that learns a generative model of reusable components across tasks, enabling rapid, update-free inference via probabilistic reasoning; it recovers components in rule and motor tasks and demonstrates one-shot adaptation.", "motivation": "To achieve data-efficient meta-learning by reusing compositional components across tasks, addressing the need to quickly adapt to new tasks with minimal experience.", "method": "Develop a generative model that captures underlying components and their statistics shared across a family of tasks; represent tasks as structured combinations of these components; cast learning as probabilistic inference, enabling solution search through highly constrained hypothesis testing without parameter updates.", "result": "Ground-truth components and statistics recovered in rule learning and motor learning tasks; demonstrates rapid inference of new solutions from single examples.", "conclusion": "Combines neural network expressivity with probabilistic inference for rapid compositional meta-learning and data-efficient adaptation."}}
{"id": "2510.02300", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02300", "abs": "https://arxiv.org/abs/2510.02300", "authors": ["Runqian Wang", "Yilun Du"], "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models", "comment": null, "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.", "AI": {"tldr": "EqM is a new generative framework that learns an equilibrium energy landscape and uses optimization-based sampling instead of diffusion-like time dynamics; it claims strong generation performance (FID 1.90 on ImageNet 256x256) and theoretical grounding for sampling from the data manifold, with applications to denoising, OOD detection, and image composition.", "motivation": "To overcome limitations of non-equilibrium, time-conditional dynamics in diffusion/flow models, by learning a unified equilibrium landscape that supports optimization-based inference and better bridges flow-based and energy-based models.", "method": "Train a model to learn the gradient of an implicit energy landscape representing the data distribution. At inference, perform gradient-based optimization (gradient descent with adaptive steps/optimizers) on the learned landscape to generate samples. This replaces time-conditional velocity dynamics with an equilibrium dynamics perspective.", "result": "Empirically surpasses diffusion/flow models, achieving a Fr\u00e9chet Inception Distance (FID) of 1.90 on ImageNet 256\u00d7256. The framework is theoretically justified to sample from the data manifold and is versatile across tasks such as denoising, OOD detection, and image composition.", "conclusion": "EqM offers a tighter bridge between flow and energy-based models, providing a simple route to optimization-driven inference and broad applicability beyond pure generation; it reframes generative modeling around equilibrium dynamics rather than non-equilibrium diffusion processes."}}
{"id": "2510.01867", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01867", "abs": "https://arxiv.org/abs/2510.01867", "authors": ["Subhamon Supantha", "Abhishek Sinha"], "title": "Universal Dynamic Regret and Constraint Violation Bounds for Constrained Online Convex Optimization", "comment": null, "summary": "We consider a generalization of the celebrated Online Convex Optimization\n(OCO) framework with online adversarial constraints. We present two algorithms\nhaving simple modular structures that yield universal dynamic regret and\ncumulative constraint violation bounds, improving upon the state-of-the-art\nresults. Our results hold in the most general case when both the cost and\nconstraint functions are chosen arbitrarily by an adversary, and the constraint\nfunctions need not contain any common feasible point. The results are\nestablished by reducing the constrained learning problem to an instance of the\nstandard OCO problem with specially constructed surrogate cost functions.", "AI": {"tldr": "Generalizes Online Convex Optimization to adversarial costs and constraints, introduces two modular algorithms with universal dynamic regret and cumulative constraint violation bounds; reduces constrained learning to standard OCO via surrogate costs.", "motivation": "Address online learning under adversarial cost and constraint functions without requiring a common feasible point, aiming for robust, universal guarantees in the most general setting.", "method": "Proposes two simple modular algorithms. Reformulates the constrained problem as a standard OCO instance by constructing surrogate cost functions that encode constraint violations, enabling analysis of dynamic regret and constraint violation.", "result": "Achieves universal dynamic regret bounds and cumulative constraint violation bounds in the fully adversarial setting, improving state-of-the-art results; applicable even when cost and constraint functions are chosen adversarially and there is no common feasible point.", "conclusion": "A simple reduction to standard OCO plus modular algorithms yields robust, universal guarantees for constrained online learning in the most general adversarial setting, broadening applicability and simplifying design."}}
{"id": "2510.01878", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01878", "abs": "https://arxiv.org/abs/2510.01878", "authors": ["Sahar Rajabi", "Nayeema Nonta", "Samanvay Vajpayee", "Sirisha Rambhatla"], "title": "Randomized Gradient Subspaces for Efficient Large Language Model Training", "comment": null, "summary": "Training large language models (LLMs) is often bottlenecked by extreme memory\ndemands, with optimizer states dominating the footprint. Recent works mitigates\nthis cost by projecting gradients into low-dimensional subspaces using\nsophisticated update strategies. In this paper, we analyze the dynamics of\ngradient space and its underlying subspaces. We find that while a small\nsubspace captures most gradient energy, a significant portion still resides in\nthe residual bulk; moreover, the influence of the core subspace diminishes over\ntime and in deeper layers. We also observe that the gradient space exhibits\nnear-flat curvature, calling for algorithms that explicitly account for this\ngeometry. Motivated by these insights, we introduce a suite of randomized\nalgorithms, GrassWalk and GrassJump, which exploit subspace and achieve\nstate-of-the-art memory savings while improving performance on LLaMA-1B and\nLLaMA-7B pretraining.", "AI": {"tldr": "We analyze gradient subspaces in large language model training, showing most gradient energy lies in a small core subspace while a substantial residual remains; gradient-space curvature is near-flat. We propose randomized algorithms GrassWalk and GrassJump that exploit subspace structure, achieving state-of-the-art memory savings and improved pretraining performance on LLaMA-1B/7B.", "motivation": "LLM training is memory-bound due to optimizer states; existing gradient-projection methods reduce memory but do not fully leverage gradient-space geometry. Understanding gradient-subspace dynamics and curvature can guide more effective memory-saving strategies.", "method": "Empirically analyze the gradient space dynamics across layers/time, identify energy distribution between a core subspace and residual bulk, observe curvature properties, and design two randomized algorithms (GrassWalk and GrassJump) that exploit subspace structure to reduce memory footprint while maintaining or improving optimization performance; validate on LLaMA-1B and LLaMA-7B pretraining.", "result": "The proposed GrassWalk and GrassJump achieve state-of-the-art memory savings with improved training performance compared to baselines during LLaMA-1B and LLaMA-7B pretraining.", "conclusion": "Gradient space in LLM training exhibits a small energy-rich core and a sizable residual, with near-flat curvature. Algorithms that explicitly account for this geometry\u2014GrassWalk and GrassJump\u2014can deliver substantial memory reductions and competitive or improved training outcomes on large-scale models."}}
{"id": "2510.01894", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01894", "abs": "https://arxiv.org/abs/2510.01894", "authors": ["Thomas Gravier", "Thomas Boyer", "Auguste Genovesio"], "title": "Multi-marginal temporal Schr\u00f6dinger Bridge Matching for video generation from unpaired data", "comment": "Under review. Code available at\n  https://github.com/tgravier/MMDSBM-pytorch . Additional experiment materials\n  available at https://mmdsbm.notion.site", "summary": "Many natural dynamic processes -- such as in vivo cellular differentiation or\ndisease progression -- can only be observed through the lens of static sample\nsnapshots. While challenging, reconstructing their temporal evolution to\ndecipher underlying dynamic properties is of major interest to scientific\nresearch. Existing approaches enable data transport along a temporal axis but\nare poorly scalable in high dimension and require restrictive assumptions to be\nmet. To address these issues, we propose \\textit{\\textbf{Multi-Marginal\ntemporal Schr\\\"odinger Bridge Matching}} (\\textbf{MMtSBM}) \\textit{for video\ngeneration from unpaired data}, extending the theoretical guarantees and\nempirical efficiency of Diffusion Schr\\\"odinger Bridge Matching\n(arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting\nalgorithm to multiple marginals in a novel factorized fashion. Experiments show\nthat MMtSBM retains theoretical properties on toy examples, achieves\nstate-of-the-art performance on real world datasets such as transcriptomic\ntrajectory inference in 100 dimensions, and for the first time recovers\ncouplings and dynamics in very high dimensional image settings. Our work\nestablishes multi-marginal Schr\\\"odinger bridges as a practical and principled\napproach for recovering hidden dynamics from static data.", "AI": {"tldr": "Multi-Marginal temporal Schr\u00f6dinger Bridge Matching (MMtSBM) extends diffusion Schr\u00f6dinger Bridge Matching to multiple marginals for video generation from unpaired data, offering scalable, high-dimensional recovery of hidden dynamics with Iterative Markovian Fitting; achieves state-of-the-art on transcriptomic trajectories and high-dimensional images.", "motivation": "Many dynamic processes are only observable as static snapshots; reconstructing temporal evolution is crucial but current transport methods are not scalable and rely on strong assumptions.", "method": "Introduce MMtSBM, derive Iterative Markovian Fitting for multiple marginals in a factorized framework, extending Diffusion Schr\u00f6dinger Bridge Matching; provide theoretical guarantees and empirical efficiency.", "result": "Maintains theoretical properties on toy examples; achieves state-of-the-art on real-world 100-dimensional transcriptomic trajectory inference; recovers couplings and dynamics in high-dimensional images.", "conclusion": "Multi-marginal Schr\u00f6dinger bridges are a practical, principled approach to recover hidden dynamics from static data, enabling scalable dynamic inference in high dimensions."}}
{"id": "2510.01899", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.01899", "abs": "https://arxiv.org/abs/2510.01899", "authors": ["Md Talha Mohsin", "Ismail Abdulrashid"], "title": "Multimodal Foundation Models for Early Disease Detection", "comment": "6 pages", "summary": "Healthcare generates diverse streams of data, including electronic health\nrecords (EHR), medical imaging, genetics, and ongoing monitoring from wearable\ndevices. Traditional diagnostic models frequently analyze these sources in\nisolation, which constrains their capacity to identify cross-modal correlations\nessential for early disease diagnosis. Our research presents a multimodal\nfoundation model that consolidates diverse patient data through an\nattention-based transformer framework. At first, dedicated encoders put each\nmodality into a shared latent space. Then, they combine them using multi-head\nattention and residual normalization. The architecture is made for pretraining\non many tasks, which makes it easy to adapt to new diseases and datasets with\nlittle extra work. We provide an experimental strategy that uses benchmark\ndatasets in oncology, cardiology, and neurology, with the goal of testing early\ndetection tasks. The framework includes data governance and model management\ntools in addition to technological performance to improve transparency,\nreliability, and clinical interpretability. The suggested method works toward a\nsingle foundation model for precision diagnostics, which could improve the\naccuracy of predictions and help doctors make decisions.", "AI": {"tldr": "A multimodal transformer-based foundation model fuses diverse healthcare data (EHR, imaging, genetics, wearables) via modality-specific encoders into a shared latent space with multi-head attention, pretrained on many tasks; aims for early disease detection with governance tools and cross-dataset adaptability.", "motivation": "Traditional models analyze data in silos, missing cross-modal correlations essential for early diagnosis; need a unified, adaptable foundation model for healthcare data.", "method": "Per-modality encoders map inputs into a shared latent space; multi-head attention with residual normalization fuses modalities; pretraining on multiple tasks; architecture supports data governance and model management tools.", "result": "Proposed experimental strategy using benchmark datasets across oncology, cardiology, and neurology to test early-detection tasks; no quantitative results reported in the abstract.", "conclusion": "Advances toward a single foundation model for precision diagnostics, with potential accuracy gains and improved clinical decision support; emphasizes transparency, reliability, and interpretability through governance and management tools."}}
{"id": "2510.01906", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01906", "abs": "https://arxiv.org/abs/2510.01906", "authors": ["Mayur Kishor Shende", "Ole-Christoffer Granmo", "Runar Helin", "Vladimir I. Zadorozhny", "Rishad Shafik"], "title": "A Methodology for Transparent Logic-Based Classification Using a Multi-Task Convolutional Tsetlin Machine", "comment": null, "summary": "The Tsetlin Machine (TM) is a novel machine learning paradigm that employs\nfinite-state automata for learning and utilizes propositional logic to\nrepresent patterns. Due to its simplistic approach, TMs are inherently more\ninterpretable than learning algorithms based on Neural Networks. The\nConvolutional TM has shown comparable performance on various datasets such as\nMNIST, K-MNIST, F-MNIST and CIFAR-2. In this paper, we explore the\napplicability of the TM architecture for large-scale multi-channel (RGB) image\nclassification. We propose a methodology to generate both local interpretations\nand global class representations. The local interpretations can be used to\nexplain the model predictions while the global class representations aggregate\nimportant patterns for each class. These interpretations summarize the\nknowledge captured by the convolutional clauses, which can be visualized as\nimages. We evaluate our methods on MNIST and CelebA datasets, using models that\nachieve 98.5\\% accuracy on MNIST and 86.56\\% F1-score on CelebA (compared to\n88.07\\% for ResNet50) respectively. We show that the TM performs competitively\nto this deep learning model while maintaining its interpretability, even in\nlarge-scale complex training environments. This contributes to a better\nunderstanding of TM clauses and provides insights into how these models can be\napplied to more complex and diverse datasets.", "AI": {"tldr": "CTM extended to RGB images achieves competitive performance on MNIST and CelebA while offering interpretable clause-based representations.", "motivation": "To combine the interpretability of Tsetlin Machines with the handling of large-scale, multi-channel image data, addressing the gap between model transparency and competitive accuracy in vision tasks.", "method": "Introduce a convolutional TM framework for multi-channel RGB inputs and propose a method to generate both local (per-region) interpretations and global class representations. Interpretations are distilled into visualizable patterns corresponding to learned convolutional clauses and used to explain predictions.", "result": "On MNIST and CelebA, the CTM achieves 98.5% accuracy on MNIST and 86.56% F1-score on CelebA, with comparisons to ResNet50 showing competitive performance while maintaining interpretability even in large-scale training environments.", "conclusion": "The work demonstrates that TM-based models can be scaled to complex vision tasks and produce human-interpretable knowledge representations, offering insights into clause behavior and potential applicability to diverse datasets."}}
{"id": "2510.01910", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01910", "abs": "https://arxiv.org/abs/2510.01910", "authors": ["Zhaoyan Wang", "Zheng Gao", "Arogya Kharel", "In-Young Ko"], "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement", "comment": "14 pages", "summary": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications,\nserving as a core technique for learning from graph-structured data, such as\ntext-attributed graphs. Yet in real-world scenarios, such graphs exhibit\ndeficiencies that substantially undermine GNN performance. While prior\nGNN-based augmentation studies have explored robustness against individual\nimperfections, a systematic understanding of how graph-native and Large\nLanguage Models (LLMs) enhanced methods behave under compound deficiencies is\nstill missing. Specifically, there has been no comprehensive investigation\ncomparing conventional approaches and recent LLM-on-graph frameworks, leaving\ntheir merits unclear. To fill this gap, we conduct the first empirical study\nthat benchmarks these two lines of methods across diverse graph deficiencies,\nrevealing overlooked vulnerabilities and challenging the assumption that LLM\naugmentation is consistently superior. Building on empirical findings, we\npropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement\n(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is\nthe first iterative paradigm that leverages Retrieval-Augmented Generation\n(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,\ndiverse augmentations and enforcing discriminative representations through\niterative graph contrastive learning. It transforms LLM augmentation for graphs\nfrom static signal injection into dynamic refinement. Extensive experiments\ndemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced\nbaselines, achieving up to 82.43% average improvement.", "AI": {"tldr": "This paper benchmarks conventional GNN augmentation methods against LLM-on-graph approaches under diverse graph deficiencies, finds that LLM-based methods are not always superior, and introduces RoGRAD, an iterative retrieval-augmented contrastive refinement framework using RAG to provide diverse, class-consistent augmentations and reinforce discriminative representations, achieving up to 82.43% average improvement.", "motivation": "There is a gap in understanding how graph-native GNN methods and LLM-enhanced graph methods perform under compounded graph deficiencies; no comprehensive comparison exists; need robust learning methods that can handle defects. The work also challenges the assumption that LLM augmentation is always beneficial.", "method": "(1) An empirical benchmark comparing conventional GNN-augmentation methods and LLM-on-graph frameworks across diverse graph deficiencies. (2) Introduction of RoGRAD (Robust Graph Learning via Retrieval-Augmented Contrastive Refinement): an iterative framework that uses Retrieval-Augmented Generation (RAG) to inject retrieval-grounded augmentations (class-consistent, diverse) and enforces discriminative representations through iterative graph contrastive learning, turning LLM augmentation into dynamic refinement.", "result": "Extensive experiments show RoGRAD outperforms both conventional GNN-based and LLM-enhanced baselines, achieving up to 82.43% average improvement.", "conclusion": "RoGRAD demonstrates that retrieval-grounded, iterative refinement is effective for robust graph learning and can surpass prior one-shot LLM-augmentation approaches; highlights the importance of dynamic, retrieval-informed augmentation and contrastive refinement for handling graph deficiencies."}}
{"id": "2510.01938", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01938", "abs": "https://arxiv.org/abs/2510.01938", "authors": ["Zhizhong Li", "Sina Sajadmanesh", "Jingtao Li", "Lingjuan Lyu"], "title": "StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold", "comment": "Accepted as a spotlight at NeurIPS 2025", "summary": "Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient\ntechnique for fine-tuning large-scale pre-trained models. However, it still\nlags behind full fine-tuning in performance, partly due to its insufficient\nexploitation of the geometric structure underlying low-rank manifolds. In this\npaper, we propose a geometry-aware extension of LoRA that uses a three-factor\ndecomposition $U\\!SV^\\top$. Analogous to the structure of singular value\ndecomposition (SVD), it separates the adapter's input and output subspaces, $V$\nand $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie\non the Stiefel manifold, ensuring their orthonormality throughout the training.\nTo optimize on the Stiefel manifold, we employ a flexible and modular geometric\noptimization design that converts any Euclidean optimizer to a Riemannian one.\nIt enables efficient subspace learning while remaining compatible with existing\nfine-tuning pipelines. Empirical results across a wide range of downstream\ntasks, including commonsense reasoning, math and code generation, image\nclassification, and image generation, demonstrate the superior performance of\nour approach against the recent state-of-the-art variants of LoRA. Code is\navailable at https://github.com/SonyResearch/stella.", "AI": {"tldr": "A geometry-aware extension of LoRA (three-factor U S V^T) with orthonormal U,V on the Stiefel manifold and a Riemannian optimization framework, yielding improved performance across diverse tasks while remaining compatible with existing fine-tuning pipelines; code available.", "motivation": "LoRA is parameter-efficient but underperforms full fine-tuning due to not exploiting the geometric structure of low-rank manifolds. The paper proposes a geometry-aware extension to better exploit subspace geometry.", "method": "A three-factor decomposition U S V^T where U and V are constrained to the Stiefel manifold (orthonormal subspaces) and S is a scaling factor. The optimization converts any Euclidean optimizer into a Riemannian one to optimize on the Stiefel manifold, enabling efficient subspace learning while staying compatible with existing fine-tuning pipelines.", "result": "Empirical results across diverse tasks\u2014commonsense reasoning, math and code generation, image classification, and image generation\u2014show the geometry-aware LoRA outperforms recent state-of-the-art LoRA variants. Code is available at the provided GitHub repository.", "conclusion": "Geometry-aware LoRA improves performance by better exploiting the geometry of low-rank manifolds through orthonormal subspace learning and Riemannian optimization, while remaining compatible with standard fine-tuning workflows across modalities; promising for broad applicability."}}
{"id": "2510.01969", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01969", "abs": "https://arxiv.org/abs/2510.01969", "authors": ["Camilo Andr\u00e9s Garc\u00eda Trillos", "Nicol\u00e1s Garc\u00eda Trillos"], "title": "Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions", "comment": null, "summary": "We consider adversarially robust classification in a multiclass setting under\narbitrary loss functions and derive dual and barycentric reformulations of the\ncorresponding learner-agnostic robust risk minimization problem. We provide\nexplicit characterizations for important cases such as the cross-entropy loss,\nloss functions with a power form, and the quadratic loss, extending in this way\navailable results for the 0-1 loss. These reformulations enable efficient\ncomputation of sharp lower bounds for adversarial risks and facilitate the\ndesign of robust classifiers beyond the 0-1 loss setting. Our paper uncovers\ninteresting connections between adversarial robustness, $\\alpha$-fair packing\nproblems, and generalized barycenter problems for arbitrary positive measures\nwhere Kullback-Leibler and Tsallis entropies are used as penalties. Our\ntheoretical results are accompanied with illustrative numerical experiments\nwhere we obtain tighter lower bounds for adversarial risks with the\ncross-entropy loss function.", "AI": {"tldr": "Proposes dual and barycentric reformulations for learner-agnostic robust risk minimization in multiclass classification under arbitrary losses, with explicit results for cross-entropy, power-form, and quadratic losses; extends 0-1 loss results, enables tight lower bounds and robust classifier design, and links robustness to alpha-fair packing and entropy-regularized barycenter problems; numerical experiments show tighter lower bounds for cross-entropy.", "motivation": "To enable principled, computationally tractable analysis and design of adversarially robust classifiers in multiclass settings under general loss functions, and to extend robustness insights beyond the 0-1 loss setting.", "method": "Derives dual and barycentric reformulations of learner-agnostic robust risk minimization; provides explicit characterizations for cross-entropy, power-form, and quadratic losses; establishes connections to alpha-fair packing and generalized barycenter problems using Kullback-Leibler and Tsallis entropies as penalties; supports illustrative numerical experiments.", "result": "Yields reformulations that allow efficient computation of sharp lower bounds on adversarial risk; facilitates robust classifier design beyond 0-1 loss; reveals structural connections to entropy-regularized optimization; empirical evidence shows tighter lower bounds for cross-entropy.", "conclusion": "This work broadens robust multiclass learning to arbitrary losses through dual and barycentric reformulations, linking adversarial robustness with entropy-regularized optimization and alpha-fair concepts, and demonstrates practical benefits via tighter risk bounds and improved guidance for robust classifier design."}}
{"id": "2510.01970", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01970", "abs": "https://arxiv.org/abs/2510.01970", "authors": ["Yuanyuan Yao", "Yuhan Shi", "Lu Chen", "Ziquan Fang", "Yunjun Gao", "Leong Hou U", "Yushuai Li", "Tianyi Li"], "title": "Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection", "comment": null, "summary": "Multivariate time series (MTS) anomaly detection identifies abnormal patterns\nwhere each timestamp contains multiple variables. Existing MTS anomaly\ndetection methods fall into three categories: reconstruction-based,\nprediction-based, and classifier-based methods. However, these methods face two\nkey challenges: (1) Unsupervised learning methods, such as reconstruction-based\nand prediction-based methods, rely on error thresholds, which can lead to\ninaccuracies; (2) Semi-supervised methods mainly model normal data and often\nunderuse anomaly labels, limiting detection of subtle anomalies;(3) Supervised\nlearning methods, such as classifier-based approaches, often fail to capture\nlocal relationships, incur high computational costs, and are constrained by the\nscarcity of labeled data. To address these limitations, we propose Moon, a\nsupervised modality conversion-based multivariate time series anomaly detection\nframework. Moon enhances the efficiency and accuracy of anomaly detection while\nproviding detailed anomaly analysis reports. First, Moon introduces a novel\nmultivariate Markov Transition Field (MV-MTF) technique to convert numeric time\nseries data into image representations, capturing relationships across\nvariables and timestamps. Since numeric data retains unique patterns that\ncannot be fully captured by image conversion alone, Moon employs a\nMultimodal-CNN to integrate numeric and image data through a feature fusion\nmodel with parameter sharing, enhancing training efficiency. Finally, a\nSHAP-based anomaly explainer identifies key variables contributing to\nanomalies, improving interpretability. Extensive experiments on six real-world\nMTS datasets demonstrate that Moon outperforms six state-of-the-art methods by\nup to 93% in efficiency, 4% in accuracy and, 10.8% in interpretation\nperformance.", "AI": {"tldr": "Moon is a supervised, modality-conversion framework for MTS anomaly detection that converts numeric data to images via MV-MTF, fuses numeric and image features with a Multimodal-CNN, and uses SHAP for explanations, achieving improved efficiency, accuracy, and interpretability over baselines.", "motivation": "Existing MTS anomaly detection methods struggle with unsupervised error-threshold reliance (reconstruction/prediction), underuse of anomaly labels in semi-supervised settings, and high cost/limited local relationship modeling in supervised methods. A unified, efficient, and interpretable supervised approach that leverages both numeric and image representations is needed.", "method": "Introduce MV-MTF to convert multivariate time series into image representations capturing cross-variable and temporal relationships; employ a Multimodal-CNN with parameter sharing to fuse numeric features and image features; apply a SHAP-based anomaly explainer for interpretability; train under a supervised modality-conversion framework; evaluate on six real-world MTS datasets.", "result": "Moon outperforms six state-of-the-art methods by up to 93% in efficiency, 4% in accuracy, and 10.8% in interpretation performance across six real-world MTS datasets.", "conclusion": "The proposed Moon framework provides a more effective and interpretable approach to MTS anomaly detection by combining modality conversion, cross-data fusion, and explainable AI, addressing key limitations of existing methods."}}
{"id": "2510.01987", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01987", "abs": "https://arxiv.org/abs/2510.01987", "authors": ["Samuel Maddock", "Graham Cormode", "Carsten Maple"], "title": "Private Federated Multiclass Post-hoc Calibration", "comment": null, "summary": "Calibrating machine learning models so that predicted probabilities better\nreflect the true outcome frequencies is crucial for reliable decision-making\nacross many applications. In Federated Learning (FL), the goal is to train a\nglobal model on data which is distributed across multiple clients and cannot be\ncentralized due to privacy concerns. FL is applied in key areas such as\nhealthcare and finance where calibration is strongly required, yet federated\nprivate calibration has been largely overlooked. This work introduces the\nintegration of post-hoc model calibration techniques within FL. Specifically,\nwe transfer traditional centralized calibration methods such as histogram\nbinning and temperature scaling into federated environments and define new\nmethods to operate them under strong client heterogeneity. We study (1) a\nfederated setting and (2) a user-level Differential Privacy (DP) setting and\ndemonstrate how both federation and DP impacts calibration accuracy. We propose\nstrategies to mitigate degradation commonly observed under heterogeneity and\nour findings highlight that our federated temperature scaling works best for\nDP-FL whereas our weighted binning approach is best when DP is not required.", "AI": {"tldr": "Federated learning calibration is explored by adapting centralized post-hoc methods (histogram binning, temperature scaling) to FL, including user-level DP. The study shows heterogeneity degrades calibration and proposes strategies; temperature scaling is most effective under DP-FL, while weighted binning excels without DP.", "motivation": "Calibrated probabilities are essential for reliable decisions. FL introduces privacy constraints and data heterogeneity across clients, which can harm probability calibration. Centralized calibration methods have not been thoroughly studied in FL under DP or heterogeneity.", "method": "Transfer and adapt centralized calibration techniques (histogram binning and temperature scaling) to federated settings, develop new federated calibration variants to handle client heterogeneity, and analyze two scenarios: standard FL and user-level DP-FL. Propose mitigation strategies to reduce calibration degradation due to heterogeneity.", "result": "Calibration accuracy degrades with increasing client heterogeneity. The proposed federated calibration variants mitigate this degradation. In DP-FL, federated temperature scaling performs best; in non-DP FL, weighted histogram binning performs best.", "conclusion": "Tailored post-hoc calibration methods are needed for FL. Temperature scaling is favorable under user-level DP in FL, while weighted binning is preferable when DP is not required. Effective mitigation strategies are essential to maintain calibration under heterogeneity."}}
{"id": "2510.01988", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01988", "abs": "https://arxiv.org/abs/2510.01988", "authors": ["Marcin Mo\u017cejko", "Adam Bielecki", "Jurand Pr\u0105dzy\u0144ski", "Marcin Traskowski", "Antoni Janowski", "Karol Jurasz", "Micha\u0142 Kucharczyk", "Hyun-Su Lee", "Marcelo Der Torossian Torres", "Cesar de la Fuente-Nunez", "Paulina Szymczak", "Micha\u0142 Kmicikiewicz", "Ewa Szczurek"], "title": "PepCompass: Navigating peptide embedding spaces using Riemannian Geometry", "comment": null, "summary": "Antimicrobial peptide discovery is challenged by the astronomical size of\npeptide space and the relative scarcity of active peptides. Generative models\nprovide continuous latent \"maps\" of peptide space, but conventionally ignore\ndecoder-induced geometry and rely on flat Euclidean metrics, rendering\nexploration and optimization distorted and inefficient. Prior manifold-based\nremedies assume fixed intrinsic dimensionality, which critically fails in\npractice for peptide data. Here, we introduce PepCompass, a geometry-aware\nframework for peptide exploration and optimization. At its core, we define a\nUnion of $\\kappa$-Stable Riemannian Manifolds $\\mathbb{M}^{\\kappa}$, a family\nof decoder-induced manifolds that captures local geometry while ensuring\ncomputational stability. We propose two local exploration methods: Second-Order\nRiemannian Brownian Efficient Sampling, which provides a convergent\nsecond-order approximation to Riemannian Brownian motion, and Mutation\nEnumeration in Tangent Space, which reinterprets tangent directions as discrete\namino-acid substitutions. Combining these yields Local Enumeration Bayesian\nOptimization (LE-BO), an efficient algorithm for local activity optimization.\nFinally, we introduce Potential-minimizing Geodesic Search (PoGS), which\ninterpolates between prototype embeddings along property-enriched geodesics,\nbiasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro\nvalidation confirms the effectiveness of PepCompass: PoGS yields four novel\nseeds, and subsequent optimization with LE-BO discovers 25 highly active\npeptides with broad-spectrum activity, including against resistant bacterial\nstrains. These results demonstrate that geometry-informed exploration provides\na powerful new paradigm for antimicrobial peptide design.", "AI": {"tldr": "PepCompass is a geometry-aware framework for antimicrobial peptide design that models the decoder-induced geometry of peptide latent spaces via a Union of \u03ba-Stable Riemannian Manifolds, enabling efficient local exploration and optimization. It introduces LE-BO (Local Enumeration Bayesian Optimization) built from Second-Order Riemannian Brownian Efficient Sampling and Mutation Enumeration in Tangent Space, plus PoGS (Potential-minimizing Geodesic Search) to bias discovery toward favorable seeds. In vitro validation yields novel seeds and 25 highly active peptides, including against resistant strains, demonstrating the approach's effectiveness.", "motivation": "Antimicrobial peptide discovery is hindered by the enormous size of peptide space and scarcity of actives. Generative models produce latent maps but typically ignore the manifold geometry induced by decoders and rely on flat Euclidean metrics, leading to inefficient exploration. Prior manifold methods assume fixed intrinsic dimensionality, which fails for peptides. A geometry-aware, stable framework that captures local structure and accommodates variable dimensionality is needed to enable efficient search and optimization of active peptides.", "method": "Define a Union of \u03ba-Stable Riemannian Manifolds M^\u03ba to capture decoder-induced local geometry with stability. Develop two local exploration techniques: (1) Second-Order Riemannian Brownian Efficient Sampling, a convergent second-order approximation to Riemannian Brownian motion; (2) Mutation Enumeration in Tangent Space, treating tangent directions as discrete amino-acid substitutions. Combine these into Local Enumeration Bayesian Optimization (LE-BO) for local activity optimization. Introduce Potential-minimizing Geodesic Search (PoGS) to interpolate between prototype embeddings along property-enriched geodesics, biasing toward seeds with favorable activity.", "result": "Empirical validation shows PoGS identifies four novel seeds, and LE-BO, guided by these seeds, discovers 25 highly active, broad-spectrum peptides, including activity against resistant bacterial strains.", "conclusion": "Geometry-informed exploration offers a powerful new paradigm for antimicrobial peptide design by accurately modeling decoder-induced geometry, accommodating variable intrinsic dimensionality, and enabling efficient local optimization and targeted seed-driven exploration."}}
{"id": "2510.02014", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02014", "abs": "https://arxiv.org/abs/2510.02014", "authors": ["Guolei Zeng", "Hezhe Qiao", "Guoguo Ai", "Jinsong Guo", "Guansong Pang"], "title": "Normality Calibration in Semi-supervised Graph Anomaly Detection", "comment": "17 pages", "summary": "Graph anomaly detection (GAD) has attracted growing interest for its crucial\nability to uncover irregular patterns in broad applications. Semi-supervised\nGAD, which assumes a subset of annotated normal nodes available during\ntraining, is among the most widely explored application settings. However, the\nnormality learned by existing semi-supervised GAD methods is limited to the\nlabeled normal nodes, often inclining to overfitting the given patterns. These\ncan lead to high detection errors, such as high false positives. To overcome\nthis limitation, we propose GraphNC , a graph normality calibration framework\nthat leverages both labeled and unlabeled data to calibrate the normality from\na teacher model (a pre-trained semi-supervised GAD model) jointly in anomaly\nscore and node representation spaces. GraphNC includes two main components,\nanomaly score distribution alignment (ScoreDA) and perturbation-based normality\nregularization (NormReg). ScoreDA optimizes the anomaly scores of our model by\naligning them with the score distribution yielded by the teacher model. Due to\naccurate scores in most of the normal nodes and part of the anomaly nodes in\nthe teacher model, the score alignment effectively pulls the anomaly scores of\nthe normal and abnormal classes toward the two ends, resulting in more\nseparable anomaly scores. Nevertheless, there are inaccurate scores from the\nteacher model. To mitigate the misleading by these scores, NormReg is designed\nto regularize the graph normality in the representation space, making the\nrepresentations of normal nodes more compact by minimizing a\nperturbation-guided consistency loss solely on the labeled nodes.", "AI": {"tldr": "GraphNC is a two-component framework for semi-supervised graph anomaly detection that uses a teacher model to calibrate anomaly scores and a perturbation-based regularizer to refine node representations, leading to better normality calibration and more separable anomaly scores.", "motivation": "Semi-supervised GAD often overfits to the limited labeled normal nodes, failing to generalize to unlabeled data and causing high false positives. A method that leverages both labeled and unlabeled data to calibrate normality can improve detection accuracy.", "method": "GraphNC comprises two components: (1) ScoreDA (anomaly score distribution alignment) which aligns the student model's anomaly scores with the score distribution produced by a teacher model, pushing normal and abnormal scores toward the two extremes; (2) NormReg (perturbation-based normality regularization) which applies a perturbation-guided consistency loss on labeled nodes to make normal node representations more compact.", "result": "ScoreDA leads to more separable anomaly scores by aligning toward the teacher's score distribution, while NormReg mitigates misleading teacher scores by regularizing representation space, yielding more compact normal representations and improved calibration.", "conclusion": "GraphNC offers a practical framework to calibrate graph normality using both labeled and unlabeled data, combining score-level and representation-level regularization to enhance semi-supervised GAD performance."}}
{"id": "2510.02017", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02017", "abs": "https://arxiv.org/abs/2510.02017", "authors": ["Aida Tayebi", "Ali Khodabandeh Yalabadi", "Mehdi Yazdani-Jahromi", "Ozlem Ozmen Garibay"], "title": "FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data", "comment": "Accepted to NeurIPS 2025 - Reliable ML Workshop", "summary": "As AI systems become more embedded in everyday life, the development of fair\nand unbiased models becomes more critical. Considering the social impact of AI\nsystems is not merely a technical challenge but a moral imperative. As\nevidenced in numerous research studies, learning fair and robust\nrepresentations has proven to be a powerful approach to effectively debiasing\nalgorithms and improving fairness while maintaining essential information for\nprediction tasks. Representation learning frameworks, particularly those that\nutilize self-supervised and contrastive learning, have demonstrated superior\nrobustness and generalizability across various domains. Despite the growing\ninterest in applying these approaches to tabular data, the issue of fairness in\nthese learned representations remains underexplored. In this study, we\nintroduce a contrastive learning framework specifically designed to address\nbias and learn fair representations in tabular datasets. By strategically\nselecting positive pair samples and employing supervised and self-supervised\ncontrastive learning, we significantly reduce bias compared to existing\nstate-of-the-art contrastive learning models for tabular data. Our results\ndemonstrate the efficacy of our approach in mitigating bias with minimum\ntrade-off in accuracy and leveraging the learned fair representations in\nvarious downstream tasks.", "AI": {"tldr": "A contrastive learning framework for fair representations in tabular data that reduces bias with minimal accuracy loss, using carefully chosen positive pairs and supervised/self-supervised contrastive objectives to improve fairness across downstream tasks.", "motivation": "Fairness in AI is critical, especially for tabular data; current representation learning (self-supervised/contrastive) shows robustness but fairness in such learned representations remains underexplored; need methods to debias while preserving performance.", "method": "Proposes a contrastive learning framework for tabular data that selects positive pair samples strategically and combines supervised and self-supervised contrastive objectives to learn fair representations.", "result": "Significant bias reduction compared to state-of-the-art contrastive models for tabular data, with little to no loss in accuracy; representations improve downstream fairness-sensitive tasks.", "conclusion": "Demonstrates the viability of contrastive learning for debiasing tabular data representations, achieving fairness without sacrificing essential predictive information; suitable for real-world downstream applications."}}
{"id": "2510.02049", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02049", "abs": "https://arxiv.org/abs/2510.02049", "authors": ["Jinshu Huang", "Haibin Su", "Xue-Cheng Tai", "Chunlin Wu"], "title": "Mathematical Modeling and Convergence Analysis of Deep Neural Networks with Dense Layer Connectivities in Deep Learning", "comment": null, "summary": "In deep learning, dense layer connectivity has become a key design principle\nin deep neural networks (DNNs), enabling efficient information flow and strong\nperformance across a range of applications. In this work, we model densely\nconnected DNNs mathematically and analyze their learning problems in the\ndeep-layer limit. For a broad applicability, we present our analysis in a\nframework setting of DNNs with densely connected layers and general non-local\nfeature transformations (with local feature transformations as special cases)\nwithin layers, which is called dense non-local (DNL) framework and includes\nstandard DenseNets and variants as special examples. In this formulation, the\ndensely connected networks are modeled as nonlinear integral equations, in\ncontrast to the ordinary differential equation viewpoint commonly adopted in\nprior works. We study the associated training problems from an optimal control\nperspective and prove convergence results from the network learning problem to\nits continuous-time counterpart. In particular, we show the convergence of\noptimal values and the subsequence convergence of minimizers, using a piecewise\nlinear extension and $\\Gamma$-convergence analysis. Our results provide a\nmathematical foundation for understanding densely connected DNNs and further\nsuggest that such architectures can offer stability of training deep models.", "AI": {"tldr": "Dense non-local (DNL) networks generalize densely connected architectures by modeling them as nonlinear integral equations; the paper proves convergence from discrete network training to a continuous-time optimal-control limit using piecewise-linear extension and Gamma-convergence, implying stability benefits and a mathematical foundation for densely connected DNNs.", "motivation": "Dense connectivity enhances information flow and performance, but there is limited rigorous understanding of training dynamics in the deep-layer limit. By formulating DNL as a nonlinear integral equation and studying the training problem from an optimal-control perspective, the work aims to provide a solid mathematical foundation and to justify the stability and robustness of densely connected architectures.", "method": "Define densely connected networks within a dense non-local (DNL) framework, allowing non-local feature transformations inside layers and modeling the network as nonlinear integral equations. Analyze the training problem as an optimal control problem, employ a piecewise linear extension to connect discrete networks to a continuous-time limit, and apply Gamma-convergence to establish convergence of optimal values and subsequence convergence of minimizers.", "result": "The authors establish convergence of the discrete training problem to its continuous-time counterpart: optimal values converge and minimizers have subsequence convergence under the Gamma-convergence framework. The DNL framework encompasses standard DenseNets and variants.", "conclusion": "This provides a mathematical foundation for densely connected DNNs, suggesting these architectures can offer stability during training in deep models and offering a bridge between discrete networks and continuous-time optimal-control formulations."}}
{"id": "2510.02056", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02056", "abs": "https://arxiv.org/abs/2510.02056", "authors": ["Benjamin Wiriyapong", "Oktay Karaku\u015f", "Kirill Sidorov"], "title": "Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference", "comment": "2 Figures and 2 tables", "summary": "Normalising-flow variational inference (VI) can approximate complex\nposteriors, yet single-flow models often behave inconsistently across\nqualitatively different distributions. We propose Adaptive Mixture Flow\nVariational Inference (AMF-VI), a heterogeneous mixture of complementary flows\n(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of\nindividual flows, and (ii) adaptive global weight estimation via\nlikelihood-driven updates, without per-sample gating or architectural changes.\nEvaluated on six canonical posterior families of banana, X-shape, two-moons,\nrings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower\nnegative log-likelihood than each single-flow baseline and delivers stable\ngains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),\nindicating improved robustness across shapes and modalities. The procedure is\nefficient and architecture-agnostic, incurring minimal overhead relative to\nstandard flow training, and demonstrates that adaptive mixtures of diverse\nflows provide a reliable route to robust VI across diverse posterior families\nwhilst preserving each expert's inductive bias.", "AI": {"tldr": "Adaptive Mixture Flow VI (AMF-VI) combines heterogeneous normalizing flows (MAF, RealNVP, RBIG) in a two-stage training regime to robustly approximate complex posteriors. It yields lower NLL and better transport metrics across diverse posterior shapes with minimal overhead.", "motivation": "Single-flow VI often fails to robustly capture qualitatively different distributions; there is a need for a robust, architecture-agnostic method that can leverage diverse inductive biases.", "method": "Two-stage training: (i) sequential expert training of individual flows; (ii) adaptive global weight estimation via likelihood-driven updates. No per-sample gating or architectural changes; the mixture is learned globally and is architecture-agnostic.", "result": "AMF-VI achieves consistently lower negative log-likelihood than each single-flow baseline across six posterior families (banana, X-shape, two-moons, rings, bimodal, five-mode), with stable improvements in Wasserstein-2 and MMD, indicating robust performance across shapes and modalities; overhead is minimal.", "conclusion": "Adaptive mixtures of diverse flows offer a reliable route to robust VI across diverse posterior families while preserving each expert's inductive bias; the approach is efficient and architecture-agnostic and scales to multiple posterior types."}}
{"id": "2510.02084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02084", "abs": "https://arxiv.org/abs/2510.02084", "authors": ["Kuiye Ding", "Fanda Fan", "Zheya Wang", "Hongxiao Li", "Yifan Wang", "Lei Wang", "Chunjie Luo", "Jianfeng Zhan"], "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting", "comment": null, "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.", "AI": {"tldr": "KAIROS is a non-autoregressive time series forecasting framework for Web apps that models segment-level multi-peak distributions, enabling just-in-time inference, zero-shot generalization, and competitive accuracy with much lower inference cost.", "motivation": "Web-scale time series forecasting needs to be fast and responsive to support real-time resource planning, cache placement, and anomaly response; autoregressive methods suffer from error accumulation and higher latency, motivating a scalable non-autoregressive approach.", "method": "Introduce KAIROS, a non-autoregressive model that directly models segment-level multi-peak distributions for time series forecasts. It avoids error accumulation and supports just-in-time inference. Trained on a large-scale corpus, it improves over existing non-autoregressive models that tend to produce over-smoothed predictions.", "result": "KAIROS demonstrates strong zero-shot generalization on six benchmarks, delivering forecasting performance comparable to state-of-the-art foundation models of similar scale while requiring a fraction of their inference cost.", "conclusion": "Non-autoregressive design is a scalable paradigm for foundation models in time series, and KAIROS shows the feasibility and efficiency of this approach for real-time, web-scale forecasting."}}
{"id": "2510.02073", "categories": ["cs.LG", "physics.bio-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02073", "abs": "https://arxiv.org/abs/2510.02073", "authors": ["Jens Behrmann", "Maria R. Cervera", "Antoine Wehenkel", "Andrew C. Miller", "Albert Cerussi", "Pranay Jain", "Vivek Venugopal", "Shijie Yan", "Guillermo Sapiro", "Luca Pegolotti", "J\u00f6rn-Henrik Jacobsen"], "title": "Inferring Optical Tissue Properties from Photoplethysmography using Hybrid Amortized Inference", "comment": null, "summary": "Smart wearables enable continuous tracking of established biomarkers such as\nheart rate, heart rate variability, and blood oxygen saturation via\nphotoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer\nphysiological information, as recent deep learning (DL) studies demonstrate.\nHowever, DL models often rely on features with unclear physiological meaning,\ncreating a tension between predictive power, clinical interpretability, and\nsensor design. We address this gap by introducing PPGen, a biophysical model\nthat relates PPG signals to interpretable physiological and optical parameters.\nBuilding on PPGen, we propose hybrid amortized inference (HAI), enabling fast,\nrobust, and scalable estimation of relevant physiological parameters from PPG\nsignals while correcting for model misspecification. In extensive in-silico\nexperiments, we show that HAI can accurately infer physiological parameters\nunder diverse noise and sensor conditions. Our results illustrate a path toward\nPPG models that retain the fidelity needed for DL-based features while\nsupporting clinical interpretation and informed hardware design.", "AI": {"tldr": "Introduce PPGen, a biophysical model linking PPG signals to interpretable physiological/optical parameters, and Hybrid Amortized Inference (HAI) for fast, robust parameter estimation, addressing interpretability and sensor design in DL contexts.", "motivation": "DL-based PPG analysis often relies on features with unclear physiological meaning, creating tension between predictive power, clinical interpretability, and hardware design; a biophysical, interpretable model is needed.", "method": "Develop PPGen to relate PPG to interpretable physiological/optical parameters; build Hybrid Amortized Inference (HAI) to estimate parameters from PPG signals while compensating for model misspecification.", "result": "In extensive in-silico experiments, HAI accurately infers physiological parameters across diverse noise and sensor conditions.", "conclusion": "PPGen+HAI offer a path toward DL-friendly but physiologically interpretable PPG models that support clinical interpretation and hardware-informed design."}}
{"id": "2510.02081", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02081", "abs": "https://arxiv.org/abs/2510.02081", "authors": ["Zhaoyi Li", "Jingtao Ding", "Yong Li", "Shihua Li"], "title": "Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions", "comment": null, "summary": "Flow Matching (FM) algorithm achieves remarkable results in generative tasks\nespecially in robotic manipulation. Building upon the foundations of diffusion\nmodels, the simulation-free paradigm of FM enables simple and efficient\ntraining, but inherently introduces a train-inference gap. Specifically, we\ncannot assess the model's output during the training phase. In contrast, other\ngenerative models including Variational Autoencoder (VAE), Normalizing Flow and\nGenerative Adversarial Networks (GANs) directly optimize on the reconstruction\nloss. Such a gap is particularly evident in scenarios that demand high\nprecision, such as robotic manipulation. Moreover, we show that FM's\nover-pursuit of straight predefined paths may introduce some serious problems\nsuch as stiffness into the system. These motivate us to fine-tune FM via\nMaximum Likelihood Estimation of reconstructions - an approach made feasible by\nFM's underlying smooth ODE formulation, in contrast to the stochastic\ndifferential equations (SDEs) used in diffusion models. This paper first\ntheoretically analyzes the relation between training loss and inference error\nin FM. Then we propose a method of fine-tuning FM via Maximum Likelihood\nEstimation of reconstructions, which includes both straightforward fine-tuning\nand residual-based fine-tuning approaches. Furthermore, through specifically\ndesigned architectures, the residual-based fine-tuning can incorporate the\ncontraction property into the model, which is crucial for the model's\nrobustness and interpretability. Experimental results in image generation and\nrobotic manipulation verify that our method reliably improves the inference\nperformance of FM.", "AI": {"tldr": "Flow Matching (FM) has a train-inference gap and can produce stiff trajectories due to over-predicted straight paths. The paper fine-tunes FM using Maximum Likelihood Estimation (MLE) of reconstructions, including a residual-based variant that enforces contraction, aiming to improve inference accuracy in image generation and robotic manipulation.", "motivation": "To address the mismatch between training objectives and inference quality in diffusion-based Flow Matching, and to alleviate issues like stiffness from overly straight paths. The work leverages FM\u2019s ODE-based formulation to bridge training and inference without SDEs.", "method": "Provide theoretical analysis on how training loss relates to inference error in FM. Propose fine-tuning FM via Maximum Likelihood Estimation of reconstructions (MLE fine-tuning), including straightforward and residual-based approaches. The residual-based variant uses specially designed architectures to enforce contraction properties for robustness and interpretability.", "result": "Experimental results show reliable improvement in inference performance for FM in both image generation and robotic manipulation tasks.", "conclusion": "MLE-based fine-tuning mitigates the training-inference gap in Flow Matching, and contraction-aware, residual-based designs further enhance robustness and interpretability, validating the approach across generative and manipulation tasks."}}
{"id": "2510.02180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02180", "abs": "https://arxiv.org/abs/2510.02180", "authors": ["Silvia Sapora", "Devon Hjelm", "Alexander Toshev", "Omar Attia", "Bogdan Mazoure"], "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning", "comment": null, "summary": "Inverse Reinforcement Learning aims to recover reward models from expert\ndemonstrations, but traditional methods yield \"black-box\" models that are\ndifficult to interpret and debug. In this work, we introduce GRACE (Generating\nRewards As CodE), a method for using Large Language Models within an\nevolutionary search to reverse-engineer an interpretable, code-based reward\nfunction directly from expert trajectories. The resulting reward function is\nexecutable code that can be inspected and verified. We empirically validate\nGRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns\nhighly accurate rewards, even in complex, multi-task settings. Further, we\ndemonstrate that the resulting reward leads to strong policies, compared to\nboth competitive Imitation Learning and online RL approaches with ground-truth\nrewards. Finally, we show that GRACE is able to build complex reward APIs in\nmulti-task setups.", "AI": {"tldr": "GRACE generates executable reward functions via LLMs in an evolutionary search, making IRL rewards interpretable and competitive on BabyAI/AndroidWorld.", "motivation": "Inverse Reinforcement Learning often yields black-box rewards. There is a need for interpretable, debuggable reward functions that can be inspected and validated, especially in multi-task contexts.", "method": "GRACE uses Large Language Models within an evolutionary search to reverse-engineer reward functions as executable code directly from expert trajectories, enabling inspectable reward APIs; validated on BabyAI and AndroidWorld with multi-task extensions.", "result": "GRACE learns highly accurate rewards efficiently, producing strong policies that compete with imitation learning and online RL baselines; it also constructs complex reward APIs for multi-task setups.", "conclusion": "GRACE provides interpretable, verifiable reward functions that can be inspected and Debugged, enabling complex multi-task reward APIs and competitive policy performance."}}
{"id": "2510.02096", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02096", "abs": "https://arxiv.org/abs/2510.02096", "authors": ["Damian Falk", "Konstantin Sch\u00fcrholt", "Konstantinos Tzevelekakis", "L\u00e9o Meynent", "Damian Borth"], "title": "Learning Model Representations Using Publicly Available Model Hubs", "comment": null, "summary": "The weights of neural networks have emerged as a novel data modality, giving\nrise to the field of weight space learning. A central challenge in this area is\nthat learning meaningful representations of weights typically requires large,\ncarefully constructed collections of trained models, typically referred to as\nmodel zoos. These model zoos are often trained ad-hoc, requiring large\ncomputational resources, constraining the learned weight space representations\nin scale and flexibility. In this work, we drop this requirement by training a\nweight space learning backbone on arbitrary models downloaded from large,\nunstructured model repositories such as Hugging Face. Unlike curated model\nzoos, these repositories contain highly heterogeneous models: they vary in\narchitecture and dataset, and are largely undocumented. To address the\nmethodological challenges posed by such heterogeneity, we propose a new weight\nspace backbone designed to handle unstructured model populations. We\ndemonstrate that weight space representations trained on models from Hugging\nFace achieve strong performance, often outperforming backbones trained on\nlaboratory-generated model zoos. Finally, we show that the diversity of the\nmodel weights in our training set allows our weight space model to generalize\nto unseen data modalities. By demonstrating that high-quality weight space\nrepresentations can be learned in the wild, we show that curated model zoos are\nnot indispensable, thereby overcoming a strong limitation currently faced by\nthe weight space learning community.", "AI": {"tldr": "A weight-space learning backbone trained on unstructured models from repositories can learn robust weight representations without curated model zoos and generalize to unseen modalities.", "motivation": "To overcome the reliance on large, curated model zoos for learning weight-space representations, addressing heterogeneity and lack of documentation in real-world model pools.", "method": "Train a weight-space backbone on arbitrary models downloaded from large repositories (e.g., Hugging Face), designed to handle highly heterogeneous, unstructured populations. Evaluate against backbones trained on laboratory-created zoos and test cross-domain generalization to unseen data modalities.", "result": "Weight-space representations learned from models sourced from Hugging Face achieve strong performance, often outperforming backbones trained on curated lab zoos. The diversity in training weights enables generalization to unseen data modalities.", "conclusion": "High-quality weight-space representations can be learned in the wild from uncurated model pools, making curated model zoos unnecessary and enabling scalable, flexible weight-space learning."}}
{"id": "2510.02202", "categories": ["cs.LG", "cs.AI", "68Txx"], "pdf": "https://arxiv.org/pdf/2510.02202", "abs": "https://arxiv.org/abs/2510.02202", "authors": ["Matthew A. Reyna", "Zuzana Koscova", "Jan Pavlus", "Soheil Saghafi", "James Weigle", "Andoni Elola", "Salman Seyedi", "Kiersten Campbell", "Qiao Li", "Ali Bahrami Rad", "Ant\u00f4nio H. Ribeiro", "Antonio Luiz P. Ribeiro", "Reza Sameni", "Gari D. Clifford"], "title": "Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025", "comment": "13 pages, 2 figures", "summary": "Objective: Chagas disease is a parasitic infection that is endemic to South\nAmerica, Central America, and, more recently, the U.S., primarily transmitted\nby insects. Chronic Chagas disease can cause cardiovascular diseases and\ndigestive problems. Serological testing capacities for Chagas disease are\nlimited, but Chagas cardiomyopathy often manifests in ECGs, providing an\nopportunity to prioritize patients for testing and treatment. Approach: The\nGeorge B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic\napproaches for identifying Chagas disease from electrocardiograms (ECGs). Main\nresults: This Challenge provides multiple innovations. First, we leveraged\nseveral datasets with labels from patient reports and serological testing,\nprovided a large dataset with weak labels and smaller datasets with strong\nlabels. Second, we augmented the data to support model robustness and\ngeneralizability to unseen data sources. Third, we applied an evaluation metric\nthat captured the local serological testing capacity for Chagas disease to\nframe the machine learning problem as a triage task. Significance: Over 630\nparticipants from 111 teams submitted over 1300 entries during the Challenge,\nrepresenting diverse approaches from academia and industry worldwide.", "AI": {"tldr": "ECG-based triage for Chagas disease via the PhysioNet Challenge 2025, leveraging weak and strong labels, data augmentation, and a serology-aware evaluation metric; global participation and diverse algorithmic approaches.", "motivation": "Chagas disease testing is constrained by limited serology capacity; ECGs showing cardiomyopathy can serve as a non-invasive screening tool to prioritize testing and treatment, especially in resource-limited settings.", "method": "Utilized multiple datasets with weak labels (patient reports) and smaller, strongly labeled datasets; applied data augmentation to improve robustness and generalization across unseen data sources; framed the problem as a triage task using a serology-capacity-aware evaluation metric.", "result": "Engaged 630+ participants from 111 teams with over 1300 entries, introducing innovations in weak/strong label handling, data augmentation, and triage-oriented evaluation; demonstrated broad methodological diversity and potential for scalable Chagas screening.", "conclusion": "The challenge showcases the feasibility and value of ECG-based triage for Chagas disease, highlighting the importance of label strategies, data augmentation, and serology-aware metrics, and it catalyzes global collaboration for scalable screening solutions."}}
{"id": "2510.02107", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02107", "abs": "https://arxiv.org/abs/2510.02107", "authors": ["Klaus-Rudolf Kladny", "Bernhard Sch\u00f6lkopf", "Michael Muehlebach"], "title": "PENEX: AdaBoost-Inspired Neural Network Regularization", "comment": null, "summary": "AdaBoost sequentially fits so-called weak learners to minimize an exponential\nloss, which penalizes mislabeled data points more severely than other loss\nfunctions like cross-entropy. Paradoxically, AdaBoost generalizes well in\npractice as the number of weak learners grows. In the present work, we\nintroduce Penalized Exponential Loss (PENEX), a new formulation of the\nmulti-class exponential loss that is theoretically grounded and, in contrast to\nthe existing formulation, amenable to optimization via first-order methods. We\ndemonstrate both empirically and theoretically that PENEX implicitly maximizes\nmargins of data points. Also, we show that gradient increments on PENEX\nimplicitly parameterize weak learners in the boosting framework. Across\ncomputer vision and language tasks, we show that PENEX exhibits a regularizing\neffect often better than established methods with similar computational cost.\nOur results highlight PENEX's potential as an AdaBoost-inspired alternative for\neffective training and fine-tuning of deep neural networks.", "AI": {"tldr": "PENEX is a new penalized exponential loss for multi-class boosting that is optimization-friendly and promotes margin maximization, yielding regularization benefits and serving as an AdaBoost-inspired alternative for training deep nets.", "motivation": "AdaBoost uses exponential loss and often generalizes well despite increasing model capacity; but existing multi-class exponential losses are hard to optimize with first-order methods. PENEX aims for theoretical grounding and optimization-friendly properties to enable margin-based boosting in modern settings.", "method": "Introduce Penalized Exponential Loss (PENEX) for multi-class problems; show it is amenable to first-order optimization; prove that PENEX implicitly maximizes data margins; show that gradient increments correspond to weak learner updates within a boosting framework; validate empirically on vision and language tasks and compare regularization effects.", "result": "Theoretical evidence that PENEX maximizes margins and that its gradient increments naturally parameterize weak learners; empirical results across vision and language tasks demonstrate PENEX provides regularization benefits, often outperforming comparable methods at similar computational cost.", "conclusion": "PENEX offers a viable AdaBoost-inspired alternative for training and fine-tuning deep neural networks, leveraging boosting principles with a first-order-optimization-friendly loss to improve generalization and regularization."}}
{"id": "2510.02212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02212", "abs": "https://arxiv.org/abs/2510.02212", "authors": ["Hanyang Zhao", "Dawen Liang", "Wenpin Tang", "David Yao", "Nathan Kallus"], "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning", "comment": null, "summary": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks.", "AI": {"tldr": "DiFFPO is a unified RL-based framework for training diffusion LLMs to reason better and faster, using off-policy surrogate RL, two-stage likelihood with importance sampling, and jointly trained samplers to cut inference compute, showing improved accuracy at lower NFEs on math/planning tasks.", "motivation": "Address inefficiencies and limited reasoning quality in diffusion LLMs; unify existing baselines (e.g., d1) under a more sample-efficient RL paradigm; enable adaptive inference control to reduce compute while preserving or improving performance.", "method": "1) Train surrogate policies via off-policy RL to approximate the true dLLM policy; 2) Use a two-stage likelihood approximation with importance sampling; 3) Introduce joint training of samplers/controllers to allocate inference thresholds per prompt, optimizing for fewer function evaluations; 4) Train open-source diffusion LLMs on math and planning benchmarks.", "result": "Improved sample efficiency and task performance; better accuracies with fewer NFEs; superior Pareto frontier for inference-time compute across math/planning benchmarks.", "conclusion": "DiFFPO provides a cohesive framework that jointly optimizes model policy and inference control, achieving faster and better reasoning in diffusion LLMs and demonstrating effectiveness on standard tasks."}}
{"id": "2510.02115", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02115", "abs": "https://arxiv.org/abs/2510.02115", "authors": ["Milad Firoozeh", "Nader Dashti", "Mohammad Ali Hatefi"], "title": "Hybrid Deep Learning Modeling Approach to Predict Natural Gas Consumption of Home Subscribers on Limited Data", "comment": null, "summary": "Today, natural gas, as a clean fuel and the best alternative to crude oil,\ncovers a significant part of global demand. Iran is one of the largest\ncountries with energy resources and in terms of gas is the second-largest\ncountry in the world. But, due to the increase in population and energy\nconsumption, it faces problems such as pressure drops and gas outages yearly in\ncold seasons and therefore it is necessary to control gas consumption,\nespecially in the residential sector, which has the largest share in Iran. This\nstudy aims to analyze and predict gas consumption for residential customers in\nZanjan province, Iran, using machine learning models, including LSTM, GRU, and\na hybrid BiLSTM-XGBoost model. The dataset consists of gas consumption and\nmeteorology data collected over six years, from 2017 to 2022. The models were\ntrained and evaluated based on their ability to accurately predict consumption\npatterns. The results indicate that the hybrid BiLSTM-XGBoost model\noutperformed the other models in terms of accuracy, with lower Root Mean\nSquared Error (RMSE), Mean Absolute Percentage Error (MAPE) values, and Mean\nPercentage Error (MPE). Additionally, the Hybrid model demonstrated robust\nperformance, particularly in scenarios with limited data. The findings suggest\nthat machine learning approaches, particularly hybrid models, can be\neffectively utilized to manage and predict gas consumption, contributing to\nmore efficient resource management and reducing seasonal shortages. This study\nhighlights the importance of incorporating geographical and climatic factors in\npredictive modeling, as these significantly influence gas usage across\ndifferent regions.", "AI": {"tldr": "Hybrid BiLSTM-XGBoost model outperforms LSTM and GRU in predicting residential gas consumption in Zanjan, Iran, using six years of meteorological data; robust with limited data, enabling better resource management and reducing seasonal shortages.", "motivation": "Growing energy demand and seasonal gas outages in Iran motivate improved prediction of residential gas consumption. Iran's large gas resources and climate variability make accurate forecasting important for efficient resource management, especially in the residential sector.", "method": "Comparative evaluation of machine learning models (LSTM, GRU, and a hybrid BiLSTM-XGBoost) using a dataset of gas consumption and meteorological data from 2017\u20132022. Models were trained and evaluated on their ability to predict consumption patterns, with performance assessed via RMSE, MAPE, and MPE. The study highlights the value of incorporating geographical and climatic factors.", "result": "The hybrid BiLSTM-XGBoost model achieved superior accuracy, with lower RMSE, MAPE, and MPE than the LSTM and GRU models. It also demonstrated robustness when data were limited, indicating strong generalization in various data-scarce scenarios.", "conclusion": "Hybrid, data-fusion ML approaches are effective for predicting residential gas consumption and managing resources, particularly in regions with variable climate. The inclusion of geographic and climate information enhances predictive performance and can help mitigate seasonal shortages."}}
{"id": "2510.02116", "categories": ["cs.LG", "cs.DB", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02116", "abs": "https://arxiv.org/abs/2510.02116", "authors": ["John N. Daras"], "title": "Ensemble Threshold Calibration for Stable Sensitivity Control", "comment": "10 pages, 6 tables", "summary": "Precise recall control is critical in large-scale spatial conflation and\nentity-matching tasks, where missing even a few true matches can break\ndownstream analytics, while excessive manual review inflates cost. Classical\nconfidence-interval cuts such as Clopper-Pearson or Wilson provide lower bounds\non recall, but they routinely overshoot the target by several percentage points\nand exhibit high run-to-run variance under skewed score distributions. We\npresent an end-to-end framework that achieves exact recall with sub-percent\nvariance over tens of millions of geometry pairs, while remaining TPU-friendly.\nOur pipeline starts with an equigrid bounding-box filter and compressed sparse\nrow (CSR) candidate representation, reducing pair enumeration by two orders of\nmagnitude. A deterministic xxHash bootstrap sample trains a lightweight neural\nranker; its scores are propagated to all remaining pairs via a single forward\npass and used to construct a reproducible, score-decile-stratified calibration\nset. Four complementary threshold estimators - Clopper-Pearson, Jeffreys,\nWilson, and an exact quantile - are aggregated via inverse-variance weighting,\nthen fused across nine independent subsamples. This ensemble reduces threshold\nvariance compared to any single method. Evaluated on two real cadastral\ndatasets (approximately 6.31M and 67.34M pairs), our approach consistently hits\na recall target within a small error, decreases redundant verifications\nrelative to other calibrations, and runs end-to-end on a single TPU v3 core.", "AI": {"tldr": "An end-to-end framework to achieve exact recall with sub-percent variance for large-scale spatial conflation and entity matching, leveraging a scalable pipeline and ensemble threshold estimators.", "motivation": "Traditional confidence-interval-based recall bounds (e.g., Clopper-Pearson, Wilson) often overshoot or vary widely under skewed score distributions, causing missed true matches or excessive verification costs. Precise, low-variance recall control at scale is needed for reliable downstream analytics.", "method": "A scalable pipeline: (1) equigrid bounding-box filter and CSR representation to drastically reduce candidate pairs; (2) deterministic xxHash bootstrap to train a lightweight neural ranker; (3) propagate ranks to all remaining pairs with a single forward pass; (4) build a reproducible, score-decile-stratified calibration set; (5) ensemble threshold estimators\u2014Clopper-Pearson, Jeffreys, Wilson, exact quantile\u2014via inverse-variance weighting, fused across nine subsamples; (6) end-to-end TPU-friendly implementation; evaluated on two cadastral datasets (\u22486.31M and 67.34M pairs).", "result": "The framework consistently achieves the target recall with sub-percent variance, reduces redundant verifications compared with other calibration methods, and runs end-to-end on a single TPU v3 core.", "conclusion": "The proposed approach delivers reproducible, low-variance recall control at scale for spatial conflation and entity matching, balancing accuracy with computational cost, and is TPU-friendly; it shows promise for other large-scale matching problems given similar data characteristics."}}
{"id": "2510.02117", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.02117", "abs": "https://arxiv.org/abs/2510.02117", "authors": ["Samhita Pal", "James O'quinn", "Kaveh Aryan", "Heather Pua", "James P. Long", "Amir Asiaee"], "title": "DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding", "comment": null, "summary": "We study structure learning for linear Gaussian SEMs in the presence of\nlatent confounding. Existing continuous methods excel when errors are\nindependent, while deconfounding-first pipelines rely on pervasive factor\nstructure or nonlinearity. We propose \\textsc{DECOR}, a single likelihood-based\nand fully differentiable estimator that jointly learns a DAG and a correlated\nnoise model. Our theory gives simple sufficient conditions for global parameter\nidentifiability: if the mixed graph is bow free and the noise covariance has a\nuniform eigenvalue margin, then the map from $(\\B,\\OmegaMat)$ to the\nobservational covariance is injective, so both the directed structure and the\nnoise are uniquely determined. The estimator alternates a smooth-acyclic graph\nupdate with a convex noise update and can include a light bow complementarity\npenalty or a post hoc reconciliation step. On synthetic benchmarks that vary\nconfounding density, graph density, latent rank, and dimension with $n<p$,\n\\textsc{DECOR} matches or outperforms strong baselines and is especially robust\nwhen confounding is non-pervasive, while remaining competitive under\npervasiveness.", "AI": {"tldr": "DECOR is a differentiable, single-shot estimator for learning DAGs in linear Gaussian SEMs with latent confounding; it jointly models the structure and a correlated noise term, with identifiability guaranteed under bow-free graphs and a uniform eigenvalue margin, and shows competitive performance across synthetic regimes.", "motivation": "Latent confounding complicates structure learning in linear SEMs; existing continuous methods rely on error independence or require deconfounding steps with strong assumptions. A single likelihood-based approach that jointly learns the DAG and noise model aims to be both principled and robust.", "method": "DECOR is a single likelihood-based, fully differentiable estimator that alternates a smooth-acyclic graph update with a convex noise update. It jointly learns the DAG and a correlated noise model, with optional bow complementarity penalty or post hoc reconciliation. Theoretical identifiability is established: if the mixed graph is bow-free and the noise covariance has a uniform eigenvalue margin, the map from (B,\u03a9) to the observational covariance is injective.", "result": "Empirically, DECOR matches or outperforms strong baselines on synthetic benchmarks, across varying confounding density, graph density, latent rank, and dimensionality (n<p). It is particularly robust when confounding is non-pervasive, while remaining competitive under pervasive confounding.", "conclusion": "DECOR provides a principled, identifiable, and practical approach for structure learning in linear Gaussian SEMs with latent confounding. It achieves strong empirical performance with theoretical identifiability under bow-free conditions and uniform eigenvalue margin, and offers flexibility via a bow penalty or reconciliation step."}}
{"id": "2510.02245", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02245", "abs": "https://arxiv.org/abs/2510.02245", "authors": ["Runzhe Zhan", "Yafu Li", "Zhi Wang", "Xiaoye Qu", "Dongrui Liu", "Jing Shao", "Derek F. Wong", "Yu Cheng"], "title": "ExGRPO: Learning to Reason from Experience", "comment": null, "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.", "AI": {"tldr": "ExGRPO proposes experiential grouping and a mixed-policy objective for RLVR to reuse valuable experiences; leveraging rollout correctness and entropy to rank experiences, it improves reasoning performance and stability across 1.5B-8B models.", "motivation": "Address computational inefficiency and instability of on-policy RLVR due to discarding rollout experiences; investigate what makes a reasoning episode valuable for large reasoning models; enable reusable experiences to improve learning dynamics.", "method": "Introduce ExGRPO (Experiential Group Relative Policy Optimization) that groups and prioritizes valuable experiences; uses rollout correctness and entropy as indicators of value; employs a mixed-policy objective combining exploration and experience exploitation; tested on five backbone models.", "result": "Consistent performance gains over on-policy RLVR (average +3.5/7.6 points on math/general benchmarks); improved stability on both strong and weak models where on-policy methods fail.", "conclusion": "Principled experience management is essential for efficient and scalable RLVR; the approach shows that selective exposure to valuable experiences can improve reasoning performance and training stability."}}
{"id": "2510.02142", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.02142", "abs": "https://arxiv.org/abs/2510.02142", "authors": ["Lena Podina", "Christina Humer", "Alexandre Duval", "Victor Schmidt", "Ali Ramlaoui", "Shahana Chatterjee", "Yoshua Bengio", "Alex Hernandez-Garcia", "David Rolnick", "F\u00e9lix Therrien"], "title": "Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study", "comment": "5 pages, 2 figures. Accepted to NeurIPS AI for Materials Workshop\n  2025", "summary": "Efficient and inexpensive energy storage is essential for accelerating the\nadoption of renewable energy and ensuring a stable supply, despite fluctuations\nin sources such as wind and solar. Electrocatalysts play a key role in hydrogen\nenergy storage (HES), allowing the energy to be stored as hydrogen. However,\nthe development of affordable and high-performance catalysts for this process\nremains a significant challenge. We introduce Catalyst GFlowNet, a generative\nmodel that leverages machine learning-based predictors of formation and\nadsorption energy to design crystal surfaces that act as efficient catalysts.\nWe demonstrate the performance of the model through a proof-of-concept\napplication to the hydrogen evolution reaction, a key reaction in HES, for\nwhich we successfully identified platinum as the most efficient known catalyst.\nIn future work, we aim to extend this approach to the oxygen evolution\nreaction, where current optimal catalysts are expensive metal oxides, and open\nthe search space to discover new materials. This generative modeling framework\noffers a promising pathway for accelerating the search for novel and efficient\ncatalysts.", "AI": {"tldr": "Catalyst GFlowNet: a ML-guided generative framework that designs catalytic crystal surfaces using formation and adsorption energy predictors; proof-of-concept for hydrogen evolution identifies platinum as the best known catalyst; future work extends to oxygen evolution and broader material space.", "motivation": "Efficient, inexpensive energy storage is vital for integrating renewable energy; hydrogen energy storage relies on effective catalysts, but affordable high-performance catalysts remain a major challenge.", "method": "Introduce Catalyst GFlowNet, a generative model steered by ML-based predictors of formation energy and adsorption energy to generate crystal surfaces as potential catalysts; apply to hydrogen evolution reaction (HER) and demonstrate platinum as the most efficient known catalyst.", "result": "The framework successfully identifies platinum as the most efficient known catalyst for HER within the explored design space, validating the approach as a proof-of-concept for ML-guided catalyst discovery; sets the stage for extending the search to other reactions and materials.", "conclusion": "A ML-driven generative framework can accelerate the discovery of efficient, cost-effective catalysts for energy storage; planned work includes extending to oxygen evolution reaction (OER) and expanding to new materials, potentially enabling faster identification of high-performance catalysts."}}
{"id": "2510.02148", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02148", "abs": "https://arxiv.org/abs/2510.02148", "authors": ["Jianing Qi", "Hao Tang", "Zhigang Zhu"], "title": "Policy Gradient Guidance Enables Test Time Control", "comment": null, "summary": "We introduce Policy Gradient Guidance (PGG), a simple extension of\nclassifier-free guidance from diffusion models to classical policy gradient\nmethods. PGG augments the policy gradient with an unconditional branch and\ninterpolates conditional and unconditional branches, yielding a test-time\ncontrol knob that modulates behavior without retraining. We provide a\ntheoretical derivation showing that the additional normalization term vanishes\nunder advantage estimation, leading to a clean guided policy gradient update.\nEmpirically, we evaluate PGG on discrete and continuous control benchmarks. We\nfind that conditioning dropout-central to diffusion guidance-offers gains in\nsimple discrete tasks and low sample regimes, but dropout destabilizes\ncontinuous control. Training with modestly larger guidance ($\\gamma>1$)\nconsistently improves stability, sample efficiency, and controllability. Our\nresults show that guidance, previously confined to diffusion policies, can be\nadapted to standard on-policy methods, opening new directions for controllable\nonline reinforcement learning.", "AI": {"tldr": "PGG extends diffusion-model guidance to policy gradient RL by adding an unconditional branch and interpolating conditional/unconditional updates, enabling test-time control without retraining; a normalization term vanishes under advantage estimation; gamma-guided updates improve stability and efficiency, with mixed effects from dropout-based conditioning.", "motivation": "Enable controllable online reinforcement learning with minimal retraining by importing guidance ideas from diffusion models into classical policy gradients, addressing stability and data efficiency in on-policy methods.", "method": "Augment policy gradient with an unconditional branch and interpolate between conditional and unconditional updates. Derive that the extra normalization term cancels under advantage estimation, yielding a clean guided policy gradient update. Introduce a guidance strength parameter (gamma) and test-time control knob. Empirically evaluate on discrete and continuous benchmarks; analyze dropout-based conditioning effects and stability.", "result": "In discrete tasks and low-sample regimes, dropout-conditioned guidance offers gains; in continuous control, dropout can destabilize. Using a modestly larger guidance (gamma>1) consistently improves stability, sample efficiency, and controllability across tasks.", "conclusion": "Guidance concepts can be transferred from diffusion policies to standard on-policy RL, enabling controllable online reinforcement learning without retraining and opening new directions for controllable agents."}}
{"id": "2510.02149", "categories": ["cs.LG", "math.OC", "stat.ML", "68T05 (Primary), 62L05, 68W27 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.02149", "abs": "https://arxiv.org/abs/2510.02149", "authors": ["Alexander Ryabchenko", "Wenlong Mou"], "title": "Reinforcement Learning with Action-Triggered Observations", "comment": null, "summary": "We study reinforcement learning problems where state observations are\nstochastically triggered by actions, a constraint common in many real-world\napplications. This framework is formulated as Action-Triggered Sporadically\nTraceable Markov Decision Processes (ATST-MDPs), where each action has a\nspecified probability of triggering a state observation. We derive tailored\nBellman optimality equations for this framework and introduce the\naction-sequence learning paradigm in which agents commit to executing a\nsequence of actions until the next observation arrives. Under the linear MDP\nassumption, value-functions are shown to admit linear representations in an\ninduced action-sequence feature map. Leveraging this structure, we propose\noff-policy estimators with statistical error guarantees for such feature maps\nand introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered\nsettings. ST-LSVI-UCB achieves regret $\\widetilde\nO(\\sqrt{Kd^3(1-\\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the\nfeature dimension, and $\\gamma$ the discount factor (per-step episode\nnon-termination probability). Crucially, this work establishes the theoretical\nfoundation for learning with sporadic, action-triggered observations while\ndemonstrating that efficient learning remains feasible under such observation\nconstraints.", "AI": {"tldr": "Introduces Action-Triggered Sporadically Traceable MDPs (ATST-MDPs) where actions probabilistically trigger state observations; develops tailored Bellman equations, action-sequence learning, and ST-LSVI-UCB; proves a sublinear regret bound under linear MDP assumptions, showing efficient learning despite sporadic observations.", "motivation": "Real-world RL settings often have observations arriving only when triggered by actions; this constraint complicates planning and learning; there is a need for theoretical foundations and practical algorithms that operate under sporadic, action-triggered observations.", "method": "Formulates ATST-MDPs with a specified probability that an action triggers a state observation; derives action-triggered Bellman optimality equations; defines an action-sequence learning paradigm where agents commit to executing a sequence of actions until the next observation arrives; under the linear MDP assumption, shows value-functions admit linear representations in an induced action-sequence feature map; develops off-policy estimators with statistical error guarantees for these feature maps; introduces ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered settings.", "result": "Establishes a theoretical foundation for learning with sporadic, action-triggered observations and demonstrates that efficient learning remains feasible under such observation constraints; derives regret bound \u007ftenilde\u007f O(sqrt(K d^3 (1-\u007f gamma)^{-3})) with K episodes, d feature dimension, and gamma discount factor.", "conclusion": "The paper shows that learning under action-triggered, sporadic observations is theoretically sound and practically feasible, expanding the RL toolkit for environments with observation constraints and enabling efficient learning via the proposed ATST-MDP framework and ST-LSVI-UCB algorithm."}}
{"id": "2510.02265", "categories": ["cs.LG", "cs.AI", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02265", "abs": "https://arxiv.org/abs/2510.02265", "authors": ["Yalin E. Sagduyu", "Tugba Erpek", "Kemal Davaslioglu", "Sastry Kompella"], "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning", "comment": null, "summary": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime.", "AI": {"tldr": "A model-free RL framework (Q-learning + DQN) enables a transmitter-receiver to learn to avoid reactive jamming by adapting power, modulation, and channel selection, achieving robust throughput without requiring prior channel/jammer models.", "motivation": "Reactive jammers that adapt their channels and sensing thresholds create a dynamic, nonstationary environment. A learning-based approach is needed to sustain throughput without prior knowledge of the spectrum or jammer strategies.", "method": "Formulated as a reinforcement-learning problem where the transmitter-receiver pair selects transmit power, modulation, and channel. Q-learning handles discrete jamming-event states, while Deep Q-Networks (DQN) handle continuous states based on received power. The study experiments with different reward functions and action sets to assess adaptability.", "result": "The RL framework enables rapid adaptation to spectrum dynamics, sustaining high data rates even as channels and jamming policies change over time.", "conclusion": "Model-free RL approaches are effective for mitigating reactive jamming in dynamic spectrum environments and can inform the design of adaptive physical-layer strategies without prior knowledge of the jammer"}}
{"id": "2510.02174", "categories": ["cs.LG", "math.OC", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02174", "abs": "https://arxiv.org/abs/2510.02174", "authors": ["Stefano Bruno", "Youngsik Hwang", "Jaehyeon An", "Sotirios Sabanis", "Dong-Young Lim"], "title": "Flatness-Aware Stochastic Gradient Langevin Dynamics", "comment": null, "summary": "Generalization in deep learning is closely tied to the pursuit of flat minima\nin the loss landscape, yet classical Stochastic Gradient Langevin Dynamics\n(SGLD) offers no mechanism to bias its dynamics toward such low-curvature\nsolutions. This work introduces Flatness-Aware Stochastic Gradient Langevin\nDynamics (fSGLD), designed to efficiently and provably seek flat minima in\nhigh-dimensional nonconvex optimization problems. At each iteration, fSGLD uses\nthe stochastic gradient evaluated at parameters perturbed by isotropic Gaussian\nnoise, commonly referred to as Random Weight Perturbation (RWP), thereby\noptimizing a randomized-smoothing objective that implicitly captures curvature\ninformation. Leveraging these properties, we prove that the invariant measure\nof fSGLD stays close to a stationary measure concentrated on the global\nminimizers of a loss function regularized by the Hessian trace whenever the\ninverse temperature and the scale of random weight perturbation are properly\ncoupled. This result provides a rigorous theoretical explanation for the\nbenefits of random weight perturbation. In particular, we establish\nnon-asymptotic convergence guarantees in Wasserstein distance with the best\nknown rate and derive an excess-risk bound for the Hessian-trace regularized\nobjective. Extensive experiments on noisy-label and large-scale vision tasks,\nin both training-from-scratch and fine-tuning settings, demonstrate that fSGLD\nachieves superior or comparable generalization and robustness to baseline\nalgorithms while maintaining the computational cost of SGD, about half that of\nSAM. Hessian-spectrum analysis further confirms that fSGLD converges to\nsignificantly flatter minima.", "AI": {"tldr": "Flatness-aware SGLD (fSGLD) biases SGD toward flat minima via random weight perturbations, with theoretical guarantees and competitive/generalization performance at SGD-like cost (about half that of SAM).", "motivation": "Generalization in deep learning is linked to flat minima in the loss landscape, but classical SGLD lacks a mechanism to bias dynamics toward low-curvature solutions.", "method": "fSGLD perturbs parameters with isotropic Gaussian noise (Random Weight Perturbation) and optimizes a randomized-smoothing objective that encodes curvature information. It couples inverse temperature and perturbation scale to ensure the invariant measure concentrates near global minimizers of a Hessian-trace-regularized loss, and provides non-asymptotic Wasserstein convergence guarantees.", "result": "The authors prove invariant measures stay close to stationary measures on Hessian-trace-regularized global minimizers; provide best-known-rate non-asymptotic Wasserstein convergence and an excess-risk bound. Empirically, fSGLD achieves superior or comparable generalization and robustness vs baselines on noisy-label and large-scale vision tasks, with SGD-like cost (about half of SAM). Hessian-spectrum analyses show convergence to significantly flatter minima.", "conclusion": "fSGLD offers a theoretically justified mechanism to prefer flat minima via random weight perturbations, yielding practical performance gains with computational cost close to SGD and substantially less than SAM."}}
{"id": "2510.02279", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02279", "abs": "https://arxiv.org/abs/2510.02279", "authors": ["Mykyta Ielanskyi", "Kajetan Schweighofer", "Lukas Aichberger", "Sepp Hochreiter"], "title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation", "comment": null, "summary": "Hallucinations are a common issue that undermine the reliability of large\nlanguage models (LLMs). Recent studies have identified a specific subset of\nhallucinations, known as confabulations, which arise due to predictive\nuncertainty of LLMs. To detect confabulations, various methods for estimating\npredictive uncertainty in natural language generation (NLG) have been\ndeveloped. These methods are typically evaluated by correlating uncertainty\nestimates with the correctness of generated text, with question-answering (QA)\ndatasets serving as the standard benchmark. However, commonly used approximate\ncorrectness functions have substantial disagreement between each other and,\nconsequently, in the ranking of the uncertainty estimation methods. This allows\none to inflate the apparent performance of uncertainty estimation methods. We\npropose using several alternative risk indicators for risk correlation\nexperiments that improve robustness of empirical assessment of UE algorithms\nfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge\nvariants leads to reducing the evaluation biases. Furthermore, we explore\nstructured tasks as well as out of distribution and perturbation detection\ntasks which provide robust and controllable risk indicators. Finally, we\npropose to use an Elo rating of uncertainty estimation methods to give an\nobjective summarization over extensive evaluation settings.", "AI": {"tldr": "Proposes robust, multi-faceted evaluation of uncertainty estimation in NLG to combat bias from flawed correctness signals; introduces multiple risk indicators, multi-judge averaging, structured/OOD/perturbation tasks, and Elo-based ranking.", "motivation": "Hallucinations and confabulations in LLMs undermine reliability. Existing uncertainty evaluation relies on flawed approximate correctness signals, which can bias method ranking. There is a need for robust, unbiased evaluation protocols for uncertainty estimation in NLG.", "method": "Introduce alternative risk indicators for risk correlation experiments; marginalize over multiple LLM-as-a-judge variants to reduce evaluation bias; apply evaluation to structured tasks, out-of-distribution (OOD) and perturbation detection tasks; propose an Elo rating system to summarize performance across many settings.", "result": "The approach yields reduced evaluation biases in QA tasks by averaging over multiple LLM-judge variants; structured, OOD, and perturbation tasks provide robust risk indicators; an Elo rating is proposed as an objective summary of uncertainty estimation methods across diverse settings.", "conclusion": "Robust evaluation frameworks for uncertainty estimation in NLG can mitigate biases caused by flawed correctness signals, with marginalization over judges, diverse task settings, and Elo-based summaries enhancing robustness and comparability of UE methods."}}
{"id": "2510.02206", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02206", "abs": "https://arxiv.org/abs/2510.02206", "authors": ["Daniel Gallo Fern\u00e1ndez"], "title": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling", "comment": null, "summary": "Sequence-to-sequence models have become central in Artificial Intelligence,\nparticularly following the introduction of the transformer architecture. While\ninitially developed for Natural Language Processing, these models have\ndemonstrated utility across domains, including Computer Vision. Such models\nrequire mechanisms to exchange information along the time dimension, typically\nusing recurrent or self-attention layers. However, self-attention scales\nquadratically with sequence length, limiting its practicality for very long\nsequences.\n  We introduce Poolformer, a sequence-to-sequence model that replaces\nself-attention with recurrent layers and incorporates pooling operations to\nreduce sequence length. Poolformer is defined recursively using SkipBlocks,\nwhich contain residual blocks, a down-pooling layer, a nested SkipBlock, an\nup-pooling layer, and additional residual blocks. We conduct extensive\nexperiments to support our architectural choices.\n  Our results show that pooling greatly accelerates training, improves\nperceptual metrics (FID and IS), and prevents overfitting. Our experiments also\nsuggest that long-range dependencies are handled by deep layers, while shallow\nlayers take care of short-term features.\n  Evaluated on raw audio, which naturally features long sequence lengths,\nPoolformer outperforms state-of-the-art models such as SaShiMi and Mamba.\nFuture directions include applications to text and vision, as well as\nmulti-modal scenarios, where a Poolformer-based LLM could effectively process\ndense representations of images and videos.", "AI": {"tldr": "Poolformer replaces self-attention with pooling-based recurrent layers in a recursive SkipBlock architecture to efficiently model very long sequences, achieving faster training and improved perceptual metrics on raw audio, and outperforming SOTA models like SaShiMi and Mamba; potential extensions to text, vision, and multimodal settings.", "motivation": "Self-attention scales quadratically with sequence length, limiting practicality for long sequences. A pooling-based, recurrent alternative aims to accelerate training, reduce memory, and capture long-range dependencies for audio and other modalities.", "method": "Introduce Poolformer with SkipBlocks consisting of residual blocks, a down-pooling layer, a nested SkipBlock, an up-pooling layer, and additional residual blocks. Replace standard self-attention with pooling operations to exchange information along the time dimension. Use a recursive architecture and extensive experiments to validate design choices.", "result": "Pooling accelerates training, improves perceptual metrics (FID and IS), and prevents overfitting. Long-range dependencies are captured by deeper layers, while shallow layers handle short-term features. On raw audio, Poolformer outperforms state-of-the-art models like SaShiMi and Mamba.", "conclusion": "Pooling-based sequence modeling is a viable alternative to self-attention for long sequences. Potential applications include text and vision, as well as multi-modal scenarios, where Poolformer-based models (e.g., LLMs) could process dense representations of images and videos."}}
{"id": "2510.02209", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02209", "abs": "https://arxiv.org/abs/2510.02209", "authors": ["Yanxu Chen", "Zijun Yao", "Yantao Liu", "Jin Ye", "Jianing Yu", "Lei Hou", "Juanzi Li"], "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.", "AI": {"tldr": "StockBench is a contamination-free, multi-month stock trading benchmark for evaluating LLM agents; results show most models underperform simple buy-and-hold, though some show potential in returns and risk management; released as an open-source resource.", "motivation": "LLMs have shown promise as autonomous agents, but finance lacks dynamic, multi-step evaluation benchmarks. A realistic trading environment is needed to assess sequential decision-making, risk management, and real economic value beyond static knowledge tasks.", "method": "A contamination-free benchmark where agents receive daily market signals (prices, fundamentals, news) and make sequential buy/sell/hold decisions over multi-month horizons. Evaluation uses financial metrics (cumulative return, maximum drawdown, Sortino ratio) across several models (proprietary and open-weight).", "result": "Most LLM agents struggle to outperform a buy-and-hold strategy, but several models show potential to deliver higher returns and better risk control. Highlights that excelling at static financial knowledge does not guarantee trading success. StockBench is released as an open-source resource to support reproducibility.", "conclusion": "StockBench provides a realistic, reusable framework for evaluating LLM-powered financial agents, revealing both challenges and opportunities in translating static knowledge into dynamic trading performance and guiding future research."}}
{"id": "2510.02286", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02286", "abs": "https://arxiv.org/abs/2510.02286", "authors": ["Ruohao Guo", "Afshin Oroojlooy", "Roshan Sridhar", "Miguel Ballesteros", "Alan Ritter", "Dan Roth"], "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks", "comment": null, "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.", "AI": {"tldr": "DialTree-RPO proposes an on-policy reinforcement learning framework with tree search to autonomously discover diverse multi-turn attack strategies for LLM safety, achieving a notable improvement in attack success and revealing novel tactics.", "motivation": "There is a gap in current safety evaluation: most work focuses on single-turn attacks or manual/templated data; multi-turn, strategically crafted dialogues reveal greater vulnerabilities and require scalable, automated exploration.", "method": "Treat dialogue as a sequential decision-making process and use on-policy reinforcement learning integrated with tree search to explore diverse multi-turn attack trajectories without manual curated data; optimize policies to maximize attack success across turns.", "result": "The approach yields over 25.9% higher adversarial success rate (ASR) across 10 target models compared to prior state-of-the-art methods and uncovers new attack strategies by learning optimal dialogue policies that maximize success over multiple turns.", "conclusion": "An effective framework for automated discovery of multi-turn adversarial strategies, underscoring the importance of sequential planning and RL-based exploration in safety evaluation and suggesting directions to fortify LLM defenses against such attacks."}}
{"id": "2510.02215", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02215", "abs": "https://arxiv.org/abs/2510.02215", "authors": ["Mertcan Cokbas", "Ziteng Liu", "Zeyi Tao", "Chengkai Zhang", "Elder Veliz", "Qin Huang", "Ellie Wen", "Huayu Li", "Qiang Jin", "Murat Duman", "Benjamin Au", "Guy Lebanon", "Sagar Chordia"], "title": "C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale Recommendation Systems", "comment": "Submitted to ICLR 2026", "summary": "Training large-scale recommendation models under a single global objective\nimplicitly assumes homogeneity across user populations. However, real-world\ndata are composites of heterogeneous cohorts with distinct conditional\ndistributions. As models increase in scale and complexity and as more data is\nused for training, they become dominated by central distribution patterns,\nneglecting head and tail regions. This imbalance limits the model's learning\nability and can result in inactive attention weights or dead neurons. In this\npaper, we reveal how the attention mechanism can play a key role in\nfactorization machines for shared embedding selection, and propose to address\nthis challenge by analyzing the substructures in the dataset and exposing those\nwith strong distributional contrast through auxiliary learning. Unlike previous\nresearch, which heuristically applies weighted labels or multi-task heads to\nmitigate such biases, we leverage partially conflicting auxiliary labels to\nregularize the shared representation. This approach customizes the learning\nprocess of attention layers to preserve mutual information with minority\ncohorts while improving global performance. We evaluated C2AL on massive\nproduction datasets with billions of data points each for six SOTA models.\nExperiments show that the factorization machine is able to capture fine-grained\nuser-ad interactions using the proposed method, achieving up to a 0.16%\nreduction in normalized entropy overall and delivering gains exceeding 0.30% on\ntargeted minority cohorts.", "AI": {"tldr": "C2AL introduces partially conflicting auxiliary learning to regulate shared embeddings via attention in factorization machines, aiming to preserve minority-cohort information and improve overall performance in heterogeneous, large-scale recommender data.", "motivation": "Training under a single global objective can ignore minority/cohort-specific distributions in large, heterogeneous data. The attention mechanism offers a path to selectively share embeddings, but without principled regularization models risk dead neurons and biased attention toward dominant cohorts.", "method": "Identify substructures with strong distributional contrast in the data and apply auxiliary learning using partially conflicting labels to regularize the shared representation and adapt attention accordingly, instead of heuristic reweighting or multi-task heads. Evaluated on massive production datasets across six SOTA models.", "result": "The approach enables the factorization machine to capture fine-grained user-ad interactions, achieving up to a 0.16% reduction in normalized entropy overall and gains over 0.30% on targeted minority cohorts.", "conclusion": "Partial-conflict auxiliary learning for attention-based sharing provides a principled regularization that preserves mutual information with minority cohorts while improving global performance, offering a scalable alternative to heuristic bias mitigation in large-scale recommender models."}}
{"id": "2510.02297", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02297", "abs": "https://arxiv.org/abs/2510.02297", "authors": ["Wentao Zhang", "Yang Young Lu", "Yuntian Deng"], "title": "Interactive Training: Feedback-Driven Neural Network Optimization", "comment": "EMNLP 2025 Demo", "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.", "AI": {"tldr": "Real-time interactive training framework enabling feedback-driven interventions via a control server to adjust hyperparameters, data, and checkpoints during training, improving stability and adaptability.", "motivation": "Address rigidity of traditional fixed-recipe training that lacks responsiveness to instabilities and emergent issues; enable dynamic responses from humans or automated agents.", "method": "Proposes Interactive Training framework with a control server mediating communication between users/AI agents and the training process; supports dynamic adjustments to optimizer hyperparameters, training data, and model checkpoints; demonstrated through three case studies.", "result": "Demonstrates superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving requirements; suggests potential for autonomous AI agents to monitor logs and proactively resolve instabilities.", "conclusion": "Open-source framework enabling real-time, feedback-driven training, paving the way for a future where AI agents autonomously monitor and optimize training dynamics."}}
{"id": "2510.02216", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.02216", "abs": "https://arxiv.org/abs/2510.02216", "authors": ["Zeqi Ye", "Minshuo Chen"], "title": "Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification", "comment": "49 pages, 4 figures. Accepted as a poster at NeurIPS 2025", "summary": "Imputation methods play a critical role in enhancing the quality of practical\ntime-series data, which often suffer from pervasive missing values. Recently,\ndiffusion-based generative imputation methods have demonstrated remarkable\nsuccess compared to autoregressive and conventional statistical approaches.\nDespite their empirical success, the theoretical understanding of how well\ndiffusion-based models capture complex spatial and temporal dependencies\nbetween the missing values and observed ones remains limited. Our work\naddresses this gap by investigating the statistical efficiency of conditional\ndiffusion transformers for imputation and quantifying the uncertainty in\nmissing values. Specifically, we derive statistical sample complexity bounds\nbased on a novel approximation theory for conditional score functions using\ntransformers, and, through this, construct tight confidence regions for missing\nvalues. Our findings also reveal that the efficiency and accuracy of imputation\nare significantly influenced by the missing patterns. Furthermore, we validate\nthese theoretical insights through simulation and propose a mixed-masking\ntraining strategy to enhance the imputation performance.", "AI": {"tldr": "This work develops a theory for conditional diffusion transformer imputation, offering sample\u2011size bounds and uncertainty quantification, showing that missing-value patterns influence performance, validated by simulations and aided by a mixed\u2011masking training strategy.", "motivation": "Practical time-series often have missing values. While diffusion-based generative imputation has shown empirical success, there is limited theoretical understanding of how well these models capture spatial/temporal dependencies and quantify uncertainty in the missing entries.", "method": "Derives statistical sample complexity bounds via a novel approximation theory for conditional score functions using transformers; constructs tight confidence regions for missing values; analyzes how missing patterns affect efficiency; validates via simulations; proposes a mixed-masking training strategy to boost imputation performance.", "result": "Theoretical insights indicate that imputation efficiency and accuracy depend on the missing data pattern. Tight confidence regions for missing values are constructed. Simulation validates the theory. The mixed-masking training strategy is proposed to enhance performance.", "conclusion": "The paper advances the theoretical understanding of diffusion-based imputation by linking sample efficiency and uncertainty quantification to missingness patterns, and offers a practical training strategy (mixed masking) to improve performance, supported by simulation evidence."}}
{"id": "2510.02224", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02224", "abs": "https://arxiv.org/abs/2510.02224", "authors": ["Ethan Baron", "Boris Oreshkin", "Ruijun Ma", "Hanyu Zhang", "Kari Torkkola", "Michael W. Mahoney", "Andrew Gordon Wilson", "Tatiana Konstantinova"], "title": "Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models", "comment": null, "summary": "Many time series applications require access to multi-step forecast\ntrajectories in the form of sample paths. Recently, time series foundation\nmodels have leveraged multi-step lookahead predictions to improve the quality\nand efficiency of multi-step forecasts. However, these models only predict\nindependent marginal distributions for each time step, rather than a full joint\npredictive distribution. To generate forecast sample paths with realistic\ncorrelation structures, one typically resorts to autoregressive sampling, which\ncan be extremely expensive. In this paper, we present a copula-based approach\nto efficiently generate accurate, correlated sample paths from existing\nmulti-step time series foundation models in one forward pass. Our copula-based\napproach generates correlated sample paths orders of magnitude faster than\nautoregressive sampling, and it yields improved sample path quality by\nmitigating the snowballing error phenomenon.", "AI": {"tldr": "Proposes a copula-based method to generate correlated multi-step forecast sample paths in one forward pass, achieving speedups over autoregressive sampling while improving path quality.", "motivation": "Current time series foundation models predict only independent marginal distributions per time step, lacking a joint predictive distribution. Generating correlated sample paths via autoregressive sampling is computationally expensive and prone to error accumulation (snowballing).", "method": "Utilize a copula to couple the per-step marginals from existing multi-step foundation models to produce coherent joint sample paths in a single forward pass, avoiding iterative autoregressive sampling.", "result": "The approach is orders of magnitude faster than autoregressive sampling and yields improved sample-path quality by mitigating snowballing errors.", "conclusion": "A copula-based framework enables efficient, accurate generation of joint multi-step forecast sample paths from time series foundation models without iterative sampling."}}
{"id": "2510.02305", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.02305", "abs": "https://arxiv.org/abs/2510.02305", "authors": ["Tyler Farghly", "Peter Potaptchik", "Samuel Howard", "George Deligiannidis", "Jakiw Pidstrigach"], "title": "Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive", "comment": null, "summary": "Diffusion models have achieved state-of-the-art performance, demonstrating\nremarkable generalisation capabilities across diverse domains. However, the\nmechanisms underpinning these strong capabilities remain only partially\nunderstood. A leading conjecture, based on the manifold hypothesis, attributes\nthis success to their ability to adapt to low-dimensional geometric structure\nwithin the data. This work provides evidence for this conjecture, focusing on\nhow such phenomena could result from the formulation of the learning problem\nthrough score matching. We inspect the role of implicit regularisation by\ninvestigating the effect of smoothing minimisers of the empirical score\nmatching objective. Our theoretical and empirical results confirm that\nsmoothing the score function -- or equivalently, smoothing in the log-density\ndomain -- produces smoothing tangential to the data manifold. In addition, we\nshow that the manifold along which the diffusion model generalises can be\ncontrolled by choosing an appropriate smoothing.", "AI": {"tldr": "Smoothing the score function in score-matching induces manifold-aligned regularisation, explaining diffusion models' generalisation and enabling control of the generalisation manifold via smoothing.", "motivation": "To explain why diffusion models generalise well by relating it to the manifold hypothesis and score-matching regularisation.", "method": "Theoretical and empirical analysis of smoothing minimisers of the empirical score matching objective; evaluation of smoothing in the log-density domain and its geometric effect on the data manifold.", "result": "Smoothing the score function yields smoothing tangential to the data manifold; the learned diffusion generalisation manifold can be steered by the choice of smoothing.", "conclusion": "The work provides evidence that implicit regularisation through score-matching smoothing explains and controls diffusion model generalisation on low-dimensional manifolds."}}
{"id": "2510.02228", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02228", "abs": "https://arxiv.org/abs/2510.02228", "authors": ["Maximilian Beck", "Kajetan Schweighofer", "Sebastian B\u00f6ck", "Sebastian Lehner", "Sepp Hochreiter"], "title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity", "comment": "Code and data available at\n  https://github.com/NX-AI/xlstm_scaling_laws", "summary": "Scaling laws play a central role in the success of Large Language Models\n(LLMs), enabling the prediction of model performance relative to compute\nbudgets prior to training. While Transformers have been the dominant\narchitecture, recent alternatives such as xLSTM offer linear complexity with\nrespect to context length while remaining competitive in the billion-parameter\nregime. We conduct a comparative investigation on the scaling behavior of\nTransformers and xLSTM along the following lines, providing insights to guide\nfuture model design and deployment. First, we study the scaling behavior for\nxLSTM in compute-optimal and over-training regimes using both IsoFLOP and\nparametric fit approaches on a wide range of model sizes (80M-7B) and number of\ntraining tokens (2B-2T). Second, we examine the dependence of optimal model\nsizes on context length, a pivotal aspect that was largely ignored in previous\nwork. Finally, we analyze inference-time scaling characteristics. Our findings\nreveal that in typical LLM training and inference scenarios, xLSTM scales\nfavorably compared to Transformers. Importantly, xLSTM's advantage widens as\ntraining and inference contexts grow.", "AI": {"tldr": "xLSTM shows favorable scaling compared to Transformers for LLMs, with its advantage increasing as context length grows, across compute-optimal and over-training regimes.", "motivation": "To understand how alternative architectures like xLSTM scale relative to Transformers under compute budgets and varying context lengths, to inform model design and deployment decisions for large-scale language models.", "method": "Systematic scaling analysis comparing Transformers and xLSTM across model sizes (80M\u20137B) and training tokens (2B\u20132T) using compute-optimal and over-training regimes. Uses IsoFLOP and parametric fit approaches, examines optimal model size versus context length, and analyzes inference-time scaling.", "result": "xLSTM demonstrates favorable scaling relative to Transformers in typical LLM training and inference scenarios, with its advantage increasing as the context length (both training and inference) grows.", "conclusion": "xLSTM\u2019s linear context-length scalability yields stronger scaling advantages than Transformers in practical LLM settings, suggesting it as a competitive alternative for future model design and deployment, especially in long-context regimes."}}
{"id": "2510.02236", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02236", "abs": "https://arxiv.org/abs/2510.02236", "authors": ["Ricardo Misael Ayala Molina", "Hyame Assem Alameddine", "Makan Pourzandi", "Chadi Assi"], "title": "PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks", "comment": "13 pages, 7 figures, 4 tables, journal paper", "summary": "Network Slices (NSs) are virtual networks operating over a shared physical\ninfrastructure, each designed to meet specific application requirements while\nmaintaining consistent Quality of Service (QoS). In Fifth Generation (5G)\nnetworks, User Equipment (UE) can connect to and seamlessly switch between\nmultiple NSs to access diverse services. However, this flexibility, known as\nInter-Slice Switching (ISS), introduces a potential vulnerability that can be\nexploited to launch Distributed Slice Mobility (DSM) attacks, a form of\nDistributed Denial of Service (DDoS) attack. To secure 5G networks and their\nNSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an\nanomaly detection solution that leverages Positive Unlabeled Learning (PUL) and\nincorporates a combination of Long Short-Term Memory Autoencoders and K-Means\nclustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership\nProject (3GPP) key performance indicators and performance measurement counters\nas features for its machine learning models to detect DSM attack variants while\nmaintaining robustness in the presence of contaminated training data. When\nevaluated on data collected from our 5G testbed based on the open-source\nfree5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;\nPUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training\ndatasets with 10% to 40% attack contamination, consistently outperforming its\ncounterpart Inter-Slice Defender and other PUL based solutions combining\nOne-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.", "AI": {"tldr": "A defense against DSM attacks in 5G network slices using a PU-learning based detector that combines LSTM autoencoders and K-means, achieving high F1-scores even with contaminated training data.", "motivation": "DSM attacks via inter-slice switching threaten QoS and uptime in 5G network slicing; conventional supervised methods struggle when training data is contaminated, necessitating robust anomaly detection leveraging unlabeled data.", "method": "Positive Unlabeled Learning (PUL) framework featuring Long Short-Term Memory Autoencoders and K-Means clustering, using 3GPP KPIs and performance counters as features; evaluated on a 5G testbed with free5GC and UERANSIM.", "result": "F1-scores exceed 98.50% on training data with 10%-40% attack contamination; outperforms Inter-Slice Defender and other PU-based baselines using OCSVM/RF/XGBoost.", "conclusion": "The proposed PUL-Inter-Slice Defender provides robust DSM attack detection in 5G NSs under label contamination and data variability, with strong performance advantages over existing PU-based methods."}}
{"id": "2510.02239", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02239", "abs": "https://arxiv.org/abs/2510.02239", "authors": ["Kaja Gruntkowska", "Yassine Maziane", "Zheng Qu", "Peter Richt\u00e1rik"], "title": "Drop-Muon: Update Less, Converge Faster", "comment": null, "summary": "Conventional wisdom in deep learning optimization dictates updating all\nlayers at every step-a principle followed by all recent state-of-the-art\noptimizers such as Muon. In this work, we challenge this assumption, showing\nthat full-network updates can be fundamentally suboptimal, both in theory and\nin practice. We introduce a non-Euclidean Randomized Progressive Training\nmethod-Drop-Muon-a simple yet powerful framework that updates only a subset of\nlayers per step according to a randomized schedule, combining the efficiency of\nprogressive training with layer-specific non-Euclidean updates for top-tier\nperformance. We provide rigorous convergence guarantees under both layer-wise\nsmoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and\nstochastic gradient settings, marking the first such results for progressive\ntraining in the stochastic and non-smooth regime. Our cost analysis further\nreveals that full-network updates are not optimal unless a very specific\nrelationship between layer smoothness constants holds. Through controlled CNN\nexperiments, we empirically demonstrate that Drop-Muon consistently outperforms\nfull-network Muon, achieving the same accuracy up to $1.4\\times$ faster in\nwall-clock time. Together, our results suggest a shift in how large-scale\nmodels can be efficiently trained, challenging the status quo and offering a\nhighly efficient, theoretically grounded alternative to full-network updates.", "AI": {"tldr": "Drop-Muon introduces randomized, layer-wise updates (Drop-Muon) with non-Euclidean updates for progressive training, showing faster convergence than full-network updates under layer-wise smoothness assumptions; provides convergence guarantees in stochastic and non-smooth settings and supports a cost argument suggesting full updates are suboptimal except in specific conditions; empirical CNNs show up to 1.4x faster wall-clock time at equal accuracy.", "motivation": "Question the conventional wisdom that full-network updates are always optimal; seek more sample- and time-efficient training by updating only a subset of layers per step with principled scheduling and non-Euclidean updates.", "method": "Propose Drop-Muon: a randomized progressive training framework that updates a subset of layers per step based on a randomized schedule and applies non-Euclidean (e.g., non-Euclidean geometry-based) updates. Provide convergence guarantees under layer-wise smoothness and layer-wise (L^0, L^1)-smoothness for both deterministic and stochastic gradients. Conduct a cost analysis comparing full-network vs. partial updates. Validate with controlled CNN experiments.", "result": "Theoretical results establish convergence guarantees for progressive training in stochastic and non-smooth regimes; cost analysis indicates full-network updates are optimal only under specific relationships between layer smoothness constants. Empirical CNN experiments show Drop-Muon consistently outperforms full Muon, achieving the same accuracy up to 1.4\u00d7 faster in wall-clock time.", "conclusion": "Layer-wise, randomized progressive training with non-Euclidean updates can outperform traditional full-network optimizers, offering a theoretically grounded and practically efficient alternative for training large-scale models. This challenges the default of updating all layers each step and highlights the importance of scheduling and geometry-aware updates."}}
{"id": "2510.02259", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.02259", "abs": "https://arxiv.org/abs/2510.02259", "authors": ["Tobias Kreiman", "Yutong Bai", "Fadi Atieh", "Elizabeth Weaver", "Eric Qu", "Aditi S. Krishnapriyan"], "title": "Transformers Discover Molecular Structure Without Graph Priors", "comment": null, "summary": "Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinates$\\unicode{x2013}$without predefined graphs or physical\npriors$\\unicode{x2013}$can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatterns$\\unicode{x2013}$such as attention weights that decay inversely with\ninteratomic distance$\\unicode{x2013}$and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling.", "AI": {"tldr": "Transformers trained directly on Cartesian coordinates can match fixed-graph GNNs for molecular energies and forces, revealing emergent physically meaningful patterns and scalable performance without hard-coded graph priors.", "motivation": "Probe whether unmodified Transformers can learn molecular energies/forces without predefined graphs or physical priors, and compare to state-of-the-art equivariant GNNs under matched compute budgets.", "method": "Train a standard Transformer on Cartesian coordinates to predict energies and forces; compare to a state-of-the-art equivariant GNN on the OMol25 dataset under matched training compute; analyze learned patterns (e.g., attention) and scaling behavior.", "result": "The Transformer achieves competitive energy and force MAEs under matched compute on OMol25; attention weights decay inversely with interatomic distance and adapt across molecular environments without fixed biases; scaling with training resources yields predictable improvements consistent with broader scaling laws.", "conclusion": "Hard-coded graph inductive biases are not strictly necessary for competitive molecular modeling; Transformers can learn physically meaningful representations and offer scalable, standardized architectures for molecular modeling."}}
{"id": "2510.02274", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02274", "abs": "https://arxiv.org/abs/2510.02274", "authors": ["Kyoungjun Park", "Yifan Yang", "Changhan Ge", "Lili Qiu", "Shiqi Jiang"], "title": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps", "comment": null, "summary": "Modeling radio frequency (RF) signal propagation is essential for\nunderstanding the environment, as RF signals offer valuable insights beyond the\ncapabilities of RGB cameras, which are limited by the visible-light spectrum,\nlens coverage, and occlusions. It is also useful for supporting wireless\ndiagnosis, deployment, and optimization. However, accurately predicting RF\nsignals in complex environments remains a challenge due to interactions with\nobstacles such as absorption and reflection. We introduce Diffusion^2, a\ndiffusion-based approach that uses 3D point clouds to model the propagation of\nRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.\nTo effectively capture RF-related features from 3D data, we present the RF-3D\nEncoder, which encapsulates the complexities of 3D geometry along with\nsignal-specific details. These features undergo multi-scale embedding to\nsimulate the actual RF signal dissemination process. Our evaluation, based on\nsynthetic and real-world measurements, demonstrates that Diffusion^2 accurately\nestimates the behavior of RF signals in various frequency bands and\nenvironmental conditions, with an error margin of just 1.9 dB and 27x faster\nthan existing methods, marking a significant advancement in the field. Refer to\nhttps://rfvision-project.github.io/ for more information.", "AI": {"tldr": "Diffusion^2 uses diffusion-based modeling with a 3D point-cloud RF-3D Encoder to predict RF propagation across Wi-Fi to millimeter-wave bands, achieving 1.9 dB error and 27x speedups over prior methods on synthetic and real data.", "motivation": "RF propagation modeling is essential for wireless diagnosis, deployment, and optimization, and current RGB-based methods struggle in environments with complex obstacles due to absorption and reflection. A physics-informed, scalable approach that covers a wide frequency range is needed.", "method": "A diffusion-based framework (Diffusion^2) that operates on 3D point clouds. It uses an RF-3D Encoder to capture geometry and signal-specific features, followed by multi-scale embedding to simulate RF dissemination. The model is trained/evaluated on synthetic and real-world measurements across multiple frequency bands.", "result": "Diffusion^2 accurately estimates RF signal behavior across Wi-Fi to millimeter-wave bands under various environmental conditions, with an error of about 1.9 dB and speed improvements of ~27\u00d7 over existing methods.", "conclusion": "Diffusion^2 represents a significant advancement in RF propagation modeling by integrating diffusion processes with detailed 3D geometry representations, unlocking more accurate and efficient wireless analysis and deployment in diverse environments."}}
{"id": "2510.02278", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02278", "abs": "https://arxiv.org/abs/2510.02278", "authors": ["Fedor Velikonivtsev", "Oleg Platonov", "Gleb Bazhenov", "Liudmila Prokhorenkova"], "title": "Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks", "comment": null, "summary": "Traffic forecasting on road networks is a complex task of significant\npractical importance that has recently attracted considerable attention from\nthe machine learning community, with spatiotemporal graph neural networks\n(GNNs) becoming the most popular approach. The proper evaluation of traffic\nforecasting methods requires realistic datasets, but current publicly available\nbenchmarks have significant drawbacks, including the absence of information\nabout road connectivity for road graph construction, limited information about\nroad properties, and a relatively small number of road segments that falls\nshort of real-world applications. Further, current datasets mostly contain\ninformation about intercity highways with sparsely located sensors, while city\nroad networks arguably present a more challenging forecasting task due to much\ndenser roads and more complex urban traffic patterns. In this work, we provide\na more complete, realistic, and challenging benchmark for traffic forecasting\nby releasing datasets representing the road networks of two major cities, with\nthe largest containing almost 100,000 road segments (more than a 10-fold\nincrease relative to existing datasets). Our datasets contain rich road\nfeatures and provide fine-grained data about both traffic volume and traffic\nspeed, allowing for building more holistic traffic forecasting systems. We show\nthat most current implementations of neural spatiotemporal models for traffic\nforecasting have problems scaling to datasets of our size. To overcome this\nissue, we propose an alternative approach to neural traffic forecasting that\nuses a GNN without a dedicated module for temporal sequence processing, thus\nachieving much better scalability, while also demonstrating stronger\nforecasting performance. We hope our datasets and modeling insights will serve\nas a valuable resource for research in traffic forecasting.", "AI": {"tldr": "New city-scale road-graph benchmarks (up to ~100k road segments) with rich features for traffic forecasting; proposes a scalable GNN approach that omits a dedicated temporal module, addressing scalability issues in existing neural spatiotemporal models.", "motivation": "Current traffic forecasting benchmarks are limited: missing road connectivity, sparse/limited road properties, and datasets far smaller than real-world urban networks. There is a need for realistic, dense benchmarks and scalable models.", "method": "Release two city-scale road network datasets with detailed traffic volume and speed features; evaluate existing neural spatiotemporal GNNs on these datasets; propose a scalable GNN architecture without a specialized temporal processing component to improve scalability and performance.", "result": "Demonstrates that state-of-the-art neural spatiotemporal models struggle to scale to large city-scale datasets; the proposed approach achieves better scalability and stronger forecasting performance on the new benchmarks; datasets enable holistic modeling of traffic dynamics.", "conclusion": "Providing large, realistic benchmarks and a scalable modeling approach will advance traffic forecasting research and facilitate more effective evaluation and deployment in real urban settings."}}
{"id": "2510.02302", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02302", "abs": "https://arxiv.org/abs/2510.02302", "authors": ["Qin Shi", "Amber Yijia Zheng", "Qifan Song", "Raymond A. Yeh"], "title": "Knowledge Distillation Detection for Open-weights Models", "comment": "NeurIPS 2025", "summary": "We propose the task of knowledge distillation detection, which aims to\ndetermine whether a student model has been distilled from a given teacher,\nunder a practical setting where only the student's weights and the teacher's\nAPI are available. This problem is motivated by growing concerns about model\nprovenance and unauthorized replication through distillation. To address this\ntask, we introduce a model-agnostic framework that combines data-free input\nsynthesis and statistical score computation for detecting distillation. Our\napproach is applicable to both classification and generative models.\nExperiments on diverse architectures for image classification and text-to-image\ngeneration show that our method improves detection accuracy over the strongest\nbaselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image\ngeneration. The code is available at\nhttps://github.com/shqii1j/distillation_detection.", "AI": {"tldr": "A model-agnostic framework detects whether a student model has been distilled from a teacher using only student weights and the teacher API, via data-free input synthesis and statistical scoring; applicable to classification and generative models, with strong empirical gains on benchmarks; code available.", "motivation": "Growing concerns about model provenance and unauthorized replication through distillation in settings where training data and full access to teachers are limited.", "method": "A model-agnostic pipeline that combines data-free input synthesis with statistical score computation to detect distillation, applicable to both classification and generative models.", "result": "Significant detection performance gains over strong baselines: 59.6% improvement on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image generation.", "conclusion": "The proposed framework enables practical, data-free detection of knowledge distillation across model types, contributing to provenance verification and defenses against distillation-based replication."}}
{"id": "2510.02308", "categories": ["cs.LG", "math.DG"], "pdf": "https://arxiv.org/pdf/2510.02308", "abs": "https://arxiv.org/abs/2510.02308", "authors": ["Dhruv Kohli", "Sawyer J. Robertson", "Gal Mishne", "Alexander Cloninger"], "title": "Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization", "comment": null, "summary": "Estimating the tangent spaces of a data manifold is a fundamental problem in\ndata analysis. The standard approach, Local Principal Component Analysis\n(LPCA), struggles in high-noise settings due to a critical trade-off in\nchoosing the neighborhood size. Selecting an optimal size requires prior\nknowledge of the geometric and noise characteristics of the data that are often\nunavailable. In this paper, we propose a spectral method, Laplacian Eigenvector\nGradient Orthogonalization (LEGO), that utilizes the global structure of the\ndata to guide local tangent space estimation. Instead of relying solely on\nlocal neighborhoods, LEGO estimates the tangent space at each data point by\northogonalizing the gradients of low-frequency eigenvectors of the graph\nLaplacian. We provide two theoretical justifications of our method. First, a\ndifferential geometric analysis on a tubular neighborhood of a manifold shows\nthat gradients of the low-frequency Laplacian eigenfunctions of the tube align\nclosely with the manifold's tangent bundle, while an eigenfunction with high\ngradient in directions orthogonal to the manifold lie deeper in the spectrum.\nSecond, a random matrix theoretic analysis also demonstrates that low-frequency\neigenvectors are robust to sub-Gaussian noise. Through comprehensive\nexperiments, we demonstrate that LEGO yields tangent space estimates that are\nsignificantly more robust to noise than those from LPCA, resulting in marked\nimprovements in downstream tasks such as manifold learning, boundary detection,\nand local intrinsic dimension estimation.", "AI": {"tldr": "LEGO uses gradients of low-frequency Laplacian eigenvectors to orthogonalize local tangent spaces, yielding robust tangent estimates against noise compared to LPCA.", "motivation": "LPCA's sensitivity to noise and the need to pick neighborhood size without prior geometric/noise knowledge hinder reliable tangent-space estimation; a method leveraging global data structure is desirable.", "method": "Construct a graph Laplacian from the data, obtain low-frequency eigenvectors, compute their gradients, and orthogonalize these gradients to estimate the tangent space at each data point. The paper provides two theoretical justifications: (1) differential-geometric analysis in a tubular neighborhood showing alignment of gradients with the tangent bundle; (2) random-matrix theory showing low-frequency eigenvectors are robust to sub-Gaussian noise.", "result": "LEGO yields tangent-space estimates that are more robust to noise than LPCA, leading to improvements in downstream tasks such as manifold learning, boundary detection, and local intrinsic-dimension estimation.", "conclusion": "Spectral gradient orthogonalization provides a robust, globally informed approach to tangent-space estimation, with solid theoretical backing and practical benefits across several data-analysis tasks."}}
{"id": "2510.02312", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02312", "abs": "https://arxiv.org/abs/2510.02312", "authors": ["Anna Kuzina", "Maciej Pioro", "Paul N. Whatmough", "Babak Ehteshami Bejnordi"], "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation", "comment": "Preprint. Under Review", "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.", "AI": {"tldr": "KaVa distills knowledge from a teacher\u2019s compressed KV-cache into a latent-reasoning student via self-distillation, using continuous latent tokens to align stepwise KV trajectories for efficient latent reasoning.", "motivation": "To overcome the high cost and artifacts of explicit chain-of-thought traces by providing scalable supervision for latent reasoning, addressing the lack of effective guidance for latent traces on complex natural-language reasoning tasks.", "method": "Distill from a teacher\u2019s compressed KV-cache into a latent-reasoning student using continuous latent tokens. Align stepwise KV trajectories through self-distillation, treating abstract KV-cache knowledge as supervisory signal despite lacking direct token correspondence. Evaluate against strong latent baselines across natural-language traces and scale to larger backbones.", "result": "KaVa consistently outperforms strong latent baselines, exhibits less degradation when moving from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency.", "conclusion": "Compressed KV-cache distillation provides a scalable supervision signal for latent reasoning, enabling CoT-level accuracy with latent inference efficiency and improving deployability."}}
