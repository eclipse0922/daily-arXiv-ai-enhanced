<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]
- [cs.LG](#cs.LG) [Total: 146]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.AI](#cs.AI) [Total: 51]
- [cs.RO](#cs.RO) [Total: 41]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andr√©s Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: A zero-shot, plug-and-play solver (LVTINO) for high-definition video restoration using Video Consistency Models (VCMs). It overcomes frame-by-frame limitations of image LDM priors by enforcing temporal causality, achieving state-of-the-art reconstruction quality with few neural evaluations and strong measurement consistency.


<details>
  <summary>Details</summary>
Motivation: To address temporal inconsistency and limited spatial detail when applying image-based diffusion priors frame-by-frame to HD video restoration; leverage Video Consistency Models to capture temporal dependencies and improve efficiency.

Method: Develop LVTINO, a zero-shot/plug-and-play inverse solver conditioned on VCM priors. It bypasses automatic differentiation, uses a lightweight conditioning mechanism, and relies on few neural function evaluations to ensure measurement consistency and smooth temporal transitions across frames.

Result: Significant perceptual improvements over frame-by-frame image LDM methods, achieving state-of-the-art video reconstruction quality and improved computational efficiency; establishes a new benchmark in reconstruction fidelity and efficiency across diverse video inverse problems.

Conclusion: LVTINO demonstrates that integrating Video Consistency Models into zero-shot video restoration yields superior temporal coherence and detail with low computational cost, enabling practical, high-quality HD video restoration using pre-trained LDM priors.

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: Three-stage style-extraction method for injecting a single-reference style into pretrained text-to-image models, via a style encoder and style projection layer, enabling fine-grained style-guided generation; introduces Style30k-captions dataset.


<details>
  <summary>Details</summary>
Motivation: Fine-grained styles are hard to describe with natural language; aligning stylized reference images with textual guidance is challenging; objective to maximize generative capability without altering model architecture.

Method: Three-stage training; use a style encoder and style projection layer to align style representations with textual representations; train on Style30k-captions dataset consisting of image, style labels, and text descriptions to supervise the components.

Result: Not stated in the abstract.

Conclusion: Not stated in the abstract.

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: A dataset and approach for struggle detection in skill learning using temporal action localization; EvoStruggle shows transferable struggle cues across tasks with room for improvement.


<details>
  <summary>Details</summary>
Motivation: Understanding how struggle evolves during skill acquisition to optimize learning and enable adaptive assistance; existing datasets neglect the temporal evolution of struggle.

Method: Collected 61.68 hours of video (2,793 videos, 5,385 annotated struggle segments) from 76 participants across 18 tasks in four activities (tying knots, origami, tangram puzzles, shuffling cards). Participants repeated tasks five times to capture skill evolution. Framed as temporal action localization to identify start/end struggle segments and evaluated generalization to unseen tasks/activities.

Result: Temporal action localization models detected struggle segments with cross-task mAP of 34.56% and cross-activity mAP of 19.24%, indicating transferable yet task/activity-dependent struggle cues. Dataset is publicly available.

Conclusion: Struggle signals transfer across diverse skill domains but remain challenging to detect; EvoStruggle provides a valuable resource for advancing adaptive learning and assistive systems, with temporal localization as a viable detection method.

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: A lightweight residual U-Net-based foundation model (SPUS) learns via autoregressive pretraining to solve a wide range of PDEs, achieving state-of-the-art generalization on unseen tasks with far fewer parameters and minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current PDE foundation models rely on large transformers, which are resource-intensive. There is a need for compact, parameter-efficient models that generalize across diverse PDEs and require limited fine-tuning data.

Method: Propose SPUS, a compact residual U-Net architecture pretrained autoregressively to mimic numerical solver behavior, trained on diverse fluid dynamics PDEs, and evaluated on six unseen downstream PDEs from various physical systems.

Result: SPUS achieves state-of-the-art generalization on unseen PDEs while using significantly fewer parameters and minimal fine-tuning data.

Conclusion: SPUS demonstrates the viability of a parameter-efficient FM for solving diverse PDEs, highlighting the potential of lightweight architectures for broad PDE solvers.

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo is an RL-based framework that optimizes identity diversity in multi-human generation, outperforming existing models on DiverseHumans with high unique-face accuracy and strong identity spread, without extra annotations.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models produce realistic images but fail on prompts involving multiple humans, causing duplicated faces, merged identities, and miscounts. There is a need for a scalable, annotation-free method that ensures identity diversity and accurate counts.

Method: DisCo fine-tunes flow-matching models using Group-Relative Policy Optimization (GRPO) with a compositional reward: penalize intra-image facial similarity, discourage cross-sample identity repetition, enforce correct person counts, and preserve visual fidelity via human-preference signals; trained with a single-stage curriculum to stabilize learning and requires no additional annotations.

Result: On the DiverseHumans Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global Identity Spread, outperforming both open-source and proprietary methods (e.g., Gemini, GPT-Image) while maintaining competitive perceptual quality.

Conclusion: DisCo provides a scalable, annotation-free solution that resolves the identity crisis in generative models and sets a new benchmark for compositional multi-human generation.

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: Proposes a hierarchical geographic embedding for worldwide visual geo-localization and fuses query appearance with a semantic segmentation map to form a robust representation, achieving state-of-the-art performance across 25 metrics on five benchmarks, with ablations showing gains mainly from combining geographic and visual cues.


<details>
  <summary>Details</summary>
Motivation: To improve localizing a photo to its exact global location using only visual content, addressing limitations of existing learned geographic representations and the need for robust, scalable representations that can leverage both appearance and semantic information.

Method: Model the world as a hierarchy of geographic embeddings and align the visual query with a learned geographic representation. Introduce a method to efficiently fuse the query's appearance features with its semantic segmentation map, creating a unified visual representation. Evaluate across five benchmark datasets, comparing to prior SOTA methods and LVLMs, with ablation studies.

Result: Achieves improved all-time bests in 22 out of 25 metrics across five benchmarks compared to prior SOTA methods and recent LVLMs. Ablation studies indicate gains arise from the combination of geographic and visual representations.

Conclusion: Hierarchical geographic embeddings combined with appearance-segmentation fusion provide a robust and scalable improvement for worldwide visual geo-localization, surpassing prior SOTA and demonstrating the value of integrating geographic structure with rich visual/semantic cues.

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: A data-efficient, principled subset selection method for LVLM instruction tuning named XMAS, clustering examples by cross-modal attention trajectory to reduce data without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Data redundancy in large LVLM instruction-tuning datasets and the lack of effective data selection methods for LVLMs; aim to reduce dataset size and training cost while maintaining performance.

Method: Compute attention matrices for examples during fine-tuning on a small proxy LVLM; measure top singular value trajectories; cluster by these trajectories; sample a balanced subset from clusters; train LVLM on the subset (XMAS).

Result: Can discard 50% of LLaVA-665k and 85% of Vision-Flan; preserve LLaVA-1.5-7B performance across 10 downstream tasks; training speedup of 1.2x; about 30% more data reduction than the best baseline for LLaVA-665k.

Conclusion: XMAS demonstrates a principled approach to data-efficient LVLM instruction tuning via gradient-informed redundancy reduction based on cross-modal attention dynamics; enables substantial data reduction with maintained performance and faster training.

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*RƒÉzvan-Andrei Mati≈üan,Vincent Tao Hu,Grigory Bartosh,Bj√∂rn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception uses variational flow matching for vector-quantized image generation, combining continuous transport with discrete codebook supervision to enable uncertainty estimation and faster training.


<details>
  <summary>Details</summary>
Motivation: To bridge continuous transport methods and discrete vector-quantized supervision, improving efficiency and offering uncertainty quantification in code assignments.

Method: Extend Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in a continuous embedding space; supports temperature-controlled generation and uncertainty quantification.

Result: Faster training convergence than both continuous and discrete flow matching baselines; competitive FID on ImageNet-1k 256x256 against state-of-the-art models.

Conclusion: Variational Flow Matching can effectively merge continuous transport with discrete supervision to enhance training efficiency in image generation.

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: AortaDiff: a unified conditional diffusion + multi-task framework that jointly synthesizes CECT from NCCT and segments aortic lumen and thrombus; achieves higher PSNR and Dice than baselines; uses semi-supervised training; evaluated on 264 patients; code available.


<details>
  <summary>Details</summary>
Motivation: Reduce risks and costs of iodinated contrast in abdominal aortic aneurysm CT by enabling end-to-end synthetic imaging and segmentation that leverage shared anatomy, avoiding error accumulation from staged pipelines and handling missing labels in real-world data.

Method: A unified conditional diffusion model integrated with multi-task learning to simultaneously synthesize synthetic CECT from NCCT and segment the aortic lumen and thrombus. The model shares encoder/decoder across tasks, requires no initial coarse segmentation, and uses semi-supervised training to learn from scans with missing segmentation labels.

Result: On 264-patient cohort, synthetic CECT achieved PSNR 25.61 dB vs 23.80 dB for single-task CDM; lumen Dice 0.89 vs 0.87; thrombus Dice 0.53 vs 0.48; lumen diameter MAE 4.19 mm vs 5.78; thrombus area error 33.85% vs 41.45% (nnU-Net). Code: GitHub link.

Conclusion: The end-to-end, semi-supervised, multi-task diffusion framework outperforms single-task and multi-stage baselines, improving image quality and segmentation accuracy, and yielding more accurate clinical measurements, potentially enabling contrast reduction in AAA assessment.

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: Proposes a framework to rapidly prototype multi-modal video analysis pipelines by converting videos into a temporal semi-structured format and a frame-level, queryable knowledge graph that supports continual learning.


<details>
  <summary>Details</summary>
Motivation: Multi-modal content analysis is hard, computationally heavy, and requires substantial engineering. While many pretrained models exist for static data, fusing them for complex data like videos remains challenging and inefficient.

Method: Introduce a framework and a candidate pipeline recipe that combines pretrained models to convert videos into a temporal semi-structured data format, then translate this into a frame-level indexed knowledge graph that is queryable and supports continual learning through interactive knowledge integration.

Result: A proposed framework that enables efficient prototyping of multi-modal pipelines, producing a temporal semi-structured representation and a frame-level knowledge graph that supports querying and continual learning, with interactive knowledge incorporation.

Conclusion: The framework lowers engineering overhead for multi-modal video analysis and enables dynamic integration of domain-specific knowledge via an interactive medium, facilitating rapid prototyping and continual learning.

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT turns website functionality into reusable, high-level tools to automate browser tasks, reducing brittle step-by-step reasoning.


<details>
  <summary>Details</summary>
Motivation: Current web automation relies on fragile UI interactions and heavy LLM reasoning; humans leverage built-in website features (search, filter, sort, etc.).

Method: Reverse-engineer latent website functionality into invocable tools (e.g., search(query), create(listing), post, comment). Tools encapsulate low-level actions; agents invoke tools rather than clicking/typing.

Result: WALT achieves higher success with fewer steps and less LLM-dependent reasoning on VisualWebArena and WebArena, demonstrating robustness and generalizability.

Conclusion: A robust, generalizable paradigm for browser automation by exposing website-designed functionalities as reusable tools.

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: A semi-supervised segmentation framework (MATCH) that enforces topological consistency across perturbed predictions and uses a novel cross-prediction feature matching strategy to improve histopathology segmentation.


<details>
  <summary>Details</summary>
Motivation: In histopathology, densely packed structures and limited labeled data make it hard to capture meaningful topology; robust semi-supervised methods are needed to distinguish real biological structures from artifacts.

Method: Leverages multiple perturbed predictions via stochastic dropout and temporal training snapshots; enforces topological consistency across outputs; introduces a matching strategy that blends spatial overlap with global structural alignment to align topology across predictions.

Result: Extensive experiments show reduced topological errors and more robust, accurate segmentations; code is released.

Conclusion: The approach enables more reliable downstream analysis in histopathology by preserving topology under semi-supervision.

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: Diffusion-LPO introduces listwise preference optimization for diffusion models using the Plackett‚ÄìLuce model, leveraging ranked human feedback to improve text-to-image generation, editing, and personalization beyond pairwise DPO baselines.


<details>
  <summary>Details</summary>
Motivation: Human feedback often contains richer, implicit ranked information. Pairwise DPO underutilizes this signal; a listwise approach can better capture user preferences and improve alignment.

Method: Aggregate caption-aligned feedback into a ranked list of images, derive a listwise DPO objective under Plackett‚ÄìLuce, enforce consistency by promoting each sample over all lower-ranked ones, and apply to diffusion models for T2I, editing, and personalized alignment.

Result: Diffusion-LPO consistently outperforms pairwise DPO baselines in both visual quality and alignment with user preferences across multiple tasks.

Conclusion: Listwise preference optimization is a practical and effective enhancement for RLHF in diffusion models, suggesting potential extensions beyond current tasks and prompting further investigation into listwise signals and ranking models.

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: Bridge is a pure autoregressive unified multimodal LLM that uses a Mixture-of-Transformers to enable both understanding and generation from a single next-token model, with a semantic-to-pixel discrete representation that combines compact semantic tokens with fine-grained pixel tokens. This yields strong language alignment and precise visual descriptions with only a 7.9% increase in sequence length, achieving competitive or superior performance on multimodal tasks while using less training data and time than previous unified MLLMs.


<details>
  <summary>Details</summary>
Motivation: There is a need for unified multimodal language models that can perform both image understanding and generation within a single autoregressive framework. Existing approaches either rely on hybrids that disrupt autoregressive properties or on discrete-token generation that sacrifices either semantic alignment or pixel fidelity. A pure autoregressive, unified MLLM with efficient, high-fidelity generation and strong alignment is highly desirable.

Method: Introduce Bridge, a pure autoregressive unified MLLM built with a Mixture-of-Transformers architecture that augments pre-trained visual understanding models with generative capability, enabling both image understanding and generation within a single next-token prediction pipeline. To improve visual generation fidelity, adopt a semantic-to-pixel discrete representation that combines compact semantic tokens with fine-grained pixel tokens, increasing sequence length by 7.9% but preserving strong language alignment and detailed visual description.

Result: Extensive experiments across diverse multimodal benchmarks show Bridge achieves competitive or superior performance in both understanding and generation tasks, while requiring less training data and reduced training time compared to prior unified MLLMs.

Conclusion: Bridge demonstrates the viability of pure autoregressive unified MLLMs and the effectiveness of a semantic-to-pixel discrete representation, achieving strong language alignment and detailed visual fidelity with improved efficiency, and offering a promising direction for future research in unified multimodal modeling.

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: A hybrid CNN-Bayesian deep learning model with variational inference is proposed for oral cancer classification on small datasets, yielding high accuracy and uncertainty estimates to improve reliability and generalization in data-scarce settings.


<details>
  <summary>Details</summary>
Motivation: Oral cancer has high mortality, especially in regions with limited healthcare access. Early diagnosis is critical but constrained by scarce training data, inadequate infrastructure, and practitioner shortages. Conventional DL can be overconfident and data-hungry, reducing reliability in resource-limited environments.

Method: A hybrid architecture combining a convolutional neural network with Bayesian deep learning using variational inference to quantify uncertainty. Trained on smartphone-captured color images and evaluated on three distinct test datasets, comparing performance to traditional CNNs.

Result: Achieved 94% accuracy on a test dataset similar to training data (comparable to CNNs). Demonstrated superior generalizability on diverse datasets with 88% accuracy versus 72.94% for traditional CNNs, even with smaller training data. Uncertainty analysis showed low uncertainty for correctly classified samples and high uncertainty for misclassified ones.

Conclusion: Bayesian inference enhances reliability and generalizability of deep learning models for early oral cancer diagnosis in data-scarce environments, enabling more trustworthy decision-making in real-world, resource-limited settings.

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: CADTrans introduces a Consistent Assistant Domains Transformer for SFDA that generates invariable feature representations via an assistant domain module and a CMK-MMD objective, improving robustness to hard samples and domain bias across standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: In source-free domain adaptation, source data are unavailable, making it hard to obtain invariant features. Existing methods struggle with hard samples and domain bias when aligning target to source-like representations.

Method: CADTrans builds an assistant domain module to extract diversified representations from aggregated global attentions. It derives invariable feature representations using multiple consistent strategies across the assistant and target domains to distinguish easy vs hard samples. A conditional multi-kernel MMD (CMK-MMD) aligns hard samples with easy samples by considering same-category vs different-category pairs.

Result: Extensive experiments on Office-31, Office-Home, VISDA-C, and DomainNet-126 show significant performance gains over baselines. Code is provided at the authors' repository.

Conclusion: CADTrans offers a robust SFDA framework by constructing domain-consistent representations through an assistant domain module and employing CMK-MMD to handle hard samples and domain bias, achieving strong generalization across multiple benchmarks.

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: A retrieval-based approach guides multimodal LLMs to produce context-relevant, BLV-friendly image descriptions by leveraging historical questions from VizWiz-LF, improving relevance and user satisfaction.


<details>
  <summary>Details</summary>
Motivation: BLV users often receive long, generic descriptions from multimodal LLMs that do not align with their immediate information needs; leveraging historical BLV questions can tailor explanations to user contexts.

Method: For each image, identify similar past visual contexts in VizWiz-LF and use their associated questions to steer the MLLM's description generation. Evaluate with three human labelers revising 92 descriptions (context-aware vs. context-free) to assess relevance and preference.

Result: Context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70/92) and were preferred in 54.4% (50/92). The GitHub repository with data and analysis is publicly available.

Conclusion: Using historical BLV questions to guide MLLM generation can produce more context-relevant descriptions for BLV users; the work provides publicly available data and analysis to support future improvements.

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: A synthetic multimodal reasoning dataset, ImageNet-Think, uses 250k ImageNet21k images with two thinking-answer sequences per image, generated by two VLMs, to train/evaluate explicit multimodal reasoning in Vision-Language Models; aims for public release.


<details>
  <summary>Details</summary>
Motivation: Address the need for explicit, step-by-step multimodal reasoning in VLMs; current models struggle with interpretability and robust reasoning; leveraging synthetic thinking tokens to supervise reasoning.

Method: Construct dataset by prompting two state-of-the-art VLMs to generate thinking tokens and final descriptive answers per image; two thinking-answer sequences per image; based on ImageNet21k; collect and structure tokens; provide benchmarks.

Result: Proposes and constructs a public dataset with ~250k images and two thinking-answer sequences per image (approx. 500k sequences) to train/evaluate multimodal reasoning; public benchmarks to assess model reasoning capabilities.

Conclusion: ImageNet-Think offers a resource to foster development of VLMs with explicit multimodal reasoning and to advance understanding of multimodal reasoning mechanisms; public release planned.

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdr√≥n-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: Non-Linear Projections of the Null-Space (NPN) learn a low-dimensional, sensing-matrix-specific regularizer that constrains solutions to the projected null-space, improving reconstruction across diverse imaging inverse problems and compatible with plug-and-play, unrolled networks, DIPs, and diffusion models.


<details>
  <summary>Details</summary>
Motivation: Imaging inverse problems are ill-posed, with infinite solutions in the null-space of the sensing operator. Traditional priors (handcrafted or learned) often ignore the task-specific structure of that null-space. NPN aims to capture null-space information tailored to the sensing process to resolve ambiguity.

Method: Propose Non-Linear Projections of the Null-Space (NPN) as a regularization that uses a neural network to impose a low-dimensional projection of the sensing matrix's null-space. Integrates with reconstruction frameworks via plug-and-play methods, unrolled networks, deep image priors, and diffusion models. Provides theoretical convergence and reconstruction guarantees within these schemes.

Result: Empirical results across diverse sensing matrices show that NPN priors consistently enhance reconstruction fidelity in imaging inverse problems‚Äîincluding compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging‚Äîwhen used with plug-and-play methods, unrolled networks, deep image priors, and diffusion models.

Conclusion: NPN priors offer interpretable, flexible, and sensing-matrix-specific regularization that complements traditional image-domain priors. They enable improved reconstructions across a range of inverse problems and are compatible with multiple modern reconstruction frameworks, with theoretical convergence guarantees.

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: An interpretable genomic interpretation framework that maps raw DNA to actionable decisions via Chaos Game Representation and a Concept Bottleneck Model, enforcing concept-level predictions (GC content, CpG density, k-mer motifs) with calibration and cost-aware decision policies, achieving state-of-the-art HIV subtype classification and interpretable evidence for robotic/clinical automation.


<details>
  <summary>Details</summary>
Motivation: There is a need for end-to-end, interpretable, and reliable genomic decision systems that can be integrated into automated medical robotics. By constraining predictions to biologically meaningful concepts and calibrating uncertainty, the system aims to provide trustworthy, actionable outputs aligned with biological priors.

Method: Encode sequences using Chaos Game Representation (CGR) and route predictions through a Concept Bottleneck Model (CBM) with concepts such as GC content, CpG density, and k-mer motifs. Add concept fidelity supervision, prior consistency alignment, KL distribution matching, and uncertainty calibration. Include a cost-aware layer that translates predictions into decision policies suitable for automation.

Result: The framework achieves state-of-the-art HIV subtype classification on both in-house and LANL datasets, with superior concept prediction fidelity and favorable cost-benefit trade-offs compared to baselines. It delivers interpretable, biology-aligned evidence that can be validated against priors and supports automated decision-making.

Conclusion: This work bridges interpretable genomic modeling with automated decision-making, laying a foundation for reliable robotic and clinical automation in genomic medicine by providing both high predictive performance and concept-level interpretability validated against biological priors.

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1 introduces a reasoning-enhanced vision-language-action model that uses RLVR with GRPO, plus a 13K chain-of-thought dataset (VLA-CoT-13K), to improve reasoning and execution for better generalization and real-world robotic performance; code and data will be released.


<details>
  <summary>Details</summary>
Motivation: Current VLA models lack explicit, verifiable step-by-step reasoning and often rely on weak reward signals during fine-tuning, limiting generalization and alignment with affordances and geometric relations.

Method: An RLVR-based post-training strategy that uses verifiable rewards for region alignment, trajectory consistency, and output formatting, combined with Group Relative Policy Optimization (GRPO); creation of VLA-CoT-13K dataset with chain-of-thought supervision aligned to affordance and trajectory annotations; extensive multi-domain evaluations on in-domain/out-of-domain, simulation, and real-robot platforms.

Result: VLA-R1 demonstrates superior generalization and real-world performance compared to prior VLA methods across diverse settings.

Conclusion: Combining verifiable rewards for reasoning and chain-of-thought supervision with the VLA framework enhances reasoning quality and execution accuracy; the dataset and code release will support future research and broader adoption.

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: A joint deblurring and multi-view 3D reconstruction framework for macrophotography using per-pixel defocus kernels and differentiable rendering, achieving high-quality deblurring and faithful 3D appearance from a small number of views.


<details>
  <summary>Details</summary>
Motivation: Defocus blur severely degrades macrophotography imaging and hampers 3D reconstruction. Existing deblurring methods require many images/annotations, and there is no established multi-view 3D reconstruction approach for macrophotography.

Method: From several multi-view blurry images, jointly optimize a clear 3D object model and per-pixel defocus blur kernels. The framework uses differentiable rendering to self-supervise the optimization of both the 3D model and the blur kernels in an end-to-end manner.

Result: Experiments on a small set of multi-view images show that the method can produce high-quality deblurred images and reconstruct high-fidelity 3D appearances.

Conclusion: The proposed method demonstrates that joint deblurring and multi-view 3D reconstruction for macrophotography is feasible with differentiable rendering, reducing the data requirements while achieving accurate 3D appearance.

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: A single-step diffusion-based deblurring method, FideDiff, achieves high-fidelity restoration by enforcing temporal consistency across diffusion-like timesteps and leveraging kernel-controlled blur estimation, outperforming prior diffusion-based methods and matching state-of-the-art restoration models with efficient one-step inference.


<details>
  <summary>Details</summary>
Motivation: Diffusion models excel at high-quality restoration but suffer from slow inference and fidelity-variance. There is a need for fast, high-fidelity deblurring suitable for real-world applications by leveraging pre-trained diffusion models.

Method: Reformulate motion deblurring as a diffusion-like process where each timestep corresponds to a progressively blurred image. Train a consistency model that aligns all timesteps to a single clean image. Use matched blur trajectories for training data to enforce temporal consistency. Enhance with Kernel ControlNet for blur kernel estimation and implement adaptive timestep prediction for efficient inference.

Result: Achieves superior full-reference deblurring metrics compared with previous diffusion-based methods and matches the performance of other state-of-the-art models, while enabling single-step (fast) deblurring. The work also provides dataset and code release.

Conclusion: FideDiff demonstrates a robust approach to adapting large pre-trained diffusion models for high-fidelity image restoration, offering a practical baseline for real-world industrial applications and paving the way for efficient diffusion-based deblurring.

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: Introduces a large BI dataset (22k pages, ~199k characters, 6.7k classes) and a two-stage detection-recognition pipeline using LadderMoE on CLIP to achieve state-of-the-art recognition across domain variations and long-tail categories.


<details>
  <summary>Details</summary>
Motivation: Bronze inscriptions are crucial for early Chinese writing and archaeology, but automatic recognition is hampered by severe degradation, cross-domain variability, and extreme long-tailed class distributions; a robust, cross-domain BI recognition system is needed.

Method: A two-stage pipeline that first localizes inscriptions and then transcribes characters. Enhances a pretrained CLIP encoder with ladder-style MoE adapters (LadderMoE) to enable dynamic specialization and robustness across heterogeneous domains and rare classes.

Result: Outperforms state-of-the-art scene text recognition baselines on both single-character and full-page tasks, with superior accuracy across head, mid, tail categories and all acquisition modalities.

Conclusion: The dataset and LadderMoE-based approach establish a strong foundation for automatic bronze inscription recognition and support downstream archaeological analysis.

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA introduces parameter-efficient unsupervised domain adaptation by adding domain-specific visual reprogramming prompts to a fixed backbone, avoiding backbone fine-tuning while achieving competitive accuracy on Office-31.


<details>
  <summary>Details</summary>
Motivation: Current UDA pipelines fine-tune backbones for each source-target pair, causing linear growth in trainable parameters and memory and hindering reuse. The idea that backbones carry textural biases motivates using domain-specific prompts to bias inputs rather than modifying the model.

Method: Attach a domain-specific visual reprogramming layer to the backbone that generates visual prompts to bias input texture; train these prompts with multiple objectives to align intra- and inter-domain distributions; backbone remains fixed and reusable across domains; prompts adapt to target domains.

Result: On Office-31, VirDA achieves 92.8% mean accuracy with 1.5M trainable parameters; surpasses PDA by 1.6% using 46% of PDA's parameters; outperforms CDTrans and FixBi by 0.2% and 1.4% while using 1.7% and 2.8% of their trainable parameters; relative to PMTrans and TVT, VirDA uses ~1.7% of their parameters with accuracy trade-offs of ‚àí2.2% and ‚àí1.1%.

Conclusion: VirDA demonstrates a viable, parameter-efficient UDA approach by reusing backbones through visual reprogramming, achieving competitive accuracy with substantially reduced trainable parameters and memory.

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: Unsupervised, data-driven discrete facial encoding (DFE) using RVQ-VAE on 3DMM-extracted expressions; outperforms FACS and MAE baselines on stress, personality, and depression tasks.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitations of the Facial Action Coding System (FACS): limited coverage and costly manual annotation; need scalable, interpretable, and expressive facial representations for psychology and affective computing.

Method: Extract identity-invariant expression features via a 3D Morphable Model (3DMM); encode with a Residual Vector Quantized Variational Autoencoder (RVQ-VAE) to produce a sequence of discrete tokens from a shared codebook; use Bag-of-Words on tokens for downstream tasks; evaluate on stress detection, personality prediction, and depression detection.

Result: DFE captures more precise facial behaviors than FACS and other encoders; Bag-of-Words with learned tokens outperforms FACS pipelines and strong baselines (e.g., MAE) across tasks; broader coverage of facial displays.

Conclusion: Discrete Facial Encoding offers a scalable, interpretable, and effective alternative to FACS for psychological and affective computing applications, with potential for broad adoption and further exploration.

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: Con-NRSfM introduces a non-rigid structure-from-motion framework for monocular deformable SLAM under conformal deformations, achieving local conformal scale recovery and decoupled depth-scale estimation via graph-based optimization and self-supervised learning, outperforming prior methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing NRSfM approaches often rely on strict local assumptions (e.g., local planarity or locally linear deformations) and cannot recover the conformal scale. In monocular deformable SLAM, depth and scale are typically entangled, limiting accuracy.

Method: Propose Con-NRSfM that performs point-wise reconstruction using 2D selected image warps optimized through a graph-based framework. It eliminates constraints on conformal scale and depth, decouples depth and conformal scale, uses parallel separable iterative optimization to address sensitivity, and integrates a self-supervised encoder-decoder to generate dense textured 3D point clouds.

Result: Simulation and real-data experiments show superior reconstruction accuracy and robustness compared with existing NRSfM approaches, with dense textured 3D outputs; code will be publicly available.

Conclusion: Con-NRSfM broadens NRSfM to general conformal deformations (including isometric) by recovering the local conformal scale and decoupling depth from scale, yielding more accurate 3D reconstructions in monocular deformable SLAM; implementation will be released publicly.

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: A decoupled two-stage pipeline using a video diffusion model (UniVerse) to restore inconsistent multi-view images, then reconstruct 3D scenes, achieving strong generalization and style-controllable results.


<details>
  <summary>Details</summary>
Motivation: Current robust 3D reconstruction methods rely on dense multi-view observations and per-view degradation modeling, making robustness to diverse inconsistencies difficult. A decoupled approach leverages global scene priors learned from large-scale data to handle varied degradations with less dependence on view density.

Method: Introduce UniVerse, a unified framework based on a video diffusion model. Steps: 1) convert inconsistent images into initial videos; 2) restore them into consistent images using a specially designed video diffusion model; 3) reconstruct 3D scenes from the restored images. The diffusion model captures a general scene prior and enables style control over the reconstructed 3D scene.

Result: Extensive experiments on synthetic and real-world datasets demonstrate strong generalization and superior performance in robust reconstruction, validating the effectiveness of diffusion-based restoration for handling diverse image inconsistencies.

Conclusion: Diffusion-based restoration enables generalizable robust 3D reconstruction and simplifies optimization by decoupling restoration from reconstruction, with added capability to control the style of the reconstructed scene.

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: A lightweight end-to-end template matching framework that jointly localizes and regresses geometric pose (center, rotation, horizontal and vertical scales) using a Template-Aware Dynamic Convolution Module (TDCM) and rotation-shear augmentation, achieving real-time performance and robustness to small templates and multi-object scenes.


<details>
  <summary>Details</summary>
Motivation: Industrial inspection demands efficient, explicit pose estimation in template matching under complex backgrounds. Traditional exhaustive angle/scale search is inefficient, and many deep models only output similarity scores without explicit geometry, limiting real-world deployment.

Method: Formulates template matching as joint localization and geometric regression. Introduces Template-Aware Dynamic Convolution Module (TDCM) that injects template features at inference, uses depthwise separable convolutions and pixel shuffle for efficiency, and employs a rotation-shear-based augmentation strategy with structure-aware pseudo labels to enable geometric-annotation-free training. Adds a lightweight refinement module for local optimization of angle and scale.

Result: The model, with 3.07M parameters, achieves high precision and 14 ms inference under compound transformations. Demonstrates robustness in small-template and multi-object scenarios, indicating suitability for real-time industrial applications.

Conclusion: The approach delivers a geometry-aware, efficient template-matching solution that combines joint pose regression with dynamic template-guided matching, enabling real-time deployment and improved robustness in challenging scenes; code is available at the authors‚Äô GitHub.

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: Introduces adaptive pixel reasoning for Vision-Language Models that selectively uses pixel-level information via operation-aware fine-tuning and rollout-guided reinforcement learning, achieving higher accuracy with reduced pixel operations.


<details>
  <summary>Details</summary>
Motivation: VLMs often lose fine-grained visual information due to encoding constraints or imprecise attention. While pixel-level data can improve reasoning, overuse leads to inefficiency and distraction from irrelevant details.

Method: 1) Operation-aware supervised fine-tuning to establish baseline textual reasoning and visual operations. 2) Rollout-guided reinforcement learning using feedback from the model's own responses to decide when to invoke pixel operations based on query difficulty.

Result: On extensive multimodal reasoning benchmarks, the approach yields superior accuracy. It achieves 73.4% accuracy on HR-Bench 4K with a tool usage ratio of 20.1%, reducing tool usage by 66.5% compared to prior methods.

Conclusion: Adaptive pixel reasoning enables dynamic, query-driven use of pixel-level information, improving accuracy while significantly reducing unnecessary pixel operations.

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: ASRS offers an augmentation-sensitivity risk score for chest X-ray models by applying clinically plausible rotations and measuring embedding shifts with the RAD-DINO encoder to flag error-prone cases for selective review, improving fairness and safety.


<details>
  <summary>Details</summary>
Motivation: To address reliability gaps in medical imaging AI where subgroup performance varies and hidden within-distribution errors are not captured by aggregate metrics; existing confidence calibration and OOD methods miss subtle errors, and consistency-based approaches remain underexplored in radiology.

Method: Apply rotations of ¬±15¬∞ and ¬±30¬∞ to CXR inputs; compute embedding shifts using the RAD-DINO encoder; derive sensitivity scores and stratify samples into stability quartiles; identify high-sensitivity cases that exhibit low recall despite high AUROC and confidence; use scores for label-free selective prediction and clinician review.

Result: High-sensitivity quartiles show substantial recall drop from model performance (-0.2 to -0.3) while AUROC and confidence remain high; ASRS provides a label-free mechanism to flag error-prone cases for review, aiding fairness and safety.

Conclusion: ASRS is a practical, label-free framework for detecting error-prone CXR cases via augmentation-sensitivity analysis, enabling selective prediction and clinician triage to improve fairness and reliability in medical AI.

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: A training-free framework (FreeViS) for video stylization that preserves temporal coherence and style richness by integrating multiple stylized references into a pretrained image-to-video model, using high-frequency compensation and flow-based motion cues; outperforms baselines with a practical, economical pipeline.


<details>
  <summary>Details</summary>
Motivation: Frame-by-frame stylization harms temporal consistency; training video stylizers requires paired data and is expensive; need a method that yields high-quality, temporally coherent stylized video without training.

Method: Combine multiple stylized references with a pretrained I2V model; mitigate propagation errors; apply high-frequency compensation to constrain content/layout and motion; use flow-based motion cues to preserve textures in low-saliency areas; training-free.

Result: Higher stylization fidelity and better temporal consistency than recent baselines; strong human preference; practical and economical; code and videos available.

Conclusion: Offers a practical, training-free pipeline for high-quality, temporally coherent video stylization; lowers cost and data requirements; ready for use in content creation.

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: MedQ-Bench introduces a perception-reasoning benchmark for assessing medical image quality via multi-modal LLMs, featuring MedQ-Perception and MedQ-Reasoning tasks across five modalities, with 2,600 perceptual queries and 708 reasoning assessments; evaluates 14 MLLMs and finds preliminary, unstable perceptual/reasoning skills, signaling need for optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional scalar IQA metrics inadequately reflect human-like, expert-quality judgments in medical imaging. There is a need for a benchmark that evaluates both perceptual ability and reasoning aligned with radiologists to guide the development of reliable medical IQA systems using MLLMs.

Method: MedQ-Bench defines two complementary tasks: MedQ-Perception (low-level perceptual questions on visual attributes) and MedQ-Reasoning (no-reference and comparison-based reasoning). It spans five imaging modalities and >40 quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, including authentic clinical images, physics-based degraded reconstructions, and AI-generated images. A multi-dimensional judging protocol with four axes evaluates model outputs, and radiologist alignment validates the human-AI agreement.

Result: Evaluation of 14 state-of-the-art MLLMs shows preliminary but unstable perceptual and reasoning capabilities, with accuracy currently insufficient for reliable clinical deployment.

Conclusion: MedQ-Bench is a valuable framework to drive targeted optimization of MLLMs for medical image quality assessment and to catalyze broader exploration into human-aligned, perception-based evaluation in medical imaging.

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer predicts full occlusion and depth ordering for all scene instances from a single RGB image in one forward pass, using interactions between object queries and latent mask descriptors to jointly encode objects; open-source code provided.


<details>
  <summary>Details</summary>
Motivation: Understanding instance-wise 3D geometry from RGB is hard and current methods rely on expensive inputs (labels, masks) and inference (quadratic passes). A single RGB-based, efficient approach for holistic ordering is needed.

Method: A transformer-like framework (InstaFormer) that models interactions between object queries and latent mask descriptors representing the same objects but carrying complementary information, enabling holistic occlusion and depth order prediction in one forward pass.

Result: Comprehensive benchmarking and ablation studies demonstrate the effectiveness of the approach; the authors also provide open-source code and models.

Conclusion: Proposes a novel, efficient RGB-only method for predicting full occlusion and depth orderings across all instances in a scene in a single pass, reducing reliance on expensive inputs and multiple inferences; code is publicly available.

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler introduces a transformer-based neural style transfer method with pyramidal positional encoding and reinforcement learning to achieve scalable, high-quality stylization at near real-time speeds, demonstrated on COCO and WikiArt with notable reductions in content and style losses and fast inference.


<details>
  <summary>Details</summary>
Motivation: Standard NST methods (CNN/transformers) struggle to scale to complex styles and high-resolution inputs, requiring efficient, multi-scale context integration.

Method: A transformer framework (PyramidStyler) using Pyramidal Positional Encoding (PPE) to capture hierarchical, multi-scale local and global information, reducing computation. The approach incorporates reinforcement learning to dynamically optimize stylization. Trained on Microsoft COCO and WikiArt, with reported performance gains and inference times.

Result: Content loss reduced by 62.6% to 2.07 after 4000 epochs; style loss reduced by 57.4% to 0.86 after 4000 epochs; inference time 1.39 s. With RL, content loss 2.03 and style loss 0.75 with inference time 1.40 s (minimal speed penalty).

Conclusion: Demonstrates real-time, high-quality artistic rendering with scalable NST suitable for media and design applications; RL provides adaptive improvement with negligible speed trade-offs.

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS tackles scaling 3D Gaussian Splatting to large scenes by depth-aware partitioning, load balancing, and light-weight training tricks, achieving up to 2x faster training with maintained quality.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) offers real-time, high-fidelity 3D reconstruction but struggles with large-scale or unbounded scenes due to memory constraints and load imbalance. Existing divide-and-conquer methods introduce bottlenecks such as unbalanced partitions and inefficient coarse-to-fine pipelines that re-load the model.

Method: Introduce depth-aware partitioning to reduce preprocessing time; an optimization-based strategy to balance visible Gaussians across blocks; and lightweight techniques‚Äîvisibility cropping and selective densification‚Äîto lower training costs.

Result: Evaluations on large-scale urban and outdoor datasets show LoBE-GS achieves up to 2√ó faster end-to-end training than state-of-the-art baselines, while preserving reconstruction quality and enabling scalability to scenes infeasible for vanilla 3DGS.

Conclusion: LoBE-GS provides a scalable, efficient large-scale 3DGS pipeline by addressing partitioning, load balancing, and training-efficiency bottlenecks.

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: MemoryPack and Direct Forcing aim to improve long-form autoregressive video generation: MemoryPack for dynamic context via text+image guidance with linear complexity, and Direct Forcing for single-step training-inference alignment to reduce error propagation, yielding minute-level temporal consistency and more reliable long-form generation.


<details>
  <summary>Details</summary>
Motivation: Long-form video generation requires capturing long-range dependencies while avoiding error accumulation; existing autoregressive methods struggle with efficiency and propagation errors as video length grows.

Method: MemoryPack: learnable context-retrieval using textual and image cues as global guidance to jointly model short- and long-term dependencies with linear complexity, enabling minute-level coherence. Direct Forcing: a single-step approximating strategy that aligns training and inference to reduce error accumulation.

Result: Improved context consistency and reliability for autoregressive video models; scalable to longer videos with linear complexity and minute-level temporal consistency.

Conclusion: MemoryPack plus Direct Forcing substantially enhance long-form video generation practicality and reliability, addressing both dynamic context modeling and error accumulation.

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schr√∂der,Marius-Raphael Schl√ºter,Markus Lienkamp*

Main category: cs.CV

TL;DR: Calibrates the full predictive confidence distribution for 3D object detectors, not just the top class. Introduces two auxiliary losses to calibrate either the dominant class or the full prediction vector, evaluated with post-hoc methods and training-time losses across CenterPoint, PillarNet, and DSVT-Pillar. Best results come from combining full-vector calibration loss with isotonic regression for CenterPoint and PillarNet; DSVT-Pillar cannot be jointly calibrated for dominant and secondary predictions with the same method.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems require reliable confidence estimates for 3D object detection to support safe operation. Existing work often focuses on dominant-class calibration; this work argues for calibrating the entire predictive distribution and accounting for both dominant and secondary predictions.

Method: Introduce two auxiliary regularizing loss terms that steer calibration of the dominant prediction and of the full prediction vector, respectively. Evaluate a range of post-hoc and training-time calibration methods on CenterPoint, PillarNet, and DSVT-Pillar. Combine full-vector calibration loss with isotonic regression for improved calibration.

Result: Full-vector calibration loss plus isotonic regression yields the best calibration for CenterPoint and PillarNet on both dominant and secondary predictions. DSVT-Pillar cannot be jointly calibrated for dominant and secondary predictions with the same method.

Conclusion: Calibrating the full predictive distribution is beneficial for at least some 3D detectors, especially when paired with isotonic regression. Model-specific calibration strategies may be required, as demonstrated by DSVT-Pillar's limitation in joint calibration using a single method.

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS uses a diffusion-model prior to address the two-task optimization conflict in person search, introducing three modules (DGRPN, MSFRN, SFAN) to improve localization, shape bias mitigation, and diffusion-feature integration, achieving state-of-the-art on CUHK-SYSU and PRW.


<details>
  <summary>Details</summary>
Motivation: Current person search pipelines rely on ImageNet-pretrained backbones and share features for detection and re-id, which leads to suboptimal spatial context capture and conflicting optimization; there is a need to leverage richer priors and decouple tasks.

Method: Introduce three modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. The framework leverages a pre-trained diffusion model to provide priors while avoiding optimization conflict between detection and re-id.

Result: Sets a new state-of-the-art on two standard benchmarks (CUHK-SYSU and PRW).

Conclusion: DiffPS demonstrates that diffusion priors can supply rich spatial and semantic cues for person search, enabling better localization and re-id. By decoupling the two sub-tasks and integrating diffusion features via specialized modules, it achieves superior performance, suggesting a promising direction for diffusion-based priors in vision-and-language guided or vision tasks. 

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: Introduces FMU, a hybrid flow-matching-guided unfolding network for hyperspectral image reconstruction from compressed measurements, combining a flow-based generative prior with deep unfolding and a mean velocity loss to enforce global flow consistency; claims strong reconstruction gains and reproducibility through released code.


<details>
  <summary>Details</summary>
Motivation: HSI provides rich spatial-spectral information but is costly to acquire and reconstruct from compressed measurements is challenging; traditional compressive sensing and optimization struggle with degradation and loss of spectral details. A hybrid approach that leverages a strong generative prior within an interpretable unfolding framework could improve reconstruction quality and robustness.

Method: FMU embeds a flow-matching generative prior into a deep unfolding network for HSI reconstruction, guiding iterative updates via learned dynamics. A mean velocity loss enforces global consistency of the flow, enhancing robustness and accuracy of reconstructions.

Result: Experiments on simulated and real datasets show FMU significantly outperforms existing methods in reconstruction quality; code and models will be released for reproducibility (GitHub link provided).

Conclusion: Combining flow-matching priors with optimization-inspired unfolding yields robust, high-quality HSI reconstructions from compressed measurements, highlighting the value of integrating generative priors with interpretable frameworks.

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: Proposes an automated DIP defect detection system using YOLOv7 with ConSinGAN data augmentation; achieves 95.5% accuracy and 285 ms per detection, addressing defect-data scarcity and enabling SCADA-integrated deployment.


<details>
  <summary>Details</summary>
Motivation: Defect detection in manufacturing is time-consuming and labor-intensive, and there is a lack of defective component images, making automated detection essential for scalable quality control of DIP components.

Method: Develop a DL-based detection pipeline using digital camera optics; generate defect images with ConSinGAN to augment training data; evaluate four YOLO variants (v3, v4, v7, v9) with and without ConSinGAN; identify YOLOv7 with ConSinGAN as best; also design and describe a supervisory control and data acquisition (SCADA) system and sensor architecture.

Result: YOLOv7 with ConSinGAN achieves 95.50% accuracy and 285 ms detection time, outperforming threshold-based approaches; effective for both surface and pin-leg DIP defects; data augmentation enables detection with limited real defect samples.

Conclusion: Automated DIP defect detection is feasible and easily deployable across different defect types, particularly when using ConSinGAN-augmented data with YOLOv7, and can be integrated into SCADA systems for industrial use.

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: FoundAD learns a nonlinear projection onto the natural image manifold to detect anomalies from few examples, leveraging pre-trained foundation encoders. It enables multi-class anomaly detection with far fewer parameters and competitive performance.


<details>
  <summary>Details</summary>
Motivation: Industrial safety inspection often has scarce labeled anomaly data, and category-agnostic settings make differentiation between normal and abnormal features tough. Large pre-trained encoders help model the distribution of normal images, motivating a parameter-efficient detector that can work with few shots.

Method: A nonlinear projection operator is learned that maps embeddings onto the natural image manifold. The anomaly amount in an image is correlated with differences in the learned embeddings, and the projection operator identifies out-of-distribution regions.

Result: The approach supports multi-class anomaly detection and achieves competitive performance while using substantially fewer parameters than prior methods. It is validated across multiple foundation encoders, including DINOv3.

Conclusion: FoundAD broadens the use of foundation features in few-shot anomaly detection, offering an efficient and effective tool for detecting anomalies in industrial inspection with limited data.

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja G√ºldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: ClustViT extends ViT for segmentation by a trainable Cluster module that merges similar tokens guided by pseudo-clusters from segmentation masks, followed by a Regenerator to restore details, achieving significant compute reductions with comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers have high accuracy but quadratic attention complexity, hindering real-world robotic deployment. Token merging helps classification but is less suited for dense prediction; thus a segmentation-friendly, efficient ViT variant is needed.

Method: Introduce a trainable Cluster module that merges tokens along the network guided by pseudo-clusters from segmentation masks, then use a Regenerator module to recover fine details for downstream segmentation heads, enabling efficient dense prediction.

Result: On three datasets, ClustViT achieves up to 2.18x fewer GFLOPs and 1.64x faster inference with comparable segmentation accuracy.

Conclusion: ClustViT demonstrates an efficient, deployable ViT-based segmentation approach that reduces computation while maintaining accuracy; code and models will be released.

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT is a unified Patch-as-Decodable Token framework for MLLMs that directly generates both text and diverse visual outputs, using Visual Reference Tokens interleaved with LLM tokens, with a lightweight decoder for detection, segmentation, and grounding. It processes VRTs per forward pass and expands the embedding table dynamically, enabling better localization; trained with random VRT sampling and per-token cross-entropy; achieves state-of-the-art across four tasks, competitive with larger LLMs; code released.


<details>
  <summary>Details</summary>
Motivation: To overcome reliance on indirect representations in vision tasks (e.g., predicting coordinates as text) that limit dense predictions like segmentation, by enabling direct, unified generation of textual and visual outputs within MLLMs.

Method: Introduce Patch-as-Decodable Token (PaDT). Derived Visual Reference Tokens (VRTs) from visual patch embeddings of query images are interleaved with LLM output tokens. A lightweight decoder converts LLM outputs into detection, segmentation, and grounding predictions. VRTs are processed independently at each forward pass; embedding table expands dynamically. Training uses randomly selected VRTs for supervised fine-tuning and applies a per-token cross-entropy loss.

Result: Empirical studies show PaDT achieves state-of-the-art performance across four visual perception and understanding tasks, outperforming baselines and even some much larger MLLMs.

Conclusion: PaDT offers a unified and effective approach to enable dense vision tasks within MLLMs, bridging text and diverse visual outputs, with code available for reproducibility.

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: A new explainable AI framework TriAlignXA with Triangular Trust Index to address trust in online produce, balancing quality, timeliness, and economics via a Trust Pyramid and dual-source verification, including QR-encoded quality data.


<details>
  <summary>Details</summary>
Motivation: In online fruit/vegetable e-commerce, lack of sensory cues creates trust deficits; need transparent, explainable decision-making to support consumer trust under agricultural constraints.

Method: Proposes a Trust Pyramid and dual-source verification; introduces Triangular Trust Index (TTI); develops TriAlignXA with Bio-Adaptive, Timeliness, and Economic engines; Pre-Mapping Mechanism encodes data to QR codes; evaluate via grading experiments and theoretical analysis; multi-objective optimization.

Result: Experiments show higher grading accuracy than baselines; empirical and theoretical evidence that the framework balances the 'impossible triangle' of biology, timeliness, and economics; improved trust potential in online produce transactions.

Conclusion: Provides a practical pathway from algorithmic decision-making to consumer trust; supports building a trustworthy online produce ecosystem; extends explainable AI for supply-chain transparency.

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: Proposes 4DGS-Craft, a consistent and interactive 4D Gaussian Splatting (4DGS) editing framework that uses 4D-aware conditioning, multi-view refinement, Gaussian-region selective optimization, and an LLM-driven intent module to improve view/temporal consistency and handle complex instructions.


<details>
  <summary>Details</summary>
Motivation: Current 4DGS editing suffers from inconsistencies across viewpoints and time, drift in non-edited regions, and difficulty in following complex text instructions. There is a need for a controllable, interactive editing framework that preserves 4D geometry.

Method: Introduce a 4D-aware InstructPix2Pix model that leverages 4D VGGT geometry features from the initial scene to preserve 4D structure during edits; add a multi-view grid module for iterative refinement of multi-view inputs while jointly optimizing the 4D scene; implement a Gaussian selection mechanism to preserve non-edited regions by optimizing only Gaussians within edited regions; incorporate an LLM-based module with an instruction template to interpret user intent and decompose complex commands into atomic editing operations.

Result: Claims more consistent and controllable 4D scene editing than related works; the code will be released upon acceptance (no quantitative metrics reported in the abstract).

Conclusion: The framework demonstrates that combining 4D geometry-aware editing, multi-view refinement, selective Gaussian optimization, and LLM-driven intent understanding can yield robust, interactive 4DGS editing with improved consistency and user control.

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: PP-ATD-light introduces pixel-level masking (Pure-Pass) to skip expensive computation on pure pixels, using fixed color center points to classify pixels for fine-grained, spatially flexible masking, integrated into ATD-light to outperform CAMixer-ATD-light with similar computation.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational burden of deep SR while preserving performance and adaptability. Prior CAMixer struggles with adaptability, coarse masking, and spatial rigidity; a finer, pixel-level, and flexible masking mechanism could improve efficiency and quality.

Method: Propose Pure-Pass (PP) masking that identifies pure pixels at the pixel level and exempts them from heavy computation. Uses fixed color center points to classify pixels into categories, enabling fine-grained, spatially flexible masking with adaptive capability. Integrates PP into the state-of-the-art ATD-light model to form PP-ATD-light.

Result: PP-ATD-light achieves superior SR performance with minimal overhead and outperforms CAMixer-ATD-light in reconstruction quality and parameter efficiency when saving a similar amount of computation.

Conclusion: Pixel-level Pure-Pass masking with fixed color centers provides fine-grained, spatially flexible, and adaptive computation reduction in SR, yielding better quality-for-cost with ATD-light.

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: GPT-4o-based pipeline with a Self-correction Loop with Structured Output (SLSO) aims to generate jaw cyst findings from panoramic radiographs. It improves several findings with iterative regenerations, but is limited by small dataset and difficulties with extensive lesions.


<details>
  <summary>Details</summary>
Motivation: To reduce hallucinations and improve accuracy in AI-generated radiology findings by enforcing structured outputs and consistency through iterative self-correction.

Method: A 10-step process over 22 jaw cyst cases: input image analysis, generation of structured data, tooth-number extraction with consistency checks, regeneration when inconsistencies are detected, final finding generation with restructuring and verification; comparison against conventional Chain-of-Thought (CoT) across seven evaluation items.

Result: SLSO improved output accuracy for tooth number (66.9%), tooth movement (33.3%), and root resorption (28.6%). Consistent structured outputs achieved after up to five regenerations. Negative findings and reduced hallucinations reported; no statistical significance due to small sample; accurate learning limited for extensive lesions spanning multiple teeth.

Conclusion: SLSO shows promise toward a practical automatic finding generation system but requires further refinement and larger datasets to improve overall performance and handle extensive multi-tooth lesions.

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja P√©rez,Jaime Godoy,Abdulla Al-Kaff,Fernando Garc√≠a*

Main category: cs.CV

TL;DR: LiLa-Net: a lightweight 3D autoencoder for LiDAR point clouds that uses simplified skip connections and fewer encoder layers to reconstruct scenes from real traffic data, achieving efficient latent representations and good generalization to unseen objects.


<details>
  <summary>Details</summary>
Motivation: There is a need for compact, efficient LiDAR representations for autonomous driving that enable accurate reconstruction without heavy architectures, enabling real-time performance and better generalization to diverse traffic environments.

Method: A 3D autoencoder architecture named LiLa-Net processes LiDAR point clouds from a real semi-autonomous vehicle equipped with Velodyne. It reduces encoder depth and simplifies skip connections, aiming for a balanced latent encoding that preserves essential information while enabling accurate reconstruction of the original point cloud. Training emphasizes an efficient latent space and improved reconstruction quality with limited resources, and includes evaluation of generalization to objects outside the training traffic environment.

Result: The model achieves accurate reconstruction of the original point cloud with an efficient latent space. It demonstrates strong generalization, reconstructing objects unrelated to the training traffic environment while maintaining performance with reduced architectural complexity.

Conclusion: LiLa-Net offers a compact, resource-efficient 3D autoencoder for LiDAR data that maintains reconstruction quality and generalization while using fewer encoder layers and simplified skip connections, suggesting a favorable trade-off between model complexity and representation fidelity.

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: Open-source drone-based framework (kabr-tools) enables automated, multi-species behavioral monitoring at scale, improving granularity and reducing labor vs. ground methods.


<details>
  <summary>Details</summary>
Motivation: Traditional field observations are time-consuming, limited in scope, and labor-intensive, hindering scalable assessment of complex, multidimensional animal behaviors across landscapes.

Method: Drone-based video integrated with machine learning for detection, tracking, and behavioral classification to extract metrics like time budgets, transitions, social interactions, habitat associations, and group dynamics; validated with three case studies across 969 sequences using Plains and Grevy's zebras and mixed-species herds.

Result: Significant improvement in behavioral granularity (15% reduction in visibility loss), higher accuracy and continuity in detecting transitions, and successful analysis of 969 sequences across three case studies, revealing species-specific patterns in vigilance, habitat effect, behavioral inertia, and inter-species spatial segregation.

Conclusion: kabr-tools enables automated, scalable behavioral monitoring suitable for ecosystem-wide studies, with implications for conservation, biodiversity research, and ecological monitoring.

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: A mesh-guided, unsupervised semantic morphing framework (GaussianMorphing) that leverages 3D Gaussian Splatting anchored to mesh patches to achieve geometry- and texture-consistent 3D morphing without labels, outperforming prior methods on TexMorph.


<details>
  <summary>Details</summary>
Motivation: The field lacks effective semantic-aware 3D morphing methods that work on textured shapes without requiring pre-defined homeomorphic mappings or labeled data. There is a need to preserve local detail while maintaining global semantic coherence, across multi-view images, and to handle both geometry and texture simultaneously.

Method: GaussianMorphing unifies a deformation strategy that anchors 3D Gaussians to reconstructed mesh patches to ensure geometrically consistent transformations. It uses mesh-guided 3D Gaussian Splatting for high-fidelity geometry and appearance, and imposes topology-aware constraints to preserve texture. It also establishes unsupervised semantic correspondence via the mesh topology as a geometric prior and enforces physically plausible point trajectories to maintain structural integrity. The approach is end-to-end and does not require labeled data.

Result: On the TexMorph benchmark, GaussianMorphing substantially outperforms prior 2D/3D methods, reducing color consistency error (Delta E) by 22.2% and EI by 26.2%. Project page provided.

Conclusion: The integrated approach yields strong local detail preservation and global semantic coherence throughout morphing without labeled data, leveraging mesh topology and 3D Gaussian Splatting to bridge geometry and texture in a unified framework.

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: InPose uses a pre-trained diffusion model conditioned on rotational measurements, guided by a location-based likelihood, to achieve zero-shot pose estimation from sparse on-body sensors.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based pose estimation methods conditioned on both location and rotation generalize poorly across users due to body-size variation. A user-agnostic, zero-shot approach is desirable for sparse sensor setups.

Method: Formulate pose estimation as an inverse problem. Use a pre-trained diffusion model conditioned on rotations alone; incorporate a likelihood term derived from measured locations to guide the sampling process, yielding pose sequences that explain the sparse data.

Result: The method is reported to achieve zero-shot generalization across users and produce highly plausible pose sequences that reconcile sparse location measurements with rotational priors.

Conclusion: Zero-shot, diffusion-prior-based pose estimation is achievable by decoupling body-size-sensitive location cues from rotation priors, enabling robust inference from sparse sensor data.

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: A Vision-Guided Diffusion Model (VGDM) combines a Vision Transformer with a diffusion-based denoising process for brain tumor detection and segmentation, achieving improved boundary precision and volumetric accuracy over U-Net baselines.


<details>
  <summary>Details</summary>
Motivation: Conventional CNNs like U-Net struggle to capture long-range dependencies in complex brain tumor structures; diffusion models can refine segmentation boundaries, and integrating global context via a transformer could enhance robustness and scalability for MRI volumes.

Method: Embed a Vision Transformer at the core of a diffusion-based segmentation framework. The model uses global contextual reasoning across the entire MRI volume within the diffusion process, and iterative denoising refines voxel-level details to produce accurate tumor segmentation. This hybrid design aims to improve both volumetric accuracy and boundary precision.

Result: Experimental validation on MRI brain tumor datasets shows consistent improvements in Dice similarity coefficient and Hausdorff distance compared with baseline methods.

Conclusion: Transformer-guided diffusion models are a promising direction for brain tumor detection and segmentation, offering robustness and boundary precision improvements beyond conventional U-Net approaches.

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,R√©mi Lemoy*

Main category: cs.CV

TL;DR: First national-scale, open-access urban footprint for historic France (1925-1950) built via a dual-pass U-Net; 73% accuracy; code and data released.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of nationwide digital urban footprint data for historical France pre-1970s, enabling long-term urbanization studies.

Method: Dual-pass U-Net: first pass identifies confusion areas (text/roads) to guide targeted augmentation; second pass uses refined data and binarized first-pass output to reduce radiometric noise; HPC processing of 941 high-res tiles to produce a national raster; handles radiometric/style variation in historical maps.

Result: Produces first open-access, national-scale urban footprint dataset for 1925-1950; overall accuracy of 73%; mitigates artifacts like labels/contour lines; supports long-term urbanization research.

Conclusion: Openly releasing code, training data, and resulting raster will facilitate future research on urban dynamics in historical contexts.

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: Point-based tracking in surgical videos competes with mask-based methods for tools but underperforms on anatomical targets due to tissue similarity and ambiguous boundaries; provides actionable guidance on point placement.


<details>
  <summary>Details</summary>
Motivation: To understand reliability and failure modes of point-based tracking in complex surgical environments and provide practical guidelines.

Method: Systematic analysis on laparoscopic cholecystectomy videos focusing on three targets (gallbladder, grasper, L-hook electrocautery); compare point-based tracking against segmentation mask initialization; include qualitative analysis.

Result: Point-based tracking is competitive for surgical tools but consistently underperforms for anatomical targets due to tissue similarity and ambiguous boundaries; qualitative factors influencing tracking outcomes identified; actionable recommendations for selecting and placing tracking points to improve performance.

Conclusion: The study offers guidelines to improve point-based tracking in surgical video analysis and discusses trade-offs with segmentation mask initialization.

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: Introduces FFREEDG, a federated learning task for semantic segmentation with unlabeled client data and no access to the server's source data, and presents FRIEREN, a Vision-Language model‚Äìbased framework that uses CLIP-based text guidance and weak-to-strong consistency for robust pseudo-label training.


<details>
  <summary>Details</summary>
Motivation: Tackle domain shifts in federated semantic segmentation when client data are unlabeled and server data cannot be re-accessed; leverage powerful Vision Foundation Models to improve semantic disambiguation across domains.

Method: Pretraining on a labeled source server, then federated training across clients using only unlabeled data. FRIEREN integrates a Vision-Language decoder guided by CLIP-based text embeddings, and employs weak-to-strong consistency learning for robust local training on pseudo-labels.

Result: Experimental results on synthetic-to-real and clear-to-adverse-weather benchmarks show that FRIEREN effectively addresses FFREEDG, achieving competitive performance against established domain generalization and adaptation methods and establishing a strong baseline for future research.

Conclusion: FFREEDG is a viable and challenging benchmark for federated semantic segmentation with unlabeled data and no source access, and FRIEREN demonstrates how Vision-Language collaboration via CLIP and consistency-based pseudo-labeling can enable strong performance under such constraints.

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint introduces a structured, action-centric prompting framework for frozen vision-language models in video anomaly detection, yielding improved accuracy, interpretability, and cross-dataset/generalization without training.


<details>
  <summary>Details</summary>
Motivation: Current prompts for VLM-based VAD are often overly abstract and fail to capture fine-grained human‚Äìobject interactions and action semantics essential for anomalies; there is a need for interpretable, domain-informed prompts and training-free approaches.

Method: Organize prompts into semantically coherent groups (e.g., violence, property crimes, public safety) and craft fine-grained guiding questions that tie model predictions to discriminative visual cues; leverages action-centric knowledge with frozen VLMs, enabling training-free inference.

Result: On UCF-Crime and XD-Violence, ASK-Hint yields consistent AUC gains over prior baselines, attaining state-of-the-art performance among both fine-tuned and training-free methods; demonstrates interpretable reasoning traces and strong generalization across datasets and VLM backbones.

Conclusion: Prompt granularity and structured, action-centric prompting are crucial; ASK-Hint provides a generalizable, training-free pathway for explainable video anomaly detection.

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify introduces a lightweight Student Affinity Network and geometry-guided pooling to purify 2D VLM-derived 3D features using geometric priors from a 3D self-supervised teacher, achieving data-efficient, state-of-the-art 3D semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Transferring 2D Vision-Language Model features to 3D segmentation faces a persistent trade-off: directly projecting 2D features yields noisy, fragmented predictions, while enforcing geometric coherence requires costly training and large-scale 3D annotations. The limitation stems from treating segmentation and matching separately, failing to reconcile 2D semantics with 3D geometry.

Method: GeoPurify uses a small Student Affinity Network to purify 2D VLM-generated 3D point features by leveraging geometric priors distilled from a 3D self-supervised teacher. At inference, a Geometry-Guided Pooling module denoises the point cloud to preserve semantic-structural consistency, exploiting latent geometric information for improved data efficiency.

Result: On major 3D benchmarks, GeoPurify matches or surpasses state-of-the-art performance while using only about 1.5% of the training data, demonstrating superior data efficiency; code is available publicly.

Conclusion: Exploiting latent geometric cues within 2D-to-3D transfers via a lightweight affinity network and geometry-guided pooling can overcome the traditional trade-off, enabling high-quality 3D semantic segmentation with substantially less annotated data.

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: Noninvasive auricular vein biometrics for pig identification using smartphone imagery; SVM achieves 98.12% precision on 800 ear images from 20 mixed-breed pigs; processing ~8.3 seconds per identification; feasible for real-time farm deployment.


<details>
  <summary>Details</summary>
Motivation: Need for reliable, low-cost, noninvasive identification in small-scale farming. Ear tags and microchips are unreliable or costly and often breed-specific. Auricular vein biometrics offer a permanent, stress-free solution to digitize livestock management.

Method: Dataset: 800 ear images from 20 mixed-breed pigs (Landrace cross Pietrain and Duroc cross Pietrain). Imaging with a standard smartphone and simple back lighting. A multistage computer vision pipeline enhances vein visibility, extracts structural and spatial features, and generates biometric signatures. Classifiers (with SVM achieving the best performance) are trained to identify individual pigs.

Result: SVM-based model achieved 98.12% precision in identifying pigs across mixed-breed populations. Total processing time from imaging to classification averaged 8.3 seconds, indicating feasibility for real-time farm use.

Conclusion: The study demonstrates the practicality of auricular vein biometrics for digitizing livestock management and supports broader adoption of precision farming techniques in resource-constrained communities. The approach offers a noninvasive, cost-effective alternative to traditional identifiers.

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: Multicategory crowd counting using a Twins pyramid vision-transformer backbone with a specialized multi-class counting head and a segmentation-based Category Focus Module. It achieves large MAE reductions on VisDrone/iSAID and demonstrates ecological applications, including biodiversity monitoring.


<details>
  <summary>Details</summary>
Motivation: In dense or occluded scenes, detection-based counting struggles. There is a need for multicategory crowd counting that suppresses inter-category interference and extends counting approaches to ecological monitoring domains.

Method: A Twins pyramid vision-transformer backbone feeds a multiscale decoding-based counting head for multicategory estimation. A two-task design introduces a segmentation-based Category Focus Module to suppress inter-category cross-talk during training. The model leverages a regional loss and is evaluated on VisDrone and iSAID, with demonstration on a biodiversity monitoring dataset.

Result: The approach achieves substantial MAE reductions (33%, 43%, and 64% relative to prior multicategory methods). It also outperforms a detection-based baseline such as YOLOv11 in dense scenes, underscoring the need for crowd-counting approaches in such contexts.

Conclusion: The regional loss enables robust multi-class crowd counting across new domains, including biodiversity monitoring, enabling scalable ecological insights and conservation-relevant analyses.

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl enables precise temporal alignment of visual concepts during inference in text-to-video diffusion by steering cross-attention with a triad of signals (correlation, energy, entropy), without retraining.


<details>
  <summary>Details</summary>
Motivation: Generative video models currently lack fine-grained temporal control; users cannot specify when particular visual elements should appear in a sequence, limiting coherence and alignment with cues like actions or audio.

Method: Optimize and steer cross-attention maps during inference using three complementary objectives: (1) correlate attention‚Äôs temporal shape with a control signal, (2) boost attention where the element should be visible (energy), and (3) preserve spatial focus (entropy). No retraining or extra supervision is required.

Result: Demonstrates improved temporal control across tasks, including temporal reordering for single and multiple objects and action/audio-aligned generation, while maintaining high video quality and diversity.

Conclusion: TempoControl provides a practical, supervision-free means to temporally steer concept appearance in video generation, enabling precise timing adjustments and broader applicabilities without sacrificing quality.

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: RewardMap introduces a multi-stage RL framework with dense VQA-derived rewards and a difficulty-aware reward design to improve fine-grained visual reasoning in multimodal LLMs; achieves ~3.47% average improvement across 6 benchmarks on ReasonMap, ReasonMap-Plus, and beyond.


<details>
  <summary>Details</summary>
Motivation: ReasonMap shows that multimodal LLMs struggle with spatial reasoning in structured, information-rich visuals like transit maps, and standard RL is hampered by sparse rewards; there is a need for better cold-start training and denser supervision to develop fine-grained visual understanding.

Method: Build ReasonMap-Plus by adding dense rewards via Visual Question Answering tasks to enable effective cold-start training. Propose RewardMap with (i) a difficulty-aware reward design that provides detail rewards to alleviate sparse rewards, and (ii) a multi-stage RL scheme that boots training from simple perception to complex reasoning, offering a more effective alternative to standard SFT.

Result: Experiments show that each component of RewardMap contributes to consistent performance gains, and their combination yields the best results. Models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, indicating enhanced visual understanding and reasoning capabilities.

Conclusion: RewardMap effectively enhances visual understanding and reasoning in multimodal LLMs, offering a better cold-start strategy and denser supervision through the proposed dense reward signals and staged RL, with demonstrated generalization beyond transit-map domains.

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow introduces region-based drag editing for diffusion-based image editing using FLUX priors (DiT), outperforming point-based methods and previous region-based baselines; it uses affine-region supervision, subject personalization adapters, gradient-mask constraints, and multimodal LLMs, with ReD Bench to benchmark region-level dragging; achieves state-of-the-art results on DragBench-DR and ReD Bench.


<details>
  <summary>Details</summary>
Motivation: Drag-based editing with earlier base models caused distortions due to projection gaps from Stable Diffusion. Stronger priors from DiT-based flow matching (FLUX) enable better editing but have not been effectively leveraged for drag-based editing. There's a need for region-aware supervision to exploit structured features in DiTs, plus mechanisms to preserve background and subject fidelity and resolve task ambiguity.

Method: Proposes DragFlow: a region-based editing framework that uses affine transformations for region-level supervision within DiT/FLUX priors. Replaces point-based guidance with region-based cues; integrates IP-Adapter-like personalization adapters to maintain subject consistency; uses gradient mask-based hard constraints to preserve background fidelity; employs multimodal large language models (MLLMs) to disambiguate tasks. Introduces ReD Bench, a Region-based Dragging benchmark, and evaluates on DragBench-DR and ReD Bench, comparing against point-based and region-based baselines.

Result: DragFlow achieves substantial gains over baselines, setting a new state-of-the-art in drag-based image editing on both DragBench-DR and ReD Bench. Region-based supervision with affine transformations provides more reliable guidance for DiT features than point-based methods; personalization adapters and gradient masks help maintain subject and background fidelity. Code and datasets will be released.

Conclusion: Region-based editing with strong diffusion priors, subject personalization, and ambiguity-resolving tools significantly improves drag-based image editing, establishing a new performance baseline and providing benchmark resources for future work.

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: A training-free method F2C improves video LLM understanding by selecting temporally coherent key clips under a constant token budget, outperforming uniform sampling on three long-form video benchmarks.


<details>
  <summary>Details</summary>
Motivation: Video LLMs face the needle-in-a-haystack problem: raw video frames generate too many tokens, and frame-wise frame selection loses crucial temporal dynamics needed to reason about motion and events. Preserving temporal coherence in selection should improve understanding of video sequences.

Method: Instead of selecting isolated key frames, the approach selects short, temporally coherent key clips and uses an adaptive resolution strategy to keep a fixed token budget. This balances clip length and spatial resolution to maximize information within a constant token count. The method is training-free and referred to as F2C.

Result: On three long-form video benchmarks (Video-MME, LongVideoBench, MLVU), F2C outperforms uniform sampling by 8.1%, 5.6%, and 10.3%, respectively.

Conclusion: Preserving temporal coherence in frame selection is crucial for video understanding in VLMs. An adaptive, clip-based sampling with a fixed token budget enables scaling Video LLMs without retraining, offering practical gains for real-world video tasks.

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fern√°ndez-Gonz√°lez,Francisco-Javier D√≠az-Pernas,Hichem Saoudi,Javier Gonz√°lez-Alonso,Mario Mart√≠nez-Zarzuela*

Main category: cs.CV

TL;DR: Monocular video 3D pose estimation can approach IMU accuracy in healthy adults for 13 daily activities but shows trade-offs in precision and generalizability.


<details>
  <summary>Details</summary>
Motivation: Need for accurate, real-world motion capture for telemedicine, sports science, and rehabilitation; benchmark comparing video-based pose estimation to IMUs using the VIDIMU dataset.

Method: Evaluate state-of-the-art DL-based 3D pose pipelines (MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, NVIDIA BodyTrack) against IMU-derived joint angles computed via OpenSim inverse kinematics (Human3.6M-like 17-keypoint format) on VIDIMU‚Äôs 13 activities; healthy subjects only.

Result: MotionAGFormer achieves best metrics: RMSE 9.27¬∞ ¬± 4.80¬∞, MAE 7.86¬∞ ¬± 4.18¬∞, Pearson r 0.86 ¬± 0.15, R¬≤ 0.67 ¬± 0.28. Both video and IMU approaches are viable for out-of-lab kinematics; video models offer clinical promise but exhibit trade-offs in cost, accessibility, and precision.

Conclusion: The study clarifies where off-the-shelf video models can support clinical kinematics in healthy adults and where IMU-based estimates remain superior, providing guidelines for researchers/clinicians to develop robust, cost-effective telehealth and remote monitoring solutions.

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift enables cross-subject visual reconstruction from fMRI using diffusion-based adapters, achieving state-of-the-art results with minimal subject-specific training by fine-tuning only 17% of parameters on lightweight GPUs.


<details>
  <summary>Details</summary>
Motivation: Cross-subject decoding of visual stimuli from fMRI is hampered by substantial inter-subject variability and the brain's abstract encoding of core semantic features, making scalable and accurate reconstruction challenging.

Method: The approach combines AutoKL for low-level features and a CLIP adapter for semantics within a diffusion framework. The CLIP adapter is trained on Stable Diffusion-generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, the model is pretrained on one subject and then fine-tuned by updating only 17% of parameters (fully connected layers) while freezing the rest, enabling ~1 hour of per-subject training on three RTX 4090 GPUs.

Result: The method achieves state-of-the-art cross-subject reconstruction performance and outperforms existing methods while substantially reducing training time and computational requirements.

Conclusion: NeuroSwift demonstrates efficient cross-subject brain decoding with modular diffusion-based adapters and minimal fine-tuning, offering a scalable path toward practical neural reconstruction across diverse subjects.

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP is a self-training framework that enhances CLIP for fine-grained image classification by introducing a saliency-guided FG token via SOAP and a two-headed, LLM-informed classifier, plus Dynamic Knowledge Aggregation, achieving ~2.9% average accuracy gain across 13 benchmarks with light adaptation.


<details>
  <summary>Details</summary>
Motivation: Fine-grained classification requires local, spatially precise cues; while CLIP transfers well globally, its global [CLS] features miss subcategory cues. Prior LLM-alignment methods align descriptions to [CLS] but ignore spatial precision. A lightweight, self-training approach can exploit fine-grained signals while keeping adaptation light.

Method: Introduce SOAP within a TokenFusion module to build a saliency-guided [FG] token from patch embeddings and fuse with the [CLS] token to align coarse and fine features. Use a two-headed LLM-derived classifier: a frozen classifier yields a stable text-based prior for pseudo-labeling via multi-view alignment; a learnable classifier initialized from LLM descriptions and refined with TokenFusion. Propose Dynamic Knowledge Aggregation to convexly combine fixed LLM/CLIP priors with evolving TokenFusion logits to iteratively refine pseudo-labels.

Result: Yields a 2.90 percentage point average accuracy gain across 13 fine-grained benchmarks with light adaptation; demonstrates consistent improvement. Code available at GitHub.

Conclusion: The framework reveals latent fine-grained signals in CLIP, enabling effective unsupervised adaptation for fine-grained image classification with modest computational overhead.

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: VidGuard-R1 is a video authenticity detector that fine-tunes a multi-modal LLM (Qwen-VL) with group relative policy optimization, achieving state-of-the-art results and offering interpretable explanations on a 140k-video dataset; code released.


<details>
  <summary>Details</summary>
Motivation: To counter AI-generated video threats (misinformation and reputational harm) and to provide transparent, interpretable detections for regulators and end users.

Method: Curated 140k real/AI-generated videos with hard-discrimination design; fine-tuned Qwen-VL using GRPO and two reward models targeting temporal artifacts and generation complexity; evaluated in zero-shot and with additional training to reach ~95% accuracy; case studies show rationales.

Result: Achieves state-of-the-art zero-shot performance on benchmarks; training pushes accuracy above 95%; interpretable rationales produced; code publicly available.

Conclusion: Demonstrates effective combination of GRPO-based fine-tuning and MLLMs for accurate, interpretable video authenticity detection; contributes dataset and methodology, advancing transparency in AI-generated content detection.

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: Introduces a self-guided, teacher-assisted approach to long-horizon video generation that uses samples from self-generated sequences to steer a student model, achieving up to 20x horizon extension and 4m15s video length with improved fidelity and temporal consistency, without long-video teachers or retraining.


<details>
  <summary>Details</summary>
Motivation: To address error accumulation and quality degradation when extrapolating diffusion-based video generation beyond short-horizon teacher capabilities, while reducing computational costs of transformer-based models and avoiding the need for long-video supervision.

Method: Leverages teacher models' rich knowledge to guide the student via sampled segments from self-generated long videos; maintains temporal consistency; avoids recomputing overlapping frames; scales length up to 20x beyond teacher; relies on the base model's position embedding capacity; uses an improved benchmarking dataset; builds improved benchmark.

Result: Outperforms baselines in fidelity and consistency; achieves up to 4:15 duration (99.9% of base model's position-embedding span) and >50x longer than baseline; demonstrates strong gains on standard benchmarks and proposed improved benchmark.

Conclusion: The proposed self-forcing guidance effectively mitigates long-horizon degradation, enabling practical long video generation with preserved quality without needing long-video teachers or retraining, and the approach is scalable to very long video horizons.

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask enables physics-guided video generation by conditioning on object velocity and a two-stage training strategy that gradually removes future motion supervision, improving realistic rigid-body interactions and enabling multi-level conditioning; trained on synthetic scenes and validated on real scenes; will release code, model, and data.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with physically plausible object interactions and physics-based control, limiting applicability to robotics and simulation.

Method: Two-stage training that reduces reliance on future motion supervision via object masks; train video diffusion models on synthetic scenes of simple interactions; integrate low-level motion control with high-level textual conditioning through predictive scene descriptions.

Result: Significant improvements in object interactions on real scenes; ablations show complementary roles of low- and high-level conditioning; outperform recent models of similar size; results validated through extensive experiments.

Conclusion: Physics-guided video generation is feasible with the proposed KineMask framework, enabling realistic dynamics and controllable synthesis; the approach bridges low- and high-level conditioning and offers practical resources (code/model/data).

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: A multimodal action framework for fine-grained control using proprioception, kinesthesia, force haptics, and muscle activation; aligns modalities with a learnable feature space and introduces causality regularization, improving simulation accuracy and reducing drift.


<details>
  <summary>Details</summary>
Motivation: Current video models lack robust world models suitable for real-time, fine-grained motor control in household robots, making delicate tasks and urgent situations difficult to handle.

Method: A feature learning paradigm that aligns multiple modalities while preserving the unique information each modality provides, coupled with a regularization scheme that enhances the causality of action trajectory features to capture intricate interaction dynamics.

Result: Incorporating multimodal senses improves simulation accuracy and reduces temporal drift; extensive ablations and downstream applications demonstrate effectiveness and practicality.

Conclusion: Multimodal sensing enables fine-grained control and practical benefits for real-time robotics and simulation, offering improvements over text-conditioned models in capturing nuanced interactions.

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA extends Native Sparse Attention to video-language models, enabling long-context understanding with a hardware-aware hybrid attention strategy.


<details>
  <summary>Details</summary>
Motivation: Long context length in video-language models leads to missed transition frames and coherence loss over extended time scales; scaling attention efficiently for long videos is a key challenge.

Method: Introduce VideoNSA by adapting Native Sparse Attention to video-language modeling (Qwen2.5-VL). Train end-to-end on a 216K video-instruction dataset. Use a hardware-aware hybrid attention: dense attention for text tokens, sparse (NSA) attention for video tokens. Employ a learnable, global-local attention allocation under a fixed budget with multiple sparse attention branches, enabling dynamic attention sinks.

Result: VideoNSA outperforms token-compression and training-free sparse baselines on long-video understanding, temporal reasoning, and spatial benchmarks. Ablations show: (1) reliable scaling to 128K tokens; (2) optimal global-local attention allocation at fixed budget; (3) task-dependent branch usage patterns; (4) learnable combined sparse attention induces dynamic attention sinks.

Conclusion: VideoNSA demonstrates effective long-context video-language modeling via a hardware-aware sparse-attention approach, offering practical scaling to very long sequences and insights into how to allocate attention between global and local contexts, as well as how learnable sparse attention can adapt to tasks.

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift is a training-free noise-calibration method that conditions the diffusion denoiser on image resolution to improve low-resolution generation without changing architecture or sampling, showing notable FID gains across SD3, SD3.5, and Flux-Dev on LAION-COCO and CelebA.


<details>
  <summary>Details</summary>
Motivation: Diffusion models trained at fixed resolutions struggle to generalize to lower resolutions, and the same noise level disproportionately harms low-resolution images, causing train-test mismatch. There is a need for a simple, budget-friendly way to improve low-res generation without retraining or changing architecture.

Method: NoiseShift recalibrates the denoiser's noise conditioning as a function of target resolution. It is training-free, requires no architectural changes or sampling schedule changes, and is compatible with existing diffusion models like Stable Diffusion 3/3.5 and Flux-Dev.

Result: On LAION-COCO, FID improves (lower is better) by 15.89% for SD3.5, 8.56% for SD3, and 2.44% for Flux-Dev on average. On CelebA, improvements are 10.36% (SD3.5), 5.19% (SD3), and 3.02% (Flux-Dev).

Conclusion: NoiseShift effectively mitigates resolution-dependent artifacts in low-resolution image generation, offering a practical, training-free enhancement compatible with existing diffusion models and sampling workflows.

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: The work studies predicting dynamic physical properties from videos (elasticity, viscosity, dynamic friction), introducing datasets and evaluating three inference approaches‚Äîoracle CV cues, visual-prompt readouts on video foundation models, and prompting MLLMs‚Äîwith video foundation models nearing oracle performance while MLLMs lag but can be improved.


<details>
  <summary>Details</summary>
Motivation: Temporal physical properties require time-based cues; inferring properties like elasticity, viscosity, and friction from video hinges on temporal dynamics, posing a challenge for conventional vision models. The study contributes new benchmarks to evaluate such reasoning, including synthetic and real-world splits.

Method: Datasets: new video datasets for each property, with synthetic training/testing splits and a real-world evaluation split. Methods: (a) oracle using classical computer-vision cues tailored to each property; (b) a readout mechanism using a visual prompt and a trainable prompt vector for cross-attention on pretrained video generative/self-supervised models; (c) prompt-based strategies for multi-modal large language models (MLLMs).

Result: Video foundation models trained in generative or self-supervised regimes achieve performance close to the oracle but still behind it; MLLMs underperform relative to the other models, though prompting can improve their results.

Conclusion: Foundational video models can approximate physics cues from motion data, underscoring the value of temporal information for dynamic property inference. MLLMs show potential but require more effective prompting; the introduced datasets offer a benchmark to compare physical-property inference methods from video.

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: Introduces sounding object detection, a multimodal, object-centric task that links sounds to the specific objects involved in everyday interactions, using segmentation-guided training and a slot-attention encoder to enforce object priors, achieving state-of-the-art results on the new task and related multimodal benchmarks.


<details>
  <summary>Details</summary>
Motivation: Reflects human perception by tying sounds to the exact object involved in an interaction. The paper argues that everyday object sounds are object-specific and that models should learn to associate audio with the objects rather than general scene cues, prompting an object-centric multimodal learning framework evaluated on egocentric video.

Method: Develops an automatic pipeline to compute segmentation masks to guide training toward informative regions of object interaction. Employs a slot-attention visual encoder to enforce an object-centric prior. Proposes the sounding object detection task and evaluates on this task and existing multimodal action understanding benchmarks using in-the-wild egocentric video data.

Result: Achieves state-of-the-art performance on the new sounding object detection task and on existing multimodal action understanding tasks, demonstrating the effectiveness of object-centric, audio-visual learning.

Conclusion: The work validates that incorporating object priors and segmentation-guided training enables models to link sounds to the corresponding objects, contributing to more interpretable and human-like multimodal understanding in real-world videos.

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: A density-guided poisoning attack on 3D Gaussian Splatting (3DGS) using KDE to insert low-density Gaussian points that create illusory objects visible from poisoned views, plus adaptive noise to break multi-view consistency, and a KDE-based evaluation protocol for benchmarking; results show state-of-the-art effectiveness.


<details>
  <summary>Details</summary>
Motivation: As 3D scene representations like NeRF and 3DGS become widespread, their vulnerabilities warrant study. The abstract focuses on poisoning attacks against 3DGS, highlighting robustness concerns and the need for evaluation and defenses against image-level tampering.

Method: Identify low-density regions via Kernel Density Estimation (KDE) and inject Gaussian points into those regions to embed viewpoint-dependent illusory objects that are clearly visible from poisoned views while minimally affecting benign views. Employ an adaptive noise strategy to disrupt multi-view consistency across views. Propose a KDE-based evaluation protocol to systematically assess attack difficulty and benchmark future work.

Result: Extensive experiments show the proposed method achieves superior performance compared to state-of-the-art poisoning techniques, validating its effectiveness.

Conclusion: The work presents a stealthy, density-guided poisoning approach for 3DGS along with an KDE-based evaluation framework, offering a new toolkit for evaluating vulnerabilities and guiding defenses in 3D scene representations.

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: Introduces a principled, optimizable framework to improve multi-subject fidelity in text-to-image generation by treating sampling as a stochastic control problem and disentangling subjects via flow matching.


<details>
  <summary>Details</summary>
Motivation: T2I models excel for single entities but fail on multi-subject prompts due to attribute leakage, identity entanglement, and omissions. The work seeks a theoretical objective to steer sampling dynamics toward faithful multi-subject rendering and to unify prior heuristics.

Method: Formulates subject disentanglement as control over a trained flow-matching sampler via stochastic optimal control. Proposes two architecture-agnostic algorithms: (1) a training-free test-time controller that perturbs the base velocity with a single-pass update; (2) Adjoint Matching, a light fine-tuning rule regressing a control network to a backward adjoint signal while preserving base-model capabilities. Extends flow-matching to flow-diffusion and unifies attention heuristics.

Result: On Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both methods improve multi-subject alignment while maintaining style. Test-time control is efficient on commodity GPUs and the learned controllers generalize to unseen prompts. FOCUS achieves state-of-the-art multi-subject fidelity across models.

Conclusion: A unified, theory-backed pathway for multi-subject fidelity in diffusion-based T2I, offering practical, scalable tools (training-free control and Adjoint Matching) and a coherent link between attention cues and sampling dynamics, with broad applicability across diffusion-model families.

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [Accelerating Long-Term Molecular Dynamics with Physics-Informed Time-Series Forecasting](https://arxiv.org/abs/2510.01206)
*Hung Le,Sherif Abbas,Minh Hoang Nguyen,Van Dai Do,Huu Hiep Nguyen,Dung Nguyen*

Main category: cs.LG

TL;DR: A time-series forecasting framework for MD that predicts atomic displacements using a physics-informed loss with DFT-parametrised Morse potentials, achieving accurate, scalable simulations faster than traditional DFT.


<details>
  <summary>Details</summary>
Motivation: DFT-based MD is computationally expensive, limiting long-term, large-system simulations; integrating physics knowledge into ML can improve physical plausibility and reliability.

Method: Formulate MD as a time-series forecasting task; use advanced forecasting models to predict atomic displacements rather than absolute positions; incorporate a physics-informed loss and an inference mechanism derived from DFT-parametrised pairwise Morse potentials to penalize unphysical atomic proximity.

Result: Consistently outperforms standard baselines in simulation accuracy across diverse materials; enables stable modeling of thousands of MD steps in minutes, offering a scalable alternative to costly DFT simulations.

Conclusion: Embedding physics knowledge into ML-based MD forecasting improves reliability and precision and provides a scalable, efficient alternative to conventional DFT-based approaches.

Abstract: Efficient molecular dynamics (MD) simulation is vital for understanding
atomic-scale processes in materials science and biophysics. Traditional density
functional theory (DFT) methods are computationally expensive, which limits the
feasibility of long-term simulations. We propose a novel approach that
formulates MD simulation as a time-series forecasting problem, enabling
advanced forecasting models to predict atomic trajectories via displacements
rather than absolute positions. We incorporate a physics-informed loss and
inference mechanism based on DFT-parametrised pair-wise Morse potential
functions that penalize unphysical atomic proximity to enforce physical
plausibility. Our method consistently surpasses standard baselines in
simulation accuracy across diverse materials. The results highlight the
importance of incorporating physics knowledge to enhance the reliability and
precision of atomic trajectory forecasting. Remarkably, it enables stable
modeling of thousands of MD steps in minutes, offering a scalable alternative
to costly DFT simulations.

</details>


### [79] [Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs](https://arxiv.org/abs/2510.01218)
*Sergey Troshin,Wafaa Mohammed,Yan Meng,Christof Monz,Antske Fokkens,Vlad Niculae*

Main category: cs.LG

TL;DR: Selective sampling dynamically switches between greedy and high-temperature sampling using a sampling-risk metric, guided by a lightweight classifier trained on verifiable problems, to improve the quality‚Äìdiversity trade-off in math reasoning tasks with minimal latency overhead.


<details>
  <summary>Details</summary>
Motivation: High-temperature sampling boosts diversity but hurts precision in tasks requiring exact reasoning (e.g., math). There is a need for risk-aware sampling that preserves accuracy while maintaining diversity.

Method: Introduce selective sampling that computes a sampling risk metric at each token position and dynamically chooses between greedy decoding and high-temperature sampling. Train a lightweight classifier on a small set of verifiable problems to predict sampling risk and integrate it with the base language model with minimal latency. The classifier informs when to apply high-temperature sampling based on estimated risk.

Result: Experiments on mathematical reasoning tasks show an improved quality‚Äìdiversity trade-off under high-temperature settings, indicating that selective sampling reduces error-prone risky sampling while preserving diversity.

Conclusion: Selective sampling provides a practical, low-overhead approach to risk-aware decoding that can be integrated with existing LMs to improve accuracy in precision-demanding tasks while maintaining diversity.

Abstract: Diversity is an essential metric for evaluating the creativity of outputs
generated by language models. Temperature-based sampling is a common strategy
to increase diversity. However, for tasks that require high precision, e.g.,
mathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$
or top-$p$, degrades reasoning quality. We demonstrate that the loss of
accuracy is caused by sampling incorrect continuations in sensitive decoding
positions. To address this, in this paper, we propose \textbf{selective
sampling}, a method that dynamically switches between greedy and
high-temperature sampling based on a sampling risk metric. This risk metric
estimates the likelihood of output errors when applying high-temperature
sampling on the current token position. To predict sampling risk, we train a
lightweight classifier on a small subset of verifiable problems. The trained
classifier can be integrated with the base language model with minimal latency
overhead. Experiments on mathematical reasoning tasks demonstrate that
selective sampling enhances the quality-diversity trade-off, even in
high-temperature settings.

</details>


### [80] [Automated Extraction of Material Properties using LLM-based AI Agents](https://arxiv.org/abs/2510.01235)
*Subham Ghosh,Abhishek Tewari*

Main category: cs.LG

TL;DR: Large-scale, LLM-driven workflow autonomously extracts thermoelectric and structural properties from ~10k full-text articles, achieving high accuracy and delivering a public dataset and tools to enable scalable data-driven materials discovery beyond thermoelectrics.


<details>
  <summary>Details</summary>
Motivation: Addresses the scarcity of large, machine-readable datasets linking performance with structural context; current databases are small, manual, or biased toward first-principles results, underutilizing experimental literature.

Method: An agentic, multimodal LLM workflow with dynamic token allocation, zero-shot multi-agent extraction, and conditional table parsing. Benchmarked on 50 curated papers (GPT-4.1 best, GPT-4.1 Mini cost-efficient). Curated 27,822 temperature-resolved property records (ZT, Seebeck, conductivity, resistivity, power factor, thermal conductivity) and structural attributes (crystal class, space group, doping). Provided an interactive web explorer with semantic filters and CSV export.

Result: GPT-4.1 achieved F1 of 0.91 for thermoelectric properties and 0.82 for structural fields; GPT-4.1 Mini achieved F1 of 0.89 and 0.81 respectively. Dataset includes normalized records; analysis recapitulates known thermoelectric trends (alloys outperform oxides, p-type doping advantage) and reveals broader structure‚Äìproperty correlations. The workflow is reproducible, cost-profiled, and scalable, culminating in the largest LLM-curated thermoelectric dataset to date and an accessible community explorer.

Conclusion: Demonstrates scalable, data-driven material discovery beyond thermoelectrics, providing a reproducible extraction pipeline, a large curated dataset, and tools to facilitate community access and further discovery.

Abstract: The rapid discovery of materials is constrained by the lack of large,
machine-readable datasets that couple performance metrics with structural
context. Existing databases are either small, manually curated, or biased
toward first principles results, leaving experimental literature
underexploited. We present an agentic, large language model (LLM)-driven
workflow that autonomously extracts thermoelectric and structural-properties
from about 10,000 full-text scientific articles. The pipeline integrates
dynamic token allocation, zeroshot multi-agent extraction, and conditional
table parsing to balance accuracy against computational cost. Benchmarking on
50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91
for thermoelectric properties and 0.82 for structural fields), while GPT-4.1
Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction
of the cost, enabling practical large scale deployment. Applying this workflow,
we curated 27,822 temperature resolved property records with normalized units,
spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity,
power factor, and thermal conductivity, together with structural attributes
such as crystal class, space group, and doping strategy. Dataset analysis
reproduces known thermoelectric trends, such as the superior performance of
alloys over oxides and the advantage of p-type doping, while also surfacing
broader structure-property correlations. To facilitate community access, we
release an interactive web explorer with semantic filters, numeric queries, and
CSV export. This study delivers the largest LLM-curated thermoelectric dataset
to date, provides a reproducible and cost-profiled extraction pipeline, and
establishes a foundation for scalable, data-driven materials discovery beyond
thermoelectrics.

</details>


### [81] [RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models](https://arxiv.org/abs/2510.01240)
*Zukang Xu,Xing Hu,Qiang Wu,Dawei Yang*

Main category: cs.LG

TL;DR: RSAVQ uses information-geometry-guided error direction and channel sensitivity to achieve extremely low-bit quantization (2-bit) for LLMs, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: LLMs have enormous parameters making deployment on resource-constrained devices challenging. Vector quantization to ultra-low bits (2‚Äì4 bits) helps, but existing methods suffer from unconstrained error directions and suboptimal bit allocation.

Method: Introduce two geometry-driven components based on Fisher Information Matrix: (1) Error Direction Sensitivity Guidance (EDSG) projects quantization errors along low-sensitivity directions using the negative natural gradient to suppress error expansion; (2) Weight Channel Sensitivity Guidance (WCSG) builds a channel-wise sensitivity metric via FIM curvature to adaptively allocate bit precision per channel, aiming for a globally optimal quantization under bit constraints.

Result: RSAVQ achieves better performance than baselines on LLM quantization. In 2-bit quantization of LLaMA-3 8B, it improves perplexity by 0.4 points and zero-shot accuracy by about 1.5 percentage points compared with VPTQ and QuIP#.

Conclusion: The approach offers a practical solution for deploying LLMs in constrained environments and provides a theoretical link between information geometry and neural network quantization, advancing efficient deep learning.

Abstract: Large language models (LLMs) have demonstrated remarkable performance across
a wide range of natural language processing tasks. However, their exponentially
increasing parameters pose significant challenges for deployment on
resource-constrained devices. Vector Quantization (VQ) shows great promise for
low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key
challenges: unconstrained direction error and suboptimal bit allocation. In
this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit
quantization for LLMs. RSAVQ introduces two geometry-driven innovations that
effectively mitigate above limitations: (1) Error Direction Sensitivity
Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced
Riemannian metric to project quantization errors onto low-sensitivity
directions in the parameter space. Specifically, this projection is performed
along the negative natural gradient direction, which effectively suppresses
error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which
constructs a channel-wise sensitivity metric via FIM curvature analysis to
dynamically guide bit resource allocation. The approach facilitates a globally
optimal quantization solution within prescribed bit constraints. Experiments
demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in
2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by
0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a
practical solution for constrained environments and a theoretical bridge
between information geometry and the quantization of neural networks, advancing
efficient deep learning.

</details>


### [82] [Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks](https://arxiv.org/abs/2510.01261)
*Vedant Palit*

Main category: cs.LG

TL;DR: A defense for federated learning against poisoning/backdoor under partial observability using a trust-aware DQN, achieving better robustness-accuracy trade-off on CIFAR-10.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to poisoning/backdoor attacks, especially when client observability is partial; there is a need to fuse multi-signal evidence and sequential decision-making to update client trust and improve robustness without sacrificing accuracy.

Method: Formulates defense as a partially observable sequential decision problem; proposes a trust-aware Deep Q-Network that updates client trust from multiple signals and optimizes a long-horizon robustness-accuracy objective; empirical CIFAR-10 experiments with Dirichlet overlap, signal-budget analysis, and controller comparisons.

Result: Empirical results show baseline accuracy improvement; increased client overlap improves accuracy and reduces ASR with stable detection; decreasing observability keeps accuracy steady but ASR rises and ROC-AUC falls, showing sequential belief updates help with weaker signals; DQN achieves best robustness-accuracy trade-off vs random, linear-Q, and policy gradient baselines.

Conclusion: Trust-aware DQN yields superior robustness-accuracy trade-off under partial observability for federated learning; sequential belief updates mitigate weak signals and improve trust estimation compared to baselines.

Abstract: Federated learning is vulnerable to poisoning and backdoor attacks under
partial observability. We formulate defence as a partially observable
sequential decision problem and introduce a trust-aware Deep Q-Network that
integrates multi-signal evidence into client trust updates while optimizing a
long-horizon robustness--accuracy objective. On CIFAR-10, we (i) establish a
baseline showing steadily improving accuracy, (ii) show through a Dirichlet
sweep that increased client overlap consistently improves accuracy and reduces
ASR with stable detection, and (iii) demonstrate in a signal-budget study that
accuracy remains steady while ASR increases and ROC-AUC declines as
observability is reduced, which highlights that sequential belief updates
mitigate weaker signals. Finally, a comparison with random, linear-Q, and
policy gradient controllers confirms that DQN achieves the best
robustness--accuracy trade-off.

</details>


### [83] [RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction](https://arxiv.org/abs/2510.01262)
*Koyena Chowdhury,Paramita Koley,Abhijnan Chakraborty,Saptarshi Ghosh*

Main category: cs.LG

TL;DR: Proposes Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN) to predict average station arrival delays in a large-scale Indian Rail Network; introduces train-frequency-aware spatial attention; releases a large IRN dataset; demonstrates improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Accurate delay forecasting is crucial for efficient railway operations and higher-level traffic management. The shift from predicting individual train delays to station-level arrivals enables broader decision support, and the authors address a data and scalability gap by building a large, diverse IRN dataset.

Method: RSTGCN combines spatio-temporal graph convolutional networks with railway-specific features, notably train-frequency-aware spatial attention, to forecast the average arrival delays of all incoming trains at each station for a given time period. The study uses data from 4,735 stations across 17 zones and benchmarks against multiple state-of-the-art baselines.

Result: The approach achieves consistent improvements over baselines on standard metrics across the large-scale IRN setting, and the authors release the dataset to support further research in railway delay forecasting.

Conclusion: RSTGCN advances station-level delay prediction in large railway networks and provides a valuable open dataset to spur future research and better traffic management in rail systems.

Abstract: Accurate prediction of train delays is critical for efficient railway
operations, enabling better scheduling and dispatching decisions. While earlier
approaches have largely focused on forecasting the exact delays of individual
trains, recent studies have begun exploring station-level delay prediction to
support higher-level traffic management. In this paper, we propose the
Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed
to forecast average arrival delays of all the incoming trains at railway
stations for a particular time period. Our approach incorporates several
architectural innovations and novel feature integrations, including train
frequency-aware spatial attention, which significantly enhances predictive
performance. To support this effort, we curate and release a comprehensive
dataset for the entire Indian Railway Network (IRN), spanning 4,735 stations
across 17 zones - the largest and most diverse railway network studied to date.
We conduct extensive experiments using multiple state-of-the-art baselines,
demonstrating consistent improvements across standard metrics. Our work not
only advances the modeling of average delay prediction in large-scale rail
networks but also provides an open dataset to encourage further research in
this critical domain.

</details>


### [84] [Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency](https://arxiv.org/abs/2510.01263)
*Yaron Meirovitch,Fuming Yang,Jeff Lichtman,Nir Shavit*

Main category: cs.LG

TL;DR: Budgeted Broadcast (BB) is a sparsity method that uses per-unit traffic budgets to optimize coding entropy, balancing selectivity and audience, yielding higher entropy, decorrelation, and often improved accuracy across diverse models and tasks.


<details>
  <summary>Details</summary>
Motivation: Standard pruning ranks parameters by loss impact but may neglect global resource constraints and representation diversity. BB targets a global sparsity budget while maximizing coding entropy, aiming for a more balanced and informative sparse structure that decorrelates features and preserves performance.

Method: Define a per-unit traffic budget a_i (activity) times fan-out k_i; perform a constrained-entropy analysis to maximize coding entropy under a global budget; derive a per-unit selectivity-audience balance log((1‚àía_i)/a_i)=Œ≤ k_i; implement simple local actuators to prune fan-in (reduce activity) or fan-out (reduce broadcast); evaluate on Transformers for ASR, ResNets for face identification, 3D U-Nets for synapse prediction, and electron microscopy images.

Result: BB increases coding entropy and decorrelation in practice; yields improved accuracy at matched sparsity across multiple architectures and tasks, sometimes surpassing dense baselines; achieves state-of-the-art F1 and PR-AUC on electron microscopy under the authors‚Äô protocol; easy to integrate and promotes more diverse and efficient representations.

Conclusion: BB provides a practical, integrable pruning framework that links sparsity to information-theoretic entropy, encouraging diversity in representations and potentially better generalization. It suggests a path toward learning representations with higher entropy and reduced redundancy across varied domains.

Abstract: Most pruning methods remove parameters ranked by impact on loss (e.g.,
magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each
unit a local traffic budget (the product of its long-term on-rate $a_i$ and
fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding
entropy under a global traffic budget yields a selectivity-audience balance,
$\log\frac{1-a_i}{a_i}=\beta k_i$. BB enforces this balance with simple local
actuators that prune either fan-in (to lower activity) or fan-out (to reduce
broadcast). In practice, BB increases coding entropy and decorrelation and
improves accuracy at matched sparsity across Transformers for ASR, ResNets for
face identification, and 3D U-Nets for synapse prediction, sometimes exceeding
dense baselines. On electron microscopy images, it attains state-of-the-art F1
and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests
a path toward learning more diverse and efficient representations.

</details>


### [85] [A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab](https://arxiv.org/abs/2510.01264)
*Isaac Peterson,Christopher Allred,Jacob Morrey,Mario Harper*

Main category: cs.LG

TL;DR: IsaacLab-HARL: a scalable adversarial MARL framework built on IsaacLab with heterogeneous agents and HAPPO integration, enabling training of robust adversarial policies in high-fidelity simulations, with open-source benchmarks.


<details>
  <summary>Details</summary>
Motivation: Adversarial interactions are crucial for real-world robotic applications (pursuit-evasion, security, competitive manipulation). There is a need for scalable, realistic training in high-fidelity physics to develop robust policies.

Method: Extend the IsaacLab platform to support adversarial MARL, create a suite of environments with heterogeneous agents and asymmetric goals/capabilities, and integrate a competitive variant of Heterogeneous Agent Proximal Policy Optimization (HAPPO) for efficient training under adversarial dynamics.

Result: The framework can model and train robust policies for morphologically diverse multi-agent competition while maintaining high throughput and realistic simulations; experiments across benchmark scenarios demonstrate effectiveness.

Conclusion: The proposed IsaacLab-HARL framework enables scalable adversarial MARL in high-fidelity physics, facilitating robust policy learning for competitive, heterogeneous multi-agent systems; code and benchmarks are openly available at the referenced repository.

Abstract: Multi-Agent Reinforcement Learning (MARL) is central to robotic systems
cooperating in dynamic environments. While prior work has focused on these
collaborative settings, adversarial interactions are equally critical for
real-world applications such as pursuit-evasion, security, and competitive
manipulation. In this work, we extend the IsaacLab framework to support
scalable training of adversarial policies in high-fidelity physics simulations.
We introduce a suite of adversarial MARL environments featuring heterogeneous
agents with asymmetric goals and capabilities. Our platform integrates a
competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal
Policy Optimization (HAPPO), enabling efficient training and evaluation under
adversarial dynamics. Experiments across several benchmark scenarios
demonstrate the framework's ability to model and train robust policies for
morphologically diverse multi-agent competition while maintaining high
throughput and simulation realism. Code and benchmarks are available at:
https://github.com/DIRECTLab/IsaacLab-HARL .

</details>


### [86] [RLP: Reinforcement as a Pretraining Objective](https://arxiv.org/abs/2510.01265)
*Ali Hatamizadeh,Syeda Nahida Akter,Shrimai Prabhumoye,Jan Kautz,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Yejin Choi*

Main category: cs.LG

TL;DR: Information-driven reinforcement pretraining (RLP): treat chain-of-thought as exploratory action during pretraining; rewards information gain in predicting the next token; yields notable gains across models and math-science benchmarks.


<details>
  <summary>Details</summary>
Motivation: The dominant training pipeline (pretraining with next-token prediction, then supervised fine-tuning, then RL in a final phase) may be suboptimal for emergent reasoning. The paper proposes injecting exploration and reasoning earlier in pretraining by using a reinforcement-like objective driven by information gain.

Method: RLP defines a reward as the increase in log-likelihood of the next token when conditioning on context plus a sampled reasoning chain versus conditioning on context alone. This dense, verifier-free reward turns RL for reasoning into a pretraining objective on ordinary text, enabling scalable training across full document streams. The approach is evaluated by pretraining Qwen3-1.7B-Base and applying RLP to Nemotron-Nano-12B-v2.

Result: For Qwen3-1.7B-Base, pretraining with RLP lifts average performance across an eight-benchmark math-and-science suite by 19%, with largest gains on AIME25 and MMLU-Pro. Applying RLP to Nemotron-Nano-12B-v2 increases overall average from 42.81% to 61.32%, and raises scientific reasoning average by 23%.

Conclusion: RLP offers an information-driven pretraining objective that fuses exploration with next-token modeling, yielding scalable gains in reasoning capabilities across architectures and sizes. It suggests a promising direction to integrate reasoning emergence into pretraining rather than relegating it to post-training RL, while leaving room for further ablations, broader benchmarks, and analysis of reward design.

Abstract: The dominant paradigm for training large reasoning models starts with
pre-training using next-token prediction loss on vast amounts of data.
Reinforcement learning, while powerful in scaling reasoning, is introduced only
as the very last phase of post-training, preceded by supervised fine-tuning.
While dominant, is this an optimal way of training? In this paper, we present
RLP, an information-driven reinforcement pretraining objective, that brings the
core spirit of reinforcement learning -- exploration -- to the last phase of
pretraining. The key idea is to treat chain-of-thought as an exploratory
action, with rewards computed based on the information gain it provides for
predicting future tokens. This training objective essentially encourages the
model to think for itself before predicting what comes next, thus teaching an
independent thinking behavior earlier in the pretraining. More concretely, the
reward signal measures the increase in log-likelihood of the next token when
conditioning on both context and a sampled reasoning chain, compared to
conditioning on context alone. This approach yields a verifier-free dense
reward signal, allowing for efficient training for the full document stream
during pretraining. Specifically, RLP reframes reinforcement learning for
reasoning as a pretraining objective on ordinary text, bridging the gap between
next-token prediction and the emergence of useful chain-of-thought reasoning.
Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an
eight-benchmark math-and-science suite by 19%. With identical post-training,
the gains compound, with the largest improvements on reasoning-heavy tasks such
as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2
increases the overall average from 42.81% to 61.32% and raises the average on
scientific reasoning by 23%, demonstrating scalability across architectures and
model sizes.

</details>


### [87] [Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance](https://arxiv.org/abs/2510.01269)
*Rohan Vitthal Thorat,Juhi Singh,Rajdip Nayek*

Main category: cs.LG

TL;DR: A hybrid model-free vibration control approach combines RL with an LQR baseline to safely train on real hardware without an accurate model, showing performance gains over uncontrolled cases even when the LQR model is incorrect.


<details>
  <summary>Details</summary>
Motivation: Structural vibrations pose safety risks and maintenance costs. Traditional LQR requires accurate system models, and pure model-free RL requires risky real-world training. A safe, model-free training framework is needed.

Method: Develop a hybrid controller where the LQR policy is derived from a randomly chosen model and used to guide RL during training on the physical system. The approach remains model-free because the LQR does not rely on the true model, eliminating the need for explicit system identification.

Result: RL guided by the LQR policy outperforms the uncontrolled case and reduces exploration risks; the framework removes dependence on explicit structural models and is claimed to be the first to address training safety in RL-based vibration control; validated solution.

Conclusion: This work provides a safe, model-free path for RL-based vibration control, reducing the need for system identification and mitigating training risks, marking a novel contribution to safe RL in structural control.

Abstract: Structural vibrations induced by external excitations pose significant risks,
including safety hazards for occupants, structural damage, and increased
maintenance costs. While conventional model-based control strategies, such as
Linear Quadratic Regulator (LQR), effectively mitigate vibrations, their
reliance on accurate system models necessitates tedious system identification.
This tedious system identification process can be avoided by using a model-free
Reinforcement learning (RL) method. RL controllers derive their policies solely
from observed structural behaviour, eliminating the requirement for an explicit
structural model. For an RL controller to be truly model-free, its training
must occur on the actual physical system rather than in simulation. However,
during this training phase, the RL controller lacks prior knowledge and it
exerts control force on the structure randomly, which can potentially harm the
structure. To mitigate this risk, we propose guiding the RL controller using a
Linear Quadratic Regulator (LQR) controller. While LQR control typically relies
on an accurate structural model for optimal performance, our observations
indicate that even an LQR controller based on an entirely incorrect model
outperforms the uncontrolled scenario. Motivated by this finding, we introduce
a hybrid control framework that integrates both LQR and RL controllers. In this
approach, the LQR policy is derived from a randomly selected model and its
parameters. As this LQR policy does not require knowledge of the true or an
approximate structural model the overall framework remains model-free. This
hybrid approach eliminates dependency on explicit system models while
minimizing exploration risks inherent in naive RL implementations. As per our
knowledge, this is the first study to address the critical training safety
challenge of RL-based vibration control and provide a validated solution.

</details>


### [88] [Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations](https://arxiv.org/abs/2510.01271)
*Arend Hintze,Asadullah Najam,Jory Schossau*

Main category: cs.LG

TL;DR: Introduces an information-theoretic framework to identify information-relay nodes (information relays) in RNNs by measuring mutual information between inputs and outputs across nodes; validates on synthetic and real time-series tasks with LSTM/GRU; uses node knockout to gauge importance; aims to improve interpretability and design of RNNs.


<details>
  <summary>Details</summary>
Motivation: Enhance interpretability of RNNs by revealing internal information-flow structure and identify key nodes that preserve/transfer information, with potential to guide robust model design.

Method: Define information relays as nodes with significant mutual information transfer. Compute mutual information between input and output vectors across nodes during network operation. Apply to various RNN architectures (LSTM, GRU) on synthetic and real-world time-series classification tasks. Perform node knockout experiments to test functional importance of relays.

Result: Expose distinct information-relay patterns across architectures, revealing how information is processed and retained over time. Node knockout confirms functional importance of identified nodes. The approach provides a practical explainability tool and insights to design more robust/interpretable networks.

Conclusion: The study advances understanding of RNN internal dynamics and contributes to explainable AI by offering a method to map information flow and guide architecture design.

Abstract: Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is
crucial for advancing their interpretability and improving their design. This
study introduces an innovative information-theoretic method to identify and
analyze information-transfer nodes within RNNs, which we refer to as
\textit{information relays}. By quantifying the mutual information between
input and output vectors across nodes, our approach pinpoints critical pathways
through which information flows during network operations. We apply this
methodology to both synthetic and real-world time series classification tasks,
employing various RNN architectures, including Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns
of information relay across different architectures, offering insights into how
information is processed and maintained over time. Additionally, we conduct
node knockout experiments to assess the functional importance of identified
nodes, significantly contributing to explainable artificial intelligence by
elucidating how specific nodes influence overall network behavior. This study
not only enhances our understanding of the complex mechanisms driving RNNs but
also provides a valuable tool for designing more robust and interpretable
neural networks.

</details>


### [89] [Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning](https://arxiv.org/abs/2510.01278)
*Hengwei Zhao,Zhengzhong Tu,Zhuo Zheng,Wei Wang,Junjue Wang,Rusty Feagin,Wenzhe Jiao*

Main category: cs.LG

TL;DR: NcPU is a non-contrastive PU learning framework using NoiSNCL and PLD to learn discriminative representations under unreliable supervision without auxiliary negatives, achieving substantial improvements over state-of-the-art PU methods across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: PU learning suffers from large performance gaps to supervised methods due to unreliable supervision and limited negative information; existing methods often rely on auxiliary negatives or pre-estimated parameters, hindering performance on complex datasets (e.g., CIFAR-100). A learning framework that can operate with only positive and unlabeled data and improve representation quality is highly desirable for real-world tasks.

Method: NcPU combines two components: (1) NoiSNCL, a noisy-pair robust supervised non-contrastive loss that aligns intra-class representations despite unreliable supervision, and (2) PLD (phantom label disambiguation), a regret-based scheme that provides conservative negative supervision by updating labels. The two components are designed to iteratively benefit each other in an EM-like framework.

Result: Theoretically, NoiSNCL and PLD reinforce each other under an EM perspective. Empirically, NoiSNCL enables simple PU methods to achieve competitive performance, and NcPU delivers substantial improvements over state-of-the-art PU methods across diverse datasets, including challenging post-disaster building damage mapping scenarios.

Conclusion: NcPU offers a practical, parameter-light approach to PU learning that closes much of the gap with supervised methods, with strong empirical performance and potential real-world impact; code is planned to be open-sourced after review.

Abstract: Positive-Unlabeled (PU) learning aims to train a binary classifier (positive
vs. negative) where only limited positive data and abundant unlabeled data are
available. While widely applicable, state-of-the-art PU learning methods
substantially underperform their supervised counterparts on complex datasets,
especially without auxiliary negatives or pre-estimated parameters (e.g., a
14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the
challenge of learning discriminative representations under unreliable
supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU
learning framework that requires no auxiliary information. NcPU combines a
noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns
intra-class representations despite unreliable supervision, with a phantom
label disambiguation (PLD) scheme that supplies conservative negative
supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can
iteratively benefit each other from the perspective of the
Expectation-Maximization framework. Empirically, extensive experiments
demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive
performance; and (2) NcPU achieves substantial improvements over
state-of-the-art PU methods across diverse datasets, including challenging
datasets on post-disaster building damage mapping, highlighting its promise for
real-world applications. Code: Code will be open-sourced after review.

</details>


### [90] [Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours](https://arxiv.org/abs/2510.01288)
*Rui Melo,Rui Abreu,Corina S. Pasareanu*

Main category: cs.LG

TL;DR: Proposes microsaccade-inspired perturbations of LLM position encodings to surface internal signals of misbehavior without fine-tuning, achieving detection of factual, safety, toxicity, and backdoor issues across models efficiently.


<details>
  <summary>Details</summary>
Motivation: Analogize perceptual micro-movements to probing LLMs to reveal hidden dynamics of behavior and enable self-flagging of failures without task-specific supervision.

Method: Apply lightweight perturbations to position encoding; measure latent signals; no fine-tuning; universal probing across tasks; detect misbehavior.

Result: Experiments on multiple state-of-the-art LLMs show the perturbations surface misbehaviors; method is computationally efficient; robust across factuality, safety, toxicity, backdoor detection.

Conclusion: Pretrained LLMs contain internal evidence to flag failures; microsaccade-inspired interventions offer a pathway for detection and mitigation of undesirable behaviors; no additional supervision needed.

Abstract: We draw inspiration from microsaccades, tiny involuntary eye movements that
reveal hidden dynamics of human perception, to propose an analogous probing
method for large language models (LLMs). Just as microsaccades expose subtle
but informative shifts in vision, we show that lightweight position encoding
perturbations elicit latent signals that indicate model misbehaviour. Our
method requires no fine-tuning or task-specific supervision, yet detects
failures across diverse settings including factuality, safety, toxicity, and
backdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate
that these perturbation-based probes surface misbehaviours while remaining
computationally efficient. These findings suggest that pretrained LLMs already
encode the internal evidence needed to flag their own failures, and that
microsaccade-inspired interventions provide a pathway for detecting and
mitigating undesirable behaviours.

</details>


### [91] [ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models](https://arxiv.org/abs/2510.01290)
*Akshat Ramachandran,Marina Neseem,Charbel Sakr,Rangharajan Venkatesan,Brucek Khailany,Tushar Krishna*

Main category: cs.LG

TL;DR: ThinKV compresses the KV cache for long-output reasoning by leveraging thought-aware sparsity and a hybrid quantization-eviction policy, achieving near-lossless accuracy with <5% of the original KV cache and up to 5.8x throughput.


<details>
  <summary>Details</summary>
Motivation: Long chain-of-thought generation expands the KV cache, exhausting GPU memory and hindering inference throughput. A memory-efficient, accurate KV management strategy is needed for long-context reasoning in LLMs.

Method: A hybrid quantization-eviction framework driven by attention-sparsity-derived thought types. Tokens are quantized according to thought importance, and tokens from less critical thoughts are progressively evicted as reasoning proceeds. Implemented a kernel extending PagedAttention to reuse memory slots of evicted tokens, removing compaction overhead.

Result: Across mathematics and coding benchmarks on DeepSeek-R1-Distill, GPT-OSS, and NVIDIA AceReason, ThinKV achieves near-lossless accuracy with <5% of the original KV cache and up to 5.8x higher inference throughput compared with state-of-the-art baselines.

Conclusion: ThinKV provides a memory-efficient, high-throughput KV management solution for long-context reasoning that preserves accuracy while drastically reducing memory footprint.

Abstract: The long-output context generation of large reasoning models enables extended
chain of thought (CoT) but also drives rapid growth of the key-value (KV)
cache, quickly overwhelming GPU memory. To address this challenge, we propose
ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on
the observation that attention sparsity reveals distinct thought types with
varying importance within the CoT. It applies a hybrid quantization-eviction
strategy, assigning token precision by thought importance and progressively
evicting tokens from less critical thoughts as reasoning trajectories evolve.
Furthermore, to implement ThinKV, we design a kernel that extends
PagedAttention to enable efficient reuse of evicted tokens' memory slots,
eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,
GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show
that ThinKV achieves near-lossless accuracy with less than 5% of the original
KV cache, while improving performance with up to 5.8x higher inference
throughput over state-of-the-art baselines.

</details>


### [92] [Network-Level Vehicle Delay Estimation at Heterogeneous Signalized Intersections](https://arxiv.org/abs/2510.01292)
*Xiaobo Ma,Hyunsoo Noh,James Tokishi,Ryan Hatch*

Main category: cs.LG

TL;DR: Domain-adaptive gradient boosting for cross-site vehicle delay estimation; outperforms baselines on 57 intersections.


<details>
  <summary>Details</summary>
Motivation: Real-world heterogeneity across intersections causes distribution shifts; need transferable ML models for delay estimation.

Method: Domain adaptation framework: separate source/target domains, extract features, fine-tune with a small labeled target subset; Gradient Boosting with Balanced Weighting (GBBW) reweights source samples by similarity to target; evaluation against eight ML regressors and seven DA methods on data from 57 intersections.

Result: GBBW achieves more accurate and robust delay estimates; better reliability for signal optimization and congestion management; demonstrates transferability of ML in transportation.

Conclusion: Proposes a scalable framework that enhances cross-intersection transferability of ML-based delay estimation, enabling broader real-world deployment.

Abstract: Accurate vehicle delay estimation is essential for evaluating the performance
of signalized intersections and informing traffic management strategies. Delay
reflects congestion levels and affects travel time reliability, fuel use, and
emissions. Machine learning (ML) offers a scalable, cost-effective alternative;
However, conventional models typically assume that training and testing data
follow the same distribution, an assumption that is rarely satisfied in
real-world applications. Variations in road geometry, signal timing, and driver
behavior across intersections often lead to poor generalization and reduced
model accuracy. To address this issue, this study introduces a domain
adaptation (DA) framework for estimating vehicle delays across diverse
intersections. The framework separates data into source and target domains,
extracts key traffic features, and fine-tunes the model using a small, labeled
subset from the target domain. A novel DA model, Gradient Boosting with
Balanced Weighting (GBBW), reweights source data based on similarity to the
target domain, improving adaptability. The framework is tested using data from
57 heterogeneous intersections in Pima County, Arizona. Performance is
evaluated against eight state-of-the-art ML regression models and seven
instance-based DA methods. Results demonstrate that the GBBW framework provides
more accurate and robust delay estimates. This approach supports more reliable
traffic signal optimization, congestion management, and performance-based
planning. By enhancing model transferability, the framework facilitates broader
deployment of machine learning techniques in real-world transportation systems.

</details>


### [93] [From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review](https://arxiv.org/abs/2510.01296)
*Emma McMillian,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: cs.LG

TL;DR: A comprehensive survey of deep learning-based 3D MRI reconstruction across four model classes (point cloud, mesh-based, shape-aware, volumetric), detailing state-of-the-art techniques, datasets, metrics, clinical applicability, and future directions.


<details>
  <summary>Details</summary>
Motivation: 3D reconstruction from 2D MRI is crucial for diagnosis, treatment planning, and computational modeling; a structured survey is needed to map methods, data, evaluation, and translation to clinical practice.

Method: Categorize methods into four model classes (point cloud, mesh-based, shape-aware, volumetric). Synthesize state-of-the-art techniques and their methodological foundations; discuss limitations, anatomical applications (cardiac, neurological, lung), data influence, public datasets, computational demands, and evaluation metrics; examine clinical applicability and cross-modality training/testing considerations; outline emerging directions including multimodal integration.

Result: Provides a structured overview and synthesis of current 3D MRI reconstruction methods; identifies gaps and opportunities to improve robustness, generalization, and clinical impact; highlights multimodal and cross-modality integration as promising directions.

Conclusion: This review aims to guide researchers toward more robust, generalizable, and clinically impactful 3D reconstruction methods, emphasizing data quality, standardized evaluation, and integration across modalities.

Abstract: Deep learning-based 3-dimensional (3D) shape reconstruction from
2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly
important in medical disease diagnosis, treatment planning, and computational
modeling. This review surveys the methodological landscape of 3D MRI
reconstruction, focusing on 4 primary approaches: point cloud, mesh-based,
shape-aware, and volumetric models. For each category, we analyze the current
state-of-the-art techniques, their methodological foundation, limitations, and
applications across anatomical structures. We provide an extensive overview
ranging from cardiac to neurological to lung imaging. We also focus on the
clinical applicability of models to diseased anatomy, and the influence of
their training and testing data. We examine publicly available datasets,
computational demands, and evaluation metrics. Finally, we highlight the
emerging research directions including multimodal integration and
cross-modality frameworks. This review aims to provide researchers with a
structured overview of current 3D reconstruction methodologies to identify
opportunities for advancing deep learning towards more robust, generalizable,
and clinically impactful solutions.

</details>


### [94] [Low Rank Gradients and Where to Find Them](https://arxiv.org/abs/2510.01303)
*Rishi Sonthalia,Michael Murray,Guido Mont√∫far*

Main category: cs.LG

TL;DR: Gradient w.r.t input weights in a two-layer network trained on anisotropic, spiked data is approximately low-rank, collapsing to two rank-one terms (bulk-residue and spike interactions) across mean-field and NTK regimes; regularizers can modulate the components; empirical evidence supports the theory.


<details>
  <summary>Details</summary>
Motivation: To understand the gradient structure and training dynamics of neural networks when standard isotropy and independence assumptions are relaxed, and to assess how data geometry, scaling, and activation shape the low-rank decomposition of gradients.

Method: Theoretical analysis in a spiked data model with anisotropic, possibly ill-conditioned bulk. Examines both mean-field and neural tangent kernel scalings for a two-layer network, without requiring independent data/weights. Derives that the gradient with respect to input weights decomposes into two dominant rank-one terms aligned with (i) the bulk data-residue and (ii) the rank-one spike in the input data. Studies how data properties, scaling, and activation influence the balance between terms. Also analyzes how regularizers (weight decay, input noise, Jacobian penalties) selectively modulate these components. Empirical validation on synthetic and real datasets.

Result: The gradient with respect to input weights is approximately low-rank, dominated by two rank-one components: one aligned with the bulk data-residue and another with the input spike. The relative strength of these components is governed by data properties, scaling regime, and activation function. Standard regularizers selectively modulate these components. Experiments on synthetic and real data corroborate the theoretical predictions.

Conclusion: Low-rank structure in gradient dynamics persists under relaxed data/parameter isotropy and across MF and NTK regimes. This reveals robust, interpretable components of learning signals (bulk-residue vs. spike) and suggests potential for regularization strategies and computational efficiency by exploiting the low-rank gradient decomposition.

Abstract: This paper investigates low-rank structure in the gradients of the training
loss for two-layer neural networks while relaxing the usual isotropy
assumptions on the training data and parameters. We consider a spiked data
model in which the bulk can be anisotropic and ill-conditioned, we do not
require independent data and weight matrices and we also analyze both the
mean-field and neural-tangent-kernel scalings. We show that the gradient with
respect to the input weights is approximately low rank and is dominated by two
rank-one terms: one aligned with the bulk data-residue , and another aligned
with the rank one spike in the input data. We characterize how properties of
the training data, the scaling regime and the activation function govern the
balance between these two components. Additionally, we also demonstrate that
standard regularizers, such as weight decay, input noise and Jacobian
penalties, also selectively modulate these components. Experiments on synthetic
and real data corroborate our theoretical predictions.

</details>


### [95] [Quantum-inspired Benchmark for Estimating Intrinsic Dimension](https://arxiv.org/abs/2510.01335)
*Aritra Das,Joseph T. Iosue,Victor V. Albert*

Main category: cs.LG

TL;DR: Proposes QuIIEst, a quantum-inspired, curvature-adjusted benchmark for intrinsic-dimension estimation (IDE) on complex, topologically non-trivial manifolds, revealing IDE difficulties and potential non-manifold dimension extraction.


<details>
  <summary>Details</summary>
Motivation: Current IDE benchmarks underrepresent manifold complexity and curvature variation; IDE estimates vary widely across methods, necessitating a benchmark that introduces richer geometry (topology, curvature, noise) to stress-test IDE methods and reveal their limitations.

Method: Introduce QuIIEst: an infinite family of topologically non-trivial manifolds with known intrinsic dimension, constructed via a quantum-optical embedding of homogeneous spaces with tunable curvature and additive noise. Evaluate existing IDE methods on these manifolds and on the Hofstadter butterfly fractal, to assess performance on spaces that are not smooth manifolds.

Result: IDE methods generally perform worse on QuIIEst manifolds than on existing benchmarks under equal resources; curvature heterogeneity causes only modest additional degradation, highlighting benchmark difficulty. Some IDE methods can extract effective dimension from non-manifold spaces like Hofstadter butterfly, showing partial robustness.

Conclusion: QuIIEst provides a challenging, informative benchmark for IDE methods, promoting development of robust estimators that handle complex geometry and non-manifold spaces; extending IDE evaluation to fractal-like structures broadens applicability of intrinsic-dimension concepts in ML.

Abstract: Machine learning models can generalize well on real-world datasets. According
to the manifold hypothesis, this is possible because datasets lie on a latent
manifold with small intrinsic dimension (ID). There exist many methods for ID
estimation (IDE), but their estimates vary substantially. This warrants
benchmarking IDE methods on manifolds that are more complex than those in
existing benchmarks. We propose a Quantum-Inspired Intrinsic-dimension
Estimation (QuIIEst) benchmark consisting of infinite families of topologically
non-trivial manifolds with known ID. Our benchmark stems from a quantum-optical
method of embedding arbitrary homogeneous spaces while allowing for curvature
modification and additive noise. The IDE methods tested were generally less
accurate on QuIIEst manifolds than on existing benchmarks under identical
resource allocation. We also observe minimal performance degradation with
increasingly non-uniform curvature, underscoring the benchmark's inherent
difficulty. As a result of independent interest, we perform IDE on the fractal
Hofstadter's butterfly and identify which methods are capable of extracting the
effective dimension of a space that is not a manifold.

</details>


### [96] [On the Identifiability of Latent Action Policies](https://arxiv.org/abs/2510.01337)
*S√©bastien Lachapelle*

Main category: cs.LG

TL;DR: Analyzes identifiability in latent action policy learning (LAPO), proposes desiderata for action representations, and proves that an entropy-regularized LAPO objective identifiably yields representations meeting these desiderata under certain conditions, explaining why discrete action representations perform well in practice.


<details>
  <summary>Details</summary>
Motivation: To understand what makes latent action representations identifiable in LAPO and to explain the empirical success of discrete action representations.

Method: Formalizes desiderata for LAPO representations, analyzes statistical benefits and sources of unidentifiability, and proves an identifiability result for an entropy-regularized LAPO objective under suitable conditions.

Result: Under suitable conditions, entropy-regularized LAPO identifies action representations that satisfy the stated desiderata, providing a theoretical explanation for the practical effectiveness of discrete action representations.

Conclusion: Entropy regularization can resolve identifiability issues in LAPO; the findings offer theoretical grounding for the success of discretized action representations in practice.

Abstract: We study the identifiability of latent action policy learning (LAPO), a
framework introduced recently to discover representations of actions from video
data. We formally describe desiderata for such representations, their
statistical benefits and potential sources of unidentifiability. Finally, we
prove that an entropy-regularized LAPO objective identifies action
representations satisfying our desiderata, under suitable conditions. Our
analysis provides an explanation for why discrete action representations
perform well in practice.

</details>


### [97] [Self-Supervised Representation Learning as Mutual Information Maximization](https://arxiv.org/abs/2510.01345)
*Akhlaqur Rahman Sabby,Yi Sui,Tongzi Wu,Jesse C. Cresswell,Ga Wu*

Main category: cs.LG

TL;DR: From variational MI lower bounds, the paper derives two SSRL paradigms (SDMI and JMI) explaining architectural choices (predictor nets, stop-gradient, regularizers) and shows many existing methods are instances of these frameworks.


<details>
  <summary>Details</summary>
Motivation: Understand why SSRL architectures are effective by deriving them from first-principles, rather than treating components as heuristic additions.

Method: Starting from a variational mutual information lower bound, derive two training paradigms: Self-Distillation MI (SDMI) and Joint MI (JMI). Analyze the optimization constraints they impose (alternating vs. joint optimization, stop-gradients, symmetry). Interpret predictor networks and statistical regularizers as MI surrogates and map existing SSRL methods to the two paradigms.

Result: SDMI requires alternating optimization with stop-gradient operations; JMI enables joint optimization with symmetric architectures. The predictor nets and regularizers emerge as tractable surrogates for the MI objective. Many existing SSRL methods are instances or approximations of SDMI or JMI.

Conclusion: A principled, MI-based explanation for SSRL architectural choices, showing that different optimization structures (alternating vs. joint) naturally lead to different components, and that a broad set of methods can be unified under these two paradigms.

Abstract: Self-supervised representation learning (SSRL) has demonstrated remarkable
empirical success, yet its underlying principles remain insufficiently
understood. While recent works attempt to unify SSRL methods by examining their
information-theoretic objectives or summarizing their heuristics for preventing
representation collapse, architectural elements like the predictor network,
stop-gradient operation, and statistical regularizer are often viewed as
empirically motivated additions. In this paper, we adopt a first-principles
approach and investigate whether the learning objective of an SSRL algorithm
dictates its possible optimization strategies and model design choices. In
particular, by starting from a variational mutual information (MI) lower bound,
we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint
MI (JMI), each imposing distinct structural constraints and covering a set of
existing SSRL algorithms. SDMI inherently requires alternating optimization,
making stop-gradient operations theoretically essential. In contrast, JMI
admits joint optimization through symmetric architectures without such
components. Under the proposed formulation, predictor networks in SDMI and
statistical regularizers in JMI emerge as tractable surrogates for the MI
objective. We show that many existing SSRL methods are specific instances or
approximations of these two paradigms. This paper provides a theoretical
explanation behind the choices of different architectural components of
existing SSRL methods, beyond heuristic conveniences.

</details>


### [98] [To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking](https://arxiv.org/abs/2510.01349)
*Hannah Lawrence,Elyssa Hofgard,Vasco Portilheiro,Yuxuan Chen,Tess Smidt,Robin Walters*

Main category: cs.LG

TL;DR: A two-sample neural classifier test quantifies symmetry-breaking in data, showing symmetry-aware methods are not universally beneficial and their utility is dataset-dependent, prompting a rethink of symmetry biases in data.


<details>
  <summary>Details</summary>
Motivation: Symmetry-aware techniques (data augmentation, equivariant models) assume transformed data points remain likely under the test distribution; this work questions that assumption and seeks to diagnose when symmetry biases help or hurt.

Method: Introduce a metric for anisotropy by training a neural classifier to distinguish the original dataset from its randomly augmented version. Validate on synthetic datasets and apply to benchmark point clouds; supplement with theoretical results on invariant ridge regression in the infinite-feature limit.

Result: The metric reveals substantial symmetry-breaking (alignment) in several benchmarks; distributional symmetry-breaking can impair invariant methods even when labels are invariant; the benefits of equivariant methods are data-dependent, aiding some anisotropic datasets but not others.

Conclusion: Understanding equivariance requires reevaluating symmetry biases in data; symmetry-aware methods may not always help and their value depends on dataset anisotropy.

Abstract: Symmetry-aware methods for machine learning, such as data augmentation and
equivariant architectures, encourage correct model behavior on all
transformations (e.g. rotations or permutations) of the original dataset. These
methods can improve generalization and sample efficiency, under the assumption
that the transformed datapoints are highly probable, or "important", under the
test distribution. In this work, we develop a method for critically evaluating
this assumption. In particular, we propose a metric to quantify the amount of
anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural
classifier test that distinguishes between the original dataset and its
randomly augmented equivalent. We validate our metric on synthetic datasets,
and then use it to uncover surprisingly high degrees of alignment in several
benchmark point cloud datasets. We show theoretically that distributional
symmetry-breaking can actually prevent invariant methods from performing
optimally even when the underlying labels are truly invariant, as we show for
invariant ridge regression in the infinite feature limit. Empirically, we find
that the implication for symmetry-aware methods is dataset-dependent:
equivariant methods still impart benefits on some anisotropic datasets, but not
others. Overall, these findings suggest that understanding equivariance -- both
when it works, and why -- may require rethinking symmetry biases in the data.

</details>


### [99] [RheOFormer: A generative transformer model for simulation of complex fluids and flows](https://arxiv.org/abs/2510.01365)
*Maedeh Saberi,Amir Barati Farimani,Safa Jamali*

Main category: cs.LG

TL;DR: RheOFormer is a self-attention based generative operator learner that acts as a neural surrogate for complex non-Newtonian fluid mechanics, delivering accurate, data-efficient predictions and enabling real-time optimization.


<details>
  <summary>Details</summary>
Motivation: Non-Newtonian/viscoelastic flows involve nonlinear, history-dependent constitutive models and high computational cost. There is a need for scalable, generalizable data-driven surrogates that can adapt to varied conditions without retraining.

Method: Introduce Rheological Operator Transformer (RheOFormer), a self-attention-based generative operator learning model. It learns different spatial interactions and features of complex fluid flows. Trained on a range of viscometric and non-viscometric flows with viscoelastic and elastoviscoplastic mechanics in complex domains to predict both scalar and tensorial nonlinear mechanics and the spatiotemporal evolution of flows, even from limited data.

Result: RheOFormer accurately learns nonlinear scalar and tensorial mechanics, and predicts the spatiotemporal evolution of complex fluid flows. It shows strong generalization and computational efficiency, functioning as an effective neural surrogate.

Conclusion: RheOFormer offers a robust framework for accelerating predictive simulations of complex fluids, enabling data-driven experimentation and real-time process optimization across diverse applications.

Abstract: The ability to model mechanics of soft materials under flowing conditions is
key in designing and engineering processes and materials with targeted
properties. This generally requires solution of internal stress tensor, related
to the deformation tensor through nonlinear and history-dependent constitutive
models. Traditional numerical methods for non-Newtonian fluid dynamics often
suffer from prohibitive computational demands and poor scalability to new
problem instances. Developments in data-driven methods have mitigated some
limitations but still require retraining across varied physical conditions. In
this work, we introduce Rheological Operator Transformer (RheOFormer), a
generative operator learning method leveraging self-attention to efficiently
learn different spatial interactions and features of complex fluid flows. We
benchmark RheOFormer across a range of different viscometric and
non-viscometric flows with different types of viscoelastic and
elastoviscoplastic mechanics in complex domains against ground truth solutions.
Our results demonstrate that RheOFormer can accurately learn both scalar and
tensorial nonlinear mechanics of different complex fluids and predict the
spatio-temporal evolution of their flows, even when trained on limited
datasets. Its strong generalization capabilities and computational efficiency
establish RheOFormer as a robust neural surrogate for accelerating predictive
complex fluid simulations, advancing data-driven experimentation, and enabling
real-time process optimization across a wide range of applications.

</details>


### [100] [Selective Underfitting in Diffusion Models](https://arxiv.org/abs/2510.01378)
*Kiwhan Song,Jaeyeon Kim,Sitan Chen,Yilun Du,Sham Kakade,Vincent Sitzmann*

Main category: cs.LG

TL;DR: Selective underfitting: diffusion models approximate the score accurately in some input regions while underfitting in others, offering a region-dependent view that explains generalization and generation.


<details>
  <summary>Details</summary>
Motivation: Clarify what score diffusion models learn and reconcile global underfitting with successful generation by introducing a region-specific bias in score approximation.

Method: Define and formalize the notion of selective underfitting; characterize regions of input space where the score is well-fit versus underfit; design empirical interventions to test the hypothesis.

Result: Evidence that selective underfitting captures diffusion models' behavior and provides new insights into their generalization and generative performance.

Conclusion: Understanding diffusion models through the lens of selective underfitting yields testable predictions and advances a nuanced theoretical account of training biases in score-based generative modeling.

Abstract: Diffusion models have emerged as the principal paradigm for generative
modeling across various domains. During training, they learn the score
function, which in turn is used to generate samples at inference. They raise a
basic yet unsolved question: which score do they actually learn? In principle,
a diffusion model that matches the empirical score in the entire data space
would simply reproduce the training data, failing to generate novel samples.
Recent work addresses this question by arguing that diffusion models underfit
the empirical score due to training-time inductive biases. In this work, we
refine this perspective, introducing the notion of selective underfitting:
instead of underfitting the score everywhere, better diffusion models more
accurately approximate the score in certain regions of input space, while
underfitting it in others. We characterize these regions and design empirical
interventions to validate our perspective. Our results establish that selective
underfitting is essential for understanding diffusion models, yielding new,
testable insights into their generalization and generative performance.

</details>


### [101] [Fine-Tuning Masked Diffusion for Provable Self-Correction](https://arxiv.org/abs/2510.01384)
*Jaeyeon Kim,Seunggeun Kim,Taekyun Lee,David Z. Pan,Hyeji Kim,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: PRISM enables lightweight, inference-time self-correction for pretrained Masked Diffusion Models by learning per-token quality scores via a self-correction loss, improving generation on Sudoku, long text, and code.


<details>
  <summary>Details</summary>
Motivation: Desideratum for generative models is self-correction. While Masked Diffusion Models are strong for discrete spaces, their capacity for self-correction is underexplored. Prior approaches either overhaul architectures/training or rely on imprecise proxies, limiting applicability. A lightweight, model-agnostic solution is needed.

Method: Introduce PRISM (Plug-in Remasking for Inference-time Self-correction of Masked Diffusions). It defines a self-correction loss that provably learns per-token quality scores, without reinforcement learning or a verifier. The quality scores are computed in the same forward pass as the MDM and are used to detect low-quality tokens, without changing the MDM training or architecture. PRISM is plug-in and model-agnostic, applicable to any pretrained MDM.

Result: Empirically, PRISM improves MDM inference across domains and scales: Sudoku; unconditional text (170M); and code with LLaDA (8B). The framework yields learned per-token quality scores and effective self-correction during inference.

Conclusion: PRISM offers a lightweight, theoretically grounded, and empirically validated approach for inference-time self-correction in discrete-space diffusion models. It is model-agnostic and broadly applicable across domains, enabling improved generation without retraining.

Abstract: A natural desideratum for generative models is self-correction--detecting and
revising low-quality tokens at inference. While Masked Diffusion Models (MDMs)
have emerged as a promising approach for generative modeling in discrete
spaces, their capacity for self-correction remains poorly understood. Prior
attempts to incorporate self-correction into MDMs either require overhauling
MDM architectures/training or rely on imprecise proxies for token quality,
limiting their applicability. Motivated by this, we introduce PRISM--Plug-in
Remasking for Inference-time Self-correction of Masked Diffusions--a
lightweight, model-agnostic approach that applies to any pretrained MDM.
Theoretically, PRISM defines a self-correction loss that provably learns
per-token quality scores, without RL or a verifier. These quality scores are
computed in the same forward pass with MDM and used to detect low-quality
tokens. Empirically, PRISM advances MDM inference across domains and scales:
Sudoku; unconditional text (170M); and code with LLaDA (8B).

</details>


### [102] [Optimal Stopping vs Best-of-$N$ for Inference Time Optimization](https://arxiv.org/abs/2510.01394)
*Yusuf Kalayci,Vinod Raman,Shaddin Dughmi*

Main category: cs.LG

TL;DR: Adaptive Pandora's Box framework for LLM inference that learns when to stop generating; achieves near-optimal performance with 15‚Äì35% fewer generations than Best-of-N.


<details>
  <summary>Details</summary>
Motivation: LLMs often balance output quality and inference cost, especially when multiple generations are used. Existing approaches like Best-of-N assume known reward distributions or incur fixed costs; there is a need for online, distribution-free stopping decisions that adapt during deployment.

Method: Model each generation as opening a costly box with a random reward. Develop a UCB-style Pandora's Box algorithm that performs near the optimal Weitzman policy when the distribution is known. Extend with a Bradley-Terry‚Äìinspired reward transformation to normalize rewards across prompts, enabling online learning of stopping thresholds. Implement adaptive inference-time optimization that normalizes rewards and updates thresholds on the fly.

Result: The adaptive strategy matches Best-of-N performance while requiring 15‚Äì35% fewer generations on average across AlpacaFarm and HH-RLHF datasets, for multiple LLM‚Äìreward model pairs. The approach provides theoretical bounds close to the optimal policy and demonstrates practical efficiency gains in LLM deployment.

Conclusion: This work bridges optimal stopping theory and inference-time scaling, offering both theoretical performance guarantees and tangible efficiency improvements for deploying large language models.

Abstract: Large language model (LLM) generation often requires balancing output quality
against inference cost, especially when using multiple generations. We
introduce a new framework for inference-time optimization based on the
classical Pandora's Box problem. Viewing each generation as opening a costly
"box" with random reward, we develop algorithms that decide when to stop
generating without knowing the underlying reward distribution. Our first
contribution is a UCB-style Pandora's Box algorithm, which achieves performance
that is provably close to Weitzman's algorithm, the optimal strategy when the
distribution is known. We further adapt this method to practical LLM settings
by addressing reward scaling across prompts via a Bradley-Terry inspired
transformation. This leads to an adaptive inference-time optimization method
that normalizes rewards and learns stopping thresholds on the fly. Experiments
on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,
show that our adaptive strategy can obtain the same performance as non-adaptive
Best-of-N sampling while requiring 15-35 percent fewer generations on average.
Our results establish a principled bridge between optimal stopping theory and
inference-time scaling, providing both theoretical performance bounds and
practical efficiency gains for LLM deployment.

</details>


### [103] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: Neural-network surrogate learns CVs directly from Cartesian coordinates and provides Jacobians via automatic differentiation, replacing the need for analytical Jacobians and enabling gradient-based free energy methods with complex ML CVs.


<details>
  <summary>Details</summary>
Motivation: Free energy reconstruction methods like Gaussian Process Regression require analytic Jacobians of CVs, which bottlenecks the use of complex or machine-learned CVs. A learnable, differentiable CV representation can bypass this limitation.

Method: Train a neural network to map Cartesian coordinates to CV values and use automatic differentiation to compute Jacobians. Validate on an MgCl2 ion-pairing system with a simple distance CV and a complex coordination-number CV, assessing CV accuracy and the distribution of Jacobian errors.

Result: The method achieves high accuracy for both CVs; Jacobian errors follow a near-Gaussian distribution, making them suitable for integration into GPR pipelines. The framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs.

Conclusion: This neural-network surrogate framework broadens the applicability of gradient-based free energy simulations to complex and machine-learned CVs in biochemistry and materials simulations by providing accurate CVs and differentiable Jacobians without analytic forms.

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR)
require Jacobians of the collective variables (CVs), a bottleneck that
restricts the use of complex or machine-learned CVs. We introduce a neural
network surrogate framework that learns CVs directly from Cartesian coordinates
and uses automatic differentiation to provide Jacobians, bypassing analytical
forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for
both a simple distance CV and a complex coordination-number CV. Moreover,
Jacobian errors also followed a near-Gaussian distribution, making them
suitable for GPR pipelines. This framework enables gradient-based free energy
methods to incorporate complex and machine-learned CVs, broadening the scope of
biochemistry and materials simulations.

</details>


### [104] [Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction](https://arxiv.org/abs/2510.01407)
*Ethan G. Rogers,Cheng Wang*

Main category: cs.LG

TL;DR: Low-rank latent autoencoder with vector quantization to cut decoder compute in neural image compression while preserving quality.


<details>
  <summary>Details</summary>
Motivation: Decoder-based neural image compression relies on expensive convolutional decoders, causing high computational cost and latency; reducing decoding complexity is a key bottleneck for practical deployment.

Method: Integrate low-rank representations into the autoencoder's latent space and apply a sequence of efficient low-rank operations during reconstruction, using vector quantization to discretize latent codes.

Result: If validated, the approach dramatically reduces decoding overhead while maintaining high image fidelity, effectively alleviating the decoder bottleneck in neural compression.

Conclusion: The proposed framework offers a scalable path to efficient neural image compression with low decoding cost and preserved quality, potentially broadening practical adoption.

Abstract: Image compression and reconstruction are crucial for various digital
applications. While contemporary neural compression methods achieve impressive
compression rates, the adoption of such technology has been largely hindered by
the complexity and large computational costs of the convolution-based decoders
during data reconstruction. To address the decoder bottleneck in neural
compression, we develop a new compression-reconstruction framework based on
incorporating low-rank representation in an autoencoder with vector
quantization. We demonstrated that performing a series of computationally
efficient low-rank operations on the learned latent representation of images
can efficiently reconstruct the data with high quality. Our approach
dramatically reduces the computational overhead in the decoding phase of neural
compression/reconstruction, essentially eliminating the decoder compute
bottleneck while maintaining high fidelity of image outputs.

</details>


### [105] [Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons](https://arxiv.org/abs/2510.01439)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.LG

TL;DR: A comprehensive, taxonomy-driven review of Edge AI's evolution, current landscape, and future directions.


<details>
  <summary>Details</summary>
Motivation: To synthesize Edge AI research to guide researchers and practitioners by clarifying deployment locations, processing capabilities (e.g., TinyML, federated learning), application domains, and hardware, while addressing challenges and outlining future opportunities.

Method: Systematic literature review guided by PRISMA, developing a multi-dimensional taxonomy (deployment, processing capabilities, domains, hardware), tracing the field from content delivery networks and fog computing to on-device intelligence, and analyzing enabling technologies (hardware accelerators, software optimizations, communication protocols) and challenges.

Result: Proposes a comprehensive framework for Edge AI incorporating evolution, landscape, and future directions; highlights enabling technologies; enumerates challenges (resource constraints, security, model management, power, connectivity) and opportunities (neuromorphic hardware, continual learning, edge-cloud collaboration, trustworthiness).

Conclusion: Edge AI is set for sustained growth with advances in specialized hardware, robust learning algorithms, secure edge-cloud ecosystems, and trust-centric AI, with the framework serving as guidance for researchers and practitioners.

Abstract: Edge Artificial Intelligence (Edge AI) embeds intelligence directly into
devices at the network edge, enabling real-time processing with improved
privacy and reduced latency by processing data close to its source. This review
systematically examines the evolution, current landscape, and future directions
of Edge AI through a multi-dimensional taxonomy including deployment location,
processing capabilities such as TinyML and federated learning, application
domains, and hardware types. Following PRISMA guidelines, the analysis traces
the field from early content delivery networks and fog computing to modern
on-device intelligence. Core enabling technologies such as specialized hardware
accelerators, optimized software, and communication protocols are explored.
Challenges including resource limitations, security, model management, power
consumption, and connectivity are critically assessed. Emerging opportunities
in neuromorphic hardware, continual learning algorithms, edge-cloud
collaboration, and trustworthiness integration are highlighted, providing a
comprehensive framework for researchers and practitioners.

</details>


### [106] [SoftAdaClip: A Smooth Clipping Strategy for Fair and Private Model Training](https://arxiv.org/abs/2510.01447)
*Dorsa Soleymani,Ali Dadsetan,Frank Rudzicz*

Main category: cs.LG

TL;DR: SoftAdaClip replaces hard gradient clipping in DP-SGD with a smooth tanh transformation, yielding better fairness and utility under differential privacy, outperforming DP-SGD and Adaptive-DPSGD on healthcare and tabular datasets with significant reductions in subgroup disparities.


<details>
  <summary>Details</summary>
Motivation: Hard clipping in DP-SGD can unevenly damp learning signals for minority subpopulations, harming fairness; existing adaptive clipping helps but remains uniform and hard, potentially limiting subgroup fairness. A smooth, adaptive approach may preserve relative gradient magnitudes and improve fairness under DP.

Method: Introduce SoftAdaClip: a DP training method that uses a tanh-based smooth transformation to bound sensitivity instead of hard clipping, paired with adaptive mechanisms to adjust clipping behavior; evaluate on MIMIC-III, GOSSIS-eICU, and Adult Income.

Result: SoftAdaClip reduces subgroup disparities by up to 87% vs DP-SGD and up to 48% vs Adaptive-DPSGD; improvements are statistically significant across the tested datasets.

Conclusion: Integrating smooth gradient transformations with adaptive clipping can achieve fairer and privately trained models, suggesting that smooth clipping strategies can complement DP mechanisms to improve utility and fairness.

Abstract: Differential privacy (DP) provides strong protection for sensitive data, but
often reduces model performance and fairness, especially for underrepresented
groups. One major reason is gradient clipping in DP-SGD, which can
disproportionately suppress learning signals for minority subpopulations.
Although adaptive clipping can enhance utility, it still relies on uniform hard
clipping, which may restrict fairness. To address this, we introduce
SoftAdaClip, a differentially private training method that replaces hard
clipping with a smooth, tanh-based transformation to preserve relative gradient
magnitudes while bounding sensitivity. We evaluate SoftAdaClip on various
datasets, including MIMIC-III (clinical text), GOSSIS-eICU (structured
healthcare), and Adult Income (tabular data). Our results show that SoftAdaClip
reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48%
compared to Adaptive-DPSGD, and these reductions in subgroup disparities are
statistically significant. These findings underscore the importance of
integrating smooth transformations with adaptive mechanisms to achieve fair and
private model training.

</details>


### [107] [Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression](https://arxiv.org/abs/2510.01450)
*Yifei Zuo,Yutong Yin,Zhichen Zeng,Ang Li,Banghua Zhu,Zhaoran Wang*

Main category: cs.LG

TL;DR: Proposes Local Linear Attention (LLA), derived from test-time regression; analyzes bias-variance advantages over Softmax/Linear attention; introduces memory-efficient primitives and FlashLLA for hardware acceleration; empirically validated on test-time regression, in-context regression, associative recall, and state tracking.


<details>
  <summary>Details</summary>
Motivation: Existing efficient attention methods (Softmax, linear) trade expressivity for speed. There is a theoretical gap for expressive, test-time adaptable attention with nonparametric/statistical grounding. The work seeks a principled, scalable mechanism that handles non-stationarity and is hardware-friendly.

Method: Derive LLA from nonparametric statistics via test-time regression; perform a bias-variance trade-off analysis versus Softmax and Linear attention for associative memory; develop two memory-efficient primitives to reduce Œò(n^2 d) and Œò(n d^2) costs; introduce FlashLLA, a blockwise hardware-friendly algorithm for parallel accelerators; implement a customized inference kernel to reduce memory overhead; empirically evaluate on test-time regression, in-context regression, associative recall, and state tracking.

Result: LLA adapts to non-stationarity and outperforms strong baselines in test-time training and in-context learning; provides evidence for scalability and applicability in large-scale models.

Conclusion: LLA offers theoretical advantages and practical pathways for more expressive, scalable attention. The approach shows potential for broad impact and motivates further exploration into test-time regression-based attention and hardware-efficient implementations.

Abstract: Transformer architectures have achieved remarkable success in various
domains. While efficient alternatives to Softmax Attention have been widely
studied, the search for more expressive mechanisms grounded in theoretical
insight-even at greater computational cost-has been relatively underexplored.
In this work, we bridge this gap by proposing Local Linear Attention (LLA), a
novel attention mechanism derived from nonparametric statistics through the
lens of test-time regression. First, we show that LLA offers theoretical
advantages over Linear and Softmax Attention for associative memory via a
bias-variance trade-off analysis. Next, we address its computational challenges
and propose two memory-efficient primitives to tackle the $\Theta(n^2 d)$ and
$\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient,
blockwise algorithm that enables scalable and parallel computation on modern
accelerators. In addition, we implement and profile a customized inference
kernel that significantly reduces memory overheads. Finally, we empirically
validate the advantages and limitations of LLA on test-time regression,
in-context regression, associative recall and state tracking tasks. Experiment
results demonstrate that LLA effectively adapts to non-stationarity,
outperforming strong baselines in test-time training and in-context learning,
and exhibiting promising evidence for its scalability and applicability in
large-scale models. Code is available at
https://github.com/Yifei-Zuo/Flash-LLA.

</details>


### [108] [SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion](https://arxiv.org/abs/2510.01456)
*Brett Barkley,Preston Culbertson,David Fridovich-Keil*

Main category: cs.LG

TL;DR: SCOPED is a fast, diffusion-model OOD detector that combines score curvature and Jacobian trace into a single statistic, enabling unsupervised, threshold-free detection with far fewer forward passes. It delivers competitive accuracy and generalizes across vision and robotics with a single trained model.


<details>
  <summary>Details</summary>
Motivation: Reliable OOD detection is essential for deploying ML systems in vision, robotics, and RL. Diffusion-model-based detectors tend to be computationally expensive and domain-specific. The goal is a fast, general-purpose, unsupervised OOD method that works from a single trained diffusion model.

Method: Compute SCOPED: a test statistic that combines the Jacobian trace and the squared norm of the score function. Use Hutchinson's trace estimator to compute the trace efficiently, and require only a single forward pass and one JVP in the simplest setting. Estimate the in-distribution density of SCOPED scores via kernel density estimation to enable unsupervised, threshold-free detection.

Result: SCOPED achieves competitive or state-of-the-art precision-recall on four vision benchmarks while dramatically reducing forward passes (order of magnitude fewer than prior methods). It generalizes to robotic control tasks, identifying distribution shifts across reward functions and training regimes.

Conclusion: SCOPED provides a practical, fast, and general-purpose OOD detection foundation for diffusion models, applicable to perceptual artifacts, outlier detection in autoregressive models, exploration in RL, and dataset curation for unsupervised training.

Abstract: Out-of-distribution (OOD) detection is essential for reliable deployment of
machine learning systems in vision, robotics, reinforcement learning, and
beyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator
for Diffusion (SCOPED), a fast and general-purpose OOD detection method for
diffusion models that reduces the number of forward passes on the trained model
by an order of magnitude compared to prior methods, outperforming most
diffusion-based baselines and closely approaching the accuracy of the strongest
ones. SCOPED is computed from a single diffusion model trained once on a
diverse dataset, and combines the Jacobian trace and squared norm of the
model's score function into a single test statistic. Rather than thresholding
on a fixed value, we estimate the in-distribution density of SCOPED scores
using kernel density estimation, enabling a flexible, unsupervised test that,
in the simplest case, only requires a single forward pass and one
Jacobian-vector product (JVP), made efficient by Hutchinson's trace estimator.
On four vision benchmarks, SCOPED achieves competitive or state-of-the-art
precision-recall scores despite its low computational cost. The same method
generalizes to robotic control tasks with shared state and action spaces,
identifying distribution shifts across reward functions and training regimes.
These results position SCOPED as a practical foundation for fast and reliable
OOD detection in real-world domains, including perceptual artifacts in vision,
outlier detection in autoregressive models, exploration in reinforcement
learning, and dataset curation for unsupervised training.

</details>


### [109] [Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization](https://arxiv.org/abs/2510.01457)
*Brett Barkley,David Fridovich-Keil*

Main category: cs.LG

TL;DR: MBPO's success in data-efficient RL hinges on addressing two failure modes when using synthetic data: (1) scale mismatches between dynamics and reward models causing critic underestimation during model-policy coevolution, and (2) a poor target representation that inflates variance and yields error-prone rollouts. By addressing these issues, MBPO can outperform SAC in most DMC tasks while retaining Gym-level gains, highlighting the importance of environment-aware algorithm design and benchmark choices.


<details>
  <summary>Details</summary>
Motivation: The abstract seeks to understand when synthetic data helps in MBPO, why it sometimes hurts, and how fixing its failure modes enables policy improvement. It also questions generalization across benchmarks (Gym vs. DeepMind Control Suite) and invites taxonomy tying task structure to algorithm behavior.

Method: Empirical investigation of MBPO on seven DeepMind Control Suite tasks, diagnosing two coupled failure modes (dynamics/reward scale mismatch causing critic underestimation; poor target representation increasing variance). Demonstrates that addressing these issues enables policy improvement and enables MBPO to outperform SAC in five of seven tasks, while preserving Gym performance.

Result: After addressing the identified failure modes, MBPO achieves policy improvement over SAC in five of seven DMC tasks and maintains the strong performance previously reported in OpenAI Gym, bridging the gap between Gym results and DMC performance.

Conclusion: Benchmarks and task structure shape algorithm generalization. The authors advocate for a taxonomy linking MDP/task-level structure to failure modes, pursue unified solutions where possible, and emphasize how benchmark choices influence where algorithms generalize.

Abstract: Synthetic data is a core component of data-efficient Dyna-style model-based
reinforcement learning, yet it can also degrade performance. We study when it
helps, where it fails, and why, and we show that addressing the resulting
failure modes enables policy improvement that was previously unattainable. We
focus on Model-Based Policy Optimization (MBPO), which performs actor and
critic updates using synthetic action counterfactuals. Despite reports of
strong and generalizable sample-efficiency gains in OpenAI Gym, recent work
shows that MBPO often underperforms its model-free counterpart, Soft
Actor-Critic (SAC), in the DeepMind Control Suite (DMC). Although both suites
involve continuous control with proprioceptive robots, this shift leads to
sharp performance losses across seven challenging DMC tasks, with MBPO failing
in cases where claims of generalization from Gym would imply success. This
reveals how environment-specific assumptions can become implicitly encoded into
algorithm design when evaluation is limited. We identify two coupled issues
behind these failures: scale mismatches between dynamics and reward models that
induce critic underestimation and hinder policy improvement during model-policy
coevolution, and a poor choice of target representation that inflates model
variance and produces error-prone rollouts. Addressing these failure modes
enables policy improvement where none was previously possible, allowing MBPO to
outperform SAC in five of seven tasks while preserving the strong performance
previously reported in OpenAI Gym. Rather than aiming only for incremental
average gains, we hope our findings motivate the community to develop
taxonomies that tie MDP task- and environment-level structure to algorithmic
failure modes, pursue unified solutions where possible, and clarify how
benchmark choices ultimately shape the conditions under which algorithms
generalize.

</details>


### [110] [How Well Can Preference Optimization Generalize Under Noisy Feedback?](https://arxiv.org/abs/2510.01458)
*Shawn Im,Yixuan Li*

Main category: cs.LG

TL;DR: Noisy human feedback in preference optimization is analyzed, yielding finite-step generalization guarantees across common loss families (DPO, IPO, SLiC, etc.) under realistic noise models; the work includes empirical validation on contemporary LLMs.


<details>
  <summary>Details</summary>
Motivation: Human feedback is inherently noisy in real-world settings. Understanding how mislabeling and uncertainty affect generalization and training efficiency is essential for reliable alignment of LLMs.

Method: Develop a generalization framework for noisy preference learning with mislabeling and uncertainty. Derive finite-step generalization bounds for a broad family of preference losses (DPO, IPO, SLiC, etc.), showing how generalization decays with noise rate, data distribution, and sample size. Provide empirical validation on contemporary LLMs.

Result: The analysis yields explicit generalization guarantees under noise, quantifies how generalization degrades with increasing noise, and demonstrates the applicability of the bounds across multiple preference-loss families; empirical results corroborate the theory on modern LLMs.

Conclusion: The findings inform robust design of preference-based alignment under imperfect human feedback, highlighting the need to model and mitigate noise in practice and offering a practical framework for evaluating noisy preference learning.

Abstract: As large language models (LLMs) advance their capabilities, aligning these
models with human preferences has become crucial. Preference optimization,
which trains models to distinguish between preferred and non-preferred
responses based on human feedback, has become a crucial component for aligning
LLMs. However, most existing works assume noise-free feedback, which is
unrealistic due to the inherent errors and inconsistencies in human judgments.
This paper addresses the impact of noisy feedback on preference optimization,
providing generalization guarantees under these conditions. In particular, we
consider noise models that correspond to common real-world sources of noise,
such as mislabeling and uncertainty. Unlike traditional analyses that assume
convergence, our work focuses on finite-step preference optimization, offering
new insights that are more aligned with practical LLM training. We describe how
generalization decays with different types of noise across levels of noise
rates based on the preference data distribution and number of samples. Our
analysis for noisy preference learning applies to a broad family of preference
optimization losses such as DPO, IPO, SLiC, etc. Empirical validation on
contemporary LLMs confirms the practical relevance of our findings, offering
valuable insights for developing AI systems that align with human preferences.

</details>


### [111] [LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning](https://arxiv.org/abs/2510.01459)
*Weizhe Chen,Sven Koenig,Bistra Dilkina*

Main category: cs.LG

TL;DR: A length-aware meta-RLVR method (LSPO) that adaptively samples training data by average response length improves RL-based training of LLMs across models and datasets; with ablations and future directions.


<details>
  <summary>Details</summary>
Motivation: To address overthinking and inefficiencies in reinforcement learning with verifiable rewards (RLVR) for large language models by leveraging response-length signals to guide dynamic data sampling.

Method: Proposes LSPO, a meta-RLVR algorithm that dynamically selects training data at each step based on the average response length, integrating length signals into the data-sampling process.

Result: LSPO consistently improves learning effectiveness across multiple base models and datasets; ablation studies explore alternative ways of incorporating length signals into dynamic sampling and offer insights for future work.

Conclusion: Length-aware sampling is a promising direction for RLVR and warrants further exploration of length-based signals and sampling strategies to enhance LLM reasoning training.

Abstract: Since the release of Deepseek-R1, reinforcement learning with verifiable
rewards (RLVR) has become a central approach for training large language models
(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss
functions to make RLVR more efficient and effective. In this paper, motivated
by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy
Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects
training data at each step based on the average response length. We evaluate
LSPO across multiple base models and datasets, demonstrating that it
consistently improves learning effectiveness. In addition, we conduct a
detailed ablation study to examine alternative ways of incorporating length
signals into dynamic sampling, offering further insights and highlighting
promising directions for future research.

</details>


### [112] [The Three Regimes of Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2510.01460)
*Lu Li,Tianwei Ni,Yihao Sun,Pierre-Luc Bacon*

Main category: cs.LG

TL;DR: A stability‚Äìplasticity principle for offline-to-online RL, identifying three online-finetuning regimes with distinct stability requirements; validated by a large-scale study that shows predictions align in 45 of 63 cases, offering guidance based on whether offline data or the pretrained policy performs better.


<details>
  <summary>Details</summary>
Motivation: Offline-to-online RL often yields inconsistent results when online fine-tuning changes; there is a need for a unifying principle to decide what to preserve and how plastic to be during online adaptation.

Method: Introduce a stability‚Äìplasticity framework and classify online-finetuning into three regimes, each with different stability properties. Perform a large-scale empirical study over 63 settings to test whether the framework‚Äôs predictions hold.

Result: The framework‚Äôs predictions align with 45 of 63 cases. The results support the idea that choosing to preserve the better source‚Äîoffline data or pretrained policy‚Äîwhile maintaining sufficient plasticity can explain much of the variability and guide design choices.

Conclusion: Provides a principled framework for designing offline-to-online RL; guidance depends on the relative performance of the offline dataset and the pretrained policy. The study offers partial but substantial empirical support and points to adaptive strategies that balance stability and plasticity.

Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical
paradigm that leverages offline datasets for pretraining and online
interactions for fine-tuning. However, its empirical behavior is highly
inconsistent: design choices of online-fine tuning that work well in one
setting can fail completely in another. We propose a stability--plasticity
principle that can explain this inconsistency: we should preserve the knowledge
of pretrained policy or offline dataset during online fine-tuning, whichever is
better, while maintaining sufficient plasticity. This perspective identifies
three regimes of online fine-tuning, each requiring distinct stability
properties. We validate this framework through a large-scale empirical study,
finding that the results strongly align with its predictions in 45 of 63 cases.
This work provides a principled framework for guiding design choices in
offline-to-online RL based on the relative performance of the offline dataset
and the pretrained policy.

</details>


### [113] [Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation](https://arxiv.org/abs/2510.01471)
*Haotian Xiang,Jinwen Xu,Qin Lu*

Main category: cs.LG

TL;DR: A lightweight BO surrogate using LoRA-tuned LLMs with a variational Bayesian last layer (VBLL) and recursive weighted ensemble (ENS) to handle high-dimensional, irregular inputs, achieving strong sample efficiency on molecular optimization and other tasks.


<details>
  <summary>Details</summary>
Motivation: Bayesian optimization with Gaussian processes struggles in high-dimensional, irregular spaces (categorical/ordinal variables). A scalable, flexible surrogate is needed. Leveraging large language models with efficient fine-tuning (LoRA) and Bayesian last-layer inference can provide a powerful, data-efficient mapping from inputs to objective values, while recursive Bayesian updates and ensemble weighting enable continual adaptation.

Method: Fine-tune a large language model with Low-Rank Adaptation (LoRA); attach a Bayesian last-layer head trained via variational Bayesian last layer (VBLL); perform recursive Bayesian updates for the linear head; assemble a weighted ensemble (ENS) of multiple LoRA-VBLL surrogates with per-model weights and individual LoRA-VBLL parameters updated recursively; integrate the surrogate into a Bayesian optimization loop for querying next points.

Result: Extensive experiments show strong, sample-efficient performance on high-dimensional benchmarks and real-world molecular optimization tasks, with the ensemble approach improving robustness and adaptation over single-model surrogates.

Conclusion: LoRA-VBLL with ENS offers a lightweight, scalable surrogate for BO in high-dimensional irregular spaces, benefiting drug discovery and material design. The framework supports continual learning via recursive Bayesian updates; future work could automate hyperparameter decisions (e.g., LoRA rank, ensemble weights) and extend to broader problem classes.

Abstract: A plethora of applications entail solving black-box optimization problems
with high evaluation costs, including drug discovery, material design, as well
as hyperparameter tuning. Toward finding the global optimum of such black-box
optimization problems with sample efficiency, Bayesian optimization (BO) is a
theoretically elegant framework that relies on a probabilistic surrogate model
so as to iteratively select the query point with well-balanced
exploration-exploitation tradeoffs. The Gaussian process (GP), as the de-facto
choice for surrogate modeling, has achieved compelling performances for vanilla
BO with low-dimensional continuous variables. However, GPs fall short in coping
with high-dimensional counterparts with {\it irregular} variables (e.g.,
categorical, ordinal, etc.). To alleviate this, neural network-based surrogates
have been explored. Inspired by the powerful capabilities of LLMs, we adopt the
LLM as the surrogate to model the mapping from the high-dimensional input
variables to the objective function. To adapt to the current problem, we
leverage the low-rank adaptation (LoRA) to fine-tune the LLM parameters
together with the posterior of a linear regression head via the variational
Bayesian last layer (VBLL) framework. The resulting LoRA-VBLL is not only
computationally light compared to existing alternatives, but also admits
recursive updates. To automate the critical selection of the LoRA rank as well
as other hyperparameters, a weighted ensemble (ENS) of LoRA-VBLL surrogates has
been devised, which further accommodates continual update of the per-model
weight and individual LoRA-VBLL parameters via recursive Bayes. Extensive
experimental results demonstrate the compelling performance of the proposed
(ENS-)LoRA-VBLL approaches on various high-dimensional benchmarks and the
real-world molecular optimization tasks.

</details>


### [114] [PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search](https://arxiv.org/abs/2510.01472)
*Hengyi Zhu,Grace Li Zhang,Shaoyi Huang*

Main category: cs.LG

TL;DR: PEL-NAS is a hardware-aware NAS framework that partitions the search space by complexity, uses LLM-guided prompt co-evolution, and a zero-cost predictor to achieve higher accuracy and lower latency with dramatically reduced search cost on HW-NAS-Bench.


<details>
  <summary>Details</summary>
Motivation: To overcome exploration bias and high search cost in LLM-driven NAS for HW-NAS, and to enable efficient discovery of architectures across the latency spectrum.

Method: Three components: (1) complexity-driven partitioning engine that divides the search space by complexity to promote diversity; (2) LLM-powered architecture prompt co-evolution where the LLM updates a knowledge base of design heuristics from prior rounds and then guides evolution with prompts incorporating this knowledge; (3) a zero-cost predictor to screen candidates without full training.

Result: On HW-NAS-Bench, PEL-NAS achieves higher HV (Pareto hypervolume), lower IGD (distance to Pareto optimal set), and up to 54% lower latency at similar accuracy compared with baselines; search cost reduces from days to minutes relative to traditional supernet baselines.

Conclusion: PEL-NAS effectively mitigates exploration bias, accelerates neural architecture search for hardware constraints, and delivers improved Pareto-optimal accuracy-latency trade-offs with substantially reduced search cost.

Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) requires joint
optimization of accuracy and latency under device constraints. Traditional
supernet-based methods require multiple GPU days per dataset. Large Language
Model (LLM)-driven approaches avoid training a large supernet and can provide
quick feedback, but we observe an exploration bias: the LLM repeatedly proposes
neural network designs within limited search space and fails to discover
architectures across different latency ranges in the entire search space. To
address this issue, we propose PEL-NAS: a search space Partitioned,
architecture prompt co-Evolutionary and LLM-driven Neural Architecture Search
that can generate neural networks with high accuracy and low latency with
reduced search cost. Our proposed PEL-NAS has three key components: 1) a
complexity-driven partitioning engine that divides the search space by
complexity to enforce diversity and mitigate exploration bias; 2) an
LLM-powered architecture prompt co-evolution operator, in which the LLM first
updates a knowledge base of design heuristics based on results from the
previous round, then performs a guided evolution algorithm on architectures
with prompts that incorporate this knowledge base. Prompts and designs improve
together across rounds which avoids random guesswork and improve efficiency; 3)
a zero-cost predictor to avoid training a large number of candidates from
scratch. Experimental results show that on HW-NAS-Bench, PEL-NAS can achieve
overall higher HV, lower IGD, and up to 54% lower latency than baselines at
similar accuracy. Meanwhile, the search cost drops from days to minutes
compared with traditional supernet baselines.

</details>


### [115] [Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets](https://arxiv.org/abs/2510.01479)
*Shriram Karpoora Sundara Pandian,Ali Baheri*

Main category: cs.LG

TL;DR: Density-Ratio Weighted Behavioral Cloning for robust offline imitation from contaminated data, using a small clean reference to estimate trajectory density ratios via a binary discriminator, weighting the BC loss; provides finite-sample guarantees independent of contamination rate; empirically robust against various poisoning protocols.


<details>
  <summary>Details</summary>
Motivation: Offline RL datasets are frequently contaminated (adversarial poisoning, system errors, low-quality samples), which harms standard BC and offline RL methods. A robust imitation method is needed that can down-weight corrupted data using minimal clean supervision and without modeling the contamination process, which is essential for safety-critical applications.

Method: Train a binary discriminator to distinguish clean reference trajectories from the full dataset to estimate trajectory-level density ratios. Clip these ratios and use them as weights in the BC objective to emphasize clean expert behavior while down-weighting or discarding corrupted data. The approach does not require knowledge of the contamination mechanism. Provide finite-sample theoretical guarantees showing convergence to the clean policy with bounds independent of contamination rate. Establish an evaluation framework with multiple poisoning protocols on continuous-control benchmarks.

Result: Empirically, Weighted BC maintains near-optimal performance even at high contamination ratios and outperforms traditional BC, BCQ, and BRAC across various poisoning settings.

Conclusion: Density-Ratio Weighted Behavioral Cloning offers a robust imitation learning framework for offline RL that leverages a small clean reference set to mitigate data contamination, with theoretical guarantees and strong empirical robustness across poisoning types.

Abstract: Offline reinforcement learning (RL) enables policy optimization from fixed
datasets, making it suitable for safety-critical applications where online
exploration is infeasible. However, these datasets are often contaminated by
adversarial poisoning, system errors, or low-quality samples, leading to
degraded policy performance in standard behavioral cloning (BC) and offline RL
methods. This paper introduces Density-Ratio Weighted Behavioral Cloning
(Weighted BC), a robust imitation learning approach that uses a small, verified
clean reference set to estimate trajectory-level density ratios via a binary
discriminator. These ratios are clipped and used as weights in the BC objective
to prioritize clean expert behavior while down-weighting or discarding
corrupted data, without requiring knowledge of the contamination mechanism. We
establish theoretical guarantees showing convergence to the clean expert policy
with finite-sample bounds that are independent of the contamination rate. A
comprehensive evaluation framework is established, which incorporates various
poisoning protocols (reward, state, transition, and action) on continuous
control benchmarks. Experiments demonstrate that Weighted BC maintains
near-optimal performance even at high contamination ratios outperforming
baselines such as traditional BC, batch-constrained Q-learning (BCQ) and
behavior regularized actor-critic (BRAC).

</details>


### [116] [Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed](https://arxiv.org/abs/2510.01494)
*Isha Gupta,Rylan Schaeffer,Joshua Kazdan,Ken Liu,Sanmi Koyejo*

Main category: cs.LG

TL;DR: Adversarial transferability is domain-dependent: data-space attacks can transfer across models, but representation-space attacks generally do not unless latent geometries are aligned.


<details>
  <summary>Details</summary>
Motivation: Explain why some adversarial attacks transfer between models and others do not, especially in vision-language models, and identify the role of attack space (data vs. representation) in transfer.

Method: Provide a theoretical result in a simple setting where two networks share an input-output map but differ in representations; build representation-space attacks for image classifiers and LMs that fail to transfer; show data-space attacks on VLMs transfer and that representation-space transfer is possible only with aligned latent geometries.

Result: Proves a fundamental distinction: input-data-space attacks can transfer across models, while representation-space attacks do not transfer unless representations are geometrically aligned. Representation-space attacks in classifiers and LMs fail to transfer; data-space attacks on VLMs transfer; transfer of representation-space attacks is possible when post-projector latent geometries are sufficiently aligned.

Conclusion: Adversarial transfer is not universal; robustness must address both data-space and representation-space vulnerabilities, with attention to latent geometry alignment in model adapters and post-projectors.

Abstract: The field of adversarial robustness has long established that adversarial
examples can successfully transfer between image classifiers and that text
jailbreaks can successfully transfer between language models (LMs). However, a
pair of recent studies reported being unable to successfully transfer image
jailbreaks between vision-language models (VLMs). To explain this striking
difference, we propose a fundamental distinction regarding the transferability
of attacks against machine learning models: attacks in the input data-space can
transfer, whereas attacks in model representation space do not, at least not
without geometric alignment of representations. We then provide theoretical and
empirical evidence of this hypothesis in four different settings. First, we
mathematically prove this distinction in a simple setting where two networks
compute the same input-output map but via different representations. Second, we
construct representation-space attacks against image classifiers that are as
successful as well-known data-space attacks, but fail to transfer. Third, we
construct representation-space attacks against LMs that successfully jailbreak
the attacked models but again fail to transfer. Fourth, we construct data-space
attacks against VLMs that successfully transfer to new VLMs, and we show that
representation space attacks \emph{can} transfer when VLMs' latent geometries
are sufficiently aligned in post-projector space. Our work reveals that
adversarial transfer is not an inherent property of all attacks but contingent
on their operational domain - the shared data-space versus models' unique
representation spaces - a critical insight for building more robust models.

</details>


### [117] [Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information](https://arxiv.org/abs/2510.01499)
*Rui Ai,Yuqi Pan,David Simchi-Levi,Milind Tambe,Haifeng Xu*

Main category: cs.LG

TL;DR: Proposes Optimal Weight (OW) and Inverse Surprising Popularity (ISP) as aggregation methods for multi-agent LLM reasoning, using first- and second-order information to improve over majority voting; shows theoretical guarantees and empirical gains across synthetic data, UltraFeedback, MMLU, and ARMMAN healthcare tasks.


<details>
  <summary>Details</summary>
Motivation: Majority voting treats all model outputs equally and ignores latent heterogeneity and correlations among models, which can degrade the reliability of collective decisions in multi-agent LLM systems.

Method: Introduce two aggregation algorithms, OW and ISP, that leverage first-order and second-order statistics to weight model outputs. Provide theoretical analysis under mild assumptions showing mitigation of majority voting limitations. Validate empirically on synthetic data, standard LLM fine-tuning benchmarks (UltraFeedback, MMLU), and a real-world healthcare setting (ARMMAN).

Result: OW and ISP consistently outperform majority voting across diverse tasks, delivering practical performance gains and offering conceptual insights for designing robust multi-agent LLM pipelines.

Conclusion: The work contributes robust aggregation strategies for ensemble LLM reasoning, combining provable guarantees with empirical validation, and offers design guidance for future multi-agent LLM systems.

Abstract: With the rapid progress of multi-agent large language model (LLM) reasoning,
how to effectively aggregate answers from multiple LLMs has emerged as a
fundamental challenge. Standard majority voting treats all answers equally,
failing to consider latent heterogeneity and correlation across models. In this
work, we design two new aggregation algorithms called Optimal Weight (OW) and
Inverse Surprising Popularity (ISP), leveraging both first-order and
second-order information. Our theoretical analysis shows these methods provably
mitigate inherent limitations of majority voting under mild assumptions,
leading to more reliable collective decisions. We empirically validate our
algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as
UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all
cases, our methods consistently outperform majority voting, offering both
practical performance gains and conceptual insights for the design of robust
multi-agent LLM pipelines.

</details>


### [118] [Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control](https://arxiv.org/abs/2510.01508)
*Will Y. Zou,Jean Feng,Alexandre Kalimouttou,Jennifer Yuntong Zhang,Christopher W. Seymour,Romain Pirracchio*

Main category: cs.LG

TL;DR: Action-space design with offline conservative Q-learning and recurrent replay improves norepinephrine dosing policies for septic shock in ICU, yielding interpretability and >15% survival improvement in eICU/MIMIC data.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning in clinical decision support often faces skepticism due to safety and interpretability concerns. This work aims to enhance adoption by crafting action spaces and safe learning methods for dual vasopressor dosing in septic shock.

Method: An end-to-end offline RL approach using conservative Q-learning, a recurrent modeling component in the replay buffer to capture temporal ICU time-series dependencies, and action-space design that supports discrete, continuous, and directional dosing. Evaluation across norepinephrine dosing strategies on eICU and MIMIC datasets.

Result: Found that action-space design profoundly shapes learned policies, improving interpretability and clinical adoption while preserving efficacy. Empirical results show >15% improvement in survival probability and alignment with established clinical protocols.

Conclusion: Designing appropriate action spaces is critical for deployable RL-based CDSS dosing policies; the proposed offline RL framework yields safer, more interpretable, and more effective dosing strategies in septic shock care.

Abstract: Reinforcement learning (RL) applications in Clinical Decision Support Systems
(CDSS) frequently encounter skepticism from practitioners regarding inoperable
dosing decisions. We address this challenge with an end-to-end approach for
learning optimal drug dosing and control policies for dual vasopressor
administration in intensive care unit (ICU) patients with septic shock. For
realistic drug dosing, we apply action space design that accommodates discrete,
continuous, and directional dosing strategies in a system that combines offline
conservative Q-learning with a novel recurrent modeling in a replay buffer to
capture temporal dependencies in ICU time-series data. Our comparative analysis
of norepinephrine dosing strategies across different action space formulations
reveals that the designed action spaces improve interpretability and facilitate
clinical adoption while preserving efficacy. Empirical results1 on eICU and
MIMIC demonstrate that action space design profoundly influences learned
behavioral policies. The proposed methods achieve improved patient outcomes of
over 15% in survival improvement probability, while aligning with established
clinical protocols.

</details>


### [119] [Flock: A Knowledge Graph Foundation Model via Learning on Random Walks](https://arxiv.org/abs/2510.01510)
*Jinwoo Kim,Xingyue Huang,Krzysztof Olejniczak,Kyungbin Min,Michael Bronstein,Seunghoon Hong,ƒ∞smail ƒ∞lkan Ceylan*

Main category: cs.LG

TL;DR: Probabilistic node-relation (N-R) equivariance unlocks expressiveness in KG foundation models by preserving equivariance in distribution and breaking symmetries during inference. The paper introduces Flock, a KGFM that samples random walks, records sequences, encodes via a sequence model, and learns pooling to aggregate node/relation representations; it is a universal approximator for isomorphism-invariant link-level functions and achieves state-of-the-art on 54 knowledge graphs, including solving the Petals diagnostic dataset.


<details>
  <summary>Details</summary>
Motivation: Deterministic equivariance in existing KG foundation models constrains expressiveness, preventing the model from distinguishing structurally similar but semantically distinct relations. A probabilistic approach aims to maintain transferable equivariance while enabling discrimination in inference, improving zero-shot link prediction across novel entities and relations.

Method: Introduce probabilistic node-relation equivariance. Develop Flock: iteratively sample random walks on the KG, encode them into sequences using a recording protocol, embed sequences with a sequence model, and aggregate node/relation representations via learned pooling. Ensure the model respects probabilistic N-R equivariance and acts as a universal approximator for isomorphism-invariant link-level functions.

Result: Flock serves as a universal approximator for isomorphism-invariant link-level functions on KGs and achieves state-of-the-art results on multiple benchmarks, including solving the Petals diagnostic dataset where prior KGFMs fail and surpassing existing methods on entity and relation prediction across 54 KGs.

Conclusion: Probabilistic node-relation equivariance, coupled with the Flock architecture, enhances expressive power and generalization for zero-shot link prediction on KGs, enabling perfect diagnostic performance on Petals and strong, broad performance across diverse knowledge graphs.

Abstract: We study the problem of zero-shot link prediction on knowledge graphs (KGs),
which requires models to generalize over novel entities and novel relations.
Knowledge graph foundation models (KGFMs) address this task by enforcing
equivariance over both nodes and relations, learning from structural properties
of nodes and relations, which are then transferable to novel graphs with
similar structural properties. However, the conventional notion of
deterministic equivariance imposes inherent limits on the expressive power of
KGFMs, preventing them from distinguishing structurally similar but
semantically distinct relations. To overcome this limitation, we introduce
probabilistic node-relation equivariance, which preserves equivariance in
distribution while incorporating a principled randomization to break symmetries
during inference. Building on this principle, we present Flock, a KGFM that
iteratively samples random walks, encodes them into sequences via a recording
protocol, embeds them with a sequence model, and aggregates representations of
nodes and relations via learned pooling. Crucially, Flock respects
probabilistic node-relation equivariance and is a universal approximator for
isomorphism-invariant link-level functions over KGs. Empirically, Flock
perfectly solves our new diagnostic dataset Petals where current KGFMs fail,
and achieves state-of-the-art performances on entity- and relation prediction
tasks on 54 KGs from diverse domains.

</details>


### [120] [Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties](https://arxiv.org/abs/2510.01520)
*Hossein Sholehrasa,Xuan Xu,Doina Caragea,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.LG

TL;DR: A large-scale ML framework using OpenFDA CVM data to predict fatal vs recovery outcomes in veterinary drug-event reports, achieving high precision/recall with CatBoost and ensemble methods; enhanced minority detection with AUM pseudo-labeling; interpretable via SHAP.


<details>
  <summary>Details</summary>
Motivation: Protect animal welfare and human food safety by detecting adverse drug-event signals early to prevent violative residues; address data sparsity, high-cardinality features, and class imbalance in veterinary safety data.

Method: Data preprocessing: merge relational tables, standardize AEs with VeDDRA; normalize, impute missing values, reduce high-cardinality features; integrate physicochemical drug properties; train supervised models (RF, CatBoost, XGBoost, ExcelFormer, LLMs Gemma 3-27B, Phi 3-12B); address class imbalance with undersampling/oversampling; use ensemble methods (Voting, Stacking); apply AUM-based pseudo-labeling to uncertain/minority cases; interpret with SHAP.

Result: CatBoost and ensemble methods achieved precision, recall, and F1 around 0.95; AUM-based pseudo-labeling improved minority-class detection, notably for ExcelFormer and XGBoost; SHAP highlighted lung/heart/bronchial disorders, demographics, and drug properties as key predictors for fatal outcomes.

Conclusion: Demonstrates that robust data engineering, advanced ML, and explainable AI enable accurate, interpretable veterinary safety predictions; supports FARAD's mission for early high-risk profile detection, stronger residue risk assessment, and informed regulatory/clinical decision-making.

Abstract: The safe use of pharmaceuticals in food-producing animals is vital to protect
animal welfare and human food safety. Adverse events (AEs) may signal
unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of
violative residues in the food chain. This study introduces a predictive
framework for classifying outcomes (Death vs. Recovery) using ~1.28 million
reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary
Medicine. A preprocessing pipeline merged relational tables and standardized
AEs through VeDDRA ontologies. Data were normalized, missing values imputed,
and high-cardinality features reduced; physicochemical drug properties were
integrated to capture chemical-residue links. We evaluated supervised models,
including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language
models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as
undersampling and oversampling, with a focus on prioritizing recall for fatal
outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,
achieving precision, recall, and F1-scores of 0.95. Incorporating Average
Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved
minority-class detection, particularly in ExcelFormer and XGBoost.
Interpretability via SHAP identified biologically plausible predictors,
including lung, heart, and bronchial disorders, animal demographics, and drug
physicochemical properties. These features were strongly linked to fatal
outcomes. Overall, the framework shows that combining rigorous data
engineering, advanced machine learning, and explainable AI enables accurate,
interpretable predictions of veterinary safety outcomes. The approach supports
FARAD's mission by enabling early detection of high-risk drug-event profiles,
strengthening residue risk assessment, and informing regulatory and clinical
decision-making.

</details>


### [121] [Predictive Preference Learning from Human Interventions](https://arxiv.org/abs/2510.01545)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.LG

TL;DR: Predictive Preference Learning from Human Interventions (PPL) propagates human corrections into a horizon of future steps to guide future rollouts, improving safety and data efficiency in interactive imitation learning by leveraging implicit preferences from interventions.


<details>
  <summary>Details</summary>
Motivation: Current interactive imitation learning methods typically correct actions only at the current state and do not explicitly shape future actions, which can be hazardous. PPL uses human interventions to influence predicted future behavior, addressing safety in long-horizon rollout.

Method: Bootstrap each human intervention into a horizon of L future time steps, assuming the agent repeats the same action and the human would intervene again within the horizon. Apply preference optimization on these future states to propagate expert corrections into safety-critical regions, enhancing learning efficiency and reducing the number of demonstrations.

Result: Empirical validation on autonomous driving and robotic manipulation benchmarks shows improved learning efficiency and generality; interventions effectively influence future rollouts. Theoretical analysis indicates that an appropriate choice of horizon L balances state coverage and label correctness, bounding the algorithmic optimality gap.

Conclusion: Selecting L is crucial to trade off coverage of risky states against label noise; PPL provides a general, efficient framework for incorporating human interventions into future-safe policies, with available demos and code.

Abstract: Learning from human involvement aims to incorporate the human subject to
monitor and correct agent behavior errors. Although most interactive imitation
learning methods focus on correcting the agent's action at the current state,
they do not adjust its actions in future states, which may be potentially more
hazardous. To address this, we introduce Predictive Preference Learning from
Human Interventions (PPL), which leverages the implicit preference signals
contained in human interventions to inform predictions of future rollouts. The
key idea of PPL is to bootstrap each human intervention into L future time
steps, called the preference horizon, with the assumption that the agent
follows the same action and the human makes the same intervention in the
preference horizon. By applying preference optimization on these future states,
expert corrections are propagated into the safety-critical regions where the
agent is expected to explore, significantly improving learning efficiency and
reducing human demonstrations needed. We evaluate our approach with experiments
on both autonomous driving and robotic manipulation benchmarks and demonstrate
its efficiency and generality. Our theoretical analysis further shows that
selecting an appropriate preference horizon L balances coverage of risky states
with label correctness, thereby bounding the algorithmic optimality gap. Demo
and code are available at: https://metadriverse.github.io/ppl

</details>


### [122] [CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models](https://arxiv.org/abs/2510.01521)
*Diptyaroop Maji,Kang Yang,Prashant Shenoy,Ramesh K Sitaraman,Mani Srivastava*

Main category: cs.LG

TL;DR: CarbonX is an open-source time-series foundation model tool designed for global carbon-intensity forecasting and imputation with uncertainty estimates, using a single general model trained on historical data.


<details>
  <summary>Details</summary>
Motivation: There is a need for accurate, fine-grained carbon-intensity forecasts for decarbonization across computing and societal systems, but existing tools require grid-specific data, have limited global coverage, and lack uncertainty estimates.

Method: Leverages Time Series Foundation Models (TSFMs) trained on historical carbon-intensity data to perform zero-shot forecasting across 214 grids with a single general model, provides 21-day forecasts with 95% prediction intervals, and can be fully fine-tuned for improved imputation performance (1.2‚Äì3.9x over baselines).

Result: Zero-shot forecasting MAPE of 15.82% across 214 grids; average MAPE of 9.59% on 13 benchmark grids; tail forecasting MAPE of 16.54%; 95% coverage of prediction intervals; forecasts up to 21 days with minimal accuracy degradation; after fine-tuning, imputation outperforms statistical baselines by 1.2‚Äì3.9x.

Conclusion: CarbonX can be applied to any grid with limited data, delivering strong performance and uncertainty estimates at global scale, making it a practical tool for carbon-aware decision making.

Abstract: Computational decarbonization aims to reduce carbon emissions in computing
and societal systems such as data centers, transportation, and built
environments. This requires accurate, fine-grained carbon intensity forecasts,
yet existing tools have several key limitations: (i) they require grid-specific
electricity mix data, restricting use where such information is unavailable;
(ii) they depend on separate grid-specific models that make it challenging to
provide global coverage; and (iii) they provide forecasts without uncertainty
estimates, limiting reliability for downstream carbon-aware applications.
  In this paper, we present CarbonX, an open-source tool that leverages Time
Series Foundation Models (TSFMs) for a range of decarbonization tasks. CarbonX
utilizes the versatility of TSFMs to provide strong performance across multiple
tasks, such as carbon intensity forecasting and imputation, and across diverse
grids. Using only historical carbon intensity data and a single general model,
our tool achieves a zero-shot forecasting Mean Absolute Percentage Error (MAPE)
of 15.82% across 214 grids worldwide. Across 13 benchmark grids, CarbonX
performance is comparable with the current state-of-the-art, with an average
MAPE of 9.59% and tail forecasting MAPE of 16.54%, while also providing
prediction intervals with 95% coverage. CarbonX can provide forecasts for up to
21 days with minimal accuracy degradation. Further, when fully fine-tuned,
CarbonX outperforms the statistical baselines by 1.2--3.9X on the imputation
task. Overall, these results demonstrate that CarbonX can be used easily on any
grid with limited data and still deliver strong performance, making it a
practical tool for global-scale decarbonization.

</details>


### [123] [On Integer Programming for the Binarized Neural Network Verification Problem](https://arxiv.org/abs/2510.01525)
*Woojin Kim,James R. Luedtke*

Main category: cs.LG

TL;DR: Two IP formulation improvements for verifying binarized neural networks: a linearized multi-class objective and new valid inequalities that exploit the network's recursive structure, yielding better verification under time constraints.


<details>
  <summary>Details</summary>
Motivation: BNNs are attractive for efficiency, but verifying robustness to input perturbations is hard due to large integrality gaps in big-M IP formulations; improving the IP can enable verification for larger perturbations.

Method: 1) derive a linear objective suitable for multi-class classification within the IP; 2) develop valid inequalities that exploit the recursive, layered structure of BNNs to tighten the IP formulation and reduce the gap; analyze and test on verification tasks.

Result: The techniques enable verifying BNNs over a higher range of input perturbations within a limited time, outperforming existing IP approaches.

Conclusion: The proposed IP enhancements effectively tighten the formulation and improve practical verifiability of BNN robustness under perturbations.

Abstract: Binarized neural networks (BNNs) are feedforward neural networks with binary
weights and activation functions. In the context of using a BNN for
classification, the verification problem seeks to determine whether a small
perturbation of a given input can lead it to be misclassified by the BNN, and
the robustness of the BNN can be measured by solving the verification problem
over multiple inputs. The BNN verification problem can be formulated as an
integer programming (IP) problem. However, the natural IP formulation is often
challenging to solve due to a large integrality gap induced by big-$M$
constraints. We present two techniques to improve the IP formulation. First, we
introduce a new method for obtaining a linear objective for the multi-class
setting. Second, we introduce a new technique for generating valid inequalities
for the IP formulation that exploits the recursive structure of BNNs. We find
that our techniques enable verifying BNNs against a higher range of input
perturbation than existing IP approaches within a limited time.

</details>


### [124] [Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs](https://arxiv.org/abs/2510.01527)
*Lecheng Kong,Xiyuan Wang,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: Train LLM-based chemistry models to maximize round-trip consistency between forward and reverse mappings using Round-Trip Reinforcement Learning (RTRL) and an iterative self-improvement loop, improving both performance and consistency especially with unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Current chemical LLMs exhibit strong unidirectional memorization and round-trip inconsistency: they can describe a molecule but fail to reconstruct the original structure from generated text. Prior work shows a strong link between round-trip consistency and task performance, suggesting consistency should be a trainable objective.

Method: Introduce Round-Trip Reinforcement Learning (RTRL) that uses the success of forward-reverse transformations as a reward signal to improve consistency. Propose an iterative variant where forward and reverse mappings train each other in a self-improvement loop, enabling data-efficient learning from large amounts of unlabeled chemistry data.

Result: RTRL significantly boosts both performance and round-trip consistency compared with strong baselines across supervised, self-supervised, and synthetic data regimes in chemistry tasks.

Conclusion: Round-trip consistency can be learned as a trainable objective, offering a new path to more robust and reliable foundation models for computational chemistry by leveraging bidirectional mappings and self-improvement loops.

Abstract: Large Language Models (LLMs) are emerging as versatile foundation models for
computational chemistry, handling bidirectional tasks like reaction prediction
and retrosynthesis. However, these models often lack round-trip consistency.
For instance, a state-of-the-art chemical LLM may successfully caption a
molecule, yet be unable to accurately reconstruct the original structure from
its own generated text. This inconsistency suggests that models are learning
unidirectional memorization rather than flexible mastery. Indeed, recent work
has demonstrated a strong correlation between a model's round-trip consistency
and its performance on the primary tasks. This strong correlation reframes
consistency into a direct target for model improvement. We therefore introduce
Round-Trip Reinforcement Learning (RTRL), a novel framework that trains a model
to improve its consistency by using the success of a round-trip transformation
as a reward signal. We further propose an iterative variant where forward and
reverse mappings alternately train each other in a self-improvement loop, a
process that is highly data-efficient and notably effective with the massive
amount of unlabelled data common in chemistry. Experiments demonstrate that
RTRL significantly \textbf{boosts performance and consistency} over strong
baselines across supervised, self-supervised, and synthetic data regimes. This
work shows that round-trip consistency is not just a desirable property but a
trainable objective, offering a new path toward more robust and reliable
foundation models.

</details>


### [125] [Bypassing Prompt Guards in Production with Controlled-Release Prompting](https://arxiv.org/abs/2510.01529)
*Jaiden Fairoze,Sanjam Garg,Keewoo Lee,Mingyuan Wang*

Main category: cs.LG

TL;DR: A novel jailbreak attack circumvents lightweight prompt guards by encoding jailbreak prompts that guards cannot decode but the main LLM can; attack is effective across major production models; indicates limitations of prompt guards and suggests defenses should prevent outputs rather than inputs; identifies additional alignment issues such as data extraction and leakage.


<details>
  <summary>Details</summary>
Motivation: To ensure AI safety and alignment as LLMs become more capable, highlighting the vulnerabilities of lightweight prompt guards and the need for stronger defenses that do not rely solely on input filtering.

Method: Introduce a jailbreak technique that exploits a resource asymmetry between the prompt guard and the main LLM, encoding jailbreak prompts in a way that guards cannot decode but the main model can. Evaluate across production models (e.g., Google Gemini 2.5 Flash/Pro, DeepSeek Chat/DeepThink, Grok, Mistral Le Chat/Magistral) to demonstrate consistent bypass while preserving response quality.

Result: The attack consistently jailbreaks production models while maintaining response quality, exposing a fundamental vulnerability in lightweight prompt guards and their limited ability to constrain model output. Demonstrated across multiple protected chat interfaces, revealing a practical attack surface in modern LLM architectures.

Conclusion: Lightweight prompt guards are insufficient for robust LLM safety. Defenses should shift from blocking malicious inputs to preventing malicious outputs. The work also points to other alignment concerns, including copyrighted data extraction, training data extraction, and malicious response leakage during chain-of-thought.

Abstract: As large language models (LLMs) advance, ensuring AI safety and alignment is
paramount. One popular approach is prompt guards, lightweight mechanisms
designed to filter malicious queries while being easy to implement and update.
In this work, we introduce a new attack that circumvents such prompt guards,
highlighting their limitations. Our method consistently jailbreaks production
models while maintaining response quality, even under the highly protected chat
interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok
(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry
between the prompt guard and the main LLM, encoding a jailbreak prompt that
lightweight guards cannot decode but the main model can. This reveals an attack
surface inherent to lightweight prompt guards in modern LLM architectures and
underscores the need to shift defenses from blocking malicious inputs to
preventing malicious outputs. We additionally identify other critical alignment
issues, such as copyrighted data extraction, training data extraction, and
malicious response leakage during thinking.

</details>


### [126] [NVIDIA AI Aerial: AI-Native Wireless Communications](https://arxiv.org/abs/2510.01533)
*Kobi Cohen-Arazi,Michael Roe,Zhen Hu,Rohan Chavan,Anna Ptasznik,Joanna Lin,Joao Morais,Joseph Boccuzzi,Tommaso Balercia*

Main category: cs.LG

TL;DR: A framework that compiles Python-based ML algorithms into GPU-executable blobs to enable AI-native 6G systems, demonstrated by CNN-based channel estimation in a PUSCH receiver, validated in a digital twin and real-time testbed on NVIDIA GPUs (NVIDIA AI Aerial).


<details>
  <summary>Details</summary>
Motivation: 6G aims for AI-native wireless systems with tight DSP/ML integration across software stacks and life-cycle stages. There is a need to bridge Python ML workflows with GPU-accelerated telecom hardware to achieve scalable, high-performance AI deployment in networks.

Method: Introduce a framework that compiles Python-based algorithms into GPU-runnable blobs, leveraging the NVIDIA AI Aerial platform. Validate via a CNN-based channel estimation in the PUSCH receiver, first in a digital twin and then in a real-time testbed.

Result: Demonstrates efficiency, flexibility, and high performance on NVIDIA GPUs, enabling real-time channel estimation via a Python-trained CNN and supporting the seamless AI/ML lifecycle for 6G development.

Conclusion: The framework provides a scalable pathway to integrate AI/ML into next-generation cellular systems, underpinning the vision of natively intelligent 6G networks.

Abstract: 6G brings a paradigm shift towards AI-native wireless systems, necessitating
the seamless integration of digital signal processing (DSP) and machine
learning (ML) within the software stacks of cellular networks. This
transformation brings the life cycle of modern networks closer to AI systems,
where models and algorithms are iteratively trained, simulated, and deployed
across adjacent environments. In this work, we propose a robust framework that
compiles Python-based algorithms into GPU-runnable blobs. The result is a
unified approach that ensures efficiency, flexibility, and the highest possible
performance on NVIDIA GPUs. As an example of the capabilities of the framework,
we demonstrate the efficacy of performing the channel estimation function in
the PUSCH receiver through a convolutional neural network (CNN) trained in
Python. This is done in a digital twin first, and subsequently in a real-time
testbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform,
lays the foundation for scalable integration of AI/ML models into
next-generation cellular systems, and is essential for realizing the vision of
natively intelligent 6G networks.

</details>


### [127] [TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis](https://arxiv.org/abs/2510.01538)
*Haokun Zhao,Xiang Zhang,Jiaqi Wei,Yiwei Xu,Yuting He,Siqi Sun,Chenyu You*

Main category: cs.LG

TL;DR: TimeSeriesScientist (TSci) is an LLM-driven agentic framework with four specialized agents that automate preprocessing, model selection, fitting/validation, and reporting for general time series forecasting, achieving improved accuracy and transparent, interpretable outputs across benchmarks.


<details>
  <summary>Details</summary>
Motivation: There is a strong need for a domain-agnostic forecasting framework that minimizes labor-intensive preprocessing, validation, and ensembling, since existing methods are often tailored to specific datasets and generalize poorly, hindering scalable, interpretable forecasting.

Method: Four agents: Curator (LLM-guided diagnostics + tools to target preprocessing), Planner (narrows model-space via multi-modal diagnostics and self-planning), Forecaster (fits/validates models and adaptively selects configurations and ensembles), Reporter (compiles a transparent, comprehensive report). The framework emphasizes white-box reasoning and interpretability, with cross-task extensibility.

Result: Empirical evaluation on eight benchmarks shows TSci consistently outperforms statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2% respectively, and delivering clear, rigorous reports.

Conclusion: TSci provides a general, interpretable, and extensible forecasting workflow that reduces human intervention and improves accuracy across diverse time-series tasks.

Abstract: Time series forecasting is central to decision-making in domains as diverse
as energy, finance, climate, and public health. In practice, forecasters face
thousands of short, noisy series that vary in frequency, quality, and horizon,
where the dominant cost lies not in model fitting, but in the labor-intensive
preprocessing, validation, and ensembling required to obtain reliable
predictions. Prevailing statistical and deep learning models are tailored to
specific datasets or domains and generalize poorly. A general, domain-agnostic
framework that minimizes human intervention is urgently in demand. In this
paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic
framework for general time series forecasting. The framework comprises four
specialized agents: Curator performs LLM-guided diagnostics augmented by
external tools that reason over data statistics to choose targeted
preprocessing; Planner narrows the hypothesis space of model choice by
leveraging multi-modal diagnostics and self-planning over the input; Forecaster
performs model fitting and validation and, based on the results, adaptively
selects the best model configuration as well as ensemble strategy to make final
predictions; and Reporter synthesizes the whole process into a comprehensive,
transparent report. With transparent natural-language rationales and
comprehensive reports, TSci transforms the forecasting workflow into a
white-box system that is both interpretable and extensible across tasks.
Empirical results on eight established benchmarks demonstrate that TSci
consistently outperforms both statistical and LLM-based baselines, reducing
forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci
produces a clear and rigorous report that makes the forecasting workflow more
transparent and interpretable.

</details>


### [128] [Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code](https://arxiv.org/abs/2510.01539)
*Aniket Vashishtha,Qirun Dai,Hongyuan Mei,Amit Sharma,Chenhao Tan,Hao Peng*

Main category: cs.LG

TL;DR: Executable counterfactuals framework reveals that current LLMs struggle with the full three-step counterfactual reasoning (abduction, intervention, and prediction). There is a substantial accuracy drop (25-40%) from interventional to counterfactual tasks for state-of-the-art models, and reinforcement learning improves generalization and performance on code and math tasks, while supervised fine-tuning helps in-domain but hurts out-of-domain (OOD) tasks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLM counterfactual reasoning often skip the abductive step, reducing the problem to interventional reasoning and overestimating capabilities. A scalable, executable framework that enforces all three steps is needed to properly assess and improve causal reasoning, especially for high-stakes domains.

Method: Propose executable counterfactuals using code and math problems that require abductive inference, interventions, and predictions. Generate scalable synthetic data with varying difficulty. Evaluate different training strategies (supervised finetuning on reasoning traces vs reinforcement learning) and test generalization to out-of-domain structures (e.g., while loops) and math word problems.

Result: Found substantial performance gaps: SOTA models exhibit a 25-40% drop from interventional to counterfactual reasoning. Supervised finetuning improves in-domain (code) performance for some models (e.g., Qwen) but reduces accuracy on OOD counterfactual math problems. In contrast, reinforcement learning yields core cognitive behaviors and generalizes to new domains, providing 1.5x-2x gains on code and math problems.

Conclusion: Reinforcement learning shows strong promise for instilling robust counterfactual reasoning in LLMs, and the executable counterfactuals framework provides a scalable path to evaluating and improving LLM causal reasoning across domains, including high-stakes settings.

Abstract: Counterfactual reasoning, a hallmark of intelligence, consists of three
steps: inferring latent variables from observations (abduction), constructing
alternatives (interventions), and predicting their outcomes (prediction). This
skill is essential for advancing LLMs' causal understanding and expanding their
applications in high-stakes domains such as scientific research. However,
existing efforts in assessing LLM's counterfactual reasoning capabilities tend
to skip the abduction step, effectively reducing to interventional reasoning
and leading to overestimation of LLM performance. To address this, we introduce
executable counterfactuals, a novel framework that operationalizes causal
reasoning through code and math problems. Our framework explicitly requires all
three steps of counterfactual reasoning and enables scalable synthetic data
creation with varying difficulty, creating a frontier for evaluating and
improving LLM's reasoning. Our results reveal substantial drop in accuracy
(25-40%) from interventional to counterfactual reasoning for SOTA models like
o4-mini and Claude-4-Sonnet. To address this gap, we construct a training set
comprising counterfactual code problems having if-else condition and test on
out-of-domain code structures (e.g. having while-loop); we also test whether a
model trained on code would generalize to counterfactual math word problems.
While supervised finetuning on stronger models' reasoning traces improves
in-domain performance of Qwen models, it leads to a decrease in accuracy on OOD
tasks such as counterfactual math problems. In contrast, reinforcement learning
induces the core cognitive behaviors and generalizes to new domains, yielding
gains over the base model on both code (improvement of 1.5x-2x) and math
problems. Analysis of the reasoning traces reinforces these findings and
highlights the promise of RL for improving LLMs' counterfactual reasoning.

</details>


### [129] [MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models](https://arxiv.org/abs/2510.01549)
*Kevin Zhai,Utsav Singh,Anirudh Thatipelli,Souradip Chakraborty,Anit Kumar Sahu,Furong Huang,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.LG

TL;DR: MIRA is a training-free, inference-time alignment method for diffusion models that uses an image-space KL surrogate with a frozen backbone to prevent reward hacking, improving reward optimization without drifting from the prompt; extended via MIRA-DPO to handle non-differentiable rewards.


<details>
  <summary>Details</summary>
Motivation: Reward-oriented prompts often fail because models optimize rewards that can be gamed (reward hacking); noise-space regularization alone is insufficient to preserve prompt fidelity during alignment. An explicit image-space constraint is needed.

Method: Introduce MIRA: an inference-time, training-free approach that regularizes diffusion sampling with an image-space, score-based KL surrogate using a frozen backbone. Derive a tractable KL approximation via diffusion scores. Evaluate across SDv1.5/SDXL, multiple rewards (Aesthetic, HPSv2, PickScore), and datasets (e.g., Animal-Animal, HPDv2). Extend to non-differentiable rewards with MIRA-DPO.

Result: MIRA achieves over 60% win rate vs strong baselines while preserving prompt adherence; mechanism plots show reward gains with near-zero drift, whereas traditional noise optimization drifts with compute. DNO shows drift as compute increases.

Conclusion: MIRA provides a training-free, inference-time alignment that mitigates reward hacking through an image-space constraint with a frozen backbone, enabling reliable reward optimization without prompt drift. MIRA-DPO extends this to non-differentiable rewards.

Abstract: Diffusion models excel at generating images conditioned on text prompts, but
the resulting images often do not satisfy user-specific criteria measured by
scalar rewards such as Aesthetic Scores. This alignment typically requires
fine-tuning, which is computationally demanding. Recently, inference-time
alignment via noise optimization has emerged as an efficient alternative,
modifying initial input noise to steer the diffusion denoising process towards
generating high-reward images. However, this approach suffers from reward
hacking, where the model produces images that score highly, yet deviate
significantly from the original prompt. We show that noise-space regularization
is insufficient and that preventing reward hacking requires an explicit
image-space constraint. To this end, we propose MIRA (MItigating Reward
hAcking), a training-free, inference-time alignment method. MIRA introduces an
image-space, score-based KL surrogate that regularizes the sampling trajectory
with a frozen backbone, constraining the output distribution so reward can
increase without off-distribution drift (reward hacking). We derive a tractable
approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple
rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,
Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while
preserving prompt adherence; mechanism plots show reward gains with near-zero
drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,
mapping preference optimization to inference time with a frozen backbone,
extending MIRA to non-differentiable rewards without fine-tuning.

</details>


### [130] [Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization](https://arxiv.org/abs/2510.01555)
*Kezhao Liu,Jason Klein Liu,Mingtao Chen,Yiming Liu*

Main category: cs.LG

TL;DR: A unified gradient-based framework reconciles KL-regularized RLHF implementations, showing that the PPO-style k1-in-reward and k2-as-loss are on-policy gradient-equivalent principled reverse KL (RKL) losses; k3-as-loss is a first-order biased approximation; off-policy variants suffer from ignored importance sampling, requiring principled corrections.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning from Human Feedback commonly uses KL regularization, but different papers implement the KL term in conflicting ways (as a reward term vs. as a loss term). The aim is to clarify the principled objective and provide guidance on correct implementation to avoid bias.

Method: Introduce a unified framework that treats the KL term coefficient k_n in two ways: (i) as a detached coefficient multiplying the policy score function (k_n in reward) and (ii) as a direct loss through which gradients propagate (k_n as loss). Prove that the second view can be analyzed via an equivalent gradient coefficient in the first. Under on-policy conditions, prove gradient-equivalence between k2-as-loss and k1-in-reward, and establish k1-as-loss/principled for RKL; show that k3-as-loss is only a first-order, biased approximation. Analyze off-policy implementations and identify biases due to neglected importance sampling, offering principled corrections.

Result: Key findings include (1) k1-in-reward is the principled loss for Reverse KL regularization; (2) under on-policy conditions, k2-as-loss is gradient-equivalent to k1-in-reward, so both implement the same RKL objective; (3) k3-as-loss (as used in GRPO) is a biased approximation of the principled loss; (4) off-policy k_n-as-loss methods are biased unless importance sampling corrections are applied; (5) the work provides a gradient-based rationale for choosing and correctly implementing KL regularization in RLHF.

Conclusion: The paper provides a unified, gradient-based understanding of KL regularization in RLHF, reconciling prior implementation choices and offering principled guidelines and corrections for robust, unbiased training across on-policy and off-policy settings.

Abstract: Reinforcement Learning from Human Feedback (RLHF) leverages a
Kullback-Leibler (KL) divergence loss to stabilize training and prevent
overfitting. However, in methods such as GRPO, its implementation may be guided
by principles from numerical value estimation-a practice that overlooks the
term's functional role as an optimization loss. To analyze this issue, we
establish a unified framework that connects two seemingly distinct
implementation styles: using the mathematical term $k_n$ as a detached
coefficient for the policy's score function ('$k_n$ in reward') or as a direct
loss function through which gradients are propagated ('$k_n$ as loss'). We show
that the latter can always be analyzed via an equivalent gradient coefficient
in the former, unifying the two perspectives. Through this framework, we prove
that the conventional '$k_1$ in reward' (like in PPO) is the principled loss
for Reverse KL (RKL) regularization. We further establish a key finding: under
on-policy conditions, the '$k_2$ as loss' formulation is, in fact,
gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our
work, identifies both as the theoretically sound implementations of the RKL
objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like
in GRPO) is merely a first-order, biased approximation of the principled loss.
Furthermore, we argue that common off-policy implementations of '$k_n$ as loss'
methods are biased due to neglected importance sampling, and we propose a
principled correction. Our findings provide a comprehensive, gradient-based
rationale for choosing and correctly implementing KL regularization, paving the
way for more robust and effective RLHF systems.

</details>


### [131] [Large-Scale Bayesian Causal Discovery with Interventional Data](https://arxiv.org/abs/2510.01562)
*Seong Woo Han,Daniel Duy Vo,Brielin C. Brown*

Main category: cs.LG

TL;DR: IBCD introduces an empirical Bayesian framework for causal discovery from interventional data, modeling total causal effects with a matrix normal likelihood, using spike-and-slab horseshoe priors on edges and data-driven priors for scale-free and Erd≈ës‚ÄìR√©nyi graphs, enabling uncertainty-aware edge inference and scalable structure recovery. It shows superior performance in simulations and robust graph identification in Perturb-seq data (521 genes).


<details>
  <summary>Details</summary>
Motivation: Inferring causal DAGs with interventional perturbations is crucial in genomics, but existing methods struggle with large-scale data and lack uncertainty quantification.

Method: An empirical Bayesian approach that models the likelihood of the total causal effects matrix (approximated by a matrix normal). It uses spike-and-slab horseshoe priors on edges, and learns data-driven weights for scale-free and Erd≈ës‚ÄìR√©nyi graph structures from observational data. Each edge is treated as a latent variable to enable uncertainty-aware inference.

Result: Simulation studies show superior structure recovery compared to baselines. Application to Perturb-seq data with 521 genes demonstrates that edge posterior inclusion probabilities can identify robust, uncertainty-aware graph structures.

Conclusion: IBCD provides a scalable, uncertainty-aware framework for causal discovery with interventional data, capable of robustly identifying graph structures in large-scale genomic settings.

Abstract: Inferring the causal relationships among a set of variables in the form of a
directed acyclic graph (DAG) is an important but notoriously challenging
problem. Recently, advancements in high-throughput genomic perturbation screens
have inspired development of methods that leverage interventional data to
improve model identification. However, existing methods still suffer poor
performance on large-scale tasks and fail to quantify uncertainty. Here, we
propose Interventional Bayesian Causal Discovery (IBCD), an empirical Bayesian
framework for causal discovery with interventional data. Our approach models
the likelihood of the matrix of total causal effects, which can be approximated
by a matrix normal distribution, rather than the full data matrix. We place a
spike-and-slab horseshoe prior on the edges and separately learn data-driven
weights for scale-free and Erd\H{o}s-R\'enyi structures from observational
data, treating each edge as a latent variable to enable uncertainty-aware
inference. Through extensive simulation, we show that IBCD achieves superior
structure recovery compared to existing baselines. We apply IBCD to CRISPR
perturbation (Perturb-seq) data on 521 genes, demonstrating that edge posterior
inclusion probabilities enable identification of robust graph structures.

</details>


### [132] [TetriServe: Efficient DiT Serving for Heterogeneous Image Generation](https://arxiv.org/abs/2510.01565)
*Runyu Lu,Shiqi He,Wenxuan Tan,Shenggui Li,Ruofan Wu,Jeff J. Ma,Ang Chen,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: TetriServe uses step-level sequence parallelism and round-based scheduling to serve Diffusion Transformer (DiT) models under strict SLOs, achieving efficient GPU utilization and up to 32% better SLO attainment without harming image quality.


<details>
  <summary>Details</summary>
Motivation: DiT generation is computationally intensive, and current serving systems with fixed parallelism underutilize GPUs and fail to meet diverse deadlines for heterogeneous workloads, especially at high resolutions.

Method: Introduce step-level sequence parallelism that dynamically adjusts parallel degree per request based on deadlines; implement a round-based scheduling mechanism that discretizes time into rounds, adapts parallelism at the step level, and jointly packs requests to minimize late completions; TetriServe is the end-to-end system implementing this strategy for DiT serving.

Result: Extensive evaluation on state-of-the-art DiT models shows TetriServe achieves up to 32% higher SLO attainment compared to existing solutions, without degrading image quality.

Conclusion: Deadline-aware, dynamic parallelism with round-based scheduling can significantly improve SLO attainment and GPU efficiency for DiT serving, making high-resolution diffusion-based image generation more practical in production.

Abstract: Diffusion Transformer (DiT) models excel at generating highquality images
through iterative denoising steps, but serving them under strict Service Level
Objectives (SLOs) is challenging due to their high computational cost,
particularly at large resolutions. Existing serving systems use fixed degree
sequence parallelism, which is inefficient for heterogeneous workloads with
mixed resolutions and deadlines, leading to poor GPU utilization and low SLO
attainment.
  In this paper, we propose step-level sequence parallelism to dynamically
adjust the parallel degree of individual requests according to their deadlines.
We present TetriServe, a DiT serving system that implements this strategy for
highly efficient image generation. Specifically, TetriServe introduces a novel
round-based scheduling mechanism that improves SLO attainment: (1) discretizing
time into fixed rounds to make deadline-aware scheduling tractable, (2)
adapting parallelism at the step level and minimize GPU hour consumption, and
(3) jointly packing requests to minimize late completions. Extensive evaluation
on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher
SLO attainment compared to existing solutions without degrading image quality.

</details>


### [133] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao,Hongrui Zhang,Junde Xu,Zhou Zhang,Lingdong Shen,Minghao Sun,Ge Liu,Jinbo Xu,Wu-Jun Li,Jinren Ni,Cesar de la Fuente-Nunez,Tianfan Fu,Yejin Choi,Pheng-Ann Heng,Fang Wu*

Main category: cs.LG

TL;DR: RL with protein language models enhances sampling efficiency and uncovers latent sequence-structure-function rules in protein design; gains depend on reward fidelity, policy capacity, and task headroom; prioritize reward modeling/calibration before scaling policy size.


<details>
  <summary>Details</summary>
Motivation: To determine whether reinforcement learning can push PLMs beyond pretraining priors to uncover latent rules governing sequence-structure-function in proteins, and to evaluate this across diverse design domains.

Method: Pair reinforcement learning with protein language models across four design domains (antimicrobial peptides, kinase variants, antibodies, inverse folding) using diverse RL algorithms and model classes; assess sampling efficiency, success rates, and the interaction of task headroom, reward fidelity, and policy capacity; derive practical guidelines.

Result: Across benchmarks, RL consistently boosts success rates and sampling efficiency. Gains follow a three-factor interaction: with accurate rewards, sufficient policy capacity, and task headroom beyond supervised baselines, improvements scale; if rewards are noisy or capacity is limited, gains saturate despite exploration.

Conclusion: RL‚ÄìPLM approaches can meaningfully improve protein design when rewards are well-modeled and policy capacity matches task difficulty; prioritize reward modeling and calibration before scaling policy size, and allocate capacity where marginal gains are largest. A public implementation is provided.

Abstract: Protein language models (PLMs) have advanced computational protein science
through large-scale pretraining and scalable architectures. In parallel,
reinforcement learning (RL) has broadened exploration and enabled precise
multi-objective optimization in protein design. Yet whether RL can push PLMs
beyond their pretraining priors to uncover latent sequence-structure-function
rules remains unclear. We address this by pairing RL with PLMs across four
domains: antimicrobial peptide design, kinase variant optimization, antibody
engineering, and inverse folding. Using diverse RL algorithms and model
classes, we ask if RL improves sampling efficiency and, more importantly, if it
reveals capabilities not captured by supervised learning. Across benchmarks, RL
consistently boosts success rates and sample efficiency. Performance follows a
three-factor interaction: task headroom, reward fidelity, and policy capacity
jointly determine gains. When rewards are accurate and informative, policies
have sufficient capacity, and tasks leave room beyond supervised baselines,
improvements scale; when rewards are noisy or capacity is constrained, gains
saturate despite exploration. This view yields practical guidance for RL in
protein design: prioritize reward modeling and calibration before scaling
policy size, match algorithm and regularization strength to task difficulty,
and allocate capacity where marginal gains are largest. Implementation is
available at https://github.com/chq1155/RL-PLM.

</details>


### [134] [Gradient Shaping Beyond Clipping: A Functional Perspective on Update Magnitude Control](https://arxiv.org/abs/2510.01578)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: SPAMP is a differentiable, per-layer gradient shaping framework that generalizes gradient clipping into adaptive, statistics-driven modulation to improve stability and convergence.


<details>
  <summary>Details</summary>
Motivation: Rigid hard gradient clipping thresholds ignore the evolving distribution of gradients during training; a per-layer, adaptive mechanism is needed to flexibly control update magnitudes.

Method: SPAMP tracks local gradient statistics per layer, dynamically estimates thresholds, and applies power-based transformations to modulate update magnitudes in a differentiable manner. It unifies clipping and warmup as dual mechanisms for controlling the effective update scale Œ∑_t ||g_t||.

Result: Empirical evidence on image and language tasks shows SPAMP improves stability, convergence speed, and robustness compared with existing clipping/warmup methods.

Conclusion: SPAMP offers a principled alternative to rigid heuristics, providing smooth gradient shaping that adapts to gradient dynamics and generalizes clipping across tasks.

Abstract: Gradient clipping is widely used to stabilize deep network training, but its
formulation as a hard, fixed threshold limits flexibility and ignores gradient
distribution dynamics. We propose SPAMP (Statistical Per-layer Adaptive
Modulation and Projection), a unified framework that generalizes clipping into
smooth, per-layer gradient shaping. SPAMP tracks local gradient statistics,
dynamically estimates thresholds, and applies power-based transformations to
modulate update magnitudes in a differentiable manner. This perspective recasts
clipping and warmup as dual mechanisms for controlling the effective update
scale $\eta_t \|g_t\|$, offering a principled alternative to rigid heuristics.
Extensive experiments across image and language tasks demonstrate that SPAMP
improves stability, convergence, and robustness over existing methods.

</details>


### [135] [Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression](https://arxiv.org/abs/2510.01581)
*Joykirat Singh,Justin Chih-Yao Chen,Archiki Prasad,Elias Stengel-Eskin,Akshay Nambi,Mohit Bansal*

Main category: cs.LG

TL;DR: TRAAC is an online post-training RL method that uses self-attention over a long reasoning trajectory to prune redundant steps and estimates problem difficulty to allocate reasoning budget, improving accuracy and reducing reasoning length across math and non-math tasks.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models struggle with balancing reasoning length: underthinking on hard problems leads to errors, while overthinking wastes tokens. There is under-adaptivity where response length does not adapt to task difficulty.

Method: TRAAC (Think Right with Adaptive, Attentive Compression) performs post-training RL using the model‚Äôs self-attention to identify important steps along a long reasoning chain and prune unnecessary ones. It also estimates task difficulty and uses it in rewards to train the model to allocate reasoning budget proportionally to difficulty, achieving adaptive thinking.

Result: TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a 36.8% reduction in reasoning length versus the base model, and a 7.9% accuracy gain with a 29.4% length drop versus the best RL baseline across tasks AIME, AMC, GPQA-D, BBEH. It generalizes to non-math datasets (GPQA-D, BBEH, OptimalThinkingBench) and shows fine-grained adjustments to thinking budget by difficulty.

Conclusion: Adaptive, attention-guided compression paired with task-difficulty calibration yields improved accuracy and efficiency across diverse tasks, demonstrating robust generalization and the value of aligning reasoning length with problem difficulty.

Abstract: Recent thinking models solve complex reasoning tasks by scaling test-time
compute, but this scaling must be allocated in line with task difficulty. On
one hand, short reasoning (underthinking) leads to errors on harder problems
that require extended reasoning steps; but, excessively long reasoning
(overthinking) can be token-inefficient, generating unnecessary steps even
after reaching a correct intermediate solution. We refer to this as
under-adaptivity, where the model fails to modulate its response length
appropriately given problems of varying difficulty. To address under-adaptivity
and strike a balance between under- and overthinking, we propose TRAAC (Think
Right with Adaptive, Attentive Compression), an online post-training RL method
that leverages the model's self-attention over a long reasoning trajectory to
identify important steps and prune redundant ones. TRAAC also estimates
difficulty and incorporates it into training rewards, thereby learning to
allocate reasoning budget commensurate with example difficulty. Our approach
improves accuracy, reduces reasoning steps, and enables adaptive thinking
compared to base models and other RL baselines. Across a variety of tasks
(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute
accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%
compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length
drop compared to the best RL baseline. TRAAC also shows strong generalization:
although our models are trained on math datasets, they show accuracy and
efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,
and OptimalThinkingBench. Our analysis further verifies that TRAAC provides
fine-grained adjustments to thinking budget based on difficulty and that a
combination of task-difficulty calibration and attention-based compression
yields gains across diverse tasks.

</details>


### [136] [Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation](https://arxiv.org/abs/2510.01588)
*Ziming Tang,Chengbin Hou,Tianyu Zhang,Bangxu Tian,Jinbao Wang,Hairong Lv*

Main category: cs.LG

TL;DR: NoRo‚Äîa noise-robust UPDRS prediction framework‚Äîuses contrastive learning on ordered feature bins to generate noise-robust embeddings, improving UPDRS predictions under various noise in at-home PD telemonitoring.


<details>
  <summary>Details</summary>
Motivation: PD telemonitoring at home faces patient-induced inaccuracies, environmental noise, and data transmission losses, leading to higher prediction errors. A robust, accessible UPDRS assessment from speech features is needed.

Method: Group original speech features into ordered bins to form contrastive pairs; train a multilayer perceptron encoder via contrastive learning to generate noise-robust features; concatenate these robust features with the original features as augmented features; feed the augmented features into existing UPDRS prediction models; introduce a customizable noise-injection evaluation module to assess robustness.

Result: NoRo improves the noise robustness of UPDRS prediction across various downstream models under different noisy environments, demonstrating the effectiveness of the proposed contrastive-bin feature learning approach.

Conclusion: The proposed noise-robust feature learning framework effectively enhances UPDRS prediction robustness in noisy at-home telemonitoring settings, with a novel evaluation module to simulate and assess diverse noise conditions.

Abstract: Parkinson's disease (PD) is one of the most common neurodegenerative
disorder. PD telemonitoring emerges as a novel assessment modality enabling
self-administered at-home tests of Unified Parkinson's Disease Rating Scale
(UPDRS) scores, enhancing accessibility for PD patients. However, three types
of noise would occur during measurements: (1) patient-induced measurement
inaccuracies, (2) environmental noise, and (3) data packet loss during
transmission, resulting in higher prediction errors. To address these
challenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,
the original speech features are grouped into ordered bins, based on the
continuous values of a selected feature, to construct contrastive pairs.
Second, the contrastive pairs are employed to train a multilayer perceptron
encoder for generating noise-robust features. Finally, these features are
concatenated with the original features as the augmented features, which are
then fed into the UPDRS prediction models. Notably, we further introduces a
novel evaluation approach with customizable noise injection module, and
extensive experiments show that NoRo can successfully enhance the noise
robustness of UPDRS prediction across various downstream prediction models
under different noisy environments.

</details>


### [137] [Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness](https://arxiv.org/abs/2510.01598)
*Youwei Bao,Shuhan Yang,Hyunsoo Yang*

Main category: cs.LG

TL;DR: Hardware-based STT-MTJ true random bits enable high-throughput, energy-efficient randomness for GAI, improving security against vulnerable PRNGs; scalable and practical for future large models.


<details>
  <summary>Details</summary>
Motivation: Deterministic PRNGs in generative AI create predictable patterns exploitable by attackers; conventional defenses incur high energy and latency overhead, necessitating fast, low-power true randomness.

Method: Use spin-transfer torque MTJ devices to generate true random bits, integrated via a highly parallel FPGA-assisted prototype delivering megabit-per-second TRNG. Evaluate by running in-situ within a GAN trained on CIFAR-10, and assess randomness with NIST tests. Discuss scalability toward >10^6 parallel cells for gigabit-per-second throughput.

Result: In-situ generation passes NIST randomness tests. Integrating hardware RNGs into the GAN reduces insecure outputs by up to 18.6√ó compared to a low-quality RNG baseline. The approach promises nanosecond switching, energy efficiency, and scalability toward >10^6 parallel cells, enabling gigabit-per-second throughput for large-model sampling.

Conclusion: STT-MTJ-based hardware RNGs are practical security components for next-generation GAI systems, offering high-speed, energy-efficient, scalable true randomness that can harden generative models against RNG-related vulnerabilities.

Abstract: Deterministic pseudo random number generators (PRNGs) used in generative
artificial intelligence (GAI) models produce predictable patterns vulnerable to
exploitation by attackers. Conventional defences against the vulnerabilities
often come with significant energy and latency overhead. Here, we embed
hardware-generated true random bits from spin-transfer torque magnetic tunnel
junctions (STT-MTJs) to address the challenges. A highly parallel,
FPGA-assisted prototype computing system delivers megabit-per-second true
random numbers, passing NIST randomness tests after in-situ operations with
minimal overhead. Integrating the hardware random bits into a generative
adversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to
18.6 times compared to the low-quality random number generators (RNG) baseline.
With nanosecond switching speed, high energy efficiency, and established
scalability, our STT-MTJ-based system holds the potential to scale beyond 106
parallel cells, achieving gigabit-per-second throughput suitable for large
language model sampling. This advancement highlights spintronic RNGs as
practical security components for next-generation GAI systems.

</details>


### [138] [Posterior Collapse as a Phase Transition in Variational Autoencoders](https://arxiv.org/abs/2510.01621)
*Zhen Li,Fan Zhang,Zheng Zhang,Yu Chen*

Main category: cs.LG

TL;DR: Posterior collapse in VAEs is identified as a phase transition driven by data structure and hyperparameters; a critical boundary where KL divergence to the prior changes discontinuously.


<details>
  <summary>Details</summary>
Motivation: To reinterpret posterior collapse through statistical physics, clarifying when latent inference is meaningful and how data/model interplay governs trainability.

Method: Stability analysis of the trivial posterior solution; derive conditions for a critical hyperparameter threshold; analyze KL divergence; empirical validation on synthetic and real datasets.

Result: There exists a phase boundary separating meaningful latent inference from collapse; the boundary shows a discontinuity in KL divergence; collapse is an emergent phenomenon rather than mere optimization failure.

Conclusion: Understanding posterior collapse as a phase transition yields insights into representational capacity and training dynamics of VAEs, linking data structure with variational constraints, with implications for model design.

Abstract: We investigate the phenomenon of posterior collapse in variational
autoencoders (VAEs) from the perspective of statistical physics, and reveal
that it constitutes a phase transition governed jointly by data structure and
model hyper-parameters. By analyzing the stability of the trivial solution
associated with posterior collapse, we identify a critical hyper-parameter
threshold. This critical boundary, separating meaningful latent inference from
collapse, is characterized by a discontinuity in the KL divergence between the
approximate posterior and the prior distribution. We validate this critical
behavior on both synthetic and real-world datasets, confirming the existence of
a phase transition. Our results demonstrate that posterior collapse is not
merely an optimization failure, but rather an emerging phase transition arising
from the interplay between data structure and variational constraints. This
perspective offers new insights into the trainability and representational
capacity of deep generative models.

</details>


### [139] [Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead](https://arxiv.org/abs/2510.01624)
*Feiyang Kang,Michael Kuchnik,Karthik Padthe,Marin Vlastelica,Ruoxi Jia,Carole-Jean Wu,Newsha Ardalani*

Main category: cs.LG

TL;DR: High SFT scores do not reliably predict RL post-training gains; generalization loss and Pass@large k are stronger predictors; RL on SFT-improved models can underperform base models; diverse, varied-length data may yield better post-RL outcomes.


<details>
  <summary>Details</summary>
Motivation: Question the common assumption that better supervised fine-tuning (SFT) performance translates into superior post-training RL outcomes, and identify practical predictors for RL success to guide data and budget decisions.

Method: Train hundreds of models (up to 12B params) with SFT and RLVR (GRPO) across multiple families (Llama3, Mistral-Nemo, Qwen3) and seven math benchmarks, with up to 256 repetitions and over $1M GPU-hours. Evaluate correlations between pre/post SFT metrics and RL performance. Compare baselines using different data, lengths, and budgets. Propose alternative metrics (generalization loss on held-out reasoning, Pass@large k) and validate their predictive power.

Result: SFT performance is not reliably predictive of RL gains. Generalization loss and Pass@large k provide substantially better predictive power (up to a 0.5 gain in R^2 and Spearman correlations). In some cases, RL on SFT-boosted models underperforms RL on the base model. Short-only SFT data can be worse for RL, even with the same SFT budget; diverse-length data often yields better RL outcomes. An open-source evaluation tool will be released.

Conclusion: Relying on SFT scores as a proxy for RL success is unreliable. Instead, use generalization loss and Pass@k as robust predictors for RL outcomes, and adopt data strategies that emphasize varied-length, diverse exemplars. The approach and tooling support broad adoption and benchmarking in post-training for reasoning LLMs.

Abstract: In post-training for reasoning Large Language Models (LLMs), the current
state of practice trains LLMs in two independent stages: Supervised Fine-Tuning
(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as
``RL'' below). In this work, we challenge whether high SFT scores translate to
improved performance after RL. We provide extensive counter-examples where this
is not true. We find high SFT scores can be biased toward simpler or more
homogeneous data and are not reliably predictive of subsequent RL gains or
scaled-up post-training effectiveness. In some cases, RL training on models
with improved SFT performance could lead to substantially worse outcome
compared to RL on the base model without SFT. We study alternative metrics and
identify generalization loss on held-out reasoning examples and Pass@large k
performance to provide strong proxies for the RL outcome. We trained hundreds
of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive
evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU
hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple
state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL
performance, prediction based on generalization loss and Pass@large k achieves
substantial higher precision, improving $R^2$ coefficient and Spearman's rank
correlation coefficient by up to 0.5 (2x). This provides strong utility for
broad use cases. For example, in most experiments, we find SFT training on
unique examples for a one epoch underperforms training on half examples for two
epochs, either after SFT or SFT-then-RL; With the same SFT budget, training
only on short examples may lead to better SFT performance, though, it often
leads to worse outcome after RL compared to training on examples with varying
lengths. Evaluation tool will be open-sourced.

</details>


### [140] [Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls](https://arxiv.org/abs/2510.01631)
*Feiyang Kang,Newsha Ardalani,Michael Kuchnik,Youssef Emad,Mostafa Elhoushi,Shubhabrata Sengupta,Shang-Wen Li,Ramya Raghavendra,Ruoxi Jia,Carole-Jean Wu*

Main category: cs.LG

TL;DR: Synthetic data can aid LLM pre-training when mixed with natural data, but pure synthetic data effects vary by type: rephrased data offers conditional benefit and speedups with mixtures; textbook-style synthetic data can harm performance, especially at small budgets; optimal synthetic ratios depend on model size and budget, around ~30% for rephrased data; larger generator size does not guarantee gains; evidence on model collapse is mixed but rephrased data shows no degradation at foreseeable scales.


<details>
  <summary>Details</summary>
Motivation: LLM scaling is data-limited; synthetic data promises a path to expand effective training data without collecting more natural data. The study rigorously evaluates how different synthetic data types and their mixtures with natural data influence pre-training performance at scale.

Method: Large-scale empirical analysis across >1000 LLMs and >100k GPU hours using a unified protocol and scaling laws. Comparisons include natural web data, rephrased synthetic data, textbooks-style synthetic data, and mixtures. Variables include model size, data budget, and synthetic data fraction. Metrics focus on validation loss and downstream performance, with attention to potential model collapse in single-round training.

Result: Pre-training on rephrased synthetic data alone is not faster than using natural web data. A mixture of 1/3 rephrased synthetic and 2/3 natural web achieves 5‚Äì10x speedup in reaching the same validation loss at larger data budgets. Textbook-style synthetic data alone leads to higher downstream loss, especially at small budgets. Best synthetic contribution for rephrased data converges around ~30% of the training data, depending on model size and budget. Larger generator models do not necessarily outperform ~8B-parameter models. Mixed results on model collapse: rephrased data shows no degradation in foreseeable scales, while mixtures of textbook-style generated data exhibit patterns aligned with model collapse.

Conclusion: Synthetic data can offer conditional benefits in LLM pre-training, particularly when using rephrased data in well-chosen mixtures with natural data. The study provides practical guidance on synthetic data ratios and cautions against overreliance on larger generators or on using textbook-style data in isolation. It helps demystify the role of synthetic data in large-scale pre-training.

Abstract: Training data plays a crucial role in Large Language Models (LLM) scaling,
yet high quality data is of limited supply. Synthetic data techniques offer a
potential path toward sidestepping these limitations. We conduct a large-scale
empirical investigation (>1000 LLMs with >100k GPU hours) using a unified
protocol and scaling laws, comparing natural web data, diverse synthetic types
(rephrased text, generated textbooks), and mixtures of natural and synthetic
data. Specifically, we found pre-training on rephrased synthetic data
\textit{alone} is not faster than pre-training on natural web texts; while
pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts
can speed up 5-10x (to reach the same validation loss) at larger data budgets.
Pre-training on textbook-style synthetic data \textit{alone} results in notably
higher loss on many downstream domains especially at small data budgets. "Good"
ratios of synthetic data in training data mixtures depend on the model size and
data budget, empirically converging to ~30% for rephrased synthetic data.
Larger generator models do not necessarily yield better pre-training data than
~8B-param models. These results contribute mixed evidence on "model collapse"
during large-scale single-round (n=1) model training on synthetic
data--training on rephrased synthetic data shows no degradation in performance
in foreseeable scales whereas training on mixtures of textbook-style
pure-generated synthetic data shows patterns predicted by "model collapse". Our
work demystifies synthetic data in pre-training, validates its conditional
benefits, and offers practical guidance.

</details>


### [141] [CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning](https://arxiv.org/abs/2510.01634)
*Ryan Y. Lin,Siddhartha Ojha,Nicholas Bai*

Main category: cs.LG

TL;DR: The Curvature-Adaptive Transformer (CAT) dynamically routes tokens among three geometry-specific attention branches (Euclidean, hyperbolic, spherical) via a differentiable gate, enabling per-token geometric specialization. It outperforms fixed-geometry baselines on knowledge graph completion with minimal overhead, demonstrating the value of a mixture-of-geometry approach.


<details>
  <summary>Details</summary>
Motivation: Transformers implicitly rely on Euclidean geometry, which limits performance on data with non-Euclidean structure. While existing hyperbolic/spherical extensions capture certain patterns, they fix geometry a priori. A model that adaptively selects geometry per token can better handle mixed geometric properties in data.

Method: CAT introduces a per-token routing mechanism that selects among three geometric attention branches (Euclidean, hyperbolic, spherical) using a lightweight differentiable gate. Each branch uses operations tailored to its manifold; the routing network yields interpretable curvature preferences. The approach results in a mixture-of-geometry architecture with minimal parameter overhead.

Result: On knowledge graph completion benchmarks FB15k-237 and WN18RR, CAT achieves about 10% gains in MRR and Hits@10 over fixed-geometry baselines, with only ~5% more parameters and comparable inference time, indicating effective geometric adaptation for relational reasoning.

Conclusion: Dynamic, per-token geometry adaptation enables a scalable, interpretable mixture-of-geometry Transformer, outperforming single-geometry models and potentially extending benefits to language, vision, and multimodal tasks.

Abstract: Transformers achieve strong performance across diverse domains but implicitly
assume Euclidean geometry in their attention mechanisms, limiting their
effectiveness on data with non-Euclidean structure. While recent extensions to
hyperbolic and spherical spaces show promise for hierarchical and cyclical
patterns, respectively, they require committing to a single geometry a priori,
reducing flexibility when data exhibits mixed geometric properties. We
introduce the Curvature-Adaptive Transformer (CAT), a novel architecture that
dynamically learns per-token routing across three geometric attention branches
through a lightweight, differentiable gating mechanism. Unlike fixed-geometry
approaches, CAT enables adaptive geometric specialization, routing tokens to
the appropriate curvature based on their local relational structure. The
routing network provides interpretable curvature preferences while each branch
employs geometry-specific operations optimized for its respective manifold. On
knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves
approximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines
with minimal overhead (5% parameter increase, comparable inference time). These
results demonstrate that learned geometric adaptation outperforms any single
fixed geometry for complex relational reasoning, establishing CAT as a scalable
and interpretable foundation for mixture-of-geometry architectures across
language, vision, and multimodal domains.

</details>


### [142] [Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking](https://arxiv.org/abs/2510.01637)
*Liyan Xie,Muhammad Siddeek,Mohamed Seif,Andrea J. Goldsmith,Mengdi Wang*

Main category: cs.LG

TL;DR: Introduces a combinatorial watermarking framework to detect and localize post-generation edits on watermarked LLM outputs, using vocabulary partitioning, deterministic patterns, a global watermark statistic, and lightweight local statistics; proposes Type-I error rate and detection accuracy as evaluation metrics and shows strong localization across edit scenarios.


<details>
  <summary>Details</summary>
Motivation: In real-world use, LLM outputs may be edited after generation or spoofed; detecting and localizing such edits on watermarked text is critical for provenance and authenticity.

Method: Partition vocabulary into disjoint subsets; embed watermark via a deterministic combinatorial pattern during generation; use a global statistic for watermark detection; develop lightweight local statistics to flag edits; evaluate with metrics Type-I error rate and detection accuracy.

Result: Empirical evaluation on open-source LLMs across diverse editing scenarios shows strong performance in edit localization; the framework effectively detects and localizes post-generation edits.

Conclusion: A combinatorial watermark with global and local statistics is effective for detecting and localizing edits to watermarked LLM outputs, with robust evaluation metrics for edit detection and localization.

Abstract: Watermarking has become a key technique for proprietary language models,
enabling the distinction between AI-generated and human-written text. However,
in many real-world scenarios, LLM-generated content may undergo post-generation
edits, such as human revisions or even spoofing attacks, making it critical to
detect and localize such modifications. In this work, we introduce a new task:
detecting post-generation edits locally made to watermarked LLM outputs. To
this end, we propose a combinatorial pattern-based watermarking framework,
which partitions the vocabulary into disjoint subsets and embeds the watermark
by enforcing a deterministic combinatorial pattern over these subsets during
generation. We accompany the combinatorial watermark with a global statistic
that can be used to detect the watermark. Furthermore, we design lightweight
local statistics to flag and localize potential edits. We introduce two
task-specific evaluation metrics, Type-I error rate and detection accuracy, and
evaluate our method on open-source LLMs across a variety of editing scenarios,
demonstrating strong empirical performance in edit localization.

</details>


### [143] [Support Basis: Fast Attention Beyond Bounded Entries](https://arxiv.org/abs/2510.01643)
*Maryam Aliakbarpour,Vladimir Braverman,Junze Yin,Haochen Zhang*

Main category: cs.LG

TL;DR: Proposes support-basis decomposition to achieve sub-quadratic softmax attention by splitting large and small entries based on sub-Gaussian behavior, enabling exact computation on sparse parts and polynomial approximation on dense parts; extends to multi-threshold settings and provides theory linking polynomial attention with sketching.


<details>
  <summary>Details</summary>
Motivation: The quadratic cost of softmax attention limits scaling of LLMs. Prior sub-quadratic methods (e.g., bounded-entry approaches) are often invalid in practice, motivating a framework that works beyond such assumptions.

Method: Empirically show sub-Gaussian behavior in query/key entries. Decompose attention into support (sparse, exactly computed) and basis (dense, approximated with polynomials). Prove sub-quadratic runtime. Extend to a multi-threshold scheme that removes distributional assumptions. Provide theoretical justification for polynomial attention using sketching, aligning with ICML 2024 work.

Result: Sub-quadratic runtime guaranteed with rigorous analysis. The approach handles non-bounded entries, with exact processing of sparse components and polynomial approximations of dense components. Extends to multi-threshold settings that relax distribution assumptions. The paper also offers the first theoretical grounding for approximating softmax attention via a combination of polynomial attentions built with sketching.

Conclusion: Support-basis decomposition broadens the scope of efficient attention beyond bounded-entry restrictions, delivering practical algorithms with strong theoretical guarantees and connecting polynomial-attention ideas to established sketching techniques.

Abstract: The quadratic complexity of softmax attention remains a central bottleneck in
scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a
sub-quadratic attention approximation algorithm, but it works only under the
restrictive bounded-entry assumption. Since this assumption rarely holds in
practice, its applicability to modern LLMs is limited.
  In this paper, we introduce support-basis decomposition, a new framework for
efficient attention approximation beyond bounded entries. We empirically
demonstrate that the entries of the query and key matrices exhibit sub-Gaussian
behavior. Our approach uses this property to split large and small entries,
enabling exact computation on sparse components and polynomial approximation on
dense components. We establish rigorous theoretical guarantees, proving a
sub-quadratic runtime, and extend the method to a multi-threshold setting that
eliminates all distributional assumptions. Furthermore, we provide the first
theoretical justification for the empirical success of polynomial attention
[Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be
closely approximated by a combination of multiple polynomial attentions with
sketching.

</details>


### [144] [Source-Free Cross-Domain Continual Learning](https://arxiv.org/abs/2510.01649)
*Muhammad Tanzil Furqon,Mahardhika Pratama,Igor ≈†krjanc,Lin Liu,Habibullah Habibullah,Kutluyil Dogancay*

Main category: cs.LG

TL;DR: Proposes REFEREE, a rehearsal-free, source-free cross-domain continual learning framework that pairs a source-pretrained model with a vision-language model, using frequency-aware prompts to produce robust low-frequency augmentations, uncertainty-weighted pseudo-labels, and kernel-LDA classification to combat forgetting, achieving strong results without source data.


<details>
  <summary>Details</summary>
Motivation: Privacy constraints and practical deployment scenarios often forbid access to labeled source-domain data for continual learning. Existing cross-domain methods assume a source domain available for training, which is unrealistic in many settings. The paper aims to enable effective cross-domain continual learning without source samples, handling domain shift, noisy pseudo labels, and catastrophic forgetting.

Method: Leverage synergy between a source-pretrained model and a large-scale vision-language model. Introduce a frequency-aware prompting scheme that emphasizes low-frequency components and suppresses high-frequency ones to produce frequency-aware augmented samples robust to noisy pseudo labels. Apply an uncertainty-aware weighting strategy where the mean and covariance of predictions are weighted by uncertainty to mitigate noisy pseudo labels. Address catastrophic forgetting by using kernel linear discriminant analysis (KLDA) with a frozen backbone and classification via linear discriminant analysis guided by a random kernel method.

Result: Numerical studies show REFEREE outperforms prior methods that have access to source-domain data by significant margins, despite not using source samples.

Conclusion: REFEREE effectively tackles source-free cross-domain continual learning by combining a source-pretrained model, a vision-language model, frequency-aware prompting, uncertainty-weighted pseudo-labels, and KLDA-based classification, offering strong performance gains under privacy constraints.

Abstract: Although existing cross-domain continual learning approaches successfully
address many streaming tasks having domain shifts, they call for a fully
labeled source domain hindering their feasibility in the privacy constrained
environments. This paper goes one step ahead with the problem of source-free
cross-domain continual learning where the use of source-domain samples are
completely prohibited. We propose the idea of rehearsal-free frequency-aware
dynamic prompt collaborations (REFEREE) to cope with the absence of labeled
source-domain samples in realm of cross-domain continual learning. REFEREE is
built upon a synergy between a source-pre-trained model and a large-scale
vision-language model, thus overcoming the problem of sub-optimal
generalizations when relying only on a source pre-trained model. The domain
shift problem between the source domain and the target domain is handled by a
frequency-aware prompting technique encouraging low-frequency components while
suppressing high-frequency components. This strategy generates frequency-aware
augmented samples, robust against noisy pseudo labels. The noisy pseudo-label
problem is further addressed with the uncertainty-aware weighting strategy
where the mean and covariance matrix are weighted by prediction uncertainties,
thus mitigating the adverse effects of the noisy pseudo label. Besides, the
issue of catastrophic forgetting (CF) is overcome by kernel linear discriminant
analysis (KLDA) where the backbone network is frozen while the classification
is performed using the linear discriminant analysis approach guided by the
random kernel method. Our rigorous numerical studies confirm the advantage of
our approach where it beats prior arts having access to source domain samples
with significant margins.

</details>


### [145] [The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM](https://arxiv.org/abs/2510.01650)
*Kwanhee Lee,Hyeondo Jang,Dongyeop Lee,Dan Alistarh,Namhoon Lee*

Main category: cs.LG

TL;DR: Elsa, an ADMM-based constrained pruning method, achieves up to 90% sparsity in LLMs with high fidelity, outperforming prior surrogates; Elsa_L extends to 27B with convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Current pruning methods rely on surrogate objectives and struggle to surpass ~60% sparsity without accuracy loss, hindering practical efficiency gains for LLMs.

Method: Formulates pruning as a constrained optimization problem solved with ADMM, identifies surrogate objective limitations, introduces Elsa and Elsa_L (quantized) with convergence guarantees.

Result: Attains up to 90% sparsity across models; 7.8x lower perplexity than best prior method on LLaMA-2-7B at 90% sparsity; Elsa_L scales to 27B with convergence guarantees.

Conclusion: Breaks through the sparsity barrier, advancing LLM sparsity; suggests substantial opportunities for further exploration in this direction.

Abstract: Neural network pruning is a promising technique to mitigate the excessive
computational and memory requirements of large language models (LLMs). Despite
its promise, however, progress in this area has diminished, as conventional
methods are seemingly unable to surpass moderate sparsity levels (50-60%)
without severely degrading model accuracy. This work breaks through the current
impasse, presenting a principled and effective method called $\texttt{Elsa}$,
which achieves extreme sparsity levels of up to 90% while retaining high model
fidelity. This is done by identifying several limitations in current practice,
all of which can be traced back to their reliance on a surrogate objective
formulation. $\texttt{Elsa}$ tackles this issue directly and effectively via
standard and well-established constrained optimization techniques based on
ADMM. Our extensive experiments across a wide range of models and scales show
that $\texttt{Elsa}$ achieves substantial improvements over existing methods;
e.g., it achieves 7.8$\times$ less perplexity than the best existing method on
LLaMA-2-7B at 90% sparsity. Furthermore, we present
$\texttt{Elsa}_{\text{-L}}$, a quantized variant that scales to extremely large
models (27B), and establish its theoretical convergence guarantees. These
results highlight meaningful progress in advancing the frontier of LLM
sparsity, while promising that significant opportunities for further
advancement may remain in directions that have so far attracted limited
exploration.

</details>


### [146] [Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning](https://arxiv.org/abs/2510.01656)
*Jiashun Liu,Johan Obando-Ceron,Han Lu,Yancheng He,Weixun Wang,Wenbo Su,Bo Zheng,Pablo Samuel Castro,Aaron Courville,Ling Pan*

Main category: cs.LG

TL;DR: AsyPPO introduces lightweight, diverse mini-critics for RL with LLMs, restoring the critic role efficiently, and uses inter-critic uncertainty to refine updates, achieving solid gains over PPO with minimal data.


<details>
  <summary>Details</summary>
Motivation: RL4LLMs rely on average baselines; traditional value functions are costly and error-prone with sparse rewards and long horizons. An architectural approach to restore critics could improve stability and performance at scale.

Method: Train multiple small critics on disjoint prompt shards; use inter-critic uncertainty to (a) mask advantages where critics agree and gradients are small, and (b) filter high-divergence states from entropy regularization; evaluate on open-source data with ~5k samples; compare to PPO/GRPO.

Result: AsyPPO yields stability and performance gains across benchmarks, e.g., >6% on Qwen3-4b-Base and ~3% on Qwen3-8b-Base/14b-Base over classic PPO, without extra tricks.

Conclusion: Architectural design enabling scalable, efficient RL for LLMs by distributing critics and leveraging their uncertainty can outperform traditional PPO baselines; importance of critic-enabled architectures.

Abstract: Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing
them with average advantage baselines. This shift is largely pragmatic:
conventional value functions are computationally expensive to train at LLM
scale and often fail under sparse rewards and long reasoning horizons. We
revisit this bottleneck from an architectural perspective and introduce
Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable
framework that restores the critics role while remaining efficient in
large-model settings. AsyPPO employs a set of lightweight mini-critics, each
trained on disjoint prompt shards. This design encourages diversity while
preserving calibration, reducing value-estimation bias. Beyond robust
estimation, AsyPPO leverages inter-critic uncertainty to refine the policy
update: (i) masking advantages in states where critics agree and gradients add
little learning signal, and (ii) filtering high-divergence states from entropy
regularization, suppressing spurious exploration. After training on open-source
data with only 5,000 samples, AsyPPO consistently improves learning stability
and performance across multiple benchmarks over strong baselines, such as GRPO,
achieving performance gains of more than six percent on Qwen3-4b-Base and about
three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without
additional tricks. These results highlight the importance of architectural
innovations for scalable, efficient algorithms.

</details>


### [147] [Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing](https://arxiv.org/abs/2510.01658)
*Amin Jalali,Milad Soltany,Michael Greenspan,Ali Etemad*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose TimeHUT, a novel method for learning time-series representations
by hierarchical uniformity-tolerance balancing of contrastive representations.
Our method uses two distinct losses to learn strong representations with the
aim of striking an effective balance between uniformity and tolerance in the
embedding space. First, TimeHUT uses a hierarchical setup to learn both
instance-wise and temporal information from input time-series. Next, we
integrate a temperature scheduler within the vanilla contrastive loss to
balance the uniformity and tolerance characteristics of the embeddings.
Additionally, a hierarchical angular margin loss enforces instance-wise and
temporal contrast losses, creating geometric margins between positive and
negative pairs of temporal sequences. This approach improves the coherence of
positive pairs and their separation from the negatives, enhancing the capture
of temporal dependencies within a time-series sample. We evaluate our approach
on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and
multivariate classification, as well as Yahoo and KPI datasets for anomaly
detection. The results demonstrate that TimeHUT outperforms prior methods by
considerable margins on classification, while obtaining competitive results for
anomaly detection. Finally, detailed sensitivity and ablation studies are
performed to evaluate different components and hyperparameters of our method.

</details>


### [148] [Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value](https://arxiv.org/abs/2510.01663)
*Wangxuan Fan,Ching Wang,Siqi Li,Nan Liu*

Main category: cs.LG

TL;DR: ShapKAN uses Shapley-value-based pruning for Kolmogorov‚ÄìArnold Networks to achieve shift-invariant, interpretable network compression without sacrificing practical performance.


<details>
  <summary>Details</summary>
Motivation: To recover and preserve interpretable feature‚Äìoutcome relationships in KANs while achieving compression; magnitude-based pruning is unreliable due to sensitivity to input coordinate shifts.

Method: Compute Shapley value contributions of each node to the network's outputs, select low-contribution nodes for pruning while maintaining shift-invariance, and evaluate on synthetic and real datasets to compare against magnitude-based pruning.

Result: ShapKAN preserves actual node importance rankings across input parameterizations and enables effective compression, maintaining interpretability advantages of KANs in resource-constrained settings.

Conclusion: ShapKAN provides a robust, shift-invariant, attribution-based pruning framework for KANs that enhances interpretability and enables deployment in environments with limited resources.

Abstract: For many real-world applications, understanding feature-outcome relationships
is as crucial as achieving high predictive accuracy. While traditional neural
networks excel at prediction, their black-box nature obscures underlying
functional relationships. Kolmogorov--Arnold Networks (KANs) address this by
employing learnable spline-based activation functions on edges, enabling
recovery of symbolic representations while maintaining competitive performance.
However, KAN's architecture presents unique challenges for network pruning.
Conventional magnitude-based methods become unreliable due to sensitivity to
input coordinate shifts. We propose \textbf{ShapKAN}, a pruning framework using
Shapley value attribution to assess node importance in a shift-invariant
manner. Unlike magnitude-based approaches, ShapKAN quantifies each node's
actual contribution, ensuring consistent importance rankings regardless of
input parameterization. Extensive experiments on synthetic and real-world
datasets demonstrate that ShapKAN preserves true node importance while enabling
effective network compression. Our approach improves KAN's interpretability
advantages, facilitating deployment in resource-constrained environments.

</details>


### [149] [Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.01677)
*Han Wu,Yanming Sun,Yunhe Yang,Derek F. Wong*

Main category: cs.LG

TL;DR: Introduces Adaptive Gated Fusion Network (AGFN) for multimodal sentiment analysis that uses dual gates based on information entropy and modality importance to adaptively weight modalities, improving robustness to noisy/missing/conflicting cues and enhancing discrimination of subtle emotions.


<details>
  <summary>Details</summary>
Motivation: Multimodal sentiment analysis often suffers when modalities have varying quality (noise, missing data, semantic conflicts). Simple fusion fails to account for these variations, leading to suboptimal performance.

Method: AGFN employs a dual gate fusion mechanism that uses information entropy and modality importance to adaptively weight features. Fusion occurs after unimodal encoding and cross-modal interaction, attenuating noise and prioritizing informative cues in a simple yet efficient architecture.

Result: On CMU-MOSI and CMU-MOSEI, AGFN significantly outperforms strong baselines in accuracy, demonstrating robust performance and improved ability to discern subtle emotions. Visualization indicates enhanced generalization by learning a broader feature distribution and reducing the correlation between feature location and prediction error.

Conclusion: AGFN provides a simple, efficient, and robust approach to adaptive multimodal fusion, improving accuracy and generalization in MSA by modulating modality contributions according to entropy and importance.

Abstract: Multimodal sentiment analysis (MSA) leverages information fusion from diverse
modalities (e.g., text, audio, visual) to enhance sentiment prediction.
However, simple fusion techniques often fail to account for variations in
modality quality, such as those that are noisy, missing, or semantically
conflicting. This oversight leads to suboptimal performance, especially in
discerning subtle emotional nuances. To mitigate this limitation, we introduce
a simple yet efficient \textbf{A}daptive \textbf{G}ated \textbf{F}usion
\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion
mechanism based on information entropy and modality importance. This mechanism
mitigates the influence of noisy modalities and prioritizes informative cues
following unimodal encoding and cross-modal interaction. Experiments on
CMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong
baselines in accuracy, effectively discerning subtle emotions with robust
performance. Visualization analysis of feature representations demonstrates
that AGFN enhances generalization by learning from a broader feature
distribution, achieved by reducing the correlation between feature location and
prediction error, thereby decreasing reliance on specific locations and
creating more robust multimodal feature representations.

</details>


### [150] [PASTA: A Unified Framework for Offline Assortment Learning](https://arxiv.org/abs/2510.01693)
*Juncheng Dong,Weibin Mo,Zhengling Qi,Cong Shi,Ethan X. Fang,Vahid Tarokh*

Main category: cs.LG

TL;DR: PASTA framework for offline data-driven assortment optimization using pessimism; provides finite-sample regret bounds for MNL and nested logit; minimax-optimal; only requires that the offline data contain an optimal assortment.


<details>
  <summary>Details</summary>
Motivation: In offline data-driven settings, firms lack prior knowledge of the choice model and face data coverage issues. The goal is to produce provably good assortments from historical data with limited coverage.

Method: Introduce Pessimistic Assortment Optimization (PASTA) that applies a pessimism principle to optimize expected revenue under general choice models. Requires only that the offline data distribution contains an optimal assortment, not full coverage of all assortments. Theoretical results include finite-sample regret bounds for MNL and nested logit models and a minimax regret lower bound.

Result: Finite-sample regret bounds for offline assortment optimization across several common choice models (MNL and nested logit) are established. A minimax lower bound is derived, showing PASTA is minimax optimal in sample and model complexity. Numerical experiments show the method outperforms baselines.

Conclusion: PASTA provides a robust, data-driven solution for offline assortment optimization with provable guarantees; it relaxes data-coverage requirements and achieves minimax-optimal performance under multiple choice models; supported by empirical results.

Abstract: We study a broad class of assortment optimization problems in an offline and
data-driven setting. In such problems, a firm lacks prior knowledge of the
underlying choice model, and aims to determine an optimal assortment based on
historical customer choice data. The combinatorial nature of assortment
optimization often results in insufficient data coverage, posing a significant
challenge in designing provably effective solutions. To address this, we
introduce a novel Pessimistic Assortment Optimization (PASTA) framework that
leverages the principle of pessimism to achieve optimal expected revenue under
general choice models. Notably, PASTA requires only that the offline data
distribution contains an optimal assortment, rather than providing the full
coverage of all feasible assortments. Theoretically, we establish the first
finite-sample regret bounds for offline assortment optimization across several
widely used choice models, including the multinomial logit and nested logit
models. Additionally, we derive a minimax regret lower bound, proving that
PASTA is minimax optimal in terms of sample and model complexity. Numerical
experiments further demonstrate that our method outperforms existing baseline
approaches.

</details>


### [151] [Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport](https://arxiv.org/abs/2510.01706)
*Shaan Shah,Meenakshi Khosla*

Main category: cs.LG

TL;DR: HOT provides a unified hierarchical optimal transport framework to jointly infer soft, globally-consistent layer couplings and neuron-level transport plans, yielding a single network-wide alignment score and handling depth mismatches.


<details>
  <summary>Details</summary>
Motivation: Standard representational similarity methods suffer from asymmetric, one-to-one layer mappings, lack a global score, and fail when networks differ in depth; these issues stem from ignoring global activation structure.

Method: Formulate hierarchical optimal transport that jointly infers soft layer-to-layer couplings and neuron-level transport plans under marginal constraints, allowing source neurons to distribute mass across multiple target layers, yielding a global alignment score.

Result: Across vision models, large language models, and human visual cortex data, HOT matches or surpasses pairwise methods, reveals smooth hierarchical correspondences (early layers map to early layers; deeper layers preserve relative order), and resolves depth mismatches via mass distribution.

Conclusion: HOT enables richer, more interpretable network comparisons, robust to architectural and depth differences, derived from global optimization rather than greedy layer-wise mapping.

Abstract: Standard representational similarity methods align each layer of a network to
its best match in another independently, producing asymmetric results, lacking
a global alignment score, and struggling with networks of different depths.
These limitations arise from ignoring global activation structure and
restricting mappings to rigid one-to-one layer correspondences. We propose
Hierarchical Optimal Transport (HOT), a unified framework that jointly infers
soft, globally consistent layer-to-layer couplings and neuron-level transport
plans. HOT allows source neurons to distribute mass across multiple target
layers while minimizing total transport cost under marginal constraints. This
yields both a single alignment score for the entire network comparison and a
soft transport plan that naturally handles depth mismatches through mass
distribution. We evaluate HOT on vision models, large language models, and
human visual cortex recordings. Across all domains, HOT matches or surpasses
standard pairwise matching in alignment quality. Moreover, it reveals smooth,
fine-grained hierarchical correspondences: early layers map to early layers,
deeper layers maintain relative positions, and depth mismatches are resolved by
distributing representations across multiple layers. These structured patterns
emerge naturally from global optimization without being imposed, yet are absent
in greedy layer-wise methods. HOT thus enables richer, more interpretable
comparisons between representations, particularly when networks differ in
architecture or depth.

</details>


### [152] [ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning](https://arxiv.org/abs/2510.01712)
*Aidan Acquah,Shing Chan,Aiden Doherty*

Main category: cs.LG

TL;DR: Self-supervised ActiNet with HMM smoothing outperforms RF+HMM for wrist-worn HAR in CAPTURE-24 data, with higher macro F1 (0.82) and Cohen's kappa (0.86) and consistent across age/sex subgroups; supports using ActiNet for epidemiological activity labeling.


<details>
  <summary>Details</summary>
Motivation: To determine whether combining self-supervised learning with hidden Markov model smoothing can meaningfully improve activity-label classification and affect estimated activity intensity distributions in large-scale epidemiological studies using wrist accelerometer data.

Method: Train ActiNet (18-layer modified ResNet-V2) in a self-supervised fashion on CAPTURE-24 data (n=151), followed by HMM smoothing. Compare against a baseline Random Forest + HMM using identical data and evaluation (5-fold stratified group cross-validation). Subgroup analyses by age and sex.

Result: ActiNet achieved mean macro F1 of 0.82 and Cohen's kappa of 0.86, outperforming RF+HMM (0.77 F1, 0.81 kappa). Findings were consistent across age/sex subgroups.

Conclusion: ActiNet + HMM is a promising approach for reliable activity intensity labeling in epidemiological studies, enabling more accurate extraction of activity patterns from wrist accelerometers and potentially improving downstream health association analyses.

Abstract: The use of reliable and accurate human activity recognition (HAR) models on
passively collected wrist-accelerometer data is essential in large-scale
epidemiological studies that investigate the association between physical
activity and health outcomes. While the use of self-supervised learning has
generated considerable excitement in improving HAR, it remains unknown the
extent to which these models, coupled with hidden Markov models (HMMs), would
make a tangible improvement to classification performance, and the effect this
may have on the predicted daily activity intensity compositions. Using 151
CAPTURE-24 participants' data, we trained the ActiNet model, a self-supervised,
18-layer, modified ResNet-V2 model, followed by hidden Markov model (HMM)
smoothing to classify labels of activity intensity. The performance of this
model, evaluated using 5-fold stratified group cross-validation, was then
compared to a baseline random forest (RF) + HMM, established in existing
literature. Differences in performance and classification outputs were compared
with different subgroups of age and sex within the Capture-24 population. The
ActiNet model was able to distinguish labels of activity intensity with a mean
macro F1 score of 0.82, and mean Cohen's kappa score of 0.86. This exceeded the
performance of the RF + HMM, trained and validated on the same dataset, with
mean scores of 0.77 and 0.81, respectively. These findings were consistent
across subgroups of age and sex. These findings encourage the use of ActiNet
for the extraction of activity intensity labels from wrist-accelerometer data
in future epidemiological studies.

</details>


### [153] [Latency-aware Multimodal Federated Learning over UAV Networks](https://arxiv.org/abs/2510.01717)
*Shaba Shaon,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: A UAV-enabled federated multimodal learning framework minimizes system latency via joint optimization (scheduling, power, trajectory, resource allocation) with a BCD-SCA algorithm, backed by convergence analysis, outperforming baselines in latency and training accuracy.


<details>
  <summary>Details</summary>
Motivation: To achieve fast, accurate federated learning in UAV networks by leveraging multimodal sensing while addressing the nonconvex, combinatorial nature of the optimization problem.

Method: Propose an iterative optimization framework combining Block Coordinate Descent (BCD) and Successive Convex Approximation (SCA) to jointly optimize UAV sensing scheduling, power control, trajectory planning, and base station resource management for FML; provide theoretical convergence analysis under a non-convex loss.

Result: The framework yields high-quality approximate solutions and demonstrates reduced system latency and improved model training performance compared with existing approaches across various data settings in numerical experiments.

Conclusion: UAV-assisted federated multimodal learning with coordinated resource management and multimodal sensing is effective for latency-constrained FL; the proposed BCD-SCA algorithm provides convergence guarantees and practical performance gains.

Abstract: This paper investigates federated multimodal learning (FML) assisted by
unmanned aerial vehicles (UAVs) with a focus on minimizing system latency and
providing convergence analysis. In this framework, UAVs are distributed
throughout the network to collect data, participate in model training, and
collaborate with a base station (BS) to build a global model. By utilizing
multimodal sensing, the UAVs overcome the limitations of unimodal systems,
enhancing model accuracy, generalization, and offering a more comprehensive
understanding of the environment. The primary objective is to optimize FML
system latency in UAV networks by jointly addressing UAV sensing scheduling,
power control, trajectory planning, resource allocation, and BS resource
management. To address the computational complexity of our latency minimization
problem, we propose an efficient iterative optimization algorithm combining
block coordinate descent and successive convex approximation techniques, which
provides high-quality approximate solutions. We also present a theoretical
convergence analysis for the UAV-assisted FML framework under a non-convex loss
function. Numerical experiments demonstrate that our FML framework outperforms
existing approaches in terms of system latency and model training performance
under different data settings.

</details>


### [154] [Accelerating Attention with Basis Decomposition](https://arxiv.org/abs/2510.01718)
*Jialin Zhao*

Main category: cs.LG

TL;DR: BD Attention provides a lossless, exact reformulation of attention using Basis Decomposition, enabling architecture-agnostic speedups with negligible impact on model performance.


<details>
  <summary>Details</summary>
Motivation: Attention computations are a bottleneck in large language and vision-language models; the goal is a provably exact acceleration that does not degrade outputs or require retraining.

Method: Basis Decomposition restructures multi-head projections into a compact form via a simple matrix identity, preserving exact outputs. This yields a lossless reformulation of attention and is architecture-agnostic. Offline preparation is required.

Result: On DeepSeek-V2-Lite (16B, FP16): ~4 seconds offline preparation; on modern GPUs: ~32% faster key/value projections and ~25% smaller weights, with end-to-end perplexity increases of 0.02% (FP16) or 0.0004% (FP32).

Conclusion: BD Attention is the first theoretically exact method for lossless attention acceleration, complementary to existing engineering optimizations and architecture-agnostic. The authors provide code.

Abstract: Attention is a core operation in large language models (LLMs) and
vision-language models (VLMs). We present BD Attention (BDA), the first
lossless algorithmic reformulation of attention. BDA is enabled by a simple
matrix identity from Basis Decomposition (BD), which restructures multi-head
projections into a compact form while preserving exact outputs. Unlike
I/O-aware system optimizations such as FlashAttention, BDA provides a
mathematically guaranteed acceleration that is architecture-agnostic. On
DeepSeek-V2-Lite (16B, FP16), BDA requires only 4s of offline preparation with
no retraining required and, on modern GPUs, achieves 32% faster key/value
projections and 25% smaller weights, while increasing end-to-end perplexity
(PPL) by just 0.02% (FP16) or 0.0004% (FP32), a negligible effect on model
performance. These results position BDA as the first theoretically exact method
for lossless attention acceleration that is complementary to existing
engineering-level optimizations. Our code is available at
https://github.com/abcbdf/basis-decomposition-official.

</details>


### [155] [Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation](https://arxiv.org/abs/2510.01721)
*Saptarshi Mandal,Yashaswini Murthy,R. Srikant*

Main category: cs.LG

TL;DR: First robust TD learning with linear function approximation under distributional robustness; model-free, no generative access; two-time-scale updates with a target network; achieves tilde O(1/epsilon^2) sample complexity for epsilon-accurate value estimates; extends to robust Q-learning.


<details>
  <summary>Details</summary>
Motivation: Robust reinforcement learning under model uncertainty lacks non-asymptotic guarantees when using function approximation. Existing results are limited to tabular MDPs or require restrictive discount-factor assumptions. This work aims to provide the first robust TD learning guarantee with linear function approximation under distributional uncertainty (TV and Wasserstein-l distance).

Method: Propose a two-time-scale stochastic approximation algorithm with an outer-loop target-network update for robust TD learning with linear function approximation. Uncertainty is modeled via total-variation distance and Wasserstein-l distance sets. The algorithm is model-free and does not require generative access. The analysis yields convergence and non-asymptotic guarantees, with the outer loop stabilizing the target. Extends naturally to robust Q-learning.

Result: Establishes an O~(1/Œµ^2) sample complexity to obtain an Œµ-accurate value estimate under robust TD with linear function approximation for TV and Wasserstein distance uncertainty sets. The algorithm is model-free and does not require access to the true MDP; shows robustness to model deviations within the specified sets.

Conclusion: This work closes a key gap between practical robustness seen in RL applications and theoretical guarantees for robust RL with function approximation. The ideas generalize to robust Q-learning, indicating a viable path for provable robustness in modern RL settings.

Abstract: Distributionally robust reinforcement learning (DRRL) focuses on designing
policies that achieve good performance under model uncertainties. In
particular, we are interested in maximizing the worst-case long-term discounted
reward, where the data for RL comes from a nominal model while the deployed
environment can deviate from the nominal model within a prescribed uncertainty
set. Existing convergence guarantees for robust temporal-difference (TD)
learning for policy evaluation are limited to tabular MDPs or are dependent on
restrictive discount-factor assumptions when function approximation is used. We
present the first robust TD learning with linear function approximation, where
robustness is measured with respect to the total-variation distance and
Wasserstein-l distance uncertainty set. Additionally, our algorithm is both
model-free and does not require generative access to the MDP. Our algorithm
combines a two-time-scale stochastic-approximation update with an outer-loop
target-network update. We establish an $\tilde{O}(1/\epsilon^2)$ sample
complexity to obtain an $\epsilon$-accurate value estimate. Our results close a
key gap between the empirical success of robust RL algorithms and the
non-asymptotic guarantees enjoyed by their non-robust counterparts. The key
ideas in the paper also extend in a relatively straightforward fashion to
robust Q-learning with function approximation.

</details>


### [156] [Workplace Location Choice Model based on Deep Neural Network](https://arxiv.org/abs/2510.01723)
*Tanay Rastogi,Anders Karlstr√∂m*

Main category: cs.LG

TL;DR: DNNs can model workplace location choices and may outperform traditional DCMs in some aspects, but each method has strengths depending on distance and attribute emphasis; model choice should align with analysis goals.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional discrete choice models in mirroring individual decision processes in workplace location, and to evaluate whether DNNs provide a robust alternative.

Method: Empirical comparison between DNNs and traditional DCMs on workplace location data, examining impact of job opportunities, individual attributes, and geographic distance; assess short vs long distance performance.

Result: Compared DNNs and DCMs; DNNs outperform DCMs in certain aspects and show strong generalization; both reproduce job opportunities' effects; DCMs better explain short-distance and individual-attribute effects; DNNs perform well for longer distances.

Conclusion: Model choice should be tailored to application; DNNs are a promising robust alternative, with strengths for complex patterns and longer-distance decisions, while DCMs remain advantageous for short distances and attribute-focused analyses.

Abstract: Discrete choice models (DCMs) have long been used to analyze workplace
location decisions, but they face challenges in accurately mirroring individual
decision-making processes. This paper presents a deep neural network (DNN)
method for modeling workplace location choices, which aims to better understand
complex decision patterns and provides better results than traditional discrete
choice models (DCMs). The study demonstrates that DNNs show significant
potential as a robust alternative to DCMs in this domain. While both models
effectively replicate the impact of job opportunities on workplace location
choices, the DNN outperforms the DCM in certain aspects. However, the DCM
better aligns with data when assessing the influence of individual attributes
on workplace distance. Notably, DCMs excel at shorter distances, while DNNs
perform comparably to both data and DCMs for longer distances. These findings
underscore the importance of selecting the appropriate model based on specific
application requirements in workplace location choice analysis.

</details>


### [157] [Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD](https://arxiv.org/abs/2510.01744)
*Lea Demelius,Dominik Kowald,Simone Kopeinik,Roman Kern,Andreas Tr√ºgler*

Main category: cs.LG

TL;DR: Hyperparameter tuning directly on DP models does not reliably fix fairness gaps in DPSGD; it can improve utility-fairness trade-offs compared to re-using non-private hyperparameters, but adds privacy leakage and is not a robust fairness remedy.


<details>
  <summary>Details</summary>
Motivation: To assess whether optimizing hyperparameters for DP models generalizes to fairness outcomes, and whether this can mitigate DPSGD's disparate impact across metrics.

Method: Empirical analysis across a wide range of hyperparameters and multiple performance/fairness metrics, comparing the disparate impact of DPSGD on each metric; evaluating under-DPSGD hyperparameter optimization vs re-using non-private hyperparameters; extending evaluation to DPSGD-Global-Adapt variant.

Result: Disparate impact varies by metric; improvement in one metric does not guarantee improvements in others. Directly optimizing hyperparameters on DP models does not reliably reduce DPSGD's disparate impact, but can improve utility-fairness trade-offs relative to re-using non-private hyperparameters. Hyperparameter tuning incurs extra privacy leakage. DPSGD-Global-Adapt may not be robust to hyperparameter choice.

Conclusion: Hyperparameter tuning under differential privacy is not a robust solution for fairness; careful balancing of privacy, utility, and fairness is required, and alternative approaches beyond hyperparameter optimization are needed.

Abstract: Differential privacy (DP) is a prominent method for protecting information
about individuals during data analysis. Training neural networks with
differentially private stochastic gradient descent (DPSGD) influences the
model's learning dynamics and, consequently, its output. This can affect the
model's performance and fairness. While the majority of studies on the topic
report a negative impact on fairness, it has recently been suggested that
fairness levels comparable to non-private models can be achieved by optimizing
hyperparameters for performance directly on differentially private models
(rather than re-using hyperparameters from non-private models, as is common
practice). In this work, we analyze the generalizability of this claim by 1)
comparing the disparate impact of DPSGD on different performance metrics, and
2) analyzing it over a wide range of hyperparameter settings. We highlight that
a disparate impact on one metric does not necessarily imply a disparate impact
on another. Most importantly, we show that while optimizing hyperparameters
directly on differentially private models does not mitigate the disparate
impact of DPSGD reliably, it can still lead to improved utility-fairness
trade-offs compared to re-using hyperparameters from non-private models. We
stress, however, that any form of hyperparameter tuning entails additional
privacy leakage, calling for careful considerations of how to balance privacy,
utility and fairness. Finally, we extend our analyses to DPSGD-Global-Adapt, a
variant of DPSGD designed to mitigate the disparate impact on accuracy, and
conclude that this alternative may not be a robust solution with respect to
hyperparameter choice.

</details>


### [158] [Learning Regularization Functionals for Inverse Problems: A Comparative Study](https://arxiv.org/abs/2510.01755)
*Johannes Hertrich,Hok Shing Wong,Alexander Denker,Stanislas Ducotterd,Zhenghan Fang,Markus Haltmeier,≈Ωeljko Kereta,Erich Kobler,Oscar Leong,Mohammad Sadegh Salehi,Carola-Bibiane Sch√∂nlieb,Johannes Schwab,Zakhar Shumaylov,Jeremias Sulam,German Sh√¢ma Wache,Martin Zach,Yasi Zhang,Matthias J. Ehrhardt,Sebastian Neumayer*

Main category: cs.LG

TL;DR: A unified framework that collects and harmonizes code for learned regularization methods in imaging inverse problems, enabling systematic comparison and practical guidelines.


<details>
  <summary>Details</summary>
Motivation: There are many learned regularization frameworks with diverse architectures and training strategies, but direct comparison is hindered by non-modular, disparate implementations. A unified view is needed to fairly assess strengths, limitations, and future potential.

Method: Aggregate and unify the available code into a common framework; provide concise descriptions of each method; perform systematic comparative analysis; derive practical guidelines for practitioners.

Result: Enables fair, systematic comparisons across methods; highlights design trade-offs, strengths, and limitations; yields insights into future potential and actionable guidelines.

Conclusion: The unified view serves as a valuable resource for researchers and practitioners to evaluate, compare, and advance learned regularization approaches for imaging inverse problems.

Abstract: In recent years, a variety of learned regularization frameworks for solving
inverse problems in imaging have emerged. These offer flexible modeling
together with mathematical insights. The proposed methods differ in their
architectural design and training strategies, making direct comparison
challenging due to non-modular implementations. We address this gap by
collecting and unifying the available code into a common framework. This
unified view allows us to systematically compare the approaches and highlight
their strengths and limitations, providing valuable insights into their future
potential. We also provide concise descriptions of each method, complemented by
practical guidelines.

</details>


### [159] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: Unsupervised Dynamic Feature Selection (DFS) enhances latent representations by per-instance removal of noisy or redundant features, boosting generalization in clustering and image generation with minimal extra cost.


<details>
  <summary>Details</summary>
Motivation: Latent representations are prone to contamination by noise and task-irrelevant features, which harms performance and generalization. An unsupervised DFS approach avoids reliance on labels and can generalize across datasets and domains.

Method: An unsupervised framework that, for each instance, identifies and removes misleading or redundant information in images so that only the most relevant features contribute to the latent space, thereby improving the quality of latent representations without using labeled data.

Result: Experiments on image datasets show significant improvements in generalization across tasks such as clustering and image generation, with only a slight increase in computational cost.

Conclusion: Unsupervised DFS is effective for enhancing latent representations and generalization without labeled data, and is broadly applicable across domains.

Abstract: Latent representations are critical for the performance and robustness of
machine learning models, as they encode the essential features of data in a
compact and informative manner. However, in vision tasks, these representations
are often affected by noisy or irrelevant features, which can degrade the
model's performance and generalization capabilities. This paper presents a
novel approach for enhancing latent representations using unsupervised Dynamic
Feature Selection (DFS). For each instance, the proposed method identifies and
removes misleading or redundant information in images, ensuring that only the
most relevant features contribute to the latent space. By leveraging an
unsupervised framework, our approach avoids reliance on labeled data, making it
broadly applicable across various domains and datasets. Experiments conducted
on image datasets demonstrate that models equipped with unsupervised DFS
achieve significant improvements in generalization performance across various
tasks, including clustering and image generation, while incurring a minimal
increase in the computational cost.

</details>


### [160] [Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX](https://arxiv.org/abs/2510.01764)
*Waris Radji,Thomas Michel,Hector Piteau*

Main category: cs.LG

TL;DR: GPU-accelerated CHIP-8 arcade RL environments in JAX (Octax): fast, faithful, scalable benchmarks for RL on GPUs.


<details>
  <summary>Details</summary>
Motivation: Need diverse, challenging RL benchmarks that are tractable and scalable. Classic CPU-bound emulators limit large-scale experimentation; a GPU-friendly, end-to-end JAX solution with faithful game mechanics is desirable.

Method: Re-implement CHIP-8 arcade games as image-based environments in JAX (end-to-end on GPU); modular design; supports puzzle/action/strategy genres; scalable across multiple games; compare performance to CPU emulators; enable generation of new environments (potentially via LLMs).

Result: Orders-of-magnitude speedups over CPU emulators while maintaining perfect fidelity to original mechanics; successful training of RL agents across multiple games; improved training speed and scalability; modular architecture enables easy extension with new games or LLM-generated environments.

Conclusion: Octax delivers a high-performance, faithful, GPU-based suite for RL research that enables large-scale experimentation and easy extensibility for additional games or novel environments.

Abstract: Reinforcement learning (RL) research requires diverse, challenging
environments that are both tractable and scalable. While modern video games may
offer rich dynamics, they are computationally expensive and poorly suited for
large-scale experimentation due to their CPU-bound execution. We introduce
Octax, a high-performance suite of classic arcade game environments implemented
in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely
adopted as a benchmark in RL research. Octax provides the JAX community with a
long-awaited end-to-end GPU alternative to the Atari benchmark, offering
image-based environments, spanning puzzle, action, and strategy genres, all
executable at massive scale on modern GPUs. Our JAX-based implementation
achieves orders-of-magnitude speedups over traditional CPU emulators while
maintaining perfect fidelity to the original game mechanics. We demonstrate
Octax's capabilities by training RL agents across multiple games, showing
significant improvements in training speed and scalability compared to existing
solutions. The environment's modular design enables researchers to easily
extend the suite with new games or generate novel environments using large
language models, making it an ideal platform for large-scale RL
experimentation.

</details>


### [161] [Neural non-canonical Hamiltonian dynamics for long-time simulations](https://arxiv.org/abs/2510.01788)
*Cl√©mentine Court√®s,Emmanuel Franck,Michael Kraus,Laurent Navoret,L√©opold Tr√©mant*

Main category: cs.LG

TL;DR: This work studies learning non-canonical Hamiltonian dynamics with structure-preserving methods, identifies gauge-related instability when coupling potential-based models with degenerate variational integrators, and proposes two training strategies (direct vector-field learning and learning a time-discrete scheme) validated on complex dynamics such as gyrokinetic guiding-center dynamics.


<details>
  <summary>Details</summary>
Motivation: Long-term predictions require preservation of geometric structure in both the learned model and the numerical integrator. Prior work focused on either a potential-based architecture or degenerate variational integrators; combining them introduces instabilities due to gauge dependencies.

Method: Propose two training strategies: (i) directly learn the vector field, or (ii) learn the time-discrete dynamics through the numerical scheme (train the scheme itself). Analyze the gauge-dependency issue and evaluate the approaches on numerical test cases.

Result: Identifies gauge-dependency‚Äìdriven instability when mixing potential-based models with degenerate variational integrators. Proposes two training strategies to address this problem. Numerical experiments across several test problems show the methods can learn and represent complex physical dynamics, including guiding-center dynamics from gyrokinetic plasma physics.

Conclusion: Two practical training strategies are viable to stabilize learning and preserve geometric structure in non-canonical Hamiltonian dynamics, enabling more reliable long-term simulations; demonstrated on test cases including guiding-center dynamics.

Abstract: This work focuses on learning non-canonical Hamiltonian dynamics from data,
where long-term predictions require the preservation of structure both in the
learned model and in numerical schemes. Previous research focused on either
facet, respectively with a potential-based architecture and with degenerate
variational integrators, but new issues arise when combining both. In
experiments, the learnt model is sometimes numerically unstable due to the
gauge dependency of the scheme, rendering long-time simulations impossible. In
this paper, we identify this problem and propose two different training
strategies to address it, either by directly learning the vector field or by
learning a time-discrete dynamics through the scheme. Several numerical test
cases assess the ability of the methods to learn complex physical dynamics,
like the guiding center from gyrokinetic plasma physics.

</details>


### [162] [Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation](https://arxiv.org/abs/2510.01793)
*Adil Koeken,Alexander Ziller,Moritz Knolle,Daniel Rueckert*

Main category: cs.LG

TL;DR: Post-hoc privacy filters for synthetic chest X-ray datasets show limited specificity and consistency, with high sensitivity only for real images and poor detection of near-duplicates from training data, implying insufficient protection of patient privacy.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate whether post-hoc privacy filtering can reliably protect patient privacy in privacy-preserving synthetic data pipelines for medical imaging.

Method: A systematic evaluation of a filtering pipeline applied to chest X-ray synthesis, assessing the filters' ability to distinguish real images from synthetic or near-duplicates of training data, and measuring specificity, consistency, and sensitivity.

Result: Filters exhibit limited specificity and consistency; high sensitivity only for real images but fail to reliably detect near-duplicates generated from training data, leading to potential privacy leakage and a false sense of security.

Conclusion: Current post-hoc privacy filters are insufficient for robust privacy protection in synthetic medical image generation; substantial advances in filter design are needed before deployment in sensitive clinical applications.

Abstract: The generation of privacy-preserving synthetic datasets is a promising avenue
for overcoming data scarcity in medical AI research. Post-hoc privacy filtering
techniques, designed to remove samples containing personally identifiable
information, have recently been proposed as a solution. However, their
effectiveness remains largely unverified. This work presents a rigorous
evaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary
to claims from the original publications, our results demonstrate that current
filters exhibit limited specificity and consistency, achieving high sensitivity
only for real images while failing to reliably detect near-duplicates generated
from training data. These results demonstrate a critical limitation of post-hoc
filtering: rather than effectively safeguarding patient privacy, these methods
may provide a false sense of security while leaving unacceptable levels of
patient information exposed. We conclude that substantial advances in filter
design are needed before these methods can be confidently deployed in sensitive
applications.

</details>


### [163] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: Introduces G^2RPO (Granular-GRPO), a reinforcement-learning forflow models framework with granular, multi-scale reward evaluation and singular stochastic sampling to tightly align per-step rewards with SDE perturbations, yielding strong improvements over base GRPO methods.


<details>
  <summary>Details</summary>
Motivation: Existing online RL for diffusion/flow models uses stochastic sampling to explore denoising directions but suffers from sparse/narrow rewards and misalignment between reward signals and the noise injected during SDE steps, leading to inefficient exploration and suboptimal alignment with human preferences.

Method: Proposes two innovations: (1) Singular Stochastic Sampling to maintain a strong correlation between injected noise and the observed reward for each SDE perturbation, enabling faithful per-step reward signaling; (2) Multi-Granularity Advantage Integration to aggregate advantages computed at multiple diffusion scales, reducing fixed-granularity bias and yielding a richer evaluation of sampling directions. Both are integrated into the Granular-GRPO framework.

Result: Empirical evaluations on multiple reward models (in-domain and out-of-domain) show that G^2RPO significantly outperforms existing flow-based GRPO baselines, demonstrating improved robustness and reward-alignment.

Conclusion: G^2RPO provides precise, comprehensive reward assessments for sampling directions in online RL for flow models and effectively mitigates reward sparsity and perturbation-reward misalignment through singular stochastic sampling and multi-granularity integration, leading to stronger downstream performance.

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow
models has recently emerged as a promising approach for aligning generative
models with human preferences. Stochastic sampling via Stochastic Differential
Equations (SDE) is employed during the denoising process to generate diverse
denoising directions for RL exploration. While existing methods effectively
explore potential high-value samples, they suffer from sub-optimal preference
alignment due to sparse and narrow reward signals. To address these challenges,
we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves
precise and comprehensive reward assessments of sampling directions in
reinforcement learning of flow models. Specifically, a Singular Stochastic
Sampling strategy is introduced to support step-wise stochastic exploration
while enforcing a high correlation between the reward and the injected noise,
thereby facilitating a faithful reward for each SDE perturbation. Concurrently,
to eliminate the bias inherent in fixed-granularity denoising, we introduce a
Multi-Granularity Advantage Integration module that aggregates advantages
computed at multiple diffusion scales, producing a more comprehensive and
robust evaluation of the sampling directions. Experiments conducted on various
reward models, including both in-domain and out-of-domain evaluations,
demonstrate that our $\text{G}^2$RPO significantly outperforms existing
flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [164] [Rethinking the shape convention of an MLP](https://arxiv.org/abs/2510.01796)
*Meng-Hsi Chen,Yu-Ang Lee,Feng-Ting Liao,Da-shan Shiu*

Main category: cs.LG

TL;DR: Hourglass MLPs invert the standard skip-connection pattern by placing skip connections in expanded dimensions while bottleneck residuals operate in narrow spaces, enabling refinement in high-dimensional spaces with fixed random input projections. They outperform conventional MLP designs on image-generative tasks, with deeper, wider-skipped Hourglass configurations favored as parameter budgets grow.


<details>
  <summary>Details</summary>
Motivation: Challenge the traditional narrow-wide-narrow MLP layout to exploit high-dimensional spaces for incremental refinement while maintaining efficiency; explore how skip-connection placement affects learning dynamics and Pareto efficiency; propose a design with potential applicability to other residual architectures such as Transformers.

Method: Introduce Hourglass MLP blocks with skip connections at expanded widths and residual paths through narrow bottlenecks. Start with a fixed random projection lifting inputs to expanded dimensions, enabling efficient training and inference. Evaluate on generative image tasks across datasets, performing systematic architectural search to map performance-parameter Pareto frontiers and compare to conventional MLP designs.

Result: Hourglass MLPs consistently achieve superior Pareto frontiers over conventional designs. With larger parameter budgets, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks, revealing a distinct scaling pattern from traditional MLPs. The approach suggests notable efficiency gains and broader applicability beyond MLPs (e.g., Transformers and other residual architectures).

Conclusion: Skip-connection placement materially affects performance and efficiency. Hourglass (wide-narrow-wide) designs warrant reconsideration in modern architectures, offering a path to improved Pareto efficiency and potential cross-domain applicability.

Abstract: Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow
design where skip connections operate at the input/output dimensions while
processing occurs in expanded hidden spaces. We challenge this convention by
proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections
operate at expanded dimensions while residual computation flows through narrow
bottlenecks. This inversion leverages higher-dimensional spaces for incremental
refinement while maintaining computational efficiency through parameter-matched
designs. Implementing Hourglass MLPs requires an initial projection to lift
input signals to expanded dimensions. We propose that this projection can
remain fixed at random initialization throughout training, enabling efficient
training and inference implementations. We evaluate both architectures on
generative tasks over popular image datasets, characterizing
performance-parameter Pareto frontiers through systematic architectural search.
Results show that Hourglass architectures consistently achieve superior Pareto
frontiers compared to conventional designs. As parameter budgets increase,
optimal Hourglass configurations favor deeper networks with wider skip
connections and narrower bottlenecks-a scaling pattern distinct from
conventional MLPs. Our findings suggest reconsidering skip connection placement
in modern architectures, with potential applications extending to Transformers
and other residual networks.

</details>


### [165] [Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction](https://arxiv.org/abs/2510.01817)
*Adam Filipek*

Main category: cs.LG

TL;DR: Sparse Query Attention (SQA) reduces Transformer FLOPs by cutting the number of Query heads, yielding up to 3x throughput improvements on long sequences with minimal quality loss, and complements existing MHA optimizations (MQA/GQA).


<details>
  <summary>Details</summary>
Motivation: The standard Multi-Head Attention (MHA) has quadratic complexity in sequence length, creating a bottleneck for long-context modeling and training. While MQA and GQA alleviate memory bandwidth during autoregressive inference by sharing Key/Value projections, they do not reduce the core FLOPs for attention score computation.

Method: Introduce Sparse Query Attention that reduces the number of Query heads rather than Key/Value heads. Provide theoretical foundation, a mathematical formulation, and a family of architectural variants. Assess performance on very long sequences (32k‚Äì200k tokens).

Result: Achieves significant throughput improvements of up to 3x in computation-bound scenarios such as pre-training, fine-tuning, and encoder-based tasks, with only minimal degradation in model quality in preliminary small-scale experiments.

Conclusion: SQA offers a complementary optimization path to existing attention efficiency methods, reducing compute by decreasing query-head count. Discovered during Reactive Transformer development, it has potential to enable more efficient and scalable transformers and warrants further exploration.

Abstract: The Transformer architecture, underpinned by the Multi-Head Attention (MHA)
mechanism, has become the de facto standard for state-of-the-art models in
artificial intelligence. However, the quadratic computational complexity of MHA
with respect to sequence length presents a significant barrier to scaling,
particularly for applications involving long contexts. Prevailing solutions,
such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have
effectively addressed the memory bandwidth bottleneck that dominates
autoregressive inference latency by sharing Key and Value projections. While
highly successful, these methods do not reduce the fundamental number of
floating-point operations (FLOPs) required for the attention score computation,
which remains a critical bottleneck for training and full-sequence processing.
This paper introduces Sparse Query Attention (SQA), a novel attention
architecture that pursues an alternative and complementary optimization path.
Instead of reducing Key/Value heads, SQA reduces the number of Query heads.
This architectural modification directly decreases the computational complexity
of the attention mechanism by a factor proportional to the reduction in query
heads, thereby lowering the overall FLOPs. This work presents the theoretical
foundation of SQA, its mathematical formulation, and a family of architectural
variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate
that SQA can achieve significant throughput improvements of up to 3x in
computation-bound scenarios such as model pre-training, fine-tuning, and
encoder-based tasks, with only a minimal impact on model quality in preliminary
smallscale experiments. SQA was discovered serendipitously during the
development of the upcoming Reactive Transformer architecture, suggesting its
potential as a powerful tool for building more efficient and scalable models

</details>


### [166] [Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning](https://arxiv.org/abs/2510.01824)
*Olivier Goudet,Quentin Suire,Adrien Go√´ffon,Fr√©d√©ric Saubion,Sylvain Lamprier*

Main category: cs.LG

TL;DR: An order-invariant, multivariate autoregressive model for black-box combinatorial optimization is trained with random generation orders (information-preserving dropout) to promote order-invariance and diversity; it uses GRPO for stable, scale-invariant policy gradients, achieving strong performance and robustness across benchmarks.


<details>
  <summary>Details</summary>
Motivation: EDAs rely on explicit dependency graphs, which are costly and may fail to capture complex interactions. There is a need for an approach that is invariant to variable ordering, scalable, and sample-efficient for large-scale combinatorial optimization.

Method: Train a multivariate autoregressive generative model without a fixed order by sampling random generation orders during training (order randomization). This acts as information-preserving dropout. Adapt Generalized Reinforcement Policy Optimization (GRPO) to provide stable policy-gradient updates using scale-invariant advantages.

Result: The method frequently achieves the best performance across a wide range of benchmark algorithms and problem instances of varying sizes and consistently avoids catastrophic failures.

Conclusion: Order-invariant RL with order randomization improves search-space diversity, focuses on relevant variable dependencies, and yields robust, sample-efficient optimization performance compared to traditional EDAs.

Abstract: We introduce an order-invariant reinforcement learning framework for
black-box combinatorial optimization. Classical estimation-of-distribution
algorithms (EDAs) often rely on learning explicit variable dependency graphs,
which can be costly and fail to capture complex interactions efficiently. In
contrast, we parameterize a multivariate autoregressive generative model
trained without a fixed variable ordering. By sampling random generation orders
during training - a form of information-preserving dropout - the model is
encouraged to be invariant to variable order, promoting search-space diversity
and shaping the model to focus on the most relevant variable dependencies,
improving sample efficiency. We adapt Generalized Reinforcement Policy
Optimization (GRPO) to this setting, providing stable policy-gradient updates
from scale-invariant advantages. Across a wide range of benchmark algorithms
and problem instances of varying sizes, our method frequently achieves the best
performance and consistently avoids catastrophic failures.

</details>


### [167] [Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets](https://arxiv.org/abs/2510.01842)
*Yannis Belkhiter,Seshu Tirupathi,Giulio Zizzo,Sachin Sharma,John D. Kelleher*

Main category: cs.LG

TL;DR: A pre-hoc AutoML approach using traditional models and LLM agents to intelligently pre-select models, reducing the search space and computational cost while still identifying the best model for each dataset.


<details>
  <summary>Details</summary>
Motivation: AutoML often relies on exhaustive post-hoc hyperparameter searches, which are computationally expensive. A pre-hoc prediction strategy can cut search space by leveraging dataset descriptions and statistics, potentially via LLMs, to choose a smaller set of candidate models before training.

Method: Combine traditional machine learning models with LLM-driven agents to interpret dataset descriptions and statistical metadata, generating a reduced candidate model subset. Apply this pre-selection to the AWS AutoGluon portfolio (175 tabular classification datasets from OpenML), then evaluate whether the selected subset can still yield the best-performing model with significantly less computation.

Result: The approach significantly reduces computational overhead associated with AutoML searches while still selecting the best model for each dataset, demonstrating the viability of pre-hoc model selection for AutoML workflows.

Conclusion: Pre-hoc predictions using traditional models and LLM-guided guidance can effectively prune the AutoML search space without sacrificing model quality. This shift can lead to faster AutoML workflows and lower resource usage, with potential for broader integration into AutoML pipelines.

Abstract: The field of AutoML has made remarkable progress in post-hoc model selection,
with libraries capable of automatically identifying the most performing models
for a given dataset. Nevertheless, these methods often rely on exhaustive
hyperparameter searches, where methods automatically train and test different
types of models on the target dataset. Contrastingly, pre-hoc prediction
emerges as a promising alternative, capable of bypassing exhaustive search
through intelligent pre-selection of models. Despite its potential, pre-hoc
prediction remains under-explored in the literature. This paper explores the
intersection of AutoML and pre-hoc model selection by leveraging traditional
models and Large Language Model (LLM) agents to reduce the search space of
AutoML libraries. By relying on dataset descriptions and statistical
information, we reduce the AutoML search space. Our methodology is applied to
the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark
containing 175 tabular classification datasets available on OpenML. The
proposed approach offers a shift in AutoML workflows, significantly reducing
computational overhead, while still selecting the best model for the given
dataset.

</details>


### [168] [Learning Representations Through Contrastive Neural Model Checking](https://arxiv.org/abs/2510.01853)
*Vladimir Krsmanovic,Matthias Cosler,Mohamed Ghanem,Bernd Finkbeiner*

Main category: cs.LG

TL;DR: CNML uses model checking as supervision to learn aligned representations of systems and specifications via a self-supervised contrastive objective, improving retrieval tasks and transferring to downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Formal verification benefits from learned representations, but representation learning is underexplored in this domain. The paper aims to connect model checking with representation learning to enable better cross-modal understanding between specifications and systems.

Method: Jointly embed logical specifications and system models into a shared latent space using a self-supervised contrastive objective, optimizing to align correct spec-system pairs while distinguishing negatives; evaluated on industry-inspired retrieval tasks in cross-modal and intra-modal settings.

Result: CNML outperforms both algorithmic and neural baselines on industry-inspired retrieval tasks in cross-modal and intra-modal settings; learned representations transfer to downstream tasks and generalize to more complex formulas.

Conclusion: Model checking can serve as an effective objective for learning representations for formal languages, enabling better alignment between specifications and systems and benefiting downstream tasks.

Abstract: Model checking is a key technique for verifying safety-critical systems
against formal specifications, where recent applications of deep learning have
shown promise. However, while ubiquitous for vision and language domains,
representation learning remains underexplored in formal verification. We
introduce Contrastive Neural Model Checking (CNML), a novel method that
leverages the model checking task as a guiding signal for learning aligned
representations. CNML jointly embeds logical specifications and systems into a
shared latent space through a self-supervised contrastive objective. On
industry-inspired retrieval tasks, CNML considerably outperforms both
algorithmic and neural baselines in cross-modal and intra-modal settings.We
further show that the learned representations effectively transfer to
downstream tasks and generalize to more complex formulas. These findings
demonstrate that model checking can serve as an objective for learning
representations for formal languages.

</details>


### [169] [Test-Time Anchoring for Discrete Diffusion Posterior Sampling](https://arxiv.org/abs/2510.02291)
*Litu Rout,Andreas Lugmayr,Yasamin Jafarian,Srivatsan Varadharajan,Constantine Caramanis,Sanjay Shakkottai,Ira Kemelmacher-Shlizerman*

Main category: cs.LG

TL;DR: Anchored Posterior Sampling (APS) for masked discrete diffusion models enables training-free posterior sampling for image recovery from noisy measurements, achieving state-of-the-art performance among discrete diffusion samplers and enabling training-free stylization and text-guided editing.


<details>
  <summary>Details</summary>
Motivation: Discrete diffusion offers a unified, faster, and training-free Bayesian framework for discrete data; however existing posterior samplers face derivative-free guidance sparsity, continuous relaxations limits, and high-dimensional sampling challenges.

Method: Two innovations: (1) quantized expectation to provide gradient-like guidance directly in discrete embedding space; (2) anchored remasking to adaptively decode and stabilize sampling. Built on masked diffusion foundation models to enable posterior sampling without retraining.

Result: Empirically achieves state-of-the-art performance among discrete diffusion samplers on linear and nonlinear inverse problems across standard benchmarks; demonstrates utility in training-free stylization and text-guided editing.

Conclusion: APS advances training-free posterior sampling for discrete diffusion models, enabling effective image recovery and downstream tasks without task-specific retraining, with broad applicability.

Abstract: We study the problem of posterior sampling using pretrained discrete
diffusion foundation models, aiming to recover images from noisy measurements
without retraining task-specific models. While diffusion models have achieved
remarkable success in generative modeling, most advances rely on continuous
Gaussian diffusion. In contrast, discrete diffusion offers a unified framework
for jointly modeling categorical data such as text and images. Beyond
unification, discrete diffusion provides faster inference, finer control, and
principled training-free Bayesian inference, making it particularly well-suited
for posterior sampling. However, existing approaches to discrete diffusion
posterior sampling face severe challenges: derivative-free guidance yields
sparse signals, continuous relaxations limit applicability, and split Gibbs
samplers suffer from the curse of dimensionality. To overcome these
limitations, we introduce Anchored Posterior Sampling (APS) for masked
diffusion foundation models, built on two key innovations -- quantized
expectation for gradient-like guidance in discrete embedding space, and
anchored remasking for adaptive decoding. Our approach achieves
state-of-the-art performance among discrete diffusion samplers across linear
and nonlinear inverse problems on the standard benchmarks. We further
demonstrate the benefits of our approach in training-free stylization and
text-guided editing.

</details>


### [170] [Explicit Discovery of Nonlinear Symmetries from Dynamic Data](https://arxiv.org/abs/2510.01855)
*Lexiang Hu,Yikang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: LieNLSD is the first method to identify nonlinear Lie symmetry generators (count and explicit expressions) for differential equations by framing the prolongation criterion as a linear system in a coefficient matrix and solving via SVD, enabling explicit nonlinear symmetry discovery and improved neural PDE modeling.


<details>
  <summary>Details</summary>
Motivation: Symmetry is central to equivariant networks and the discovery of governing equations, but existing methods largely handle only linear symmetries and fail to provide the Lie algebra subspace, hindering nonlinear symmetry discovery.

Method: Specify a library of infinitesimal group actions (functions). Use the prolonged infinitesimal criterion, which is linear in the coefficient matrix. Build a linear system from data (central differences and the Jacobian of a trained NN) and solve it with SVD to recover the coefficient matrix, the number of generators, and their explicit expressions.

Result: LieNLSD can determine the number of infinitesimal generators with nonlinear terms and their expressions. It shows qualitative advantages over existing methods on tasks like top quark tagging and various dynamic systems, and improves the long rollout accuracy of neural PDE solvers by over 20%; it also supports guiding data augmentation. Code and data are publicly available.

Conclusion: LieNLSD provides a first-principles, scalable framework to discover nonlinear Lie algebra generators and their explicit forms, enabling more expressive symmetry discovery and aiding symmetry-aware learning and physics-guided modeling.

Abstract: Symmetry is widely applied in problems such as the design of equivariant
networks and the discovery of governing equations, but in complex scenarios, it
is not known in advance. Most previous symmetry discovery methods are limited
to linear symmetries, and recent attempts to discover nonlinear symmetries fail
to explicitly get the Lie algebra subspace. In this paper, we propose LieNLSD,
which is, to our knowledge, the first method capable of determining the number
of infinitesimal generators with nonlinear terms and their explicit
expressions. We specify a function library for the infinitesimal group action
and aim to solve for its coefficient matrix, proving that its prolongation
formula for differential equations, which governs dynamic data, is also linear
with respect to the coefficient matrix. By substituting the central differences
of the data and the Jacobian matrix of the trained neural network into the
infinitesimal criterion, we get a system of linear equations for the
coefficient matrix, which can then be solved using SVD. On top quark tagging
and a series of dynamic systems, LieNLSD shows qualitative advantages over
existing methods and improves the long rollout accuracy of neural PDE solvers
by over 20% while applying to guide data augmentation. Code and data are
available at https://github.com/hulx2002/LieNLSD.

</details>


### [171] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: CNS identifies concept-related neurons in diffusion models to enable incremental personalization with minimal forgetting and fusion-free operation, achieving state-of-the-art results with fewer parameter updates.


<details>
  <summary>Details</summary>
Motivation: Real-world incremental updates of diffusion models are desirable but computationally expensive; there is a need for methods to personalize models while mitigating catastrophic forgetting and preserving zero-shot generation.

Method: Identify and fine-tune a small set of concept neurons in diffusion models that encode the target concepts, integrate them in an incremental continual-learning framework, and jointly preserve knowledge of previous concepts, all while avoiding fusion-based memory expansions.

Result: On real-world datasets, CNS achieves state-of-the-art performance for single- and multi-concept personalization with minimal parameter updates; reduces memory storage and processing time; preserves zero-shot text-to-image generation.

Conclusion: CNS provides an effective, efficient continual-learning approach for diffusion-model personalization via targeted neuron selection, balancing plasticity and stability with low memory overhead and fusion-free operation.

Abstract: Updating diffusion models in an incremental setting would be practical in
real-world applications yet computationally challenging. We present a novel
learning strategy of Concept Neuron Selection (CNS), a simple yet effective
approach to perform personalization in a continual learning scheme. CNS
uniquely identifies neurons in diffusion models that are closely related to the
target concepts. In order to mitigate catastrophic forgetting problems while
preserving zero-shot text-to-image generation ability, CNS finetunes concept
neurons in an incremental manner and jointly preserves knowledge learned of
previous concepts. Evaluation of real-world datasets demonstrates that CNS
achieves state-of-the-art performance with minimal parameter adjustments,
outperforming previous methods in both single and multi-concept personalization
works. CNS also achieves fusion-free operation, reducing memory storage and
processing time for continual personalization.

</details>


### [172] [Compositional meta-learning through probabilistic task inference](https://arxiv.org/abs/2510.01858)
*Jacob J. W. Bakermans,Pablo Tano,Reidar Riveland,Charles Findling,Alexandre Pouget*

Main category: cs.LG

TL;DR: A compositional meta-learning model that learns a generative model of reusable components across tasks, enabling rapid, update-free inference via probabilistic reasoning; it recovers components in rule and motor tasks and demonstrates one-shot adaptation.


<details>
  <summary>Details</summary>
Motivation: To achieve data-efficient meta-learning by reusing compositional components across tasks, addressing the need to quickly adapt to new tasks with minimal experience.

Method: Develop a generative model that captures underlying components and their statistics shared across a family of tasks; represent tasks as structured combinations of these components; cast learning as probabilistic inference, enabling solution search through highly constrained hypothesis testing without parameter updates.

Result: Ground-truth components and statistics recovered in rule learning and motor learning tasks; demonstrates rapid inference of new solutions from single examples.

Conclusion: Combines neural network expressivity with probabilistic inference for rapid compositional meta-learning and data-efficient adaptation.

Abstract: To solve a new task from minimal experience, it is essential to effectively
reuse knowledge from previous tasks, a problem known as meta-learning.
Compositional solutions, where common elements of computation are flexibly
recombined into new configurations, are particularly well-suited for
meta-learning. Here, we propose a compositional meta-learning model that
explicitly represents tasks as structured combinations of reusable
computations. We achieve this by learning a generative model that captures the
underlying components and their statistics shared across a family of tasks.
This approach transforms learning a new task into a probabilistic inference
problem, which allows for finding solutions without parameter updates through
highly constrained hypothesis testing. Our model successfully recovers ground
truth components and statistics in rule learning and motor learning tasks. We
then demonstrate its ability to quickly infer new solutions from just single
examples. Together, our framework joins the expressivity of neural networks
with the data-efficiency of probabilistic inference to achieve rapid
compositional meta-learning.

</details>


### [173] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: EqM is a new generative framework that learns an equilibrium energy landscape and uses optimization-based sampling instead of diffusion-like time dynamics; it claims strong generation performance (FID 1.90 on ImageNet 256x256) and theoretical grounding for sampling from the data manifold, with applications to denoising, OOD detection, and image composition.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of non-equilibrium, time-conditional dynamics in diffusion/flow models, by learning a unified equilibrium landscape that supports optimization-based inference and better bridges flow-based and energy-based models.

Method: Train a model to learn the gradient of an implicit energy landscape representing the data distribution. At inference, perform gradient-based optimization (gradient descent with adaptive steps/optimizers) on the learned landscape to generate samples. This replaces time-conditional velocity dynamics with an equilibrium dynamics perspective.

Result: Empirically surpasses diffusion/flow models, achieving a Fr√©chet Inception Distance (FID) of 1.90 on ImageNet 256√ó256. The framework is theoretically justified to sample from the data manifold and is versatile across tasks such as denoising, OOD detection, and image composition.

Conclusion: EqM offers a tighter bridge between flow and energy-based models, providing a simple route to optimization-driven inference and broad applicability beyond pure generation; it reframes generative modeling around equilibrium dynamics rather than non-equilibrium diffusion processes.

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework
built from an equilibrium dynamics perspective. EqM discards the
non-equilibrium, time-conditional dynamics in traditional diffusion and
flow-based generative models and instead learns the equilibrium gradient of an
implicit energy landscape. Through this approach, we can adopt an
optimization-based sampling process at inference time, where samples are
obtained by gradient descent on the learned landscape with adjustable step
sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation
performance of diffusion/flow models empirically, achieving an FID of 1.90 on
ImageNet 256$\times$256. EqM is also theoretically justified to learn and
sample from the data manifold. Beyond generation, EqM is a flexible framework
that naturally handles tasks including partially noised image denoising, OOD
detection, and image composition. By replacing time-conditional velocities with
a unified equilibrium landscape, EqM offers a tighter bridge between flow and
energy-based models and a simple route to optimization-driven inference.

</details>


### [174] [Universal Dynamic Regret and Constraint Violation Bounds for Constrained Online Convex Optimization](https://arxiv.org/abs/2510.01867)
*Subhamon Supantha,Abhishek Sinha*

Main category: cs.LG

TL;DR: Generalizes Online Convex Optimization to adversarial costs and constraints, introduces two modular algorithms with universal dynamic regret and cumulative constraint violation bounds; reduces constrained learning to standard OCO via surrogate costs.


<details>
  <summary>Details</summary>
Motivation: Address online learning under adversarial cost and constraint functions without requiring a common feasible point, aiming for robust, universal guarantees in the most general setting.

Method: Proposes two simple modular algorithms. Reformulates the constrained problem as a standard OCO instance by constructing surrogate cost functions that encode constraint violations, enabling analysis of dynamic regret and constraint violation.

Result: Achieves universal dynamic regret bounds and cumulative constraint violation bounds in the fully adversarial setting, improving state-of-the-art results; applicable even when cost and constraint functions are chosen adversarially and there is no common feasible point.

Conclusion: A simple reduction to standard OCO plus modular algorithms yields robust, universal guarantees for constrained online learning in the most general adversarial setting, broadening applicability and simplifying design.

Abstract: We consider a generalization of the celebrated Online Convex Optimization
(OCO) framework with online adversarial constraints. We present two algorithms
having simple modular structures that yield universal dynamic regret and
cumulative constraint violation bounds, improving upon the state-of-the-art
results. Our results hold in the most general case when both the cost and
constraint functions are chosen arbitrarily by an adversary, and the constraint
functions need not contain any common feasible point. The results are
established by reducing the constrained learning problem to an instance of the
standard OCO problem with specially constructed surrogate cost functions.

</details>


### [175] [Randomized Gradient Subspaces for Efficient Large Language Model Training](https://arxiv.org/abs/2510.01878)
*Sahar Rajabi,Nayeema Nonta,Samanvay Vajpayee,Sirisha Rambhatla*

Main category: cs.LG

TL;DR: We analyze gradient subspaces in large language model training, showing most gradient energy lies in a small core subspace while a substantial residual remains; gradient-space curvature is near-flat. We propose randomized algorithms GrassWalk and GrassJump that exploit subspace structure, achieving state-of-the-art memory savings and improved pretraining performance on LLaMA-1B/7B.


<details>
  <summary>Details</summary>
Motivation: LLM training is memory-bound due to optimizer states; existing gradient-projection methods reduce memory but do not fully leverage gradient-space geometry. Understanding gradient-subspace dynamics and curvature can guide more effective memory-saving strategies.

Method: Empirically analyze the gradient space dynamics across layers/time, identify energy distribution between a core subspace and residual bulk, observe curvature properties, and design two randomized algorithms (GrassWalk and GrassJump) that exploit subspace structure to reduce memory footprint while maintaining or improving optimization performance; validate on LLaMA-1B and LLaMA-7B pretraining.

Result: The proposed GrassWalk and GrassJump achieve state-of-the-art memory savings with improved training performance compared to baselines during LLaMA-1B and LLaMA-7B pretraining.

Conclusion: Gradient space in LLM training exhibits a small energy-rich core and a sizable residual, with near-flat curvature. Algorithms that explicitly account for this geometry‚ÄîGrassWalk and GrassJump‚Äîcan deliver substantial memory reductions and competitive or improved training outcomes on large-scale models.

Abstract: Training large language models (LLMs) is often bottlenecked by extreme memory
demands, with optimizer states dominating the footprint. Recent works mitigates
this cost by projecting gradients into low-dimensional subspaces using
sophisticated update strategies. In this paper, we analyze the dynamics of
gradient space and its underlying subspaces. We find that while a small
subspace captures most gradient energy, a significant portion still resides in
the residual bulk; moreover, the influence of the core subspace diminishes over
time and in deeper layers. We also observe that the gradient space exhibits
near-flat curvature, calling for algorithms that explicitly account for this
geometry. Motivated by these insights, we introduce a suite of randomized
algorithms, GrassWalk and GrassJump, which exploit subspace and achieve
state-of-the-art memory savings while improving performance on LLaMA-1B and
LLaMA-7B pretraining.

</details>


### [176] [Multi-marginal temporal Schr√∂dinger Bridge Matching for video generation from unpaired data](https://arxiv.org/abs/2510.01894)
*Thomas Gravier,Thomas Boyer,Auguste Genovesio*

Main category: cs.LG

TL;DR: Multi-Marginal temporal Schr√∂dinger Bridge Matching (MMtSBM) extends diffusion Schr√∂dinger Bridge Matching to multiple marginals for video generation from unpaired data, offering scalable, high-dimensional recovery of hidden dynamics with Iterative Markovian Fitting; achieves state-of-the-art on transcriptomic trajectories and high-dimensional images.


<details>
  <summary>Details</summary>
Motivation: Many dynamic processes are only observable as static snapshots; reconstructing temporal evolution is crucial but current transport methods are not scalable and rely on strong assumptions.

Method: Introduce MMtSBM, derive Iterative Markovian Fitting for multiple marginals in a factorized framework, extending Diffusion Schr√∂dinger Bridge Matching; provide theoretical guarantees and empirical efficiency.

Result: Maintains theoretical properties on toy examples; achieves state-of-the-art on real-world 100-dimensional transcriptomic trajectory inference; recovers couplings and dynamics in high-dimensional images.

Conclusion: Multi-marginal Schr√∂dinger bridges are a practical, principled approach to recover hidden dynamics from static data, enabling scalable dynamic inference in high dimensions.

Abstract: Many natural dynamic processes -- such as in vivo cellular differentiation or
disease progression -- can only be observed through the lens of static sample
snapshots. While challenging, reconstructing their temporal evolution to
decipher underlying dynamic properties is of major interest to scientific
research. Existing approaches enable data transport along a temporal axis but
are poorly scalable in high dimension and require restrictive assumptions to be
met. To address these issues, we propose \textit{\textbf{Multi-Marginal
temporal Schr\"odinger Bridge Matching}} (\textbf{MMtSBM}) \textit{for video
generation from unpaired data}, extending the theoretical guarantees and
empirical efficiency of Diffusion Schr\"odinger Bridge Matching
(arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting
algorithm to multiple marginals in a novel factorized fashion. Experiments show
that MMtSBM retains theoretical properties on toy examples, achieves
state-of-the-art performance on real world datasets such as transcriptomic
trajectory inference in 100 dimensions, and for the first time recovers
couplings and dynamics in very high dimensional image settings. Our work
establishes multi-marginal Schr\"odinger bridges as a practical and principled
approach for recovering hidden dynamics from static data.

</details>


### [177] [Multimodal Foundation Models for Early Disease Detection](https://arxiv.org/abs/2510.01899)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: cs.LG

TL;DR: A multimodal transformer-based foundation model fuses diverse healthcare data (EHR, imaging, genetics, wearables) via modality-specific encoders into a shared latent space with multi-head attention, pretrained on many tasks; aims for early disease detection with governance tools and cross-dataset adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional models analyze data in silos, missing cross-modal correlations essential for early diagnosis; need a unified, adaptable foundation model for healthcare data.

Method: Per-modality encoders map inputs into a shared latent space; multi-head attention with residual normalization fuses modalities; pretraining on multiple tasks; architecture supports data governance and model management tools.

Result: Proposed experimental strategy using benchmark datasets across oncology, cardiology, and neurology to test early-detection tasks; no quantitative results reported in the abstract.

Conclusion: Advances toward a single foundation model for precision diagnostics, with potential accuracy gains and improved clinical decision support; emphasizes transparency, reliability, and interpretability through governance and management tools.

Abstract: Healthcare generates diverse streams of data, including electronic health
records (EHR), medical imaging, genetics, and ongoing monitoring from wearable
devices. Traditional diagnostic models frequently analyze these sources in
isolation, which constrains their capacity to identify cross-modal correlations
essential for early disease diagnosis. Our research presents a multimodal
foundation model that consolidates diverse patient data through an
attention-based transformer framework. At first, dedicated encoders put each
modality into a shared latent space. Then, they combine them using multi-head
attention and residual normalization. The architecture is made for pretraining
on many tasks, which makes it easy to adapt to new diseases and datasets with
little extra work. We provide an experimental strategy that uses benchmark
datasets in oncology, cardiology, and neurology, with the goal of testing early
detection tasks. The framework includes data governance and model management
tools in addition to technological performance to improve transparency,
reliability, and clinical interpretability. The suggested method works toward a
single foundation model for precision diagnostics, which could improve the
accuracy of predictions and help doctors make decisions.

</details>


### [178] [A Methodology for Transparent Logic-Based Classification Using a Multi-Task Convolutional Tsetlin Machine](https://arxiv.org/abs/2510.01906)
*Mayur Kishor Shende,Ole-Christoffer Granmo,Runar Helin,Vladimir I. Zadorozhny,Rishad Shafik*

Main category: cs.LG

TL;DR: CTM extended to RGB images achieves competitive performance on MNIST and CelebA while offering interpretable clause-based representations.


<details>
  <summary>Details</summary>
Motivation: To combine the interpretability of Tsetlin Machines with the handling of large-scale, multi-channel image data, addressing the gap between model transparency and competitive accuracy in vision tasks.

Method: Introduce a convolutional TM framework for multi-channel RGB inputs and propose a method to generate both local (per-region) interpretations and global class representations. Interpretations are distilled into visualizable patterns corresponding to learned convolutional clauses and used to explain predictions.

Result: On MNIST and CelebA, the CTM achieves 98.5% accuracy on MNIST and 86.56% F1-score on CelebA, with comparisons to ResNet50 showing competitive performance while maintaining interpretability even in large-scale training environments.

Conclusion: The work demonstrates that TM-based models can be scaled to complex vision tasks and produce human-interpretable knowledge representations, offering insights into clause behavior and potential applicability to diverse datasets.

Abstract: The Tsetlin Machine (TM) is a novel machine learning paradigm that employs
finite-state automata for learning and utilizes propositional logic to
represent patterns. Due to its simplistic approach, TMs are inherently more
interpretable than learning algorithms based on Neural Networks. The
Convolutional TM has shown comparable performance on various datasets such as
MNIST, K-MNIST, F-MNIST and CIFAR-2. In this paper, we explore the
applicability of the TM architecture for large-scale multi-channel (RGB) image
classification. We propose a methodology to generate both local interpretations
and global class representations. The local interpretations can be used to
explain the model predictions while the global class representations aggregate
important patterns for each class. These interpretations summarize the
knowledge captured by the convolutional clauses, which can be visualized as
images. We evaluate our methods on MNIST and CelebA datasets, using models that
achieve 98.5\% accuracy on MNIST and 86.56\% F1-score on CelebA (compared to
88.07\% for ResNet50) respectively. We show that the TM performs competitively
to this deep learning model while maintaining its interpretability, even in
large-scale complex training environments. This contributes to a better
understanding of TM clauses and provides insights into how these models can be
applied to more complex and diverse datasets.

</details>


### [179] [Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement](https://arxiv.org/abs/2510.01910)
*Zhaoyan Wang,Zheng Gao,Arogya Kharel,In-Young Ko*

Main category: cs.LG

TL;DR: This paper benchmarks conventional GNN augmentation methods against LLM-on-graph approaches under diverse graph deficiencies, finds that LLM-based methods are not always superior, and introduces RoGRAD, an iterative retrieval-augmented contrastive refinement framework using RAG to provide diverse, class-consistent augmentations and reinforce discriminative representations, achieving up to 82.43% average improvement.


<details>
  <summary>Details</summary>
Motivation: There is a gap in understanding how graph-native GNN methods and LLM-enhanced graph methods perform under compounded graph deficiencies; no comprehensive comparison exists; need robust learning methods that can handle defects. The work also challenges the assumption that LLM augmentation is always beneficial.

Method: (1) An empirical benchmark comparing conventional GNN-augmentation methods and LLM-on-graph frameworks across diverse graph deficiencies. (2) Introduction of RoGRAD (Robust Graph Learning via Retrieval-Augmented Contrastive Refinement): an iterative framework that uses Retrieval-Augmented Generation (RAG) to inject retrieval-grounded augmentations (class-consistent, diverse) and enforces discriminative representations through iterative graph contrastive learning, turning LLM augmentation into dynamic refinement.

Result: Extensive experiments show RoGRAD outperforms both conventional GNN-based and LLM-enhanced baselines, achieving up to 82.43% average improvement.

Conclusion: RoGRAD demonstrates that retrieval-grounded, iterative refinement is effective for robust graph learning and can surpass prior one-shot LLM-augmentation approaches; highlights the importance of dynamic, retrieval-informed augmentation and contrastive refinement for handling graph deficiencies.

Abstract: Graph Neural Networks (GNNs) are widely adopted in Web-related applications,
serving as a core technique for learning from graph-structured data, such as
text-attributed graphs. Yet in real-world scenarios, such graphs exhibit
deficiencies that substantially undermine GNN performance. While prior
GNN-based augmentation studies have explored robustness against individual
imperfections, a systematic understanding of how graph-native and Large
Language Models (LLMs) enhanced methods behave under compound deficiencies is
still missing. Specifically, there has been no comprehensive investigation
comparing conventional approaches and recent LLM-on-graph frameworks, leaving
their merits unclear. To fill this gap, we conduct the first empirical study
that benchmarks these two lines of methods across diverse graph deficiencies,
revealing overlooked vulnerabilities and challenging the assumption that LLM
augmentation is consistently superior. Building on empirical findings, we
propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement
(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is
the first iterative paradigm that leverages Retrieval-Augmented Generation
(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,
diverse augmentations and enforcing discriminative representations through
iterative graph contrastive learning. It transforms LLM augmentation for graphs
from static signal injection into dynamic refinement. Extensive experiments
demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced
baselines, achieving up to 82.43% average improvement.

</details>


### [180] [StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold](https://arxiv.org/abs/2510.01938)
*Zhizhong Li,Sina Sajadmanesh,Jingtao Li,Lingjuan Lyu*

Main category: cs.LG

TL;DR: A geometry-aware extension of LoRA (three-factor U S V^T) with orthonormal U,V on the Stiefel manifold and a Riemannian optimization framework, yielding improved performance across diverse tasks while remaining compatible with existing fine-tuning pipelines; code available.


<details>
  <summary>Details</summary>
Motivation: LoRA is parameter-efficient but underperforms full fine-tuning due to not exploiting the geometric structure of low-rank manifolds. The paper proposes a geometry-aware extension to better exploit subspace geometry.

Method: A three-factor decomposition U S V^T where U and V are constrained to the Stiefel manifold (orthonormal subspaces) and S is a scaling factor. The optimization converts any Euclidean optimizer into a Riemannian one to optimize on the Stiefel manifold, enabling efficient subspace learning while staying compatible with existing fine-tuning pipelines.

Result: Empirical results across diverse tasks‚Äîcommonsense reasoning, math and code generation, image classification, and image generation‚Äîshow the geometry-aware LoRA outperforms recent state-of-the-art LoRA variants. Code is available at the provided GitHub repository.

Conclusion: Geometry-aware LoRA improves performance by better exploiting the geometry of low-rank manifolds through orthonormal subspace learning and Riemannian optimization, while remaining compatible with standard fine-tuning workflows across modalities; promising for broad applicability.

Abstract: Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient
technique for fine-tuning large-scale pre-trained models. However, it still
lags behind full fine-tuning in performance, partly due to its insufficient
exploitation of the geometric structure underlying low-rank manifolds. In this
paper, we propose a geometry-aware extension of LoRA that uses a three-factor
decomposition $U\!SV^\top$. Analogous to the structure of singular value
decomposition (SVD), it separates the adapter's input and output subspaces, $V$
and $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie
on the Stiefel manifold, ensuring their orthonormality throughout the training.
To optimize on the Stiefel manifold, we employ a flexible and modular geometric
optimization design that converts any Euclidean optimizer to a Riemannian one.
It enables efficient subspace learning while remaining compatible with existing
fine-tuning pipelines. Empirical results across a wide range of downstream
tasks, including commonsense reasoning, math and code generation, image
classification, and image generation, demonstrate the superior performance of
our approach against the recent state-of-the-art variants of LoRA. Code is
available at https://github.com/SonyResearch/stella.

</details>


### [181] [Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions](https://arxiv.org/abs/2510.01969)
*Camilo Andr√©s Garc√≠a Trillos,Nicol√°s Garc√≠a Trillos*

Main category: cs.LG

TL;DR: Proposes dual and barycentric reformulations for learner-agnostic robust risk minimization in multiclass classification under arbitrary losses, with explicit results for cross-entropy, power-form, and quadratic losses; extends 0-1 loss results, enables tight lower bounds and robust classifier design, and links robustness to alpha-fair packing and entropy-regularized barycenter problems; numerical experiments show tighter lower bounds for cross-entropy.


<details>
  <summary>Details</summary>
Motivation: To enable principled, computationally tractable analysis and design of adversarially robust classifiers in multiclass settings under general loss functions, and to extend robustness insights beyond the 0-1 loss setting.

Method: Derives dual and barycentric reformulations of learner-agnostic robust risk minimization; provides explicit characterizations for cross-entropy, power-form, and quadratic losses; establishes connections to alpha-fair packing and generalized barycenter problems using Kullback-Leibler and Tsallis entropies as penalties; supports illustrative numerical experiments.

Result: Yields reformulations that allow efficient computation of sharp lower bounds on adversarial risk; facilitates robust classifier design beyond 0-1 loss; reveals structural connections to entropy-regularized optimization; empirical evidence shows tighter lower bounds for cross-entropy.

Conclusion: This work broadens robust multiclass learning to arbitrary losses through dual and barycentric reformulations, linking adversarial robustness with entropy-regularized optimization and alpha-fair concepts, and demonstrates practical benefits via tighter risk bounds and improved guidance for robust classifier design.

Abstract: We consider adversarially robust classification in a multiclass setting under
arbitrary loss functions and derive dual and barycentric reformulations of the
corresponding learner-agnostic robust risk minimization problem. We provide
explicit characterizations for important cases such as the cross-entropy loss,
loss functions with a power form, and the quadratic loss, extending in this way
available results for the 0-1 loss. These reformulations enable efficient
computation of sharp lower bounds for adversarial risks and facilitate the
design of robust classifiers beyond the 0-1 loss setting. Our paper uncovers
interesting connections between adversarial robustness, $\alpha$-fair packing
problems, and generalized barycenter problems for arbitrary positive measures
where Kullback-Leibler and Tsallis entropies are used as penalties. Our
theoretical results are accompanied with illustrative numerical experiments
where we obtain tighter lower bounds for adversarial risks with the
cross-entropy loss function.

</details>


### [182] [Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.01970)
*Yuanyuan Yao,Yuhan Shi,Lu Chen,Ziquan Fang,Yunjun Gao,Leong Hou U,Yushuai Li,Tianyi Li*

Main category: cs.LG

TL;DR: Moon is a supervised, modality-conversion framework for MTS anomaly detection that converts numeric data to images via MV-MTF, fuses numeric and image features with a Multimodal-CNN, and uses SHAP for explanations, achieving improved efficiency, accuracy, and interpretability over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing MTS anomaly detection methods struggle with unsupervised error-threshold reliance (reconstruction/prediction), underuse of anomaly labels in semi-supervised settings, and high cost/limited local relationship modeling in supervised methods. A unified, efficient, and interpretable supervised approach that leverages both numeric and image representations is needed.

Method: Introduce MV-MTF to convert multivariate time series into image representations capturing cross-variable and temporal relationships; employ a Multimodal-CNN with parameter sharing to fuse numeric features and image features; apply a SHAP-based anomaly explainer for interpretability; train under a supervised modality-conversion framework; evaluate on six real-world MTS datasets.

Result: Moon outperforms six state-of-the-art methods by up to 93% in efficiency, 4% in accuracy, and 10.8% in interpretation performance across six real-world MTS datasets.

Conclusion: The proposed Moon framework provides a more effective and interpretable approach to MTS anomaly detection by combining modality conversion, cross-data fusion, and explainable AI, addressing key limitations of existing methods.

Abstract: Multivariate time series (MTS) anomaly detection identifies abnormal patterns
where each timestamp contains multiple variables. Existing MTS anomaly
detection methods fall into three categories: reconstruction-based,
prediction-based, and classifier-based methods. However, these methods face two
key challenges: (1) Unsupervised learning methods, such as reconstruction-based
and prediction-based methods, rely on error thresholds, which can lead to
inaccuracies; (2) Semi-supervised methods mainly model normal data and often
underuse anomaly labels, limiting detection of subtle anomalies;(3) Supervised
learning methods, such as classifier-based approaches, often fail to capture
local relationships, incur high computational costs, and are constrained by the
scarcity of labeled data. To address these limitations, we propose Moon, a
supervised modality conversion-based multivariate time series anomaly detection
framework. Moon enhances the efficiency and accuracy of anomaly detection while
providing detailed anomaly analysis reports. First, Moon introduces a novel
multivariate Markov Transition Field (MV-MTF) technique to convert numeric time
series data into image representations, capturing relationships across
variables and timestamps. Since numeric data retains unique patterns that
cannot be fully captured by image conversion alone, Moon employs a
Multimodal-CNN to integrate numeric and image data through a feature fusion
model with parameter sharing, enhancing training efficiency. Finally, a
SHAP-based anomaly explainer identifies key variables contributing to
anomalies, improving interpretability. Extensive experiments on six real-world
MTS datasets demonstrate that Moon outperforms six state-of-the-art methods by
up to 93% in efficiency, 4% in accuracy and, 10.8% in interpretation
performance.

</details>


### [183] [Private Federated Multiclass Post-hoc Calibration](https://arxiv.org/abs/2510.01987)
*Samuel Maddock,Graham Cormode,Carsten Maple*

Main category: cs.LG

TL;DR: Federated learning calibration is explored by adapting centralized post-hoc methods (histogram binning, temperature scaling) to FL, including user-level DP. The study shows heterogeneity degrades calibration and proposes strategies; temperature scaling is most effective under DP-FL, while weighted binning excels without DP.


<details>
  <summary>Details</summary>
Motivation: Calibrated probabilities are essential for reliable decisions. FL introduces privacy constraints and data heterogeneity across clients, which can harm probability calibration. Centralized calibration methods have not been thoroughly studied in FL under DP or heterogeneity.

Method: Transfer and adapt centralized calibration techniques (histogram binning and temperature scaling) to federated settings, develop new federated calibration variants to handle client heterogeneity, and analyze two scenarios: standard FL and user-level DP-FL. Propose mitigation strategies to reduce calibration degradation due to heterogeneity.

Result: Calibration accuracy degrades with increasing client heterogeneity. The proposed federated calibration variants mitigate this degradation. In DP-FL, federated temperature scaling performs best; in non-DP FL, weighted histogram binning performs best.

Conclusion: Tailored post-hoc calibration methods are needed for FL. Temperature scaling is favorable under user-level DP in FL, while weighted binning is preferable when DP is not required. Effective mitigation strategies are essential to maintain calibration under heterogeneity.

Abstract: Calibrating machine learning models so that predicted probabilities better
reflect the true outcome frequencies is crucial for reliable decision-making
across many applications. In Federated Learning (FL), the goal is to train a
global model on data which is distributed across multiple clients and cannot be
centralized due to privacy concerns. FL is applied in key areas such as
healthcare and finance where calibration is strongly required, yet federated
private calibration has been largely overlooked. This work introduces the
integration of post-hoc model calibration techniques within FL. Specifically,
we transfer traditional centralized calibration methods such as histogram
binning and temperature scaling into federated environments and define new
methods to operate them under strong client heterogeneity. We study (1) a
federated setting and (2) a user-level Differential Privacy (DP) setting and
demonstrate how both federation and DP impacts calibration accuracy. We propose
strategies to mitigate degradation commonly observed under heterogeneity and
our findings highlight that our federated temperature scaling works best for
DP-FL whereas our weighted binning approach is best when DP is not required.

</details>


### [184] [PepCompass: Navigating peptide embedding spaces using Riemannian Geometry](https://arxiv.org/abs/2510.01988)
*Marcin Mo≈ºejko,Adam Bielecki,Jurand PrƒÖdzy≈Ñski,Marcin Traskowski,Antoni Janowski,Karol Jurasz,Micha≈Ç Kucharczyk,Hyun-Su Lee,Marcelo Der Torossian Torres,Cesar de la Fuente-Nunez,Paulina Szymczak,Micha≈Ç Kmicikiewicz,Ewa Szczurek*

Main category: cs.LG

TL;DR: PepCompass is a geometry-aware framework for antimicrobial peptide design that models the decoder-induced geometry of peptide latent spaces via a Union of Œ∫-Stable Riemannian Manifolds, enabling efficient local exploration and optimization. It introduces LE-BO (Local Enumeration Bayesian Optimization) built from Second-Order Riemannian Brownian Efficient Sampling and Mutation Enumeration in Tangent Space, plus PoGS (Potential-minimizing Geodesic Search) to bias discovery toward favorable seeds. In vitro validation yields novel seeds and 25 highly active peptides, including against resistant strains, demonstrating the approach's effectiveness.


<details>
  <summary>Details</summary>
Motivation: Antimicrobial peptide discovery is hindered by the enormous size of peptide space and scarcity of actives. Generative models produce latent maps but typically ignore the manifold geometry induced by decoders and rely on flat Euclidean metrics, leading to inefficient exploration. Prior manifold methods assume fixed intrinsic dimensionality, which fails for peptides. A geometry-aware, stable framework that captures local structure and accommodates variable dimensionality is needed to enable efficient search and optimization of active peptides.

Method: Define a Union of Œ∫-Stable Riemannian Manifolds M^Œ∫ to capture decoder-induced local geometry with stability. Develop two local exploration techniques: (1) Second-Order Riemannian Brownian Efficient Sampling, a convergent second-order approximation to Riemannian Brownian motion; (2) Mutation Enumeration in Tangent Space, treating tangent directions as discrete amino-acid substitutions. Combine these into Local Enumeration Bayesian Optimization (LE-BO) for local activity optimization. Introduce Potential-minimizing Geodesic Search (PoGS) to interpolate between prototype embeddings along property-enriched geodesics, biasing toward seeds with favorable activity.

Result: Empirical validation shows PoGS identifies four novel seeds, and LE-BO, guided by these seeds, discovers 25 highly active, broad-spectrum peptides, including activity against resistant bacterial strains.

Conclusion: Geometry-informed exploration offers a powerful new paradigm for antimicrobial peptide design by accurately modeling decoder-induced geometry, accommodating variable intrinsic dimensionality, and enabling efficient local optimization and targeted seed-driven exploration.

Abstract: Antimicrobial peptide discovery is challenged by the astronomical size of
peptide space and the relative scarcity of active peptides. Generative models
provide continuous latent "maps" of peptide space, but conventionally ignore
decoder-induced geometry and rely on flat Euclidean metrics, rendering
exploration and optimization distorted and inefficient. Prior manifold-based
remedies assume fixed intrinsic dimensionality, which critically fails in
practice for peptide data. Here, we introduce PepCompass, a geometry-aware
framework for peptide exploration and optimization. At its core, we define a
Union of $\kappa$-Stable Riemannian Manifolds $\mathbb{M}^{\kappa}$, a family
of decoder-induced manifolds that captures local geometry while ensuring
computational stability. We propose two local exploration methods: Second-Order
Riemannian Brownian Efficient Sampling, which provides a convergent
second-order approximation to Riemannian Brownian motion, and Mutation
Enumeration in Tangent Space, which reinterprets tangent directions as discrete
amino-acid substitutions. Combining these yields Local Enumeration Bayesian
Optimization (LE-BO), an efficient algorithm for local activity optimization.
Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which
interpolates between prototype embeddings along property-enriched geodesics,
biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro
validation confirms the effectiveness of PepCompass: PoGS yields four novel
seeds, and subsequent optimization with LE-BO discovers 25 highly active
peptides with broad-spectrum activity, including against resistant bacterial
strains. These results demonstrate that geometry-informed exploration provides
a powerful new paradigm for antimicrobial peptide design.

</details>


### [185] [Normality Calibration in Semi-supervised Graph Anomaly Detection](https://arxiv.org/abs/2510.02014)
*Guolei Zeng,Hezhe Qiao,Guoguo Ai,Jinsong Guo,Guansong Pang*

Main category: cs.LG

TL;DR: GraphNC is a two-component framework for semi-supervised graph anomaly detection that uses a teacher model to calibrate anomaly scores and a perturbation-based regularizer to refine node representations, leading to better normality calibration and more separable anomaly scores.


<details>
  <summary>Details</summary>
Motivation: Semi-supervised GAD often overfits to the limited labeled normal nodes, failing to generalize to unlabeled data and causing high false positives. A method that leverages both labeled and unlabeled data to calibrate normality can improve detection accuracy.

Method: GraphNC comprises two components: (1) ScoreDA (anomaly score distribution alignment) which aligns the student model's anomaly scores with the score distribution produced by a teacher model, pushing normal and abnormal scores toward the two extremes; (2) NormReg (perturbation-based normality regularization) which applies a perturbation-guided consistency loss on labeled nodes to make normal node representations more compact.

Result: ScoreDA leads to more separable anomaly scores by aligning toward the teacher's score distribution, while NormReg mitigates misleading teacher scores by regularizing representation space, yielding more compact normal representations and improved calibration.

Conclusion: GraphNC offers a practical framework to calibrate graph normality using both labeled and unlabeled data, combining score-level and representation-level regularization to enhance semi-supervised GAD performance.

Abstract: Graph anomaly detection (GAD) has attracted growing interest for its crucial
ability to uncover irregular patterns in broad applications. Semi-supervised
GAD, which assumes a subset of annotated normal nodes available during
training, is among the most widely explored application settings. However, the
normality learned by existing semi-supervised GAD methods is limited to the
labeled normal nodes, often inclining to overfitting the given patterns. These
can lead to high detection errors, such as high false positives. To overcome
this limitation, we propose GraphNC , a graph normality calibration framework
that leverages both labeled and unlabeled data to calibrate the normality from
a teacher model (a pre-trained semi-supervised GAD model) jointly in anomaly
score and node representation spaces. GraphNC includes two main components,
anomaly score distribution alignment (ScoreDA) and perturbation-based normality
regularization (NormReg). ScoreDA optimizes the anomaly scores of our model by
aligning them with the score distribution yielded by the teacher model. Due to
accurate scores in most of the normal nodes and part of the anomaly nodes in
the teacher model, the score alignment effectively pulls the anomaly scores of
the normal and abnormal classes toward the two ends, resulting in more
separable anomaly scores. Nevertheless, there are inaccurate scores from the
teacher model. To mitigate the misleading by these scores, NormReg is designed
to regularize the graph normality in the representation space, making the
representations of normal nodes more compact by minimizing a
perturbation-guided consistency loss solely on the labeled nodes.

</details>


### [186] [FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data](https://arxiv.org/abs/2510.02017)
*Aida Tayebi,Ali Khodabandeh Yalabadi,Mehdi Yazdani-Jahromi,Ozlem Ozmen Garibay*

Main category: cs.LG

TL;DR: A contrastive learning framework for fair representations in tabular data that reduces bias with minimal accuracy loss, using carefully chosen positive pairs and supervised/self-supervised contrastive objectives to improve fairness across downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Fairness in AI is critical, especially for tabular data; current representation learning (self-supervised/contrastive) shows robustness but fairness in such learned representations remains underexplored; need methods to debias while preserving performance.

Method: Proposes a contrastive learning framework for tabular data that selects positive pair samples strategically and combines supervised and self-supervised contrastive objectives to learn fair representations.

Result: Significant bias reduction compared to state-of-the-art contrastive models for tabular data, with little to no loss in accuracy; representations improve downstream fairness-sensitive tasks.

Conclusion: Demonstrates the viability of contrastive learning for debiasing tabular data representations, achieving fairness without sacrificing essential predictive information; suitable for real-world downstream applications.

Abstract: As AI systems become more embedded in everyday life, the development of fair
and unbiased models becomes more critical. Considering the social impact of AI
systems is not merely a technical challenge but a moral imperative. As
evidenced in numerous research studies, learning fair and robust
representations has proven to be a powerful approach to effectively debiasing
algorithms and improving fairness while maintaining essential information for
prediction tasks. Representation learning frameworks, particularly those that
utilize self-supervised and contrastive learning, have demonstrated superior
robustness and generalizability across various domains. Despite the growing
interest in applying these approaches to tabular data, the issue of fairness in
these learned representations remains underexplored. In this study, we
introduce a contrastive learning framework specifically designed to address
bias and learn fair representations in tabular datasets. By strategically
selecting positive pair samples and employing supervised and self-supervised
contrastive learning, we significantly reduce bias compared to existing
state-of-the-art contrastive learning models for tabular data. Our results
demonstrate the efficacy of our approach in mitigating bias with minimum
trade-off in accuracy and leveraging the learned fair representations in
various downstream tasks.

</details>


### [187] [Mathematical Modeling and Convergence Analysis of Deep Neural Networks with Dense Layer Connectivities in Deep Learning](https://arxiv.org/abs/2510.02049)
*Jinshu Huang,Haibin Su,Xue-Cheng Tai,Chunlin Wu*

Main category: cs.LG

TL;DR: Dense non-local (DNL) networks generalize densely connected architectures by modeling them as nonlinear integral equations; the paper proves convergence from discrete network training to a continuous-time optimal-control limit using piecewise-linear extension and Gamma-convergence, implying stability benefits and a mathematical foundation for densely connected DNNs.


<details>
  <summary>Details</summary>
Motivation: Dense connectivity enhances information flow and performance, but there is limited rigorous understanding of training dynamics in the deep-layer limit. By formulating DNL as a nonlinear integral equation and studying the training problem from an optimal-control perspective, the work aims to provide a solid mathematical foundation and to justify the stability and robustness of densely connected architectures.

Method: Define densely connected networks within a dense non-local (DNL) framework, allowing non-local feature transformations inside layers and modeling the network as nonlinear integral equations. Analyze the training problem as an optimal control problem, employ a piecewise linear extension to connect discrete networks to a continuous-time limit, and apply Gamma-convergence to establish convergence of optimal values and subsequence convergence of minimizers.

Result: The authors establish convergence of the discrete training problem to its continuous-time counterpart: optimal values converge and minimizers have subsequence convergence under the Gamma-convergence framework. The DNL framework encompasses standard DenseNets and variants.

Conclusion: This provides a mathematical foundation for densely connected DNNs, suggesting these architectures can offer stability during training in deep models and offering a bridge between discrete networks and continuous-time optimal-control formulations.

Abstract: In deep learning, dense layer connectivity has become a key design principle
in deep neural networks (DNNs), enabling efficient information flow and strong
performance across a range of applications. In this work, we model densely
connected DNNs mathematically and analyze their learning problems in the
deep-layer limit. For a broad applicability, we present our analysis in a
framework setting of DNNs with densely connected layers and general non-local
feature transformations (with local feature transformations as special cases)
within layers, which is called dense non-local (DNL) framework and includes
standard DenseNets and variants as special examples. In this formulation, the
densely connected networks are modeled as nonlinear integral equations, in
contrast to the ordinary differential equation viewpoint commonly adopted in
prior works. We study the associated training problems from an optimal control
perspective and prove convergence results from the network learning problem to
its continuous-time counterpart. In particular, we show the convergence of
optimal values and the subsequence convergence of minimizers, using a piecewise
linear extension and $\Gamma$-convergence analysis. Our results provide a
mathematical foundation for understanding densely connected DNNs and further
suggest that such architectures can offer stability of training deep models.

</details>


### [188] [Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference](https://arxiv.org/abs/2510.02056)
*Benjamin Wiriyapong,Oktay Karaku≈ü,Kirill Sidorov*

Main category: cs.LG

TL;DR: Adaptive Mixture Flow VI (AMF-VI) combines heterogeneous normalizing flows (MAF, RealNVP, RBIG) in a two-stage training regime to robustly approximate complex posteriors. It yields lower NLL and better transport metrics across diverse posterior shapes with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Single-flow VI often fails to robustly capture qualitatively different distributions; there is a need for a robust, architecture-agnostic method that can leverage diverse inductive biases.

Method: Two-stage training: (i) sequential expert training of individual flows; (ii) adaptive global weight estimation via likelihood-driven updates. No per-sample gating or architectural changes; the mixture is learned globally and is architecture-agnostic.

Result: AMF-VI achieves consistently lower negative log-likelihood than each single-flow baseline across six posterior families (banana, X-shape, two-moons, rings, bimodal, five-mode), with stable improvements in Wasserstein-2 and MMD, indicating robust performance across shapes and modalities; overhead is minimal.

Conclusion: Adaptive mixtures of diverse flows offer a reliable route to robust VI across diverse posterior families while preserving each expert's inductive bias; the approach is efficient and architecture-agnostic and scales to multiple posterior types.

Abstract: Normalising-flow variational inference (VI) can approximate complex
posteriors, yet single-flow models often behave inconsistently across
qualitatively different distributions. We propose Adaptive Mixture Flow
Variational Inference (AMF-VI), a heterogeneous mixture of complementary flows
(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of
individual flows, and (ii) adaptive global weight estimation via
likelihood-driven updates, without per-sample gating or architectural changes.
Evaluated on six canonical posterior families of banana, X-shape, two-moons,
rings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower
negative log-likelihood than each single-flow baseline and delivers stable
gains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),
indicating improved robustness across shapes and modalities. The procedure is
efficient and architecture-agnostic, incurring minimal overhead relative to
standard flow training, and demonstrates that adaptive mixtures of diverse
flows provide a reliable route to robust VI across diverse posterior families
whilst preserving each expert's inductive bias.

</details>


### [189] [KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting](https://arxiv.org/abs/2510.02084)
*Kuiye Ding,Fanda Fan,Zheya Wang,Hongxiao Li,Yifan Wang,Lei Wang,Chunjie Luo,Jianfeng Zhan*

Main category: cs.LG

TL;DR: KAIROS is a non-autoregressive time series forecasting framework for Web apps that models segment-level multi-peak distributions, enabling just-in-time inference, zero-shot generalization, and competitive accuracy with much lower inference cost.


<details>
  <summary>Details</summary>
Motivation: Web-scale time series forecasting needs to be fast and responsive to support real-time resource planning, cache placement, and anomaly response; autoregressive methods suffer from error accumulation and higher latency, motivating a scalable non-autoregressive approach.

Method: Introduce KAIROS, a non-autoregressive model that directly models segment-level multi-peak distributions for time series forecasts. It avoids error accumulation and supports just-in-time inference. Trained on a large-scale corpus, it improves over existing non-autoregressive models that tend to produce over-smoothed predictions.

Result: KAIROS demonstrates strong zero-shot generalization on six benchmarks, delivering forecasting performance comparable to state-of-the-art foundation models of similar scale while requiring a fraction of their inference cost.

Conclusion: Non-autoregressive design is a scalable paradigm for foundation models in time series, and KAIROS shows the feasibility and efficiency of this approach for real-time, web-scale forecasting.

Abstract: In the World Wide Web, reliable time series forecasts provide the
forward-looking signals that drive resource planning, cache placement, and
anomaly response, enabling platforms to operate efficiently as user behavior
and content distributions evolve. Compared with other domains, time series
forecasting for Web applications requires much faster responsiveness to support
real-time decision making. We present KAIROS, a non-autoregressive time series
forecasting framework that directly models segment-level multi-peak
distributions. Unlike autoregressive approaches, KAIROS avoids error
accumulation and achieves just-in-time inference, while improving over existing
non-autoregressive models that collapse to over-smoothed predictions. Trained
on the large-scale corpus, KAIROS demonstrates strong zero-shot generalization
on six widely used benchmarks, delivering forecasting performance comparable to
state-of-the-art foundation models with similar scale, at a fraction of their
inference cost. Beyond empirical results, KAIROS highlights the importance of
non-autoregressive design as a scalable paradigm for foundation models in time
series.

</details>


### [190] [Inferring Optical Tissue Properties from Photoplethysmography using Hybrid Amortized Inference](https://arxiv.org/abs/2510.02073)
*Jens Behrmann,Maria R. Cervera,Antoine Wehenkel,Andrew C. Miller,Albert Cerussi,Pranay Jain,Vivek Venugopal,Shijie Yan,Guillermo Sapiro,Luca Pegolotti,J√∂rn-Henrik Jacobsen*

Main category: cs.LG

TL;DR: Introduce PPGen, a biophysical model linking PPG signals to interpretable physiological/optical parameters, and Hybrid Amortized Inference (HAI) for fast, robust parameter estimation, addressing interpretability and sensor design in DL contexts.


<details>
  <summary>Details</summary>
Motivation: DL-based PPG analysis often relies on features with unclear physiological meaning, creating tension between predictive power, clinical interpretability, and hardware design; a biophysical, interpretable model is needed.

Method: Develop PPGen to relate PPG to interpretable physiological/optical parameters; build Hybrid Amortized Inference (HAI) to estimate parameters from PPG signals while compensating for model misspecification.

Result: In extensive in-silico experiments, HAI accurately infers physiological parameters across diverse noise and sensor conditions.

Conclusion: PPGen+HAI offer a path toward DL-friendly but physiologically interpretable PPG models that support clinical interpretation and hardware-informed design.

Abstract: Smart wearables enable continuous tracking of established biomarkers such as
heart rate, heart rate variability, and blood oxygen saturation via
photoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer
physiological information, as recent deep learning (DL) studies demonstrate.
However, DL models often rely on features with unclear physiological meaning,
creating a tension between predictive power, clinical interpretability, and
sensor design. We address this gap by introducing PPGen, a biophysical model
that relates PPG signals to interpretable physiological and optical parameters.
Building on PPGen, we propose hybrid amortized inference (HAI), enabling fast,
robust, and scalable estimation of relevant physiological parameters from PPG
signals while correcting for model misspecification. In extensive in-silico
experiments, we show that HAI can accurately infer physiological parameters
under diverse noise and sensor conditions. Our results illustrate a path toward
PPG models that retain the fidelity needed for DL-based features while
supporting clinical interpretation and informed hardware design.

</details>


### [191] [Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions](https://arxiv.org/abs/2510.02081)
*Zhaoyi Li,Jingtao Ding,Yong Li,Shihua Li*

Main category: cs.LG

TL;DR: Flow Matching (FM) has a train-inference gap and can produce stiff trajectories due to over-predicted straight paths. The paper fine-tunes FM using Maximum Likelihood Estimation (MLE) of reconstructions, including a residual-based variant that enforces contraction, aiming to improve inference accuracy in image generation and robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: To address the mismatch between training objectives and inference quality in diffusion-based Flow Matching, and to alleviate issues like stiffness from overly straight paths. The work leverages FM‚Äôs ODE-based formulation to bridge training and inference without SDEs.

Method: Provide theoretical analysis on how training loss relates to inference error in FM. Propose fine-tuning FM via Maximum Likelihood Estimation of reconstructions (MLE fine-tuning), including straightforward and residual-based approaches. The residual-based variant uses specially designed architectures to enforce contraction properties for robustness and interpretability.

Result: Experimental results show reliable improvement in inference performance for FM in both image generation and robotic manipulation tasks.

Conclusion: MLE-based fine-tuning mitigates the training-inference gap in Flow Matching, and contraction-aware, residual-based designs further enhance robustness and interpretability, validating the approach across generative and manipulation tasks.

Abstract: Flow Matching (FM) algorithm achieves remarkable results in generative tasks
especially in robotic manipulation. Building upon the foundations of diffusion
models, the simulation-free paradigm of FM enables simple and efficient
training, but inherently introduces a train-inference gap. Specifically, we
cannot assess the model's output during the training phase. In contrast, other
generative models including Variational Autoencoder (VAE), Normalizing Flow and
Generative Adversarial Networks (GANs) directly optimize on the reconstruction
loss. Such a gap is particularly evident in scenarios that demand high
precision, such as robotic manipulation. Moreover, we show that FM's
over-pursuit of straight predefined paths may introduce some serious problems
such as stiffness into the system. These motivate us to fine-tune FM via
Maximum Likelihood Estimation of reconstructions - an approach made feasible by
FM's underlying smooth ODE formulation, in contrast to the stochastic
differential equations (SDEs) used in diffusion models. This paper first
theoretically analyzes the relation between training loss and inference error
in FM. Then we propose a method of fine-tuning FM via Maximum Likelihood
Estimation of reconstructions, which includes both straightforward fine-tuning
and residual-based fine-tuning approaches. Furthermore, through specifically
designed architectures, the residual-based fine-tuning can incorporate the
contraction property into the model, which is crucial for the model's
robustness and interpretability. Experimental results in image generation and
robotic manipulation verify that our method reliably improves the inference
performance of FM.

</details>


### [192] [GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning](https://arxiv.org/abs/2510.02180)
*Silvia Sapora,Devon Hjelm,Alexander Toshev,Omar Attia,Bogdan Mazoure*

Main category: cs.LG

TL;DR: GRACE generates executable reward functions via LLMs in an evolutionary search, making IRL rewards interpretable and competitive on BabyAI/AndroidWorld.


<details>
  <summary>Details</summary>
Motivation: Inverse Reinforcement Learning often yields black-box rewards. There is a need for interpretable, debuggable reward functions that can be inspected and validated, especially in multi-task contexts.

Method: GRACE uses Large Language Models within an evolutionary search to reverse-engineer reward functions as executable code directly from expert trajectories, enabling inspectable reward APIs; validated on BabyAI and AndroidWorld with multi-task extensions.

Result: GRACE learns highly accurate rewards efficiently, producing strong policies that compete with imitation learning and online RL baselines; it also constructs complex reward APIs for multi-task setups.

Conclusion: GRACE provides interpretable, verifiable reward functions that can be inspected and Debugged, enabling complex multi-task reward APIs and competitive policy performance.

Abstract: Inverse Reinforcement Learning aims to recover reward models from expert
demonstrations, but traditional methods yield "black-box" models that are
difficult to interpret and debug. In this work, we introduce GRACE (Generating
Rewards As CodE), a method for using Large Language Models within an
evolutionary search to reverse-engineer an interpretable, code-based reward
function directly from expert trajectories. The resulting reward function is
executable code that can be inspected and verified. We empirically validate
GRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns
highly accurate rewards, even in complex, multi-task settings. Further, we
demonstrate that the resulting reward leads to strong policies, compared to
both competitive Imitation Learning and online RL approaches with ground-truth
rewards. Finally, we show that GRACE is able to build complex reward APIs in
multi-task setups.

</details>


### [193] [Learning Model Representations Using Publicly Available Model Hubs](https://arxiv.org/abs/2510.02096)
*Damian Falk,Konstantin Sch√ºrholt,Konstantinos Tzevelekakis,L√©o Meynent,Damian Borth*

Main category: cs.LG

TL;DR: A weight-space learning backbone trained on unstructured models from repositories can learn robust weight representations without curated model zoos and generalize to unseen modalities.


<details>
  <summary>Details</summary>
Motivation: To overcome the reliance on large, curated model zoos for learning weight-space representations, addressing heterogeneity and lack of documentation in real-world model pools.

Method: Train a weight-space backbone on arbitrary models downloaded from large repositories (e.g., Hugging Face), designed to handle highly heterogeneous, unstructured populations. Evaluate against backbones trained on laboratory-created zoos and test cross-domain generalization to unseen data modalities.

Result: Weight-space representations learned from models sourced from Hugging Face achieve strong performance, often outperforming backbones trained on curated lab zoos. The diversity in training weights enables generalization to unseen data modalities.

Conclusion: High-quality weight-space representations can be learned in the wild from uncurated model pools, making curated model zoos unnecessary and enabling scalable, flexible weight-space learning.

Abstract: The weights of neural networks have emerged as a novel data modality, giving
rise to the field of weight space learning. A central challenge in this area is
that learning meaningful representations of weights typically requires large,
carefully constructed collections of trained models, typically referred to as
model zoos. These model zoos are often trained ad-hoc, requiring large
computational resources, constraining the learned weight space representations
in scale and flexibility. In this work, we drop this requirement by training a
weight space learning backbone on arbitrary models downloaded from large,
unstructured model repositories such as Hugging Face. Unlike curated model
zoos, these repositories contain highly heterogeneous models: they vary in
architecture and dataset, and are largely undocumented. To address the
methodological challenges posed by such heterogeneity, we propose a new weight
space backbone designed to handle unstructured model populations. We
demonstrate that weight space representations trained on models from Hugging
Face achieve strong performance, often outperforming backbones trained on
laboratory-generated model zoos. Finally, we show that the diversity of the
model weights in our training set allows our weight space model to generalize
to unseen data modalities. By demonstrating that high-quality weight space
representations can be learned in the wild, we show that curated model zoos are
not indispensable, thereby overcoming a strong limitation currently faced by
the weight space learning community.

</details>


### [194] [Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025](https://arxiv.org/abs/2510.02202)
*Matthew A. Reyna,Zuzana Koscova,Jan Pavlus,Soheil Saghafi,James Weigle,Andoni Elola,Salman Seyedi,Kiersten Campbell,Qiao Li,Ali Bahrami Rad,Ant√¥nio H. Ribeiro,Antonio Luiz P. Ribeiro,Reza Sameni,Gari D. Clifford*

Main category: cs.LG

TL;DR: ECG-based triage for Chagas disease via the PhysioNet Challenge 2025, leveraging weak and strong labels, data augmentation, and a serology-aware evaluation metric; global participation and diverse algorithmic approaches.


<details>
  <summary>Details</summary>
Motivation: Chagas disease testing is constrained by limited serology capacity; ECGs showing cardiomyopathy can serve as a non-invasive screening tool to prioritize testing and treatment, especially in resource-limited settings.

Method: Utilized multiple datasets with weak labels (patient reports) and smaller, strongly labeled datasets; applied data augmentation to improve robustness and generalization across unseen data sources; framed the problem as a triage task using a serology-capacity-aware evaluation metric.

Result: Engaged 630+ participants from 111 teams with over 1300 entries, introducing innovations in weak/strong label handling, data augmentation, and triage-oriented evaluation; demonstrated broad methodological diversity and potential for scalable Chagas screening.

Conclusion: The challenge showcases the feasibility and value of ECG-based triage for Chagas disease, highlighting the importance of label strategies, data augmentation, and serology-aware metrics, and it catalyzes global collaboration for scalable screening solutions.

Abstract: Objective: Chagas disease is a parasitic infection that is endemic to South
America, Central America, and, more recently, the U.S., primarily transmitted
by insects. Chronic Chagas disease can cause cardiovascular diseases and
digestive problems. Serological testing capacities for Chagas disease are
limited, but Chagas cardiomyopathy often manifests in ECGs, providing an
opportunity to prioritize patients for testing and treatment. Approach: The
George B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic
approaches for identifying Chagas disease from electrocardiograms (ECGs). Main
results: This Challenge provides multiple innovations. First, we leveraged
several datasets with labels from patient reports and serological testing,
provided a large dataset with weak labels and smaller datasets with strong
labels. Second, we augmented the data to support model robustness and
generalizability to unseen data sources. Third, we applied an evaluation metric
that captured the local serological testing capacity for Chagas disease to
frame the machine learning problem as a triage task. Significance: Over 630
participants from 111 teams submitted over 1300 entries during the Challenge,
representing diverse approaches from academia and industry worldwide.

</details>


### [195] [PENEX: AdaBoost-Inspired Neural Network Regularization](https://arxiv.org/abs/2510.02107)
*Klaus-Rudolf Kladny,Bernhard Sch√∂lkopf,Michael Muehlebach*

Main category: cs.LG

TL;DR: PENEX is a new penalized exponential loss for multi-class boosting that is optimization-friendly and promotes margin maximization, yielding regularization benefits and serving as an AdaBoost-inspired alternative for training deep nets.


<details>
  <summary>Details</summary>
Motivation: AdaBoost uses exponential loss and often generalizes well despite increasing model capacity; but existing multi-class exponential losses are hard to optimize with first-order methods. PENEX aims for theoretical grounding and optimization-friendly properties to enable margin-based boosting in modern settings.

Method: Introduce Penalized Exponential Loss (PENEX) for multi-class problems; show it is amenable to first-order optimization; prove that PENEX implicitly maximizes data margins; show that gradient increments correspond to weak learner updates within a boosting framework; validate empirically on vision and language tasks and compare regularization effects.

Result: Theoretical evidence that PENEX maximizes margins and that its gradient increments naturally parameterize weak learners; empirical results across vision and language tasks demonstrate PENEX provides regularization benefits, often outperforming comparable methods at similar computational cost.

Conclusion: PENEX offers a viable AdaBoost-inspired alternative for training and fine-tuning deep neural networks, leveraging boosting principles with a first-order-optimization-friendly loss to improve generalization and regularization.

Abstract: AdaBoost sequentially fits so-called weak learners to minimize an exponential
loss, which penalizes mislabeled data points more severely than other loss
functions like cross-entropy. Paradoxically, AdaBoost generalizes well in
practice as the number of weak learners grows. In the present work, we
introduce Penalized Exponential Loss (PENEX), a new formulation of the
multi-class exponential loss that is theoretically grounded and, in contrast to
the existing formulation, amenable to optimization via first-order methods. We
demonstrate both empirically and theoretically that PENEX implicitly maximizes
margins of data points. Also, we show that gradient increments on PENEX
implicitly parameterize weak learners in the boosting framework. Across
computer vision and language tasks, we show that PENEX exhibits a regularizing
effect often better than established methods with similar computational cost.
Our results highlight PENEX's potential as an AdaBoost-inspired alternative for
effective training and fine-tuning of deep neural networks.

</details>


### [196] [DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning](https://arxiv.org/abs/2510.02212)
*Hanyang Zhao,Dawen Liang,Wenpin Tang,David Yao,Nathan Kallus*

Main category: cs.LG

TL;DR: DiFFPO is a unified RL-based framework for training diffusion LLMs to reason better and faster, using off-policy surrogate RL, two-stage likelihood with importance sampling, and jointly trained samplers to cut inference compute, showing improved accuracy at lower NFEs on math/planning tasks.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies and limited reasoning quality in diffusion LLMs; unify existing baselines (e.g., d1) under a more sample-efficient RL paradigm; enable adaptive inference control to reduce compute while preserving or improving performance.

Method: 1) Train surrogate policies via off-policy RL to approximate the true dLLM policy; 2) Use a two-stage likelihood approximation with importance sampling; 3) Introduce joint training of samplers/controllers to allocate inference thresholds per prompt, optimizing for fewer function evaluations; 4) Train open-source diffusion LLMs on math and planning benchmarks.

Result: Improved sample efficiency and task performance; better accuracies with fewer NFEs; superior Pareto frontier for inference-time compute across math/planning benchmarks.

Conclusion: DiFFPO provides a cohesive framework that jointly optimizes model policy and inference control, achieving faster and better reasoning in diffusion LLMs and demonstrating effectiveness on standard tasks.

Abstract: We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified
framework for training masked diffusion large language models (dLLMs) to reason
not only better (furious), but also faster via reinforcement learning (RL). We
first unify the existing baseline approach such as d1 by proposing to train
surrogate policies via off-policy RL, whose likelihood is much more tractable
as an approximation to the true dLLM policy. This naturally motivates a more
accurate and informative two-stage likelihood approximation combined with
importance sampling correction, which leads to generalized RL algorithms with
better sample efficiency and superior task performance. Second, we propose a
new direction of joint training efficient samplers/controllers of dLLMs policy.
Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by
letting the model learn to adaptively allocate an inference threshold for each
prompt. By jointly training the sampler, we yield better accuracies with lower
number of function evaluations (NFEs) compared to training the model only,
obtaining the best performance in improving the Pareto frontier of the
inference-time compute of dLLMs. We showcase the effectiveness of our pipeline
by training open source large diffusion language models over benchmark math and
planning tasks.

</details>


### [197] [Hybrid Deep Learning Modeling Approach to Predict Natural Gas Consumption of Home Subscribers on Limited Data](https://arxiv.org/abs/2510.02115)
*Milad Firoozeh,Nader Dashti,Mohammad Ali Hatefi*

Main category: cs.LG

TL;DR: Hybrid BiLSTM-XGBoost model outperforms LSTM and GRU in predicting residential gas consumption in Zanjan, Iran, using six years of meteorological data; robust with limited data, enabling better resource management and reducing seasonal shortages.


<details>
  <summary>Details</summary>
Motivation: Growing energy demand and seasonal gas outages in Iran motivate improved prediction of residential gas consumption. Iran's large gas resources and climate variability make accurate forecasting important for efficient resource management, especially in the residential sector.

Method: Comparative evaluation of machine learning models (LSTM, GRU, and a hybrid BiLSTM-XGBoost) using a dataset of gas consumption and meteorological data from 2017‚Äì2022. Models were trained and evaluated on their ability to predict consumption patterns, with performance assessed via RMSE, MAPE, and MPE. The study highlights the value of incorporating geographical and climatic factors.

Result: The hybrid BiLSTM-XGBoost model achieved superior accuracy, with lower RMSE, MAPE, and MPE than the LSTM and GRU models. It also demonstrated robustness when data were limited, indicating strong generalization in various data-scarce scenarios.

Conclusion: Hybrid, data-fusion ML approaches are effective for predicting residential gas consumption and managing resources, particularly in regions with variable climate. The inclusion of geographic and climate information enhances predictive performance and can help mitigate seasonal shortages.

Abstract: Today, natural gas, as a clean fuel and the best alternative to crude oil,
covers a significant part of global demand. Iran is one of the largest
countries with energy resources and in terms of gas is the second-largest
country in the world. But, due to the increase in population and energy
consumption, it faces problems such as pressure drops and gas outages yearly in
cold seasons and therefore it is necessary to control gas consumption,
especially in the residential sector, which has the largest share in Iran. This
study aims to analyze and predict gas consumption for residential customers in
Zanjan province, Iran, using machine learning models, including LSTM, GRU, and
a hybrid BiLSTM-XGBoost model. The dataset consists of gas consumption and
meteorology data collected over six years, from 2017 to 2022. The models were
trained and evaluated based on their ability to accurately predict consumption
patterns. The results indicate that the hybrid BiLSTM-XGBoost model
outperformed the other models in terms of accuracy, with lower Root Mean
Squared Error (RMSE), Mean Absolute Percentage Error (MAPE) values, and Mean
Percentage Error (MPE). Additionally, the Hybrid model demonstrated robust
performance, particularly in scenarios with limited data. The findings suggest
that machine learning approaches, particularly hybrid models, can be
effectively utilized to manage and predict gas consumption, contributing to
more efficient resource management and reducing seasonal shortages. This study
highlights the importance of incorporating geographical and climatic factors in
predictive modeling, as these significantly influence gas usage across
different regions.

</details>


### [198] [Ensemble Threshold Calibration for Stable Sensitivity Control](https://arxiv.org/abs/2510.02116)
*John N. Daras*

Main category: cs.LG

TL;DR: An end-to-end framework to achieve exact recall with sub-percent variance for large-scale spatial conflation and entity matching, leveraging a scalable pipeline and ensemble threshold estimators.


<details>
  <summary>Details</summary>
Motivation: Traditional confidence-interval-based recall bounds (e.g., Clopper-Pearson, Wilson) often overshoot or vary widely under skewed score distributions, causing missed true matches or excessive verification costs. Precise, low-variance recall control at scale is needed for reliable downstream analytics.

Method: A scalable pipeline: (1) equigrid bounding-box filter and CSR representation to drastically reduce candidate pairs; (2) deterministic xxHash bootstrap to train a lightweight neural ranker; (3) propagate ranks to all remaining pairs with a single forward pass; (4) build a reproducible, score-decile-stratified calibration set; (5) ensemble threshold estimators‚ÄîClopper-Pearson, Jeffreys, Wilson, exact quantile‚Äîvia inverse-variance weighting, fused across nine subsamples; (6) end-to-end TPU-friendly implementation; evaluated on two cadastral datasets (‚âà6.31M and 67.34M pairs).

Result: The framework consistently achieves the target recall with sub-percent variance, reduces redundant verifications compared with other calibration methods, and runs end-to-end on a single TPU v3 core.

Conclusion: The proposed approach delivers reproducible, low-variance recall control at scale for spatial conflation and entity matching, balancing accuracy with computational cost, and is TPU-friendly; it shows promise for other large-scale matching problems given similar data characteristics.

Abstract: Precise recall control is critical in large-scale spatial conflation and
entity-matching tasks, where missing even a few true matches can break
downstream analytics, while excessive manual review inflates cost. Classical
confidence-interval cuts such as Clopper-Pearson or Wilson provide lower bounds
on recall, but they routinely overshoot the target by several percentage points
and exhibit high run-to-run variance under skewed score distributions. We
present an end-to-end framework that achieves exact recall with sub-percent
variance over tens of millions of geometry pairs, while remaining TPU-friendly.
Our pipeline starts with an equigrid bounding-box filter and compressed sparse
row (CSR) candidate representation, reducing pair enumeration by two orders of
magnitude. A deterministic xxHash bootstrap sample trains a lightweight neural
ranker; its scores are propagated to all remaining pairs via a single forward
pass and used to construct a reproducible, score-decile-stratified calibration
set. Four complementary threshold estimators - Clopper-Pearson, Jeffreys,
Wilson, and an exact quantile - are aggregated via inverse-variance weighting,
then fused across nine independent subsamples. This ensemble reduces threshold
variance compared to any single method. Evaluated on two real cadastral
datasets (approximately 6.31M and 67.34M pairs), our approach consistently hits
a recall target within a small error, decreases redundant verifications
relative to other calibrations, and runs end-to-end on a single TPU v3 core.

</details>


### [199] [DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding](https://arxiv.org/abs/2510.02117)
*Samhita Pal,James O'quinn,Kaveh Aryan,Heather Pua,James P. Long,Amir Asiaee*

Main category: cs.LG

TL;DR: DECOR is a differentiable, single-shot estimator for learning DAGs in linear Gaussian SEMs with latent confounding; it jointly models the structure and a correlated noise term, with identifiability guaranteed under bow-free graphs and a uniform eigenvalue margin, and shows competitive performance across synthetic regimes.


<details>
  <summary>Details</summary>
Motivation: Latent confounding complicates structure learning in linear SEMs; existing continuous methods rely on error independence or require deconfounding steps with strong assumptions. A single likelihood-based approach that jointly learns the DAG and noise model aims to be both principled and robust.

Method: DECOR is a single likelihood-based, fully differentiable estimator that alternates a smooth-acyclic graph update with a convex noise update. It jointly learns the DAG and a correlated noise model, with optional bow complementarity penalty or post hoc reconciliation. Theoretical identifiability is established: if the mixed graph is bow-free and the noise covariance has a uniform eigenvalue margin, the map from (B,Œ©) to the observational covariance is injective.

Result: Empirically, DECOR matches or outperforms strong baselines on synthetic benchmarks, across varying confounding density, graph density, latent rank, and dimensionality (n<p). It is particularly robust when confounding is non-pervasive, while remaining competitive under pervasive confounding.

Conclusion: DECOR provides a principled, identifiable, and practical approach for structure learning in linear Gaussian SEMs with latent confounding. It achieves strong empirical performance with theoretical identifiability under bow-free conditions and uniform eigenvalue margin, and offers flexibility via a bow penalty or reconciliation step.

Abstract: We study structure learning for linear Gaussian SEMs in the presence of
latent confounding. Existing continuous methods excel when errors are
independent, while deconfounding-first pipelines rely on pervasive factor
structure or nonlinearity. We propose \textsc{DECOR}, a single likelihood-based
and fully differentiable estimator that jointly learns a DAG and a correlated
noise model. Our theory gives simple sufficient conditions for global parameter
identifiability: if the mixed graph is bow free and the noise covariance has a
uniform eigenvalue margin, then the map from $(\B,\OmegaMat)$ to the
observational covariance is injective, so both the directed structure and the
noise are uniquely determined. The estimator alternates a smooth-acyclic graph
update with a convex noise update and can include a light bow complementarity
penalty or a post hoc reconciliation step. On synthetic benchmarks that vary
confounding density, graph density, latent rank, and dimension with $n<p$,
\textsc{DECOR} matches or outperforms strong baselines and is especially robust
when confounding is non-pervasive, while remaining competitive under
pervasiveness.

</details>


### [200] [ExGRPO: Learning to Reason from Experience](https://arxiv.org/abs/2510.02245)
*Runzhe Zhan,Yafu Li,Zhi Wang,Xiaoye Qu,Dongrui Liu,Jing Shao,Derek F. Wong,Yu Cheng*

Main category: cs.LG

TL;DR: ExGRPO proposes experiential grouping and a mixed-policy objective for RLVR to reuse valuable experiences; leveraging rollout correctness and entropy to rank experiences, it improves reasoning performance and stability across 1.5B-8B models.


<details>
  <summary>Details</summary>
Motivation: Address computational inefficiency and instability of on-policy RLVR due to discarding rollout experiences; investigate what makes a reasoning episode valuable for large reasoning models; enable reusable experiences to improve learning dynamics.

Method: Introduce ExGRPO (Experiential Group Relative Policy Optimization) that groups and prioritizes valuable experiences; uses rollout correctness and entropy as indicators of value; employs a mixed-policy objective combining exploration and experience exploitation; tested on five backbone models.

Result: Consistent performance gains over on-policy RLVR (average +3.5/7.6 points on math/general benchmarks); improved stability on both strong and weak models where on-policy methods fail.

Conclusion: Principled experience management is essential for efficient and scalable RLVR; the approach shows that selective exposure to valuable experiences can improve reasoning performance and training stability.

Abstract: Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm
for improving the reasoning ability of large language models. However, standard
on-policy training discards rollout experiences after a single update, leading
to computational inefficiency and instability. While prior work on RL has
highlighted the benefits of reusing past experience, the role of experience
characteristics in shaping learning dynamics of large reasoning models remains
underexplored. In this paper, we are the first to investigate what makes a
reasoning experience valuable and identify rollout correctness and entropy as
effective indicators of experience value. Based on these insights, we propose
ExGRPO (Experiential Group Relative Policy Optimization), a framework that
organizes and prioritizes valuable experiences, and employs a mixed-policy
objective to balance exploration with experience exploitation. Experiments on
five backbone models (1.5B-8B parameters) show that ExGRPO consistently
improves reasoning performance on mathematical/general benchmarks, with an
average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO
stabilizes training on both stronger and weaker models where on-policy methods
fail. These results highlight principled experience management as a key
ingredient for efficient and scalable RLVR.

</details>


### [201] [Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study](https://arxiv.org/abs/2510.02142)
*Lena Podina,Christina Humer,Alexandre Duval,Victor Schmidt,Ali Ramlaoui,Shahana Chatterjee,Yoshua Bengio,Alex Hernandez-Garcia,David Rolnick,F√©lix Therrien*

Main category: cs.LG

TL;DR: Catalyst GFlowNet: a ML-guided generative framework that designs catalytic crystal surfaces using formation and adsorption energy predictors; proof-of-concept for hydrogen evolution identifies platinum as the best known catalyst; future work extends to oxygen evolution and broader material space.


<details>
  <summary>Details</summary>
Motivation: Efficient, inexpensive energy storage is vital for integrating renewable energy; hydrogen energy storage relies on effective catalysts, but affordable high-performance catalysts remain a major challenge.

Method: Introduce Catalyst GFlowNet, a generative model steered by ML-based predictors of formation energy and adsorption energy to generate crystal surfaces as potential catalysts; apply to hydrogen evolution reaction (HER) and demonstrate platinum as the most efficient known catalyst.

Result: The framework successfully identifies platinum as the most efficient known catalyst for HER within the explored design space, validating the approach as a proof-of-concept for ML-guided catalyst discovery; sets the stage for extending the search to other reactions and materials.

Conclusion: A ML-driven generative framework can accelerate the discovery of efficient, cost-effective catalysts for energy storage; planned work includes extending to oxygen evolution reaction (OER) and expanding to new materials, potentially enabling faster identification of high-performance catalysts.

Abstract: Efficient and inexpensive energy storage is essential for accelerating the
adoption of renewable energy and ensuring a stable supply, despite fluctuations
in sources such as wind and solar. Electrocatalysts play a key role in hydrogen
energy storage (HES), allowing the energy to be stored as hydrogen. However,
the development of affordable and high-performance catalysts for this process
remains a significant challenge. We introduce Catalyst GFlowNet, a generative
model that leverages machine learning-based predictors of formation and
adsorption energy to design crystal surfaces that act as efficient catalysts.
We demonstrate the performance of the model through a proof-of-concept
application to the hydrogen evolution reaction, a key reaction in HES, for
which we successfully identified platinum as the most efficient known catalyst.
In future work, we aim to extend this approach to the oxygen evolution
reaction, where current optimal catalysts are expensive metal oxides, and open
the search space to discover new materials. This generative modeling framework
offers a promising pathway for accelerating the search for novel and efficient
catalysts.

</details>


### [202] [Policy Gradient Guidance Enables Test Time Control](https://arxiv.org/abs/2510.02148)
*Jianing Qi,Hao Tang,Zhigang Zhu*

Main category: cs.LG

TL;DR: PGG extends diffusion-model guidance to policy gradient RL by adding an unconditional branch and interpolating conditional/unconditional updates, enabling test-time control without retraining; a normalization term vanishes under advantage estimation; gamma-guided updates improve stability and efficiency, with mixed effects from dropout-based conditioning.


<details>
  <summary>Details</summary>
Motivation: Enable controllable online reinforcement learning with minimal retraining by importing guidance ideas from diffusion models into classical policy gradients, addressing stability and data efficiency in on-policy methods.

Method: Augment policy gradient with an unconditional branch and interpolate between conditional and unconditional updates. Derive that the extra normalization term cancels under advantage estimation, yielding a clean guided policy gradient update. Introduce a guidance strength parameter (gamma) and test-time control knob. Empirically evaluate on discrete and continuous benchmarks; analyze dropout-based conditioning effects and stability.

Result: In discrete tasks and low-sample regimes, dropout-conditioned guidance offers gains; in continuous control, dropout can destabilize. Using a modestly larger guidance (gamma>1) consistently improves stability, sample efficiency, and controllability across tasks.

Conclusion: Guidance concepts can be transferred from diffusion policies to standard on-policy RL, enabling controllable online reinforcement learning without retraining and opening new directions for controllable agents.

Abstract: We introduce Policy Gradient Guidance (PGG), a simple extension of
classifier-free guidance from diffusion models to classical policy gradient
methods. PGG augments the policy gradient with an unconditional branch and
interpolates conditional and unconditional branches, yielding a test-time
control knob that modulates behavior without retraining. We provide a
theoretical derivation showing that the additional normalization term vanishes
under advantage estimation, leading to a clean guided policy gradient update.
Empirically, we evaluate PGG on discrete and continuous control benchmarks. We
find that conditioning dropout-central to diffusion guidance-offers gains in
simple discrete tasks and low sample regimes, but dropout destabilizes
continuous control. Training with modestly larger guidance ($\gamma>1$)
consistently improves stability, sample efficiency, and controllability. Our
results show that guidance, previously confined to diffusion policies, can be
adapted to standard on-policy methods, opening new directions for controllable
online reinforcement learning.

</details>


### [203] [Reinforcement Learning with Action-Triggered Observations](https://arxiv.org/abs/2510.02149)
*Alexander Ryabchenko,Wenlong Mou*

Main category: cs.LG

TL;DR: Introduces Action-Triggered Sporadically Traceable MDPs (ATST-MDPs) where actions probabilistically trigger state observations; develops tailored Bellman equations, action-sequence learning, and ST-LSVI-UCB; proves a sublinear regret bound under linear MDP assumptions, showing efficient learning despite sporadic observations.


<details>
  <summary>Details</summary>
Motivation: Real-world RL settings often have observations arriving only when triggered by actions; this constraint complicates planning and learning; there is a need for theoretical foundations and practical algorithms that operate under sporadic, action-triggered observations.

Method: Formulates ATST-MDPs with a specified probability that an action triggers a state observation; derives action-triggered Bellman optimality equations; defines an action-sequence learning paradigm where agents commit to executing a sequence of actions until the next observation arrives; under the linear MDP assumption, shows value-functions admit linear representations in an induced action-sequence feature map; develops off-policy estimators with statistical error guarantees for these feature maps; introduces ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered settings.

Result: Establishes a theoretical foundation for learning with sporadic, action-triggered observations and demonstrates that efficient learning remains feasible under such observation constraints; derives regret bound tenilde O(sqrt(K d^3 (1- gamma)^{-3})) with K episodes, d feature dimension, and gamma discount factor.

Conclusion: The paper shows that learning under action-triggered, sporadic observations is theoretically sound and practically feasible, expanding the RL toolkit for environments with observation constraints and enabling efficient learning via the proposed ATST-MDP framework and ST-LSVI-UCB algorithm.

Abstract: We study reinforcement learning problems where state observations are
stochastically triggered by actions, a constraint common in many real-world
applications. This framework is formulated as Action-Triggered Sporadically
Traceable Markov Decision Processes (ATST-MDPs), where each action has a
specified probability of triggering a state observation. We derive tailored
Bellman optimality equations for this framework and introduce the
action-sequence learning paradigm in which agents commit to executing a
sequence of actions until the next observation arrives. Under the linear MDP
assumption, value-functions are shown to admit linear representations in an
induced action-sequence feature map. Leveraging this structure, we propose
off-policy estimators with statistical error guarantees for such feature maps
and introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered
settings. ST-LSVI-UCB achieves regret $\widetilde
O(\sqrt{Kd^3(1-\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the
feature dimension, and $\gamma$ the discount factor (per-step episode
non-termination probability). Crucially, this work establishes the theoretical
foundation for learning with sporadic, action-triggered observations while
demonstrating that efficient learning remains feasible under such observation
constraints.

</details>


### [204] [How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning](https://arxiv.org/abs/2510.02265)
*Yalin E. Sagduyu,Tugba Erpek,Kemal Davaslioglu,Sastry Kompella*

Main category: cs.LG

TL;DR: A model-free RL framework (Q-learning + DQN) enables a transmitter-receiver to learn to avoid reactive jamming by adapting power, modulation, and channel selection, achieving robust throughput without requiring prior channel/jammer models.


<details>
  <summary>Details</summary>
Motivation: Reactive jammers that adapt their channels and sensing thresholds create a dynamic, nonstationary environment. A learning-based approach is needed to sustain throughput without prior knowledge of the spectrum or jammer strategies.

Method: Formulated as a reinforcement-learning problem where the transmitter-receiver pair selects transmit power, modulation, and channel. Q-learning handles discrete jamming-event states, while Deep Q-Networks (DQN) handle continuous states based on received power. The study experiments with different reward functions and action sets to assess adaptability.

Result: The RL framework enables rapid adaptation to spectrum dynamics, sustaining high data rates even as channels and jamming policies change over time.

Conclusion: Model-free RL approaches are effective for mitigating reactive jamming in dynamic spectrum environments and can inform the design of adaptive physical-layer strategies without prior knowledge of the jammer

Abstract: This paper studies the problem of mitigating reactive jamming, where a jammer
adopts a dynamic policy of selecting channels and sensing thresholds to detect
and jam ongoing transmissions. The transmitter-receiver pair learns to avoid
jamming and optimize throughput over time (without prior knowledge of channel
conditions or jamming strategies) by using reinforcement learning (RL) to adapt
transmit power, modulation, and channel selection. Q-learning is employed for
discrete jamming-event states, while Deep Q-Networks (DQN) are employed for
continuous states based on received power. Through different reward functions
and action sets, the results show that RL can adapt rapidly to spectrum
dynamics and sustain high rates as channels and jamming policies change over
time.

</details>


### [205] [Flatness-Aware Stochastic Gradient Langevin Dynamics](https://arxiv.org/abs/2510.02174)
*Stefano Bruno,Youngsik Hwang,Jaehyeon An,Sotirios Sabanis,Dong-Young Lim*

Main category: cs.LG

TL;DR: Flatness-aware SGLD (fSGLD) biases SGD toward flat minima via random weight perturbations, with theoretical guarantees and competitive/generalization performance at SGD-like cost (about half that of SAM).


<details>
  <summary>Details</summary>
Motivation: Generalization in deep learning is linked to flat minima in the loss landscape, but classical SGLD lacks a mechanism to bias dynamics toward low-curvature solutions.

Method: fSGLD perturbs parameters with isotropic Gaussian noise (Random Weight Perturbation) and optimizes a randomized-smoothing objective that encodes curvature information. It couples inverse temperature and perturbation scale to ensure the invariant measure concentrates near global minimizers of a Hessian-trace-regularized loss, and provides non-asymptotic Wasserstein convergence guarantees.

Result: The authors prove invariant measures stay close to stationary measures on Hessian-trace-regularized global minimizers; provide best-known-rate non-asymptotic Wasserstein convergence and an excess-risk bound. Empirically, fSGLD achieves superior or comparable generalization and robustness vs baselines on noisy-label and large-scale vision tasks, with SGD-like cost (about half of SAM). Hessian-spectrum analyses show convergence to significantly flatter minima.

Conclusion: fSGLD offers a theoretically justified mechanism to prefer flat minima via random weight perturbations, yielding practical performance gains with computational cost close to SGD and substantially less than SAM.

Abstract: Generalization in deep learning is closely tied to the pursuit of flat minima
in the loss landscape, yet classical Stochastic Gradient Langevin Dynamics
(SGLD) offers no mechanism to bias its dynamics toward such low-curvature
solutions. This work introduces Flatness-Aware Stochastic Gradient Langevin
Dynamics (fSGLD), designed to efficiently and provably seek flat minima in
high-dimensional nonconvex optimization problems. At each iteration, fSGLD uses
the stochastic gradient evaluated at parameters perturbed by isotropic Gaussian
noise, commonly referred to as Random Weight Perturbation (RWP), thereby
optimizing a randomized-smoothing objective that implicitly captures curvature
information. Leveraging these properties, we prove that the invariant measure
of fSGLD stays close to a stationary measure concentrated on the global
minimizers of a loss function regularized by the Hessian trace whenever the
inverse temperature and the scale of random weight perturbation are properly
coupled. This result provides a rigorous theoretical explanation for the
benefits of random weight perturbation. In particular, we establish
non-asymptotic convergence guarantees in Wasserstein distance with the best
known rate and derive an excess-risk bound for the Hessian-trace regularized
objective. Extensive experiments on noisy-label and large-scale vision tasks,
in both training-from-scratch and fine-tuning settings, demonstrate that fSGLD
achieves superior or comparable generalization and robustness to baseline
algorithms while maintaining the computational cost of SGD, about half that of
SAM. Hessian-spectrum analysis further confirms that fSGLD converges to
significantly flatter minima.

</details>


### [206] [Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation](https://arxiv.org/abs/2510.02279)
*Mykyta Ielanskyi,Kajetan Schweighofer,Lukas Aichberger,Sepp Hochreiter*

Main category: cs.LG

TL;DR: Proposes robust, multi-faceted evaluation of uncertainty estimation in NLG to combat bias from flawed correctness signals; introduces multiple risk indicators, multi-judge averaging, structured/OOD/perturbation tasks, and Elo-based ranking.


<details>
  <summary>Details</summary>
Motivation: Hallucinations and confabulations in LLMs undermine reliability. Existing uncertainty evaluation relies on flawed approximate correctness signals, which can bias method ranking. There is a need for robust, unbiased evaluation protocols for uncertainty estimation in NLG.

Method: Introduce alternative risk indicators for risk correlation experiments; marginalize over multiple LLM-as-a-judge variants to reduce evaluation bias; apply evaluation to structured tasks, out-of-distribution (OOD) and perturbation detection tasks; propose an Elo rating system to summarize performance across many settings.

Result: The approach yields reduced evaluation biases in QA tasks by averaging over multiple LLM-judge variants; structured, OOD, and perturbation tasks provide robust risk indicators; an Elo rating is proposed as an objective summary of uncertainty estimation methods across diverse settings.

Conclusion: Robust evaluation frameworks for uncertainty estimation in NLG can mitigate biases caused by flawed correctness signals, with marginalization over judges, diverse task settings, and Elo-based summaries enhancing robustness and comparability of UE methods.

Abstract: Hallucinations are a common issue that undermine the reliability of large
language models (LLMs). Recent studies have identified a specific subset of
hallucinations, known as confabulations, which arise due to predictive
uncertainty of LLMs. To detect confabulations, various methods for estimating
predictive uncertainty in natural language generation (NLG) have been
developed. These methods are typically evaluated by correlating uncertainty
estimates with the correctness of generated text, with question-answering (QA)
datasets serving as the standard benchmark. However, commonly used approximate
correctness functions have substantial disagreement between each other and,
consequently, in the ranking of the uncertainty estimation methods. This allows
one to inflate the apparent performance of uncertainty estimation methods. We
propose using several alternative risk indicators for risk correlation
experiments that improve robustness of empirical assessment of UE algorithms
for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge
variants leads to reducing the evaluation biases. Furthermore, we explore
structured tasks as well as out of distribution and perturbation detection
tasks which provide robust and controllable risk indicators. Finally, we
propose to use an Elo rating of uncertainty estimation methods to give an
objective summarization over extensive evaluation settings.

</details>


### [207] [Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling](https://arxiv.org/abs/2510.02206)
*Daniel Gallo Fern√°ndez*

Main category: cs.LG

TL;DR: Poolformer replaces self-attention with pooling-based recurrent layers in a recursive SkipBlock architecture to efficiently model very long sequences, achieving faster training and improved perceptual metrics on raw audio, and outperforming SOTA models like SaShiMi and Mamba; potential extensions to text, vision, and multimodal settings.


<details>
  <summary>Details</summary>
Motivation: Self-attention scales quadratically with sequence length, limiting practicality for long sequences. A pooling-based, recurrent alternative aims to accelerate training, reduce memory, and capture long-range dependencies for audio and other modalities.

Method: Introduce Poolformer with SkipBlocks consisting of residual blocks, a down-pooling layer, a nested SkipBlock, an up-pooling layer, and additional residual blocks. Replace standard self-attention with pooling operations to exchange information along the time dimension. Use a recursive architecture and extensive experiments to validate design choices.

Result: Pooling accelerates training, improves perceptual metrics (FID and IS), and prevents overfitting. Long-range dependencies are captured by deeper layers, while shallow layers handle short-term features. On raw audio, Poolformer outperforms state-of-the-art models like SaShiMi and Mamba.

Conclusion: Pooling-based sequence modeling is a viable alternative to self-attention for long sequences. Potential applications include text and vision, as well as multi-modal scenarios, where Poolformer-based models (e.g., LLMs) could process dense representations of images and videos.

Abstract: Sequence-to-sequence models have become central in Artificial Intelligence,
particularly following the introduction of the transformer architecture. While
initially developed for Natural Language Processing, these models have
demonstrated utility across domains, including Computer Vision. Such models
require mechanisms to exchange information along the time dimension, typically
using recurrent or self-attention layers. However, self-attention scales
quadratically with sequence length, limiting its practicality for very long
sequences.
  We introduce Poolformer, a sequence-to-sequence model that replaces
self-attention with recurrent layers and incorporates pooling operations to
reduce sequence length. Poolformer is defined recursively using SkipBlocks,
which contain residual blocks, a down-pooling layer, a nested SkipBlock, an
up-pooling layer, and additional residual blocks. We conduct extensive
experiments to support our architectural choices.
  Our results show that pooling greatly accelerates training, improves
perceptual metrics (FID and IS), and prevents overfitting. Our experiments also
suggest that long-range dependencies are handled by deep layers, while shallow
layers take care of short-term features.
  Evaluated on raw audio, which naturally features long sequence lengths,
Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba.
Future directions include applications to text and vision, as well as
multi-modal scenarios, where a Poolformer-based LLM could effectively process
dense representations of images and videos.

</details>


### [208] [StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?](https://arxiv.org/abs/2510.02209)
*Yanxu Chen,Zijun Yao,Yantao Liu,Jin Ye,Jianing Yu,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: StockBench is a contamination-free, multi-month stock trading benchmark for evaluating LLM agents; results show most models underperform simple buy-and-hold, though some show potential in returns and risk management; released as an open-source resource.


<details>
  <summary>Details</summary>
Motivation: LLMs have shown promise as autonomous agents, but finance lacks dynamic, multi-step evaluation benchmarks. A realistic trading environment is needed to assess sequential decision-making, risk management, and real economic value beyond static knowledge tasks.

Method: A contamination-free benchmark where agents receive daily market signals (prices, fundamentals, news) and make sequential buy/sell/hold decisions over multi-month horizons. Evaluation uses financial metrics (cumulative return, maximum drawdown, Sortino ratio) across several models (proprietary and open-weight).

Result: Most LLM agents struggle to outperform a buy-and-hold strategy, but several models show potential to deliver higher returns and better risk control. Highlights that excelling at static financial knowledge does not guarantee trading success. StockBench is released as an open-source resource to support reproducibility.

Conclusion: StockBench provides a realistic, reusable framework for evaluating LLM-powered financial agents, revealing both challenges and opportunities in translating static knowledge into dynamic trading performance and guiding future research.

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
as autonomous agents, showing promise in reasoning, tool use, and sequential
decision-making. While prior benchmarks have evaluated LLM agents in domains
such as software engineering and scientific discovery, the finance domain
remains underexplored, despite its direct relevance to economic value and
high-stakes decision-making. Existing financial benchmarks primarily test
static knowledge through question answering, but they fall short of capturing
the dynamic and iterative nature of trading. To address this gap, we introduce
StockBench, a contamination-free benchmark designed to evaluate LLM agents in
realistic, multi-month stock trading environments. Agents receive daily market
signals -- including prices, fundamentals, and news -- and must make sequential
buy, sell, or hold decisions. Performance is assessed using financial metrics
such as cumulative return, maximum drawdown, and the Sortino ratio. Our
evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and
open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM
agents struggle to outperform the simple buy-and-hold baseline, several models
demonstrate the potential to deliver higher returns and manage risk more
effectively. These findings highlight both the challenges and opportunities in
developing LLM-powered financial agents, showing that excelling at static
financial knowledge tasks does not necessarily translate into successful
trading strategies. We release StockBench as an open-source resource to support
reproducibility and advance future research in this domain.

</details>


### [209] [Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks](https://arxiv.org/abs/2510.02286)
*Ruohao Guo,Afshin Oroojlooy,Roshan Sridhar,Miguel Ballesteros,Alan Ritter,Dan Roth*

Main category: cs.LG

TL;DR: DialTree-RPO proposes an on-policy reinforcement learning framework with tree search to autonomously discover diverse multi-turn attack strategies for LLM safety, achieving a notable improvement in attack success and revealing novel tactics.


<details>
  <summary>Details</summary>
Motivation: There is a gap in current safety evaluation: most work focuses on single-turn attacks or manual/templated data; multi-turn, strategically crafted dialogues reveal greater vulnerabilities and require scalable, automated exploration.

Method: Treat dialogue as a sequential decision-making process and use on-policy reinforcement learning integrated with tree search to explore diverse multi-turn attack trajectories without manual curated data; optimize policies to maximize attack success across turns.

Result: The approach yields over 25.9% higher adversarial success rate (ASR) across 10 target models compared to prior state-of-the-art methods and uncovers new attack strategies by learning optimal dialogue policies that maximize success over multiple turns.

Conclusion: An effective framework for automated discovery of multi-turn adversarial strategies, underscoring the importance of sequential planning and RL-based exploration in safety evaluation and suggesting directions to fortify LLM defenses against such attacks.

Abstract: Despite recent rapid progress in AI safety, current large language models
remain vulnerable to adversarial attacks in multi-turn interaction settings,
where attackers strategically adapt their prompts across conversation turns and
pose a more critical yet realistic challenge. Existing approaches that discover
safety vulnerabilities either rely on manual red-teaming with human experts or
employ automated methods using pre-defined templates and human-curated attack
data, with most focusing on single-turn attacks. However, these methods did not
explore the vast space of possible multi-turn attacks, failing to consider
novel attack trajectories that emerge from complex dialogue dynamics and
strategic conversation planning. This gap is particularly critical given recent
findings that LLMs exhibit significantly higher vulnerability to multi-turn
attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy
reinforcement learning framework integrated with tree search that autonomously
discovers diverse multi-turn attack strategies by treating the dialogue as a
sequential decision-making problem, enabling systematic exploration without
manually curated data. Through extensive experiments, our approach not only
achieves more than 25.9% higher ASR across 10 target models compared to
previous state-of-the-art approaches, but also effectively uncovers new attack
strategies by learning optimal dialogue policies that maximize attack success
across multiple turns.

</details>


### [210] [C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale Recommendation Systems](https://arxiv.org/abs/2510.02215)
*Mertcan Cokbas,Ziteng Liu,Zeyi Tao,Chengkai Zhang,Elder Veliz,Qin Huang,Ellie Wen,Huayu Li,Qiang Jin,Murat Duman,Benjamin Au,Guy Lebanon,Sagar Chordia*

Main category: cs.LG

TL;DR: C2AL introduces partially conflicting auxiliary learning to regulate shared embeddings via attention in factorization machines, aiming to preserve minority-cohort information and improve overall performance in heterogeneous, large-scale recommender data.


<details>
  <summary>Details</summary>
Motivation: Training under a single global objective can ignore minority/cohort-specific distributions in large, heterogeneous data. The attention mechanism offers a path to selectively share embeddings, but without principled regularization models risk dead neurons and biased attention toward dominant cohorts.

Method: Identify substructures with strong distributional contrast in the data and apply auxiliary learning using partially conflicting labels to regularize the shared representation and adapt attention accordingly, instead of heuristic reweighting or multi-task heads. Evaluated on massive production datasets across six SOTA models.

Result: The approach enables the factorization machine to capture fine-grained user-ad interactions, achieving up to a 0.16% reduction in normalized entropy overall and gains over 0.30% on targeted minority cohorts.

Conclusion: Partial-conflict auxiliary learning for attention-based sharing provides a principled regularization that preserves mutual information with minority cohorts while improving global performance, offering a scalable alternative to heuristic bias mitigation in large-scale recommender models.

Abstract: Training large-scale recommendation models under a single global objective
implicitly assumes homogeneity across user populations. However, real-world
data are composites of heterogeneous cohorts with distinct conditional
distributions. As models increase in scale and complexity and as more data is
used for training, they become dominated by central distribution patterns,
neglecting head and tail regions. This imbalance limits the model's learning
ability and can result in inactive attention weights or dead neurons. In this
paper, we reveal how the attention mechanism can play a key role in
factorization machines for shared embedding selection, and propose to address
this challenge by analyzing the substructures in the dataset and exposing those
with strong distributional contrast through auxiliary learning. Unlike previous
research, which heuristically applies weighted labels or multi-task heads to
mitigate such biases, we leverage partially conflicting auxiliary labels to
regularize the shared representation. This approach customizes the learning
process of attention layers to preserve mutual information with minority
cohorts while improving global performance. We evaluated C2AL on massive
production datasets with billions of data points each for six SOTA models.
Experiments show that the factorization machine is able to capture fine-grained
user-ad interactions using the proposed method, achieving up to a 0.16%
reduction in normalized entropy overall and delivering gains exceeding 0.30% on
targeted minority cohorts.

</details>


### [211] [Interactive Training: Feedback-Driven Neural Network Optimization](https://arxiv.org/abs/2510.02297)
*Wentao Zhang,Yang Young Lu,Yuntian Deng*

Main category: cs.LG

TL;DR: Real-time interactive training framework enabling feedback-driven interventions via a control server to adjust hyperparameters, data, and checkpoints during training, improving stability and adaptability.


<details>
  <summary>Details</summary>
Motivation: Address rigidity of traditional fixed-recipe training that lacks responsiveness to instabilities and emergent issues; enable dynamic responses from humans or automated agents.

Method: Proposes Interactive Training framework with a control server mediating communication between users/AI agents and the training process; supports dynamic adjustments to optimizer hyperparameters, training data, and model checkpoints; demonstrated through three case studies.

Result: Demonstrates superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving requirements; suggests potential for autonomous AI agents to monitor logs and proactively resolve instabilities.

Conclusion: Open-source framework enabling real-time, feedback-driven training, paving the way for a future where AI agents autonomously monitor and optimize training dynamics.

Abstract: Traditional neural network training typically follows fixed, predefined
optimization recipes, lacking the flexibility to dynamically respond to
instabilities or emerging training issues. In this paper, we introduce
Interactive Training, an open-source framework that enables real-time,
feedback-driven intervention during neural network training by human experts or
automated AI agents. At its core, Interactive Training uses a control server to
mediate communication between users or agents and the ongoing training process,
allowing users to dynamically adjust optimizer hyperparameters, training data,
and model checkpoints. Through three case studies, we demonstrate that
Interactive Training achieves superior training stability, reduced sensitivity
to initial hyperparameters, and improved adaptability to evolving user needs,
paving the way toward a future training paradigm where AI agents autonomously
monitor training logs, proactively resolve instabilities, and optimize training
dynamics.

</details>


### [212] [Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification](https://arxiv.org/abs/2510.02216)
*Zeqi Ye,Minshuo Chen*

Main category: cs.LG

TL;DR: This work develops a theory for conditional diffusion transformer imputation, offering sample‚Äësize bounds and uncertainty quantification, showing that missing-value patterns influence performance, validated by simulations and aided by a mixed‚Äëmasking training strategy.


<details>
  <summary>Details</summary>
Motivation: Practical time-series often have missing values. While diffusion-based generative imputation has shown empirical success, there is limited theoretical understanding of how well these models capture spatial/temporal dependencies and quantify uncertainty in the missing entries.

Method: Derives statistical sample complexity bounds via a novel approximation theory for conditional score functions using transformers; constructs tight confidence regions for missing values; analyzes how missing patterns affect efficiency; validates via simulations; proposes a mixed-masking training strategy to boost imputation performance.

Result: Theoretical insights indicate that imputation efficiency and accuracy depend on the missing data pattern. Tight confidence regions for missing values are constructed. Simulation validates the theory. The mixed-masking training strategy is proposed to enhance performance.

Conclusion: The paper advances the theoretical understanding of diffusion-based imputation by linking sample efficiency and uncertainty quantification to missingness patterns, and offers a practical training strategy (mixed masking) to improve performance, supported by simulation evidence.

Abstract: Imputation methods play a critical role in enhancing the quality of practical
time-series data, which often suffer from pervasive missing values. Recently,
diffusion-based generative imputation methods have demonstrated remarkable
success compared to autoregressive and conventional statistical approaches.
Despite their empirical success, the theoretical understanding of how well
diffusion-based models capture complex spatial and temporal dependencies
between the missing values and observed ones remains limited. Our work
addresses this gap by investigating the statistical efficiency of conditional
diffusion transformers for imputation and quantifying the uncertainty in
missing values. Specifically, we derive statistical sample complexity bounds
based on a novel approximation theory for conditional score functions using
transformers, and, through this, construct tight confidence regions for missing
values. Our findings also reveal that the efficiency and accuracy of imputation
are significantly influenced by the missing patterns. Furthermore, we validate
these theoretical insights through simulation and propose a mixed-masking
training strategy to enhance the imputation performance.

</details>


### [213] [Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models](https://arxiv.org/abs/2510.02224)
*Ethan Baron,Boris Oreshkin,Ruijun Ma,Hanyu Zhang,Kari Torkkola,Michael W. Mahoney,Andrew Gordon Wilson,Tatiana Konstantinova*

Main category: cs.LG

TL;DR: Proposes a copula-based method to generate correlated multi-step forecast sample paths in one forward pass, achieving speedups over autoregressive sampling while improving path quality.


<details>
  <summary>Details</summary>
Motivation: Current time series foundation models predict only independent marginal distributions per time step, lacking a joint predictive distribution. Generating correlated sample paths via autoregressive sampling is computationally expensive and prone to error accumulation (snowballing).

Method: Utilize a copula to couple the per-step marginals from existing multi-step foundation models to produce coherent joint sample paths in a single forward pass, avoiding iterative autoregressive sampling.

Result: The approach is orders of magnitude faster than autoregressive sampling and yields improved sample-path quality by mitigating snowballing errors.

Conclusion: A copula-based framework enables efficient, accurate generation of joint multi-step forecast sample paths from time series foundation models without iterative sampling.

Abstract: Many time series applications require access to multi-step forecast
trajectories in the form of sample paths. Recently, time series foundation
models have leveraged multi-step lookahead predictions to improve the quality
and efficiency of multi-step forecasts. However, these models only predict
independent marginal distributions for each time step, rather than a full joint
predictive distribution. To generate forecast sample paths with realistic
correlation structures, one typically resorts to autoregressive sampling, which
can be extremely expensive. In this paper, we present a copula-based approach
to efficiently generate accurate, correlated sample paths from existing
multi-step time series foundation models in one forward pass. Our copula-based
approach generates correlated sample paths orders of magnitude faster than
autoregressive sampling, and it yields improved sample path quality by
mitigating the snowballing error phenomenon.

</details>


### [214] [Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive](https://arxiv.org/abs/2510.02305)
*Tyler Farghly,Peter Potaptchik,Samuel Howard,George Deligiannidis,Jakiw Pidstrigach*

Main category: cs.LG

TL;DR: Smoothing the score function in score-matching induces manifold-aligned regularisation, explaining diffusion models' generalisation and enabling control of the generalisation manifold via smoothing.


<details>
  <summary>Details</summary>
Motivation: To explain why diffusion models generalise well by relating it to the manifold hypothesis and score-matching regularisation.

Method: Theoretical and empirical analysis of smoothing minimisers of the empirical score matching objective; evaluation of smoothing in the log-density domain and its geometric effect on the data manifold.

Result: Smoothing the score function yields smoothing tangential to the data manifold; the learned diffusion generalisation manifold can be steered by the choice of smoothing.

Conclusion: The work provides evidence that implicit regularisation through score-matching smoothing explains and controls diffusion model generalisation on low-dimensional manifolds.

Abstract: Diffusion models have achieved state-of-the-art performance, demonstrating
remarkable generalisation capabilities across diverse domains. However, the
mechanisms underpinning these strong capabilities remain only partially
understood. A leading conjecture, based on the manifold hypothesis, attributes
this success to their ability to adapt to low-dimensional geometric structure
within the data. This work provides evidence for this conjecture, focusing on
how such phenomena could result from the formulation of the learning problem
through score matching. We inspect the role of implicit regularisation by
investigating the effect of smoothing minimisers of the empirical score
matching objective. Our theoretical and empirical results confirm that
smoothing the score function -- or equivalently, smoothing in the log-density
domain -- produces smoothing tangential to the data manifold. In addition, we
show that the manifold along which the diffusion model generalises can be
controlled by choosing an appropriate smoothing.

</details>


### [215] [xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity](https://arxiv.org/abs/2510.02228)
*Maximilian Beck,Kajetan Schweighofer,Sebastian B√∂ck,Sebastian Lehner,Sepp Hochreiter*

Main category: cs.LG

TL;DR: xLSTM shows favorable scaling compared to Transformers for LLMs, with its advantage increasing as context length grows, across compute-optimal and over-training regimes.


<details>
  <summary>Details</summary>
Motivation: To understand how alternative architectures like xLSTM scale relative to Transformers under compute budgets and varying context lengths, to inform model design and deployment decisions for large-scale language models.

Method: Systematic scaling analysis comparing Transformers and xLSTM across model sizes (80M‚Äì7B) and training tokens (2B‚Äì2T) using compute-optimal and over-training regimes. Uses IsoFLOP and parametric fit approaches, examines optimal model size versus context length, and analyzes inference-time scaling.

Result: xLSTM demonstrates favorable scaling relative to Transformers in typical LLM training and inference scenarios, with its advantage increasing as the context length (both training and inference) grows.

Conclusion: xLSTM‚Äôs linear context-length scalability yields stronger scaling advantages than Transformers in practical LLM settings, suggesting it as a competitive alternative for future model design and deployment, especially in long-context regimes.

Abstract: Scaling laws play a central role in the success of Large Language Models
(LLMs), enabling the prediction of model performance relative to compute
budgets prior to training. While Transformers have been the dominant
architecture, recent alternatives such as xLSTM offer linear complexity with
respect to context length while remaining competitive in the billion-parameter
regime. We conduct a comparative investigation on the scaling behavior of
Transformers and xLSTM along the following lines, providing insights to guide
future model design and deployment. First, we study the scaling behavior for
xLSTM in compute-optimal and over-training regimes using both IsoFLOP and
parametric fit approaches on a wide range of model sizes (80M-7B) and number of
training tokens (2B-2T). Second, we examine the dependence of optimal model
sizes on context length, a pivotal aspect that was largely ignored in previous
work. Finally, we analyze inference-time scaling characteristics. Our findings
reveal that in typical LLM training and inference scenarios, xLSTM scales
favorably compared to Transformers. Importantly, xLSTM's advantage widens as
training and inference contexts grow.

</details>


### [216] [PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks](https://arxiv.org/abs/2510.02236)
*Ricardo Misael Ayala Molina,Hyame Assem Alameddine,Makan Pourzandi,Chadi Assi*

Main category: cs.LG

TL;DR: A defense against DSM attacks in 5G network slices using a PU-learning based detector that combines LSTM autoencoders and K-means, achieving high F1-scores even with contaminated training data.


<details>
  <summary>Details</summary>
Motivation: DSM attacks via inter-slice switching threaten QoS and uptime in 5G network slicing; conventional supervised methods struggle when training data is contaminated, necessitating robust anomaly detection leveraging unlabeled data.

Method: Positive Unlabeled Learning (PUL) framework featuring Long Short-Term Memory Autoencoders and K-Means clustering, using 3GPP KPIs and performance counters as features; evaluated on a 5G testbed with free5GC and UERANSIM.

Result: F1-scores exceed 98.50% on training data with 10%-40% attack contamination; outperforms Inter-Slice Defender and other PU-based baselines using OCSVM/RF/XGBoost.

Conclusion: The proposed PUL-Inter-Slice Defender provides robust DSM attack detection in 5G NSs under label contamination and data variability, with strong performance advantages over existing PU-based methods.

Abstract: Network Slices (NSs) are virtual networks operating over a shared physical
infrastructure, each designed to meet specific application requirements while
maintaining consistent Quality of Service (QoS). In Fifth Generation (5G)
networks, User Equipment (UE) can connect to and seamlessly switch between
multiple NSs to access diverse services. However, this flexibility, known as
Inter-Slice Switching (ISS), introduces a potential vulnerability that can be
exploited to launch Distributed Slice Mobility (DSM) attacks, a form of
Distributed Denial of Service (DDoS) attack. To secure 5G networks and their
NSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an
anomaly detection solution that leverages Positive Unlabeled Learning (PUL) and
incorporates a combination of Long Short-Term Memory Autoencoders and K-Means
clustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership
Project (3GPP) key performance indicators and performance measurement counters
as features for its machine learning models to detect DSM attack variants while
maintaining robustness in the presence of contaminated training data. When
evaluated on data collected from our 5G testbed based on the open-source
free5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;
PUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training
datasets with 10% to 40% attack contamination, consistently outperforming its
counterpart Inter-Slice Defender and other PUL based solutions combining
One-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.

</details>


### [217] [Drop-Muon: Update Less, Converge Faster](https://arxiv.org/abs/2510.02239)
*Kaja Gruntkowska,Yassine Maziane,Zheng Qu,Peter Richt√°rik*

Main category: cs.LG

TL;DR: Drop-Muon introduces randomized, layer-wise updates (Drop-Muon) with non-Euclidean updates for progressive training, showing faster convergence than full-network updates under layer-wise smoothness assumptions; provides convergence guarantees in stochastic and non-smooth settings and supports a cost argument suggesting full updates are suboptimal except in specific conditions; empirical CNNs show up to 1.4x faster wall-clock time at equal accuracy.


<details>
  <summary>Details</summary>
Motivation: Question the conventional wisdom that full-network updates are always optimal; seek more sample- and time-efficient training by updating only a subset of layers per step with principled scheduling and non-Euclidean updates.

Method: Propose Drop-Muon: a randomized progressive training framework that updates a subset of layers per step based on a randomized schedule and applies non-Euclidean (e.g., non-Euclidean geometry-based) updates. Provide convergence guarantees under layer-wise smoothness and layer-wise (L^0, L^1)-smoothness for both deterministic and stochastic gradients. Conduct a cost analysis comparing full-network vs. partial updates. Validate with controlled CNN experiments.

Result: Theoretical results establish convergence guarantees for progressive training in stochastic and non-smooth regimes; cost analysis indicates full-network updates are optimal only under specific relationships between layer smoothness constants. Empirical CNN experiments show Drop-Muon consistently outperforms full Muon, achieving the same accuracy up to 1.4√ó faster in wall-clock time.

Conclusion: Layer-wise, randomized progressive training with non-Euclidean updates can outperform traditional full-network optimizers, offering a theoretically grounded and practically efficient alternative for training large-scale models. This challenges the default of updating all layers each step and highlights the importance of scheduling and geometry-aware updates.

Abstract: Conventional wisdom in deep learning optimization dictates updating all
layers at every step-a principle followed by all recent state-of-the-art
optimizers such as Muon. In this work, we challenge this assumption, showing
that full-network updates can be fundamentally suboptimal, both in theory and
in practice. We introduce a non-Euclidean Randomized Progressive Training
method-Drop-Muon-a simple yet powerful framework that updates only a subset of
layers per step according to a randomized schedule, combining the efficiency of
progressive training with layer-specific non-Euclidean updates for top-tier
performance. We provide rigorous convergence guarantees under both layer-wise
smoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and
stochastic gradient settings, marking the first such results for progressive
training in the stochastic and non-smooth regime. Our cost analysis further
reveals that full-network updates are not optimal unless a very specific
relationship between layer smoothness constants holds. Through controlled CNN
experiments, we empirically demonstrate that Drop-Muon consistently outperforms
full-network Muon, achieving the same accuracy up to $1.4\times$ faster in
wall-clock time. Together, our results suggest a shift in how large-scale
models can be efficiently trained, challenging the status quo and offering a
highly efficient, theoretically grounded alternative to full-network updates.

</details>


### [218] [Transformers Discover Molecular Structure Without Graph Priors](https://arxiv.org/abs/2510.02259)
*Tobias Kreiman,Yutong Bai,Fadi Atieh,Elizabeth Weaver,Eric Qu,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: Transformers trained directly on Cartesian coordinates can match fixed-graph GNNs for molecular energies and forces, revealing emergent physically meaningful patterns and scalable performance without hard-coded graph priors.


<details>
  <summary>Details</summary>
Motivation: Probe whether unmodified Transformers can learn molecular energies/forces without predefined graphs or physical priors, and compare to state-of-the-art equivariant GNNs under matched compute budgets.

Method: Train a standard Transformer on Cartesian coordinates to predict energies and forces; compare to a state-of-the-art equivariant GNN on the OMol25 dataset under matched training compute; analyze learned patterns (e.g., attention) and scaling behavior.

Result: The Transformer achieves competitive energy and force MAEs under matched compute on OMol25; attention weights decay inversely with interatomic distance and adapt across molecular environments without fixed biases; scaling with training resources yields predictable improvements consistent with broader scaling laws.

Conclusion: Hard-coded graph inductive biases are not strictly necessary for competitive molecular modeling; Transformers can learn physically meaningful representations and offer scalable, standardized architectures for molecular modeling.

Abstract: Graph Neural Networks (GNNs) are the dominant architecture for molecular
machine learning, particularly for molecular property prediction and machine
learning interatomic potentials (MLIPs). GNNs perform message passing on
predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor
scheme. While this design aligns with the locality present in many molecular
tasks, a hard-coded graph can limit expressivity due to the fixed receptive
field and slows down inference with sparse graph operations. In this work, we
investigate whether pure, unmodified Transformers trained directly on Cartesian
coordinates$\unicode{x2013}$without predefined graphs or physical
priors$\unicode{x2013}$can approximate molecular energies and forces. As a
starting point for our analysis, we demonstrate how to train a Transformer to
competitive energy and force mean absolute errors under a matched training
compute budget, relative to a state-of-the-art equivariant GNN on the OMol25
dataset. We discover that the Transformer learns physically consistent
patterns$\unicode{x2013}$such as attention weights that decay inversely with
interatomic distance$\unicode{x2013}$and flexibly adapts them across different
molecular environments due to the absence of hard-coded biases. The use of a
standard Transformer also unlocks predictable improvements with respect to
scaling training resources, consistent with empirical scaling laws observed in
other domains. Our results demonstrate that many favorable properties of GNNs
can emerge adaptively in Transformers, challenging the necessity of hard-coded
graph inductive biases and pointing toward standardized, scalable architectures
for molecular modeling.

</details>


### [219] [Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps](https://arxiv.org/abs/2510.02274)
*Kyoungjun Park,Yifan Yang,Changhan Ge,Lili Qiu,Shiqi Jiang*

Main category: cs.LG

TL;DR: Diffusion^2 uses diffusion-based modeling with a 3D point-cloud RF-3D Encoder to predict RF propagation across Wi-Fi to millimeter-wave bands, achieving 1.9 dB error and 27x speedups over prior methods on synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: RF propagation modeling is essential for wireless diagnosis, deployment, and optimization, and current RGB-based methods struggle in environments with complex obstacles due to absorption and reflection. A physics-informed, scalable approach that covers a wide frequency range is needed.

Method: A diffusion-based framework (Diffusion^2) that operates on 3D point clouds. It uses an RF-3D Encoder to capture geometry and signal-specific features, followed by multi-scale embedding to simulate RF dissemination. The model is trained/evaluated on synthetic and real-world measurements across multiple frequency bands.

Result: Diffusion^2 accurately estimates RF signal behavior across Wi-Fi to millimeter-wave bands under various environmental conditions, with an error of about 1.9 dB and speed improvements of ~27√ó over existing methods.

Conclusion: Diffusion^2 represents a significant advancement in RF propagation modeling by integrating diffusion processes with detailed 3D geometry representations, unlocking more accurate and efficient wireless analysis and deployment in diverse environments.

Abstract: Modeling radio frequency (RF) signal propagation is essential for
understanding the environment, as RF signals offer valuable insights beyond the
capabilities of RGB cameras, which are limited by the visible-light spectrum,
lens coverage, and occlusions. It is also useful for supporting wireless
diagnosis, deployment, and optimization. However, accurately predicting RF
signals in complex environments remains a challenge due to interactions with
obstacles such as absorption and reflection. We introduce Diffusion^2, a
diffusion-based approach that uses 3D point clouds to model the propagation of
RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.
To effectively capture RF-related features from 3D data, we present the RF-3D
Encoder, which encapsulates the complexities of 3D geometry along with
signal-specific details. These features undergo multi-scale embedding to
simulate the actual RF signal dissemination process. Our evaluation, based on
synthetic and real-world measurements, demonstrates that Diffusion^2 accurately
estimates the behavior of RF signals in various frequency bands and
environmental conditions, with an error margin of just 1.9 dB and 27x faster
than existing methods, marking a significant advancement in the field. Refer to
https://rfvision-project.github.io/ for more information.

</details>


### [220] [Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks](https://arxiv.org/abs/2510.02278)
*Fedor Velikonivtsev,Oleg Platonov,Gleb Bazhenov,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: New city-scale road-graph benchmarks (up to ~100k road segments) with rich features for traffic forecasting; proposes a scalable GNN approach that omits a dedicated temporal module, addressing scalability issues in existing neural spatiotemporal models.


<details>
  <summary>Details</summary>
Motivation: Current traffic forecasting benchmarks are limited: missing road connectivity, sparse/limited road properties, and datasets far smaller than real-world urban networks. There is a need for realistic, dense benchmarks and scalable models.

Method: Release two city-scale road network datasets with detailed traffic volume and speed features; evaluate existing neural spatiotemporal GNNs on these datasets; propose a scalable GNN architecture without a specialized temporal processing component to improve scalability and performance.

Result: Demonstrates that state-of-the-art neural spatiotemporal models struggle to scale to large city-scale datasets; the proposed approach achieves better scalability and stronger forecasting performance on the new benchmarks; datasets enable holistic modeling of traffic dynamics.

Conclusion: Providing large, realistic benchmarks and a scalable modeling approach will advance traffic forecasting research and facilitate more effective evaluation and deployment in real urban settings.

Abstract: Traffic forecasting on road networks is a complex task of significant
practical importance that has recently attracted considerable attention from
the machine learning community, with spatiotemporal graph neural networks
(GNNs) becoming the most popular approach. The proper evaluation of traffic
forecasting methods requires realistic datasets, but current publicly available
benchmarks have significant drawbacks, including the absence of information
about road connectivity for road graph construction, limited information about
road properties, and a relatively small number of road segments that falls
short of real-world applications. Further, current datasets mostly contain
information about intercity highways with sparsely located sensors, while city
road networks arguably present a more challenging forecasting task due to much
denser roads and more complex urban traffic patterns. In this work, we provide
a more complete, realistic, and challenging benchmark for traffic forecasting
by releasing datasets representing the road networks of two major cities, with
the largest containing almost 100,000 road segments (more than a 10-fold
increase relative to existing datasets). Our datasets contain rich road
features and provide fine-grained data about both traffic volume and traffic
speed, allowing for building more holistic traffic forecasting systems. We show
that most current implementations of neural spatiotemporal models for traffic
forecasting have problems scaling to datasets of our size. To overcome this
issue, we propose an alternative approach to neural traffic forecasting that
uses a GNN without a dedicated module for temporal sequence processing, thus
achieving much better scalability, while also demonstrating stronger
forecasting performance. We hope our datasets and modeling insights will serve
as a valuable resource for research in traffic forecasting.

</details>


### [221] [Knowledge Distillation Detection for Open-weights Models](https://arxiv.org/abs/2510.02302)
*Qin Shi,Amber Yijia Zheng,Qifan Song,Raymond A. Yeh*

Main category: cs.LG

TL;DR: A model-agnostic framework detects whether a student model has been distilled from a teacher using only student weights and the teacher API, via data-free input synthesis and statistical scoring; applicable to classification and generative models, with strong empirical gains on benchmarks; code available.


<details>
  <summary>Details</summary>
Motivation: Growing concerns about model provenance and unauthorized replication through distillation in settings where training data and full access to teachers are limited.

Method: A model-agnostic pipeline that combines data-free input synthesis with statistical score computation to detect distillation, applicable to both classification and generative models.

Result: Significant detection performance gains over strong baselines: 59.6% improvement on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image generation.

Conclusion: The proposed framework enables practical, data-free detection of knowledge distillation across model types, contributing to provenance verification and defenses against distillation-based replication.

Abstract: We propose the task of knowledge distillation detection, which aims to
determine whether a student model has been distilled from a given teacher,
under a practical setting where only the student's weights and the teacher's
API are available. This problem is motivated by growing concerns about model
provenance and unauthorized replication through distillation. To address this
task, we introduce a model-agnostic framework that combines data-free input
synthesis and statistical score computation for detecting distillation. Our
approach is applicable to both classification and generative models.
Experiments on diverse architectures for image classification and text-to-image
generation show that our method improves detection accuracy over the strongest
baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image
generation. The code is available at
https://github.com/shqii1j/distillation_detection.

</details>


### [222] [Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization](https://arxiv.org/abs/2510.02308)
*Dhruv Kohli,Sawyer J. Robertson,Gal Mishne,Alexander Cloninger*

Main category: cs.LG

TL;DR: LEGO uses gradients of low-frequency Laplacian eigenvectors to orthogonalize local tangent spaces, yielding robust tangent estimates against noise compared to LPCA.


<details>
  <summary>Details</summary>
Motivation: LPCA's sensitivity to noise and the need to pick neighborhood size without prior geometric/noise knowledge hinder reliable tangent-space estimation; a method leveraging global data structure is desirable.

Method: Construct a graph Laplacian from the data, obtain low-frequency eigenvectors, compute their gradients, and orthogonalize these gradients to estimate the tangent space at each data point. The paper provides two theoretical justifications: (1) differential-geometric analysis in a tubular neighborhood showing alignment of gradients with the tangent bundle; (2) random-matrix theory showing low-frequency eigenvectors are robust to sub-Gaussian noise.

Result: LEGO yields tangent-space estimates that are more robust to noise than LPCA, leading to improvements in downstream tasks such as manifold learning, boundary detection, and local intrinsic-dimension estimation.

Conclusion: Spectral gradient orthogonalization provides a robust, globally informed approach to tangent-space estimation, with solid theoretical backing and practical benefits across several data-analysis tasks.

Abstract: Estimating the tangent spaces of a data manifold is a fundamental problem in
data analysis. The standard approach, Local Principal Component Analysis
(LPCA), struggles in high-noise settings due to a critical trade-off in
choosing the neighborhood size. Selecting an optimal size requires prior
knowledge of the geometric and noise characteristics of the data that are often
unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector
Gradient Orthogonalization (LEGO), that utilizes the global structure of the
data to guide local tangent space estimation. Instead of relying solely on
local neighborhoods, LEGO estimates the tangent space at each data point by
orthogonalizing the gradients of low-frequency eigenvectors of the graph
Laplacian. We provide two theoretical justifications of our method. First, a
differential geometric analysis on a tubular neighborhood of a manifold shows
that gradients of the low-frequency Laplacian eigenfunctions of the tube align
closely with the manifold's tangent bundle, while an eigenfunction with high
gradient in directions orthogonal to the manifold lie deeper in the spectrum.
Second, a random matrix theoretic analysis also demonstrates that low-frequency
eigenvectors are robust to sub-Gaussian noise. Through comprehensive
experiments, we demonstrate that LEGO yields tangent space estimates that are
significantly more robust to noise than those from LPCA, resulting in marked
improvements in downstream tasks such as manifold learning, boundary detection,
and local intrinsic dimension estimation.

</details>


### [223] [KaVa: Latent Reasoning via Compressed KV-Cache Distillation](https://arxiv.org/abs/2510.02312)
*Anna Kuzina,Maciej Pioro,Paul N. Whatmough,Babak Ehteshami Bejnordi*

Main category: cs.LG

TL;DR: KaVa distills knowledge from a teacher‚Äôs compressed KV-cache into a latent-reasoning student via self-distillation, using continuous latent tokens to align stepwise KV trajectories for efficient latent reasoning.


<details>
  <summary>Details</summary>
Motivation: To overcome the high cost and artifacts of explicit chain-of-thought traces by providing scalable supervision for latent reasoning, addressing the lack of effective guidance for latent traces on complex natural-language reasoning tasks.

Method: Distill from a teacher‚Äôs compressed KV-cache into a latent-reasoning student using continuous latent tokens. Align stepwise KV trajectories through self-distillation, treating abstract KV-cache knowledge as supervisory signal despite lacking direct token correspondence. Evaluate against strong latent baselines across natural-language traces and scale to larger backbones.

Result: KaVa consistently outperforms strong latent baselines, exhibits less degradation when moving from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency.

Conclusion: Compressed KV-cache distillation provides a scalable supervision signal for latent reasoning, enabling CoT-level accuracy with latent inference efficiency and improving deployability.

Abstract: Large Language Models (LLMs) excel at multi-step reasoning problems with
explicit chain-of-thought (CoT), but verbose traces incur significant
computational costs and memory overhead, and often carry redundant, stylistic
artifacts. Latent reasoning has emerged as an efficient alternative that
internalizes the thought process, but it suffers from a critical lack of
supervision, limiting its effectiveness on complex, natural-language reasoning
traces. In this work, we propose KaVa, the first framework that bridges this
gap by distilling knowledge directly from a compressed KV-cache of the teacher
into a latent-reasoning student via self-distillation, leveraging the
representational flexibility of continuous latent tokens to align stepwise KV
trajectories. We show that the abstract, unstructured knowledge within
compressed KV-cache, which lacks direct token correspondence, can serve as a
rich supervisory signal for a latent reasoning student. Empirically, the
approach consistently outperforms strong latent baselines, exhibits markedly
smaller degradation from equation-only to natural-language traces, and scales
to larger backbones while preserving efficiency. These results establish
compressed KV-cache distillation as a scalable supervision signal for latent
reasoning, combining the accuracy of CoT-trained teachers with the efficiency
and deployability of latent inference.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [224] [Minimum Selective Subset on Unit Disk Graphs and Circle Graphs](https://arxiv.org/abs/2510.01931)
*Bubai Manna*

Main category: cs.CG

TL;DR: MSS seeks a minimum-size selective subset of vertices under a per-color locality constraint; the problem is hard to approximate on general graphs (log-APX-hard even for c=2) and has a PTAS in unit disk graphs, with MSS NP-complete there for arbitrary c; also log-APX-hard on circle graphs (c=2).


<details>
  <summary>Details</summary>
Motivation: To understand the computational and approximation complexity of selecting a small subset of vertices that ensures, within each color class, every vertex has a same-color neighbor either in the subset or outside its color class.

Method: Prove hardness via reductions showing log-APX-hardness on general graphs (and circle graphs); establish a PTAS for unit disk graphs that does not rely on an explicit geometric embedding and extends to arbitrary c; prove NP-completeness of MSS in unit disk graphs for arbitrary c.

Result: Hardness results: MSS is log-APX-hard on general graphs, even when c=2; no PTAS unless P=NP. Positive result: a PTAS exists for unit disk graphs (not requiring geometric input) for any c. MSS is NP-complete in unit disk graphs for arbitrary c. Additional result: MSS is log-APX-hard on circle graphs, even for c=2.

Conclusion: MSS exhibits strong inapproximability in general graphs, with limited positive tractability in structured geometric graphs like unit disk graphs (where a PTAS exists). The problem remains hard in circle graphs, and NP-complete in unit disk graphs for arbitrary color counts, indicating nuanced complexity across graph classes.

Abstract: In a connected simple graph G = (V(G),E(G)), each vertex is assigned one of c
colors, where V(G) can be written as a union of a total of c subsets
V_{1},...,V_{c} and V_{i} denotes the set of vertices of color i. A subset S of
V(G) is called a selective subset if, for every i, every vertex v in V_{i} has
at least one nearest neighbor in $S \cup (V(G) \setminus V_{i})$ that also lies
in V_{i}. The Minimum Selective Subset (MSS) problem asks for a selective
subset of minimum size.
  We show that the MSS problem is log-APX-hard on general graphs, even when
c=2. As a consequence, the problem does not admit a polynomial-time
approximation scheme (PTAS) unless P = NP. On the positive side, we present a
PTAS for unit disk graphs, which works without requiring a geometric
representation and applies for arbitrary c. We further prove that MSS remains
NP-complete in unit disk graphs for arbitrary c. In addition, we show that the
MSS problem is log-APX-hard on circle graphs, even when c=2.

</details>


### [225] [Bifurcation: How to Explore a Tree](https://arxiv.org/abs/2510.01939)
*Sariel Har-Peled*

Main category: cs.CG

TL;DR: A bifurcation-based approach to parametric search recast as a tree exploration problem yields near-optimal efficiency on a specific tree with k branching nodes: time O(n sqrt(k)) and oracle calls O(sqrt(k) + log n).


<details>
  <summary>Details</summary>
Motivation: To accelerate parametric search by exploiting a reversible exploration (bifurcation) framework that leverages cheap rollbacks and near-linear decider time, potentially beating standard distance-selection lower bounds.

Method: Model the problem as exploring a binary implicit tree of height n with k internal two-child nodes; use a bifurcation-based algorithm that minimizes oracle calls by performing selective exploration and rollback, achieving O(n sqrt(k)) time and O(sqrt(k) + log n) oracle calls.

Result: Improved bound from O(n sqrt(k)) time and O(sqrt(k) log n) oracle calls to O(n sqrt(k)) time and O(sqrt(k) + log n) oracle calls; matching lower bounds under certain assumptions.

Conclusion: Viewing bifurcation as a tree exploration yields a simple, potentially broadly useful algorithmic tool for parametric search, with provable efficiency gains in the stated model and potential for independent application.

Abstract: Avraham et al. [AFK+15] presented an alternative approach to parametric
search, called \emph{bifurcation}, that performs faster under certain
circumstances. Intuitively, when the underlying decider execution can be rolled
back cheaply and the decider has a near-linear running time. For some problems,
this leads to fast algorithms that beat the seemingly natural lower bound
arising from distance selection.
  Bifurcation boils down to a tree exploration problem. You are given a binary
(unfortunately implicit) tree of height $n$ and $k$ internal nodes with two
children (all other internal nodes have a single child), and assume each node
has an associated parameter value. These values are sorted in the inorder
traversal of the tree. Assume there is (say) a node (not necessarily a leaf)
that is the target node that the exploration needs to discover.
  The player starts from the root. At each step, the player can move to
adjacent nodes to the current location (i.e., one of the children or the
parent). Alternatively, the player can call an oracle on the current node,
which returns either that it is the target (thus, mission accomplished!) or
whether the target value is strictly smaller or larger than the current one.
  A naive algorithm explores the whole tree, in $O(n k)$ time, then performs
$O(\log k n)$ calls to the oracle to find the desired leaf. Avraham \etal
showed that this can be improved to $O(n \sqrt{k} )$ time, and $O( \sqrt{k}
\log n)$ oracle calls.
  Here, we improve this to $O(n \sqrt{k} )$ time, with only $ O( \sqrt{k} +
\log n)$ oracle calls. We also show matching lower bounds, under certain
assumptions. We believe our interpretation of bifurcation as a tree exploration
problem, and the associated algorithm, are of independent interest.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [226] [OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models](https://arxiv.org/abs/2510.01253)
*Jianzhang Zhang,Jialong Zhou,Chuang Liu*

Main category: cs.AI

TL;DR: OR-Toolformer fine-tunes Llama-3.1-8B-Instruct via a semi-automatic data synthesis pipeline and solver augmentation to produce API calls, achieving strong OR performance and zero-shot generalization without relying on closed APIs.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns with closed-source APIs for optimization and reasoning tasks and the high compute costs of training open models from scratch; enable accurate, generalizable OR solving with open models.

Method: Fine-tune Llama-3.1-8B-Instruct using a semi-automatic data synthesis pipeline to generate diverse OR problem-answer pairs, and augment the model with external solvers to produce API calls.

Result: On three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot evaluation on two unseen OR problem types, it attains 54% average accuracy, a 21-point improvement over the strongest baseline.

Conclusion: Tool-augmented fine-tuning of LLMs is effective for accurate and generalizable OR problem modeling and solving, offering privacy-preserving and cost-efficient alternatives to closed APIs.

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning, but
reliance on closed-source APIs for OR tasks raises privacy concerns, and
training open-source models from scratch incurs high compute costs. We
introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a
semi-automatic data synthesis pipeline that generates diverse OR problem-answer
pairs and augments the model with external solvers to produce API calls. On
three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot
evaluation on two unseen OR problem types, it attains 54% average accuracy, a
21 percentage-point improvement over the strongest baseline. These findings
validate the efficacy of tool-augmented fine-tuning LLMs for accurate and
generalizable OR problem modeling and solving.

</details>


### [227] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: ROTE uses behavioral programs synthesized with LLMs and probabilistic inference to predict human behavior, treating action understanding as program synthesis; it outperforms baselines in gridworld and a household sim.


<details>
  <summary>Details</summary>
Motivation: Existing human-modeling approaches are data-hungry or brittle due to strong rationality assumptions and heavy computation; many everyday interactions follow predictable routines that can be efficiently modeled as scripts to reduce cognitive load.

Method: ROTE leverages LLMs to generate a hypothesis space of behavioral programs (scripts) and employs probabilistic inference to reason about uncertainty over that space, tested on gridworld tasks and a large-scale embodied household simulator.

Result: ROTE achieves higher predictive accuracy than competitive baselines (including behavior cloning and LLM-based methods), with improvements up to 50% in both in-sample accuracy and out-of-sample generalization.

Conclusion: Viewing action understanding as program synthesis offers a scalable, efficient path for predicting human behavior in real-world human-AI collaboration.

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [228] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: A knowledge-base‚Äìenhanced multi-agent CA-ChemE system enables autonomous discovery in chemical engineering, with improved dialogue quality and cross-domain collaboration via a Collaboration Agent that mitigates knowledge-gap inefficiencies.


<details>
  <summary>Details</summary>
Motivation: To address limited interdisciplinary collaboration and exploration of uncharted problems in AI-augmented chemical engineering by enabling self-directed research evolution through a living digital town of collaborating agents.

Method: Integrate domain-specific knowledge bases, knowledge-enhancement technologies, and collaboration agents into a multi-agent system; employ ontology engineering within the Collaboration Agent to bridge cross-domain gaps.

Result: Dialogue quality scores improved by 10‚Äì15% across seven expert agents; Collaboration Agent yielded 8.5% gains for distant-domain pairs vs 0.8% for domain-proximate pairs (a 10.6√ó difference), revealing a knowledge-base-gap‚Äìdriven bottleneck that CA helps alleviate.

Conclusion: A carefully designed multi-agent architecture with knowledge-based enhancements can support autonomous scientific discovery in chemical engineering, though cross-domain collaboration remains bottlenecked by knowledge-base gaps without targeted collaboration agents.

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [229] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: Proposes a psychometrically grounded, multi-agent LLM debate framework as a dynamic evaluation lab, revealing emergent consensus, persona effects, and moderator influence across topics, with code released.


<details>
  <summary>Details</summary>
Motivation: Traditional benchmarks fail to capture social-cognitive dynamics when LLMs operate as autonomous agents; there is a need for controlled environments to study debate, persuasion, collaboration, and alignment risks in agentic settings.

Method: Instantiate multiple LLM-based agents with distinct personas and incentives, supervise debates with an LLM moderator, develop psychometric and semantic metrics, run hundreds of debates across topics, and analyze results to reveal emergent social dynamics and alignment-relevant patterns.

Result: The framework reveals a robust emergent tendency for agents to seek consensus, achieving high semantic agreement (mu > 0.88) without explicit instruction; personas produce stable psychometric cognitive profiles; the moderator‚Äôs persona can substantially shape debate outcomes by structuring the environment; the approach provides scalable evaluation tooling and insights.

Conclusion: Provides a blueprint for dynamic, psychometrically grounded evaluation in agentic settings, aiding understanding and shaping the social behaviors of next-generation AI agents; code and results have been released for external use.

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [230] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: AGILE treats jigsaw solving as an interactive agentic learning loop where the model generates executable code to act in a visual environment; it yields large gains on simple jigsaws and generalizes across tasks, offering a scalable approach to improving perception and reasoning in vision-language models.


<details>
  <summary>Details</summary>
Motivation: Current large vision-language models show limited perceptual and reasoning abilities. Simple jigsaw tasks expose core deficits. The scarcity and limited scalability of high-quality multimodal reinforcement learning data hinder progress. There is a need for data-efficient, scalable methods that can enhance perception and reasoning through interaction.

Method: AGILE formulates jigsaw solving as an interactive process. At each step, the model generates executable code to perform an action based on the current state. The environment provides fine-grained visual feedback to guide task completion. Through iterative observation-action cycles, the model explores and receives feedback to progressively improve perceptual and reasoning capabilities.

Result: In 2x2 jigsaw settings, accuracy improves from 9.5% to 82.8%. Across 9 general vision tasks, AGILE achieves an average improvement of 3.1%. The code and datasets are available at the provided URL, indicating reproducibility and scalability.

Conclusion: AGILE introduces a novel interactive learning paradigm for enhancing perception and reasoning in multimodal models. It offers a scalable, data-efficient approach to improve core capabilities and generalization, addressing multimodal RL data scarcity and potentially broadening the applicability of VLMs to reasoning tasks.

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [231] [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
*Tudor Achim,Alex Best,Kevin Der,Math√Øs F√©d√©rico,Sergei Gukov,Daniel Halpern-Leister,Kirsten Henningsgard,Yury Kudryashov,Alexander Meiburg,Martin Michelsen,Riley Patterson,Eric Rodriguez,Laura Scharff,Vikram Shanker,Vladmir Sicca,Hari Sowrirajan,Aidan Swope,Matyas Tamas,Vlad Tenev,Jonathan Thomm,Harold Williams,Lawrence Wu*

Main category: cs.AI

TL;DR: Hybrid AI system Aristotle merges Lean-based proof search, informal lemma generation/formalization, and a geometry solver to reach gold-medal-equivalent performance on 2025 IMO problems, with favorable scaling in automated theorem proving.


<details>
  <summary>Details</summary>
Motivation: To bridge formal verification with human-like informal mathematical reasoning for challenging problems and to improve automation and scalability in theorem proving.

Method: Architecture with three components: (1) Lean proof search for formal verification; (2) informal reasoning system that generates and formalizes lemmas; (3) a dedicated geometry solver. Integration yields an end-to-end problem-solving pipeline and improved scalability.

Result: Achieves state-of-the-art performance on IMO-style problems, attaining gold-medal-equivalent results on the 2025 IMO problems; demonstrates favorable scaling properties compared to baselines.

Conclusion: Demonstrates that a hybrid formal+informal reasoning approach with a geometry solver can yield strong automated theorem proving performance and scalability, suggesting potential extensions to other domains and future integration refinements.

Abstract: We introduce Aristotle, an AI system that combines formal verification with
informal reasoning, achieving gold-medal-equivalent performance on the 2025
International Mathematical Olympiad problems. Aristotle integrates three main
components: a Lean proof search system, an informal reasoning system that
generates and formalizes lemmas, and a dedicated geometry solver. Our system
demonstrates state-of-the-art performance with favorable scaling properties for
automated theorem proving.

</details>


### [232] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: A benchmark, MEMTRACK, evaluates long-term memory and state tracking in multi-platform enterprise agent environments using asynchronous platform-interleaved timelines, with metrics Correctness, Efficiency, and Redundancy; results show current models struggle with long-horizon memory (GPT-5 ~ 60% correctness).


<details>
  <summary>Details</summary>
Motivation: Address the gap in memory evaluation for dynamic, real-world enterprise workflows where memory must integrate across multiple platforms and asynchronous events, beyond traditional conversational benchmarks.

Method: Manual expert-driven design plus scalable agent-based synthesis to generate ecologically valid scenarios across Slack, Linear, and Git. Benchmarks provide chronologically platform-interleaved timelines with noisy, conflicting, cross-referring information and possible codebase/file-system exploration. Evaluate memory via acquisition, selection, and conflict resolution across long horizons. Dataset built both by experts and synthetic agents.

Result: Experiments with state-of-the-art LLMs and memory backends reveal challenges in long-horizon memory usage, cross-platform dependencies, and contradiction resolution. Best model (GPT-5) achieves about 60% Correctness on MEMTRACK, highlighting substantial room for improvement.

Conclusion: MEMTRACK offers an extensible framework for memory-augmented agents beyond conversational benchmarks, enabling multi-agent, multi-platform memory benchmarking in complex organizational settings and motivating further research into memory mechanisms.

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [233] [Retrieval-Augmented Framework for LLM-Based Clinical Decision Support](https://arxiv.org/abs/2510.01363)
*Leon Garza,Anantaa Kotal,Michael A. Grasso,Emre Umucu*

Main category: cs.AI

TL;DR: LLM-powered clinical decision support using a retrieval-augmented generation pipeline to assist prescribing by fusing unstructured EHR narratives with structured data, emphasizing safety, alignment, and validation.


<details>
  <summary>Details</summary>
Motivation: To address growing complexity in clinical decision-making and the expansion of EHR data, enabling data-informed prescribing without replacing clinician judgment.

Method: A retrieval-augmented generation (RAG) pipeline that combines natural language processing with structured clinical inputs (demographics, complaints, symptoms, diagnostics, treatment history) and supports local or federated data sources. Representation alignment and generation strategies are used to generate contextually relevant recommendations. Evaluation uses de-identified/synthetic datasets to assess plausibility and consistency.

Result: Preliminary evaluations indicate plausibility and consistency of outputs. Early findings suggest LLM-based tools can provide valuable decision support in prescribing workflows when constrained and rigorously validated.

Conclusion: An initial step toward integrating generative AI into real-world clinical decision-making, prioritizing transparency, safety, and alignment with established practices.

Abstract: The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.

</details>


### [234] [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
*Xinpeng Wang,Nitish Joshi,Barbara Plank,Rico Angell,He He*

Main category: cs.AI

TL;DR: TRACE is a scalable, unsupervised method to detect implicit reward hacking by truncating a model's chain-of-thought (CoT) and measuring how early the reasoning suffices to pass a verifier, with higher area under the accuracy-vs-length curve indicating shortcut use.


<details>
  <summary>Details</summary>
Motivation: Reward hacking poses a serious threat, including implicit leakage where CoT appears benign. Existing monitors struggle with unseen loopholes and lack scalable oversight; there is a need for an unsupervised detector that can reveal shortcuts in reasoning.

Method: Progressively truncate a model's CoT at multiple lengths, force a final answer, and measure the verifier-passing rate at each cutoff. Compute the area under the accuracy-vs-length curve (AUC). A short-cut (hacking) model will pass with only a small fraction of CoT, yielding a large AUC. Evaluated on math reasoning (with a 72B CoT monitor) and coding (with a 32B monitor),TRACE shows significant gains and can identify unknown loopholes during training.

Result: TRACE achieved >65% gains over the strongest 72B CoT monitor in math reasoning and >30% gains over a 32B monitor in coding, demonstrating substantial improvement over existing monitors and the ability to uncover unknown loopholes during training.

Conclusion: TRACE offers a scalable, unsupervised approach for oversight that is effective against implicit reward hacking and loopholes in CoT, improving monitoring where current methods are ineffective.

Abstract: Reward hacking, where a reasoning model exploits loopholes in a reward
function to achieve high rewards without solving the intended task, poses a
significant threat. This behavior may be explicit, i.e. verbalized in the
model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus
bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE
(Truncated Reasoning AUC Evaluation). Our key observation is that hacking
occurs when exploiting the loophole is easier than solving the actual task.
This means that the model is using less `effort' than required to achieve high
reward. TRACE quantifies effort by measuring how early a model's reasoning
becomes sufficient to pass a verifier. We progressively truncate a model's CoT
at various lengths, force the model to answer, and measure the verifier-passing
rate at each cutoff. A hacking model, which takes a shortcut, will achieve a
high passing rate with only a small fraction of its CoT, yielding a large area
under the accuracy-vs-length curve. TRACE achieves over 65% gains over our
strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B
monitor in coding. We further show that TRACE can discover unknown loopholes
during training. Overall, TRACE offers a scalable unsupervised approach for
oversight where current monitoring methods prove ineffective.

</details>


### [235] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: A simple distillation pipeline converts retrieval-guided performance into internal competence by extracting failure hints, generating improved teacher trajectories via one-shot retrieval at episode start, and training students with hints removed, yielding strong gains with fewer tokens and no runtime retrieval dependencies.


<details>
  <summary>Details</summary>
Motivation: LLM agents often fail due to unmet preconditions, redundancy, or poor environment handling. While retrieval-augmented generation can help, it relies on external memory and adds runtime cost. The work seeks a lightweight alternative that preserves retrieval benefits without ongoing dependencies.

Method: 1) Extract compact failure-derived hints from agent failures. 2) Use these hints to produce enhanced teacher trajectories through one-shot retrieval at episode start. 3) Train student models on these trajectories with hint strings removed to force internalization rather than memorization. Evaluated on ALFWorld and WebShop across 7B/14B models and ReAct/StateAct architectures.

Result: Distilled students outperform baselines on both benchmarks (ALFWorld: 91% vs 79%; WebShop: 72 vs 61). They use 10‚Äì60% fewer tokens than retrieval-augmented teachers. The approach generalizes across model scales and architectures, demonstrating that retrieval benefits can be internalized via targeted fine-tuning without permanent runtime dependencies.

Conclusion: Retrieval advantages can be effectively internalized through distillation, enabling robust multi-step task performance without external retrieval during deployment. This offers a scalable, architecture-agnostic path to improve efficiency and reduce runtime overhead while preserving gains from retrieval-informed guidance.

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [236] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: LLM agents automate end-to-end data-driven regression modeling using two frameworks (multi-agent collaboration and ReAct-based single agent). They autonomously preprocess data, build/train models, optimize hyperparameters, and quantify uncertainty; evaluated on CHF data (~25k points); outperform CHF lookup tables and match/better Bayesian-optimized DNNs developed by humans.


<details>
  <summary>Details</summary>
Motivation: Reduce manual intervention and scale data-driven engineering modeling to diverse applications by leveraging LLM-based agents to automate end-to-end workflows.

Method: Compare two LLM-agent frameworks: a multi-agent system with specialized, collaborative agents, and a single-agent ReAct-based system. Both autonomously perform data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification on a CHF regression benchmark using ~25,000 data points. Baselines include traditional CHF lookup tables and state-of-the-art Bayesian-optimized DNNs.

Result: The LLM-agent models surpass traditional CHF lookup tables and achieve predictive accuracy and uncertainty quantification on par with state-of-the-art Bayesian-optimized deep neural networks designed by human experts.

Conclusion: LLM-based agents can robustly automate complex engineering modeling tasks, reducing human workload while delivering predictive performance that meets or exceeds current standards.

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [237] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX uses LLMs to convert raw logs into ontology-grounded knowledge graphs via a lightweight log ontology, retrieval augmented generation (RAG), and iterative corrections, enabling aggregation into sessions and MITRE ATT&CK tactic prediction; validated on benchmark data and honeypot logs; shows improved precision/recall and actionable CTI extraction.


<details>
  <summary>Details</summary>
Motivation: System logs are rich but unstructured and heterogeneous; current fragmentation hinders actionable CTI. A unified, ontology-based, LLM-assisted approach can reconcile data into interoperable representations and link evidence to attacker objectives.

Method: Develop OntoLogX: a framework combining a lightweight log ontology, Retrieval Augmented Generation (RAG), and iterative correction steps; generate ontology-grounded KGs from logs; aggregate event-level KGs into sessions; use LLM to map evidence to MITRE ATT&CK tactics; evaluate across multiple KG backends on benchmark and real-world honeypot datasets.

Result: Demonstrates robust KG generation across KG backends; accurate mapping of adversarial activity to ATT&CK tactics; retrieval and correction improve precision and recall; code-oriented models effective for structured log analysis; ontology-grounded representations beneficial for actionable CTI.

Conclusion: Shows feasibility and value of combining ontologies, retrieval-augmented LLMs, and iterative corrections for structured, interoperable CTI extraction from logs; supports linking low-level evidence to high-level objectives and enhances CTI usefulness.

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [238] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: Falconer combines LLMs as planners/annotators with lightweight proxy models to perform scalable knowledge mining, achieving near-SOTA instruction-following accuracy with substantial cost reductions and speedups.


<details>
  <summary>Details</summary>
Motivation: To enable scalable, accurate knowledge mining from massive unstructured text while avoiding the high cost and brittleness of end-to-end LLMs and large, task-specific pipelines.

Method: Falconer uses LLMs as planners to decompose user instructions into executable pipelines and as annotators to generate supervision for small proxy models. It unifies classification and extraction into two atomic operations (get label and get span) and trains lightweight proxies with LLM-/human-generated supervision, evaluating with new planning and end-to-end benchmarks.

Result: Falconer closely matches state-of-the-art instruction-following accuracy while reducing inference cost by up to 90% and accelerating large-scale knowledge mining by more than 20x.

Conclusion: Falconer provides an efficient, scalable foundation for Deep Research by combining agentic LLM reasoning with lightweight proxies, reducing reliance on expensive LLMs and enabling robust, generalizable knowledge mining.

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [239] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: This paper argues for leveraging highly curated expert knowledge and explainable AI to automatically generate lessons and adaptive tutoring systems, demonstrated via a pollinator-identification case study.


<details>
  <summary>Details</summary>
Motivation: There is an underutilized potential for domain-expert knowledge in AI for education; combining expert-curated rules with XAI can create scalable, explainable, adaptive educational tools.

Method: Discuss two avenues: (1) using expert-specified solving rules with XAI to automatically generate lessons; (2) using expert-curated curricula to drive adaptive tutoring systems and enable more efficient algorithms; supports with a pollinator-identification case study.

Result: Conceptual framework with illustrative case study; no empirical results reported.

Conclusion: Integrating expert knowledge with XAI in education can enhance tutoring systems and learner experiences; highlights need for expert elicitation and further research.

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [240] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: VOGUE introduces visual-uncertainty driven exploration for multimodal RL from RLVR, shifting exploration to input space and using a KL-based uncertainty signal to improve reasoning without sacrificing exploitation.


<details>
  <summary>Details</summary>
Motivation: To address exploration failure in multimodal LLMs under RL from verifiable rewards, where visual inputs are treated as fixed and deterministic, leading to brittle policies under plausible visual variations.

Method: Treat the image as a stochastic context and quantify policy sensitivity to visual perturbations via the symmetric KL divergence between a 'raw' and a 'noisy' visual branch. Use this uncertainty signal to form an uncertainty-proportional bonus in the learning objective, augmented with a token-entropy bonus and an annealed sampling schedule. Implemented within the GRPO framework on two model scales (Qwen2.5-VL-3B/7B).

Result: On three visual math benchmarks and three general-domain reasoning benchmarks, pass@1 improves by ~2.6% and ~3.7% respectively (averages across tasks); pass@4 also improves and exploration decay during RL fine-tuning is mitigated.

Conclusion: Grounding exploration in the inherent uncertainty of visual inputs is an effective strategy for enhancing multimodal reasoning in RL-finetuned LLMs.

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [241] [AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance](https://arxiv.org/abs/2510.01474)
*Bill Marino,Rosco Hunter,Zubair Jamali,Marinos Emmanouil Kalpakos,Mudra Kashyap,Isaiah Hinton,Alexa Hanson,Maahum Nazir,Christoph Schnabl,Felix Steffek,Hongkai Wen,Nicholas D. Lane*

Main category: cs.AI

TL;DR: AIReg-Bench is introduced as the first benchmark dataset to evaluate how well LLMs can assess compliance with the EU AI Act, created by prompting an LLM to generate 120 fictional AI system documents and having legal experts annotate compliance; aims to benchmark and understand the capabilities and limits of LLM-based AIR compliance tools.


<details>
  <summary>Details</summary>
Motivation: There is no existing benchmark to measure LLM performance on AI Regulation compliance assessment. This work fills that gap by providing a labeled dataset and a framework to compare frontier LLMs against expert annotations.

Method: Two-step creation: (1) prompt an LLM with structured instructions to generate 120 plausible technical excerpts describing AI systems; (2) legal experts annotate each sample for violations of specific AIA Articles. The authors evaluate whether frontier LLMs can reproduce the expert labels, and provide dataset + evaluation code.

Result: A labeled benchmark dataset and evaluation framework that enables benchmarking LLMs on AIR compliance. Frontier LLMs are evaluated against expert annotations, establishing a starting point to understand capabilities and limitations.

Conclusion: AIReg-Bench offers a valuable resource to benchmark and compare LLM-based AIR compliance tools, highlighting opportunities and limitations, and providing code and data for future research.

Abstract: As governments move to regulate AI, there is growing interest in using Large
Language Models (LLMs) to assess whether or not an AI system complies with a
given AI Regulation (AIR). However, there is presently no way to benchmark the
performance of LLMs at this task. To fill this void, we introduce AIReg-Bench:
the first benchmark dataset designed to test how well LLMs can assess
compliance with the EU AI Act (AIA). We created this dataset through a two-step
process: (1) by prompting an LLM with carefully structured instructions, we
generated 120 technical documentation excerpts (samples), each depicting a
fictional, albeit plausible, AI system - of the kind an AI provider might
produce to demonstrate their compliance with AIR; (2) legal experts then
reviewed and annotated each sample to indicate whether, and in what way, the AI
system described therein violates specific Articles of the AIA. The resulting
dataset, together with our evaluation of whether frontier LLMs can reproduce
the experts' compliance labels, provides a starting point to understand the
opportunities and limitations of LLM-based AIR compliance assessment tools and
establishes a benchmark against which subsequent LLMs can be compared. The
dataset and evaluation code are available at
https://github.com/camlsys/aireg-bench.

</details>


### [242] [Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates](https://arxiv.org/abs/2510.01500)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: LToT separates utility from logical consistency in Tree-of-Thoughts, using mainlines for high-utility paths and laterals for diverse, consistent candidates; laterals are explored via Lateral Racing with Short-Circuit (LR-SC), a capped successive-halving across a wide lateral set; mainlines are kept narrow to allocate compute cheaply to width expansion; claims a pseudolinear lateral cost Œò(N0 log_eta N0) vs exponential growth without caps; empirical validation is forthcoming.


<details>
  <summary>Details</summary>
Motivation: Standard Tree-of-Thoughts search under large test-time budgets suffers two pathologies: breadth saturation (increasing samples yield near-duplicates) and depth myopia (noisy short-horizon utilities prune potentially payoff-rich branches). A principled way to harvest diversity without wasting compute is needed.

Method: Introduce Lateral Tree-of-Thoughts (LToT) that splits the frontier into mainlines (high-utility candidates used for exploitation) and laterals (consistent but initially low-utility candidates). Apply Lateral Racing with Short-Circuit (LR-SC): a capped successive-halving race that quickly probes a very wide lateral set with width-aware thresholds and repeat-to-confirm, promoting a branch once its envelope clears the mainline bar. Keep mainlines intentionally narrow; surrogate compute is spent on a cheap exploration of width via laterals. Theoretical bound: pseudolinear lateral cost Œò(N0 log_eta N0) (eta>1) contrasted with exponential growth of uncapped mainlines.

Result: The abstract notes empirical evaluations on benchmark tasks are in preparation for a future revision; it also provides a theoretical cost bound and qualitative claims about diversity and promotion discipline.

Conclusion: LToT offers a drop-in controller that turns large test-time budgets into principled diversity while preserving promotion discipline, mitigating breadth saturation and depth myopia without inflating overall compute.

Abstract: Modern deployments increasingly allocate large test-time compute (thousands
of tokens or many node expansions) to boost reliability. Under such budgets,
standard Tree-of-Thoughts-style search exhibits two pathologies: breadth
saturation (additional samples mostly produce near-duplicates, so width stops
growing) and depth myopia (noisy short-horizon utilities prune branches whose
payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts
(LToT), a drop-in controller that separates utility from logical consistency
and treats low-utility but consistent candidates as assets rather than waste.
The frontier is split into mainlines (high-utility candidates used for
exploitation) and laterals (consistent, initially low-utility candidates that
receive short, cheap probes before judgment). LToT explores laterals via
Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race
that spreads tiny probes across a very wide lateral set, uses width-aware
thresholds with repeat-to-confirm, and immediately promotes a branch once its
envelope clears the mainline bar; mainlines are kept intentionally narrow so
surplus compute is invested where width is cheap. We prove a pseudolinear
lateral cost $\Theta(N_0 \log_{\eta} N_0)$ with logarithmically many rungs
(initial lateral width $N_0$; culling factor $\eta>1$), in contrast to the
exponential growth of uncapped mainlines. Empirical evaluations on benchmark
tasks are in preparation and will be added in a future revision. In short, LToT
turns large test-time budgets into principled diversity while preserving
promotion discipline, mitigating saturation and myopia without inflating
compute.

</details>


### [243] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: SAEs + clustering to analyze LLM token representations for guided, balanced mathematical reasoning; uses sparse autoencoders and k-means to build a token-graph and an edge-weighted reward to enforce adherence to reasoning traces; measures diversity via clustering; balances exploitation and exploration to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: to quantify internal token representations and transitions in LLMs in order to steer reasoning and avoid extreme behaviors, thereby improving mathematical reasoning accuracy.

Method: train a sparse autoencoder to generate sparse token representations; apply k-means clustering to form token clusters and construct a graph where vertices are clusters and edges capture sequential token transitions; define an edge-weighted reward based on adherence to established reasoning traces; assess generation diversity via clustering; use the SAE as a scalable reward model during generation to balance exploitation and exploration.

Result: balancing exploitation and exploration improves accuracy in mathematical reasoning tasks; SAE-guided generation prevents extreme behaviors and yields higher-quality reasoning; the approach offers a scalable framework for reward shaping in LLM reasoning.

Conclusion: SAE-based reward shaping and clustering-derived guidance can steer LLM generations toward robust mathematical reasoning by maintaining a balance between following established reasoning traces and exploring alternative paths.

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [244] [LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning](https://arxiv.org/abs/2510.01530)
*Navapat Nananukul,Yue Zhang,Ryan Lee,Eric Boxer,Jonathan May,Vibhav Giridhar Gogate,Jay Pujara,Mayank Kejriwal*

Main category: cs.AI

TL;DR: A neurosymbolically-grounded LogT architecture combines dual symbolic graph and logic-based contexts with LLMs to tackle high-assurance reasoning (defeasible rules, negation, implication), achieving ~11.84% gains across four benchmarks and LLMs.


<details>
  <summary>Details</summary>
Motivation: High-assurance domains (law, medicine) rely on codified rules that involve non-monotonic reasoning and numerous exceptions; single facts can invalidate general rules. LLMs excel at language but struggle with rigorous, evidence-grounded inference, necessitating a hybrid approach that grounds reasoning in symbolic logic.

Method: Introduce LOGicalThought (LogT), a neurosymbolic architecture that uses an advanced logical language and reasoner alongside an LLM to construct two context representations: a dual symbolic graph context and a logic-based context. These contexts transform long-form guideline inference into a compact, grounded evaluation. Evaluated on four multi-domain benchmarks against four baselines.

Result: LogT yields an overall improvement of 11.84% across all LLMs. Gains are substantial across reasoning modes: negation +10.2%, implication +13.2%, and defeasible reasoning +5.5% over the strongest baseline.

Conclusion: The study demonstrates that a neurosymbolic, dual-context approach improves high-assurance reasoning tasks involving defeasible rules and non-monotonic logic. The two-context representation enables grounded, compact evaluation and outperforms baselines across multiple domains and LLMs; future work may explore scalability, broader domain coverage, and deeper integration with existing guidelines.

Abstract: High-assurance reasoning, particularly in critical domains such as law and
medicine, requires conclusions that are accurate, verifiable, and explicitly
grounded in evidence. This reasoning relies on premises codified from rules,
statutes, and contracts, inherently involving defeasible or non-monotonic logic
due to numerous exceptions, where the introduction of a single fact can
invalidate general rules, posing significant challenges. While large language
models (LLMs) excel at processing natural language, their capabilities in
standard inference tasks do not translate to the rigorous reasoning required
over high-assurance text guidelines. Core reasoning challenges within such
texts often manifest specific logical structures involving negation,
implication, and, most critically, defeasible rules and exceptions. In this
paper, we propose a novel neurosymbolically-grounded architecture called
LOGicalThought (LogT) that uses an advanced logical language and reasoner in
conjunction with an LLM to construct a dual symbolic graph context and
logic-based context. These two context representations transform the problem
from inference over long-form guidelines into a compact grounded evaluation.
Evaluated on four multi-domain benchmarks against four baselines, LogT improves
overall performance by 11.84% across all LLMs. Performance improves
significantly across all three modes of reasoning: by up to +10.2% on negation,
+13.2% on implication, and +5.5% on defeasible reasoning compared to the
strongest baseline.

</details>


### [245] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: InfoSeeker is an LLM decision-making framework that couples task planning with information seeking to align internal dynamics with the real world under partial observability and uncertain dynamics. It uses active information gathering actions before planning and demonstrates strong gains on a new benchmark suite (74% absolute improvement) and strong generalization across LLMs, robotics, and web navigation.


<details>
  <summary>Details</summary>
Motivation: Address the gap where existing LLM planners manage observational uncertainty but ignore discrepancies between the model's internal dynamics and the actual environment, leading to suboptimal decisions under partial observability and dynamic environments.

Method: Integrate task-oriented planning with information-seeking as controllable actions. The LLM plans actions to validate understanding, detect environmental changes, or test hypotheses, then generates or revises task plans. Evaluation on a novel partially observable benchmark suite, plus established benchmarks, with cross-LLM generalization.

Result: InfoSeeker achieves 74% absolute performance gain over prior methods with good sample efficiency, generalizes across LLMs, and outperforms baselines on robotic manipulation and web navigation benchmarks.

Conclusion: Tightly integrating planning and information seeking yields robust behavior under partial observability and dynamic environments, suggesting a promising direction for LLM-based agents that align internal models with the real world.

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [246] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: Hierarchical reasoning framework for diffusion LMs with Step-Aware Policy Optimization (SAPO), aligning denoising steps to latent reasoning to improve complex reasoning and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current RL with sparse, outcome-based rewards can reinforce flawed, coincidental reasoning paths and fail to capture the natural multi-step structure of problem solving; a theory-backed approach is needed.

Method: Formalize complex problem solving as a hierarchical selection process; identify unstructured refinement as a key failure mode; introduce SAPO with a process-based reward to guide incremental progress and align denoising with latent reasoning hierarchy; provide theoretical insights into identifiability.

Result: Empirical results show significant performance gains on challenging reasoning benchmarks and improved interpretability of the generation process.

Conclusion: A principled, hierarchy-aware training paradigm for dLLMs via SAPO yields more structured reasoning paths, mitigates flawed iterative steps, and enhances interpretability.

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [247] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: InvThink uses inverse thinking to prime LLMs to anticipate harms before answering: enumerate potential harms, analyze consequences, and generate safe outputs. Safety gains scale with model size, preserve general reasoning, and extend to high-stakes and agentic risk domains, with up to 15.7% fewer harmful responses vs SafetyPrompt. Implemented via supervised fine-tuning and reinforcement learning across three LLM families.


<details>
  <summary>Details</summary>
Motivation: Address safety in LLMs without sacrificing general reasoning by training models to think about failure modes beforehand, overcoming limitations of direct safety optimization.

Method: Inverse thinking: (1) enumerate potential harms, (2) analyze their consequences, (3) generate safe outputs that avoid risks; implemented through supervised fine-tuning and reinforcement learning across three LLM families; evaluated on standard benchmarks and high-stakes domains.

Result: Safety improvements scale with model size more than existing safety methods; up to 15.7% reduction in harmful responses versus baselines like SafetyPrompt; preserves general reasoning; effective in high-stakes domains (medicine, finance, law) and agentic risk scenarios.

Conclusion: Inverse reasoning provides a scalable, generalizable approach to safer, more capable language models.

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [248] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: AdvEvo-MARL is a co-evolutionary MARL framework that internalizes safety into task agents, using adversarial training to resist jailbreak prompts without external guards, achieving lower attack success rates and equal or improved task performance.


<details>
  <summary>Details</summary>
Motivation: Current defenses either rely on self-verification (often ineffective for cross-agent risks) or external guard modules (adding overhead and single points of failure). Integrated safety within agents aims to reduce risk from cross-agent chains and delegation while maintaining efficiency.

Method: Co-evolutionary reinforcement learning that jointly optimizes attackers (synthesizing evolving jailbreak prompts) and defenders (task agents trained to complete tasks and resist attacks) in adversarial environments. Introduces a public baseline for advantage estimation: a group-level mean-return baseline shared among agents in the same functional group to reduce variance and boost intra-group coordination.

Result: Attack success rate (ASR) kept below 20% across scenarios (vs. up to 38.33% for baselines); task accuracy on reasoning tasks improved by up to 3.67%; no external guard modules required, reducing overhead while increasing safety and often performance.

Conclusion: Integrated safety learning in task agents can simultaneously improve safety and utility in LLM-based multi-agent systems, avoiding external guards and extra system overhead.

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [249] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: AgentRec introduces a hierarchical, LLM-powered multi-agent framework for interactive conversational recommendation that uses adaptive weighting and a three-tier learning strategy to balance conversation quality and ranking objectives.


<details>
  <summary>Details</summary>
Motivation: Existing interactive recommender systems struggle with dynamic user preferences, maintaining conversation coherence, and jointly optimizing multiple objectives; there is a need for coordinated, intelligent agent architectures.

Method: AgentRec deploys hierarchical agent networks with specialized LLM agents for conversation understanding, preference modeling, context awareness, and dynamic ranking, coordinated by an adaptive weighting mechanism learned from interactions. It adopts a three-tier learning strategy: rapid response for simple queries, intelligent reasoning for complex preferences, and deep collaboration for challenging scenarios.

Result: Experiments on three real-world datasets show improvements over baselines: 2.8% higher conversation success rate, 1.9% higher NDCG@10, and 3.2% better conversation efficiency, with comparable computational costs thanks to efficient agent coordination.

Conclusion: AgentRec demonstrates consistent performance gains and effective coordination for next-generation interactive recommender systems, advancing the state of the art while maintaining practical efficiency.

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [250] [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
*Min Zeng*

Main category: cs.AI

TL;DR: PsychoBench evaluates whether LLMs can meet licensing standards for psychological counseling by testing them on a national exam-style benchmark; frontier models surpass the passing threshold while smaller open models lag, signaling potential but notable challenges for psychology-oriented LLMs.


<details>
  <summary>Details</summary>
Motivation: Assess if LLMs can function as licensed counselors by comparing their knowledge against the requirements of a U.S. counselor licensure exam.

Method: Build PsychoBench with ~2,252 single-choice questions derived from U.S. National Counselor Examination content; evaluate multiple models including GPT-4o, Llama3-70B, Gemma3-27B, Qwen2.5-7B, Mistral-7B to measure passing chances.

Result: Advanced, frontier models achieve scores well above the 70% passing threshold; smaller open models remain far below it, indicating that only cutting-edge LLMs currently meet counseling exam standards.

Conclusion: Shows promise for LLM-based counseling with frontier models, but highlights substantial challenges and the need for further work on safety, reliability, and real-world applicability beyond exam performance.

Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of industries, primarily due to their impressive generative
abilities. Yet, their potential in applications requiring cognitive abilities,
such as psychological counseling, remains largely untapped. This paper
investigates the key question: Can LLMs be effectively applied to psychological
counseling? To determine whether an LLM can effectively take on the role of a
psychological counselor, the first step is to assess whether it meets the
qualifications required for such a role, namely the ability to pass the U.S.
National Counselor Certification Exam (NCE). This is because, just as a human
counselor must pass a certification exam to practice, an LLM must demonstrate
sufficient psychological knowledge to meet the standards required for such a
role. To address this, we introduce PsychoBench, a benchmark grounded in
U.S.national counselor examinations, a licensure test for professional
counselors that requires about 70% accuracy to pass. PsychoBench comprises
approximately 2,252 carefully curated single-choice questions, crafted to
require deep understanding and broad enough to cover various sub-disciplines of
psychology. This benchmark provides a comprehensive assessment of an LLM's
ability to function as a counselor. Our evaluation shows that advanced models
such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing
threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)
remain far below it. These results suggest that only frontier LLMs are
currently capable of meeting counseling exam standards, highlighting both the
promise and the challenges of developing psychology-oriented LLMs.

</details>


### [251] [Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs](https://arxiv.org/abs/2510.01620)
*Peidong Liu,Junjiang Lin,Shaowen Wang,Yao Xu,Haiqing Li,Xuhao Xie,Siyi Wu,Hao Li*

Main category: cs.AI

TL;DR: LLM-based summarization of contextual inputs for CMDPs yields low-dimensional, semantically rich summaries that preserve decision-critical cues, enabling regret bounds, a latency-entropy trade-off, and practical gains in reward, success rate, and efficiency.


<details>
  <summary>Details</summary>
Motivation: CMDPs struggle to generalize and scale in high-dimensional or unstructured contexts due to computation and instability; a principled, scalable representation of context is needed to maintain performance while reducing cost.

Method: Compress contextual inputs with large language models into low-dimensional summaries that augment decision states (approximate context sufficiency). Derive regret bounds and a latency-entropy trade-off for CMDPs, connecting informativeness to computational cost. Validate across discrete, continuous, visual, and recommendation benchmarks.

Result: Across domains, the approach outperforms raw-context and non-context baselines, improving reward, success rate, and sample efficiency, while reducing latency and memory usage.

Conclusion: LLM-based contextual summarization offers a scalable, interpretable solution for decision-making in context-rich, resource-constrained CMDPs, balancing informativeness with computational efficiency.

Abstract: Contextual Markov Decision Processes (CMDPs) offer a framework for sequential
decision-making under external signals, but existing methods often fail to
generalize in high-dimensional or unstructured contexts, resulting in excessive
computation and unstable performance. We propose an information-theoretic
summarization approach that uses large language models (LLMs) to compress
contextual inputs into low-dimensional, semantically rich summaries. These
summaries augment states by preserving decision-critical cues while reducing
redundancy. Building on the notion of approximate context sufficiency, we
provide, to our knowledge, the first regret bounds and a latency-entropy
trade-off characterization for CMDPs. Our analysis clarifies how
informativeness impacts computational cost. Experiments across discrete,
continuous, visual, and recommendation benchmarks show that our method
outperforms raw-context and non-context baselines, improving reward, success
rate, and sample efficiency, while reducing latency and memory usage. These
findings demonstrate that LLM-based summarization offers a scalable and
interpretable solution for efficient decision-making in context-rich,
resource-constrained environments.

</details>


### [252] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: LLMs can read road networks and perform navigation-like trajectory recovery without external tools, using GLOBALTRACE (~4k real-world trajectories) to outperform baselines and show strong zero-shot generalization, though they exhibit regional and mode biases; they can be used to enhance map-based navigation with flexible user preferences.


<details>
  <summary>Details</summary>
Motivation: Assess whether LLMs can perform geospatial reasoning and map-based navigation by reading road networks, bridging natural language models with navigation tasks.

Method: Introduce GLOBALTRACE, a dataset of over 4,000 real-world trajectories across diverse regions and transportation modes. Propose a prompting framework that uses the road network as context to guide LLMs in generating valid paths, without accessing external navigation tools. Evaluate on trajectory recovery (masked GPS traces) and analyze zero-shot generalization and biases.

Result: LLMs outperform off-the-shelf baselines and specialized trajectory recovery models, with strong zero-shot generalization. Fine-grained analysis shows strong comprehension of road networks and coordinate systems but reveals systematic biases with respect to regions and transportation modes.

Conclusion: LLMs can enhance navigation experiences by reasoning over maps and accommodating user preferences, though biases exist; suggests promising integration of map-based reasoning into navigation systems and directions for mitigating biases.

Abstract: We explore the geospatial reasoning capabilities of Large Language Models
(LLMs), specifically, whether LLMs can read road network maps and perform
navigation. We frame trajectory recovery as a proxy task, which requires models
to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with
over 4,000 real-world trajectories across diverse regions and transportation
modes. Using road network as context, our prompting framework enables LLMs to
generate valid paths without accessing any external navigation tools.
Experiments show that LLMs outperform off-the-shelf baselines and specialized
trajectory recovery models, with strong zero-shot generalization. Fine-grained
analysis shows that LLMs have strong comprehension of the road network and
coordinate systems, but also pose systematic biases with respect to regions and
transportation modes. Finally, we demonstrate how LLMs can enhance navigation
experiences by reasoning over maps in flexible ways to incorporate user
preferences.

</details>


### [253] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: GuruAgents show that encoding legendary investors' philosophies into LLM prompts can yield reproducible, guru-driven trading strategies; notably Buffett GuruAgent reaches 42.2% CAGR in NASDAQ-100 backtests.


<details>
  <summary>Details</summary>
Motivation: To test whether qualitative investing wisdom can be translated into quantitative, backtestable strategies via prompt engineering and deterministic reasoning, enabling automated systematic investing.

Method: Create five GuruAgents, each mimicking a famous investor through tailored prompts; integrate financial tools with a deterministic reasoning pipeline; backtest on NASDAQ-100 constituents from Q4 2023 to Q2 2025.

Result: Buffett GuruAgent achieves the highest performance with a 42.2% CAGR, substantially outperforming benchmarks; other agents yield varied results, indicating persona-dependent behavior; overall evidence that prompt engineering can convert qualitative philosophies into quantitative strategies.

Conclusion: Prompt-driven emulation of investment gurus can produce reproducible, quantitative strategies, suggesting a novel direction for automated systematic investing; source code and data are publicly available.

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [254] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: Introduces BLIND-ACT to study Blind Goal-Directedness (BGD) in Computer-Use Agents (CUAs); shows BGD is pervasive across frontier models, with high agreement (93.75%) between LLM judges and humans; prompting reduces but does not eliminate BGD; identifies failure modes and provides a benchmark for mitigations.


<details>
  <summary>Details</summary>
Motivation: Address the pervasive bias of CUAs toward pursuing goals regardless of feasibility, safety, or context. Provide a realistic benchmark and evaluation framework to study BGD and inform mitigation strategies for safe CUA deployment.

Method: Build BLIND-ACT benchmark comprising 90 tasks within OSWorld environments to capture three BGD patterns. Use LLM-based judges to assess agent behavior and validate judgments against human annotations (93.75% agreement). Evaluate nine frontier models (e.g., Claude Sonnet, Opus 4, GPT-5 variants) and analyze BGD rates (average ~80.8%). Conduct qualitative analysis to identify failure modes and test prompting-based interventions.

Result: BLIND-ACT enables consistent measurement of BGD across models, revealing high prevalence of BGD (80.8% on average). LLM judges align closely with human judgments (93.75%). Prompting reduces BGD but does not eliminate it, indicating need for stronger training or inference-time interventions. Qualitative analysis uncovers execution-first bias, thought-action disconnect, and request-primacy as core failure modes.

Conclusion: Establishes a foundation for future research on diagnosing and mitigating BGD in CUAs, emphasizing the necessity of stronger safeguards during training and at inference to ensure safe CUA deployment.

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [255] [A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)
*Motoki Sato,Yuki Matsushita,Hidekazu Takahashi,Tomoaki Kakazu,Sou Nagata,Mizuho Ohnuma,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.AI

TL;DR: A safety-first, local, non-generative clinical QA pipeline (LENOHA) routes patient queries to clinician-curated FAQs and returns verbatim answers, achieving near-GPT-4o performance while dramatically reducing energy use and latency on on-prem hardware.


<details>
  <summary>Details</summary>
Motivation: Patients awaiting invasive procedures often have unanswered questions, but time pressure and privacy concerns limit personalized counseling. A safe, local-first system that avoids generation and preserves privacy is needed.

Method: Two-domain evaluation (tooth extraction, gastroscopy). A high-precision sentence-transformer classifier routes inputs to a clinician-curated FAQ, which provides verbatim answers (no text generation). Compared four encoders across expert-validated thresholding sets (n=400/domain) and independent test sets (n=200/domain). Energetic and latency metrics measured on-prem.

Result: E5-large-instruct (560M) achieved overall accuracy 0.983 (CI 0.964‚Äì0.991), AUC 0.996, seven errors; GPT-4o performance not statistically different; Gemini had zero errors on the test set. Non-generative path uses ~1.0 mWh per input vs ~168 mWh per small-talk reply from an 8B SLM (‚âà170√ó savings) with ~0.10 s latency on a single on‚Äëprem GPU.

Conclusion: Verbatim, clinician-curated FAQ retrieval avoids generation-induced errors while enabling privacy, sustainability, and equitable deployment in bandwidth-limited settings; the approach supports near-frontier discrimination without generation, suitable for clinical paths.

Abstract: Patients awaiting invasive procedures often have unanswered pre-procedural
questions; however, time-pressured workflows and privacy constraints limit
personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave
No One Behind Architecture), a safety-first, local-first system that routes
inputs with a high-precision sentence-transformer classifier and returns
verbatim answers from a clinician-curated FAQ for clinical queries, eliminating
free-text generation in the clinical path. We evaluated two domains (tooth
extraction and gastroscopy) using expert-reviewed validation sets
(n=400/domain) for thresholding and independent test sets (n=200/domain). Among
the four encoders, E5-large-instruct (560M) achieved an overall accuracy of
0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were
statistically indistinguishable from GPT-4o on this task; Gemini made no errors
on this test set. Energy logging shows that the non-generative clinical path
consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local
8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single
on-prem GPU. These results indicate that near-frontier discrimination and
generation-induced errors are structurally avoided in the clinical path by
returning vetted FAQ answers verbatim, supporting privacy, sustainability, and
equitable deployment in bandwidth-limited environments.

</details>


### [256] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: Shift AGI evaluation from intuition-based synthetic tasks to robust, competence-focused task execution, inspired by data science deployment practices.


<details>
  <summary>Details</summary>
Motivation: Current AGI benchmarks rely on intuitive, synthetic tasks that have historically performed poorly and poorly reflect real-world capabilities; a reliable evaluation framework is needed to credibly track progress toward AGI.

Method: Advocate an alternative design philosophy focused on evaluating robust task execution and demonstrated competence, drawing on data science practices used to show reliable deployment; provide practical examples of what robust evaluation would entail for AGI.

Result: Proposes a conceptual framework with practical examples for evaluating AGI via robust task competence; no empirical results are claimed in the abstract.

Conclusion: Adopting a deployment-oriented, competence-focused evaluation approach may yield more meaningful progress signals and better reflect real-world capabilities than intuition-based benchmarks.

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [257] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: Introduces VaPR, a hard-negative generation framework for LVLM alignment via LLM-guided response editing, creating a 30K sample dataset to improve preference-finetuning; shows consistent gains across LVLM families and open-source editors.


<details>
  <summary>Details</summary>
Motivation: Address noise and biases in synthetic preference data (notably stylistic and length biases) that degrade alignment quality when finetuning with preference data.

Method: Use LLM-guided editing to generate hard-negative responses that resemble accepted answers in style and length but contain targeted errors. Build VaPR, a 30K high-quality dataset, to fine-tune LVLMs (LLaVA-V1.5, Qwen2VL, Qwen2.5VL) with DPO/AI-feedback; evaluate across ten benchmarks; extend to open-source editors (VaPR-OS).

Result: Consistent performance gains across models: +6.5% (LLaVA), +4.0% (Qwen2VL), +1.5% (Qwen2.5VL) on ten benchmarks, notably in reasoning tasks. Scaling: performance improves with more data, with LLaVA benefiting at smaller scales. VaPR reduces the tendency to answer ‚ÄúYes‚Äù in binary questions. VaPR-OS editors reach ~99% of the full VaPR results (vs GPT-4o-synthesized data). Public release of data, models, and code.

Conclusion: Hard-negative generation effectively mitigates synthetic preference noise and yields robust gains across LVLM families and open-source editors. The VaPR dataset and framework offer scalable, transferable improvements for alignment via preference finetuning and reduce common failure modes, with strong evidence of generalization.

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [258] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-F√©lix Nothias*

Main category: cs.AI

TL;DR: A multi-agent AI system (MetaboT) translates natural language questions about metabolomics into SPARQL queries on a large knowledge graph (ENPKG), achieving high accuracy and outperforming a baseline.


<details>
  <summary>Details</summary>
Motivation: Mass spectrometry metabolomics data is vast; knowledge graphs structure data but require ontology expertise; to democratize querying, an AI assistant translating NL to SPARQL is needed.

Method: A LangChain/LangGraph-based multi-agent architecture with Entry, Validator, Supervisor, Knowledge Graph, and SPARQL Query agents; converts NL questions into SPARQL using KG ontology and IDs; queries ENPKG; evaluation on 50 curated questions against GPT-4o baseline.

Result: MetaboT achieved 83.67% accuracy on 50 questions, vs 8.16% for the baseline; demonstrates effective translation to SPARQL and retrieval of structured results.

Conclusion: MetaboT lowers barriers to semantic querying in metabolomics, enabling researchers to obtain structured KG data via natural language, with potential to drive data-driven discoveries; source code and demo available.

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [259] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: A structured, multi-agent AI framework aligned with NIST CSF 2.0, featuring graduated autonomy and a holistic cyber defense approach, with conceptual validation.


<details>
  <summary>Details</summary>
Motivation: To bridge AI agent theory with industry cybersecurity standards, enabling transparent, stepwise selection and deployment of AI solutions against cyber threats while accommodating varying maturity levels.

Method: Granular decomposition of NIST CSF 2.0 into tasks; linking AI properties (autonomy, adaptive learning, real-time responsiveness) to security requirements; defining autonomy levels (assisted, augmented, fully autonomous); outlining a unified detection, incident response, and governance strategy; conceptual validation to test alignment with real-world constraints.

Result: Conceptual validation demonstrates alignment with constraints and risk profiles, improvements in situational awareness and response speed, and enhanced adaptive risk management; establishes groundwork for empirically validated multi-agent systems.

Conclusion: Bridges theory and practice, enabling robust, standards-compliant multi-agent systems for cybersecurity with a foundation for future empirical validation.

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [260] [REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing](https://arxiv.org/abs/2510.01800)
*Thanh Ma,Tri-Tam La,Lam-Thu Le Huu,Minh-Nghi Nguyen,Khanh-Van Pham Luu,Huu-Hoa Nguyen*

Main category: cs.AI

TL;DR: REBot is an LLM-enabled advisory chatbot for academic regulation advising, powered by CatRAG, a hybrid retrieval+graph reasoning framework that fuses dense retrieval with a category-labeled knowledge graph. It achieves state-of-the-art results and is demonstrated via a web app.


<details>
  <summary>Details</summary>
Motivation: Academic regulation advising needs domain-specific, policy-aligned regulatory resources to interpret institutional policies accurately. Pure LLMs risk factual errors and poor domain alignment without structured retrieval and reasoning.

Method: Introduce CatRAG: a hybrid system combining retrieval augmented generation with graph-based reasoning. Build a hierarchical, category-labeled knowledge graph with semantic features for domain alignment. Use a lightweight intent classifier to route queries to relevant retrieval modules. Create a regulation-specific dataset and evaluate on classification and question answering tasks. Develop a web application to demonstrate real-world advising scenarios.

Result: Achieves state-of-the-art performance with an F1 score of 98.89% on the evaluation tasks. Demonstrates practical value of REBot in real-world academic advising through a web interface.

Conclusion: REBot effectively integrates retrieval and graph reasoning to deliver accurate, context-rich regulatory guidance for academic advising, with potential for real-world deployment.

Abstract: Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.

</details>


### [261] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: A conceptual design for a trustworthy co-learning model enabling bidirectional human-AI teaming in military operations, integrating adjustable autonomy, multi-layered control, bidirectional feedback, and collaborative decision-making, with exemplifications and practical recommendations.


<details>
  <summary>Details</summary>
Motivation: Rapidly evolving military threats and complex environments necessitate effective, ethical AI integration in operations. Moving beyond external system-level views to internal dynamics of human-AI teams addresses multidimensional safety, responsibility, and robustness, enabling continuous adaptation to battlefield conditions.

Method: Proposes a design framework‚Äîthe trustworthy co-learning model‚Äîbuilt around four dimensions: adjustable autonomy, multi-layered control, bidirectional feedback, and collaborative decision-making; features continuous exchanges of reasoning and uncertainties, dynamic calibration to mission state, system confidence, and environmental uncertainty; supported by concrete exemplifications and recommendations.

Result: A concrete architectural/design framework for trustworthy human-AI teaming, including exemplifications and recommendations to advance responsible deployment in military operations.

Conclusion: The framework offers a path toward more responsible, trustworthy, and robust human-AI collaboration in high-stakes military contexts by embedding co-learning and adaptable governance into the teaming model.

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [262] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: Two-stage Plan-Then-Action approach PTA-GRPO improves chain-of-thought reasoning by first distilling high-level guidance from LLMs for supervised fine-tuning, then using guidance-aware reinforcement learning to optimize both the final answer and the quality of the high-level guidance, yielding stable gains on math benchmarks across diverse models.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with global planning due to autoregressive, token-level generation, leading to local, incoherent, or inaccurate reasoning. Existing tree/RL methods incur high cost and may not yield optimal reasoning trajectories.

Method: Stage 1: Distill chain-of-thought (CoT) into compact high-level guidance with LLMs and fine-tune a model via supervised learning on that guidance. Stage 2: Apply guidance-aware reinforcement learning (Plan-Then-Action with Group Relative Policy Optimization) that jointly optimizes the final output and the quality of high-level guidance.

Result: PTA-GRPO consistently delivers stable and significant improvements across multiple mathematical reasoning benchmarks (MATH, AIME2024, AIME2025, AMC) and across diverse base models (Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, LLaMA3.2-3B).

Conclusion: The two-stage PTA-GRPO framework effectively enhances both high-level planning and fine-grained CoT reasoning, demonstrating strong generalization across models and tasks and addressing the limitations of purely autoregressive, locally-decided reasoning.

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [263] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicol√°s Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: A dense, token-level adversarial IRL reward is learned for LLM reasoning, used both as training signal and as an inference-time critic to rerank traces, improving correctness and enabling error localisation. Demonstrated on GSM8K with Llama3 and Qwen2.5, showing reward-guided reranking boosts reasoning performance and provides interpretable diagnostics.


<details>
  <summary>Details</summary>
Motivation: Current supervised fine-tuning emphasizes surface-form imitation and lacks explicit process-level supervision for multi-step reasoning. A reward that tracks reasoning steps could guide models toward correct, verifiable reasoning and enable interpretable error analysis.

Method: Train a dense, token-level reward model via adversarial inverse reinforcement learning from expert demonstrations. Use the learned reward to (i) provide step-level feedback during training to optimize a reasoning policy, and (ii) act as a critic at inference-time to rerank candidate traces under fixed compute budgets. Evaluate on GSM8K with Llama3 and Qwen2.5 backbones, comparing reward-guided reranking to baseline approaches.

Result: The dense reasoning rewards correlate with answer validity and enable localization of errors within traces. Reward-guided reranking improves predictive performance, especially for Llama-based policies, indicating the reward‚Äôs effectiveness as a learning signal for reasoning.

Conclusion: Unified reasoning rewards can serve as reusable process-level signals that enhance multi-step reasoning during both training and inference, with broad potential for improving accuracy and interpretability in LLM-based reasoning systems.

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [264] [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
*Pawe≈Ç Parys,Sairam Vaidya,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: Constrained Adaptive Rejection Sampling (CARS) makes rejection sampling for constrained language generation more efficient and distribution-preserving by adaptively pruning invalid constraint prefixes with a trie, yielding better sample efficiency and diversity across domains.


<details>
  <summary>Details</summary>
Motivation: There is a need to generate outputs that satisfy hard semantic/syntactic constraints (e.g., program fuzzing, molecular generation) without compromising the LM's distribution. Greedy constrained decoding distorts the distribution, while rejection sampling wastes computation by discarding invalid samples.

Method: Start from unconstrained LM sampling, build and update a trie of constraint-violating prefixes, and subtract their probability mass from future draws. This adaptive pruning prevents revisiting invalid prefixes, monotonically increases acceptance rates, and yields samples that exactly follow the constrained distribution.

Result: Across domains like program fuzzing and molecular generation, CARS achieves higher sample efficiency (fewer LM forward passes per valid sample) and stronger sample diversity than GCD and methods that approximate the LM distribution, while preserving the constrained distribution.

Conclusion: CARS provides a distribution-preserving, more efficient approach to constrained generation, avoiding distortion of the LM distribution and delivering better efficiency and diversity in constrained sampling.

Abstract: Language Models (LMs) are increasingly used in applications where generated
outputs must satisfy strict semantic or syntactic constraints. Existing
approaches to constrained generation fall along a spectrum: greedy constrained
decoding methods enforce validity during decoding but distort the LM's
distribution, while rejection sampling (RS) preserves fidelity but wastes
computation by discarding invalid outputs. Both extremes are problematic in
domains such as program fuzzing, where both validity and diversity of samples
are essential. We present Constrained Adaptive Rejection Sampling (CARS), an
approach that strictly improves the sample-efficiency of RS without
distributional distortion. CARS begins with unconstrained LM sampling and
adaptively rules out constraint-violating continuations by recording them in a
trie and subtracting their probability mass from future draws. This adaptive
pruning ensures that prefixes proven invalid are never revisited, acceptance
rates improve monotonically, and the resulting samples exactly follow the
constrained distribution. In experiments on a variety of domains -- e.g.,
program fuzzing and molecular generation -- CARS consistently achieves higher
efficiency -- measured in the number of LM forward passes per valid sample --
while also producing stronger sample diversity than both GCD and methods that
approximate the LM's distribution.

</details>


### [265] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,Cl√©mentine Bouleau,Vivian Tsai,Ma√´l Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: An empirical framework to evaluate how well LLMs align with collective human reasoning, using a large-scale social task to compare how different models reflect or mask human biases in group decision-making.


<details>
  <summary>Details</summary>
Motivation: To extend alignment research from individuals to groups, addressing how LLMs participate in collective decision-making and whether they reproduce, mask, or compensate for human biases.

Method: Conducted a large online experiment (N=748 groups) on the Lost at Sea task, randomly assigning groups to leader elections with either visible demographic attributes or pseudonymous aliases, then simulated matched LLM groups conditioned on the human data and benchmarked multiple models (Gemini 2.5, GPT-4.1, Claude Haiku 3.5, Gemma 3).

Result: LLM behaviors diverge: some mirror human biases, others mask or compensate for biases. Alignment with collective human behavior is context- and model-dependent, influenced by cues and inductive biases.

Conclusion: Dynamic, context-sensitive benchmarks are required to properly assess socially-aligned AI in collective reasoning; understanding collective alignment is essential as LLMs play a growing role in group decision-making.

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [266] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: A deterministic simulation framework for evaluating AI-generated peer reviews yields stable, evidence-based governance indicators‚Äîpredictable editorial outcomes and a fixed compliance rate‚Äîenabling scalable auditing of scholarly integrity.


<details>
  <summary>Details</summary>
Motivation: Growing submission volumes and unregulated AI threaten the integrity and auditability of peer review; there is a need for an objective, scalable benchmark to evaluate AI-generated reviews.

Method: A deterministic simulation framework analyzed 352 peer-review simulation reports to identify reliable system-state indicators. Findings include a majority 'Revise' rate (>50%) across disciplines, field-sensitive 'Reject' rates (up to ~45% in Health Sciences), and a stable 29% evidence-anchoring compliance rate invariant to task/domain.

Result: The framework exhibits consistent, rule-bound behavior, reducing AI stochasticity and providing a transparent, scalable tool for fairness auditing and governance in scholarly publishing.

Conclusion: The framework positions AI as part of institutional accountability, offering infrastructure to uphold trust in scholarly communication and to manage integrity risks across publishing workflows.

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [267] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: Restores textual semantics in tabular anomaly detection by enriching benchmarks with structured metadata and introducing a zero-shot LLM framework, showing semantic context improves detection and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current tabular anomaly detection benchmarks provide only raw data points and lack semantic context; anomaly definitions rely on domain knowledge. This limits research flexibility and the ability to leverage domain knowledge.

Method: Curate 20 tabular datasets enriched with structured textual metadata describing features and domain context; implement state-of-the-art anomaly detection approaches including classical, deep learning, and LLM-based methods; introduce a zero-shot LLM framework that uses semantic context without task-specific training.

Result: Semantic context improves detection performance and interpretability; ReTabAD provides a benchmark resource for systematic exploration of context-aware anomaly detection.

Conclusion: Textual metadata are valuable for context-aware AD; the benchmark enables broader research on leveraging domain knowledge and paves the way for future work in this area.

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [268] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: Systematic, multi-dimensional analysis of LLM depth usage shows depth effects are task-/metric-/model-dependent; early layers dominate in retrieval, while middle/deep layers support reasoning and coherence, especially under generation-based evaluation; most layers can be pruned under likelihood-based metrics; distillation can reshape deeper-layer roles.


<details>
  <summary>Details</summary>
Motivation: Address inconsistent claims that deep layers add little value by evaluating across diverse protocols, tasks, and architectures to understand when and how depth matters.

Method: Prune and analyze layers across diverse evaluation settings (likelihood-based vs generation-based), task categories, and architectures; quantify layer importance, examine distribution of knowledge/retrieval vs reasoning, and explore distillation to alter depth roles.

Result: Under likelihood metrics without generation, removing most layers preserves performance; initial layers are critical. Under generation-based evaluation, middle/deeper layers are crucial for reasoning and long-range coherence. Knowledge/retrieval relies on shallow layers; reasoning relies on deeper layers, which can be reshaped via distillation.

Conclusion: Depth usage in LLMs is heterogeneous and context-dependent; interpretation and compression should be task-, metric-, and model-aware.

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [269] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: Accuracy alone can misrepresent abstract reasoning in ARC-like tasks: text models may reach human accuracy but rely on shortcuts, while visual models underperform, yet still show partial abstraction; a rule-level framework is needed to assess true abstraction abilities.


<details>
  <summary>Details</summary>
Motivation: To determine whether state-of-the-art models genuinely learn and apply the abstractions targeted by ConceptARC, across modalities (text vs. visual) and tool usage, beyond surface patterns.

Method: Evaluate models on ConceptARC with varying input modalities (text vs. visual), ability to use external Python tools, and different reasoning effort. Measure output accuracy and perform fine-grained analysis of the natural-language rules models generate to explain solutions, comparing to human abstractions.

Result: Text-based models can match human accuracy, but best models rely on surface shortcuts and capture abstractions less often than humans. In visuals, accuracy drops, yet rule-level analysis shows a sizable share of abstraction-related rules, though they are often misapplied. Overall, models lag humans in abstract reasoning, and accuracy-alone assessments overestimate abstraction in text and underestimate it in vision.

Conclusion: The evaluation framework offers a more faithful, multimodal view of abstract reasoning and provides a principled way to track progress toward human-like, abstraction-centered intelligence.

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [270] [FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models](https://arxiv.org/abs/2510.02133)
*Karan Dua,Hitesh Laxmichand Patel,Puneet Mittal,Ranjeet Gupta,Amit Agarwal,Praneet Pabolu,Srikant Panda,Hansa Meghwani,Graham Horwood,Fahad Shah*

Main category: cs.AI

TL;DR: FlexDoc is a scalable synthetic data framework using stochastic schemas and parameterized sampling to generate multilingual semi-structured documents with rich annotations, boosting KIE performance and reducing annotation effort; deployed for enterprise document understanding.


<details>
  <summary>Details</summary>
Motivation: To overcome the data bottleneck in enterprise document understanding caused by privacy, legal constraints, and costly manual annotation, hindering scaling of models.

Method: Combines stochastic schemas and parameterized sampling to probabilistically model layout, visual structure, and content variability; generates diverse, realistic, multilingual semi-structured documents with annotations.

Result: When used to augment real data in KIE tasks, absolute F1 improves by up to 11%; annotation effort reduces by >90% relative to hard-template methods; validated in active deployment.

Conclusion: FlexDoc enables scalable synthetic data generation for enterprise document understanding, reducing data collection/annotation costs and accelerating model development at scale.

Abstract: Developing document understanding models at enterprise scale requires large,
diverse, and well-annotated datasets spanning a wide range of document types.
However, collecting such data is prohibitively expensive due to privacy
constraints, legal restrictions, and the sheer volume of manual annotation
needed - costs that can scale into millions of dollars. We introduce FlexDoc, a
scalable synthetic data generation framework that combines Stochastic Schemas
and Parameterized Sampling to produce realistic, multilingual semi-structured
documents with rich annotations. By probabilistically modeling layout patterns,
visual structure, and content variability, FlexDoc enables the controlled
generation of diverse document variants at scale. Experiments on Key
Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data
improves the absolute F1 Score by up to 11% when used to augment real datasets,
while reducing annotation effort by over 90% compared to traditional
hard-template methods. The solution is in active deployment, where it has
accelerated the development of enterprise-grade document understanding models
while significantly reducing data acquisition and annotation costs.

</details>


### [271] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: A benchmark and evaluation framework for Deep Research Agents (DRAs) with 214 expert questions across 10 domains; assesses long-form report outputs with semantic quality, topical focus, and retrieval trust; DRAs outperform web-search-augmented models but have room for improvement.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for DRAs fail to capture evaluation dimensions, response formatting, and scoring necessary to assess complex, multi-source reasoning; a robust, multidimensional evaluation framework is needed to assess DRA capabilities.

Method: Develop 214 expert-curated challenging queries across 10 broad domains; create manually constructed reference bundles for composite evaluation; design a multidimensional scoring system for long-form reports including semantic quality, topical focus, and retrieval trustworthiness; compare mainstream DRAs against web-search-tool-augmented reasoning models; perform extensive experiments.

Result: Mainstream DRAs outperform web-search augmented models; however, substantial room for improvement remains; the benchmark provides a solid foundation for capability assessment and architectural refinement in DRA systems.

Conclusion: The study offers a robust framework for capability assessment, architectural refinement, and paradigm advancement in DRA systems.

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [272] [UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194)
*Yuhao Sun,Zhuoer Xu,Shiwen Cui,Kun Yang,Lingyun Yu,Yongdong Zhang,Hongtao Xie*

Main category: cs.AI

TL;DR: UpSafe¬∞C proposes safety-aware upcycling of LLMs by converting safety-critical layers into a sparse Mixture-of-Experts (MoE) with a router as a soft guardrail, plus a two-stage SFT and an inference-time safety temperature to dynamically trade safety and utility, achieving better safety with competitive performance.


<details>
  <summary>Details</summary>
Motivation: Safety in LLMs is constrained by static guardrails and post-hoc alignments. There is a need for modular, controllable safety that can adapt at inference time without sacrificing utility.

Method: Identify safety-critical layers, upcycle them into a sparse MoE; router selectively activates original MLPs and safety experts. Employ a two-stage supervised fine-tuning to improve safety discrimination while preserving capabilities. Introduce a safety temperature to adjust safety-utility trade-off during inference.

Result: Empirical results show robust safety improvements against harmful and jailbreak inputs across multiple benchmarks, base models, and scales, with maintenance of competitive general task performance. Safety temperature enables fine-grained inference-time control and Pareto-optimal trade-off frontier.

Conclusion: Shifts safety design from static alignment toward dynamic, modular, and inference-aware control via architectural upcycling, enabling safer LLMs with tunable trade-offs.

Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.

</details>


### [273] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: RLVR can shrink the reasoning boundary due to negative interference and winner-take-all dynamics, driven by on-policy sampling. A simple data-curation approach that emphasizes low-likelihood problems improves Pass@k, mitigating shrinkage.


<details>
  <summary>Details</summary>
Motivation: To understand why RLVR shrinks the reasoning capacity of LLMs, diagnose the learning dynamics that cause failure, and identify remedies to preserve or expand the reasoning boundary.

Method: Theoretical analysis combined with empirical experiments on multiple mathematical reasoning benchmarks. The study identifies negative interference (solving some problems lowers success on others) and a winner-take-all effect (high-likelihood problems are disproportionately reinforced). It attributes these phenomena to on-policy sampling in standard RL objectives and proposes a data-curation algorithm that targets low-likelihood problems and evaluates its impact on Pass@k.

Result: Two main phenomena are observed: (1) negative interference reduces the probability of correct solutions for other problems, leading to a decline in Pass@k; (2) winner-take-all concentrates learning on high-likelihood, correct solutions under the base model, suppressing low-likelihood ones. On-policy sampling drives convergence toward narrow strategies. The proposed data-curation algorithm focusing on low-likelihood problems yields notable Pass@k improvements. Code is provided at the authors' repository.

Conclusion: RLVR can be fragile due to on-policy dynamics that favor a narrow set of solutions. Mitigation via targeted data curation for low-likelihood problems can enhance reasoning performance, suggesting broader applicability of such balancing strategies for RL-based instruction tuning and advising caution when relying on standard RL objectives for broad reasoning capabilities.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [274] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: Behavior Best-of-N (bBoN) scales computer-use agents by generating multiple rollouts and selecting among them via behavior narratives, improving robustness and achieving state-of-the-art performance on OSWorld with strong generalization to other platforms.


<details>
  <summary>Details</summary>
Motivation: CUAs are unreliable and highly variable on long-horizon, complex tasks; robust performance requires scalable, structured trajectory understanding and selection.

Method: Generate multiple rollouts per task, describe each rollout with behavior narratives, and select among them; perform ablations; evaluate on OSWorld, WindowsAgentArena, and AndroidWorld.

Result: Achieves SoTA performance of 69.9% on OSWorld and near-human 72%; strong generalization to WindowsAgentArena and AndroidWorld; ablations validate design choices; highlights effectiveness of scaling CUAs when trajectories are understood and selected.

Conclusion: Structured trajectory understanding paired with principled selection (bBoN) is an effective framework for scaling CUAs, enabling robust performance and broad generalization.

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [275] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: RLAD introduces a two-player RL framework that learns concise reasoning abstractions to guide solution generation, improving generalization to harder problems by focusing test-time compute on abstraction generation.


<details>
  <summary>Details</summary>
Motivation: Reasoning with large models often relies on pattern matching and verbose exploration rather than reusable procedural knowledge. There is a need for concise abstractions that capture procedural and factual knowledge to steer deduction.

Method: Propose multiple reasoning abstractions for a problem, then use RL to incentivize building a solution that leverages these abstractions. The framework, RLAD, jointly trains an abstraction generator and a solution generator in a two-player setup, enabling structured exploration and decoupled learning signals.

Result: The approach improves generalization to harder problems and reveals that allocating more test-time compute to generating abstractions yields greater gains than expanding solution generation budgets at test time.

Conclusion: Reasoning abstractions effectively guide exploration and enable more robust, scalable reasoning in RL-trained models; RLAD is a practical framework for leveraging such abstractions.

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


### [276] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: A lightweight bridge-based framework (BioX-Bridge) enables unsupervised cross-modal knowledge transfer for biosignals by aligning intermediate representations between foundation models across modalities, reducing trainable parameters by 88‚Äì99% without sacrificing, and often improving, transfer performance.


<details>
  <summary>Details</summary>
Motivation: Biosignals are interrelated, allowing tasks to be performed with different modalities. The scarcity of large labeled datasets for each modality hinders task-specific model training. Unsupervised cross-modal transfer offers a resource-efficient alternative, but existing methods rely on costly teacher-student distillation, which is impractical with large foundation models.

Method: Introduce BioX-Bridge, a lightweight bridge network that aligns intermediate representations to enable information flow between foundation models and across biosignal modalities. Propose an efficient strategy to select alignment positions for the bridge and a flexible prototype network architecture for the bridge.

Result: Across multiple biosignal modalities, tasks, and datasets, BioX-Bridge reduces trainable parameters by 88‚Äì99% while maintaining or improving transfer performance relative to state-of-the-art methods.

Conclusion: Unsupervised cross-modal transfer can be made practical and scalable for biosignals by using a compact bridge that aligns representations between foundation models across modalities, enabling efficient knowledge transfer with far fewer trainable parameters.

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [277] [Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge](https://arxiv.org/abs/2510.01348)
*Michal Werner,David ƒåapek,Tom√°≈° Musil,Ond≈ôej Franƒõk,Tom√°≈° B√°ƒça,Martin Saska*

Main category: cs.RO

TL;DR: Fully onboard, CPU-only GNSS-denied UAV system for long-range navigation (9 km) using LiDAR heightmap matching to a geo-data map, fused with odometry via a clustered particle filter; achieved kilometer-scale flights and reduced drift in real time.


<details>
  <summary>Details</summary>
Motivation: Enable reliable long-range UAV operation in GNSS-denied environments where odometry drifts and loop closures are unreliable, under strict compute constraints.

Method: Integrated perception, mapping, planning, and control. Introduces drift-correction by gradient-template matching LiDAR-derived local heightmaps to a prior geo-data heightmap; fuses this evidence with odometry in a clustered particle filter; fully onboard CPU-only implementation and evaluation in the SPRIN-D challenge.

Result: Executed kilometer-scale flights across urban, forest, and open-field terrain; substantially reduced drift compared with raw odometry; real-time performance on CPU; detailed system architecture and localization pipeline; evaluation and deployment insights.

Conclusion: Shows the feasibility of GNSS-denied UAV autonomy with lightweight, onboard localization using geo-data heightmaps and particle-filter fusion, providing practical guidelines for design and deployment in real-world environments.

Abstract: Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied
environments is challenging: integrating odometry leads to drift, loop closures
are unavailable in previously unseen areas and embedded platforms provide
limited computational power. We present a fully onboard UAV system developed
for the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km
long-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS
or prior dense mapping. The system integrates perception, mapping, planning,
and control with a lightweight drift-correction method that matches
LiDAR-derived local heightmaps to a prior geo-data heightmap via
gradient-template matching and fuses the evidence with odometry in a clustered
particle filter. Deployed during the competition, the system executed
kilometer-scale flights across urban, forest, and open-field terrain and
reduced drift substantially relative to raw odometry, while running in real
time on CPU-only hardware. We describe the system architecture, the
localization pipeline, and the competition evaluation, and we report practical
insights from field deployment that inform the design of GNSS-denied UAV
autonomy.

</details>


### [278] [Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels](https://arxiv.org/abs/2510.01357)
*Alejandro Gonzalez-Garcia,Wei Xiao,Wei Wang,Alejandro Astudillo,Wilm Decr√©,Jan Swevers,Carlo Ratti,Daniela Rus*

Main category: cs.RO

TL;DR: Proposes a MPC-CBF safe motion planning framework for autonomous vessels in narrow waterways using adaptive time-varying ellipse obstacles to reduce conservativeness and enable real-time performance.


<details>
  <summary>Details</summary>
Motivation: To enable safe, real-time navigation for autonomous vessels in constrained inland waterways where traditional planners are either too conservative or computationally heavy.

Method: Use time-varying inflated ellipse obstacles with an inflation radius that depends on the vessel‚Äìobstacle relative position and attitude. Solve an approximate motion plan via Model Predictive Control (MPC) and enforce safety with high-order Control Barrier Functions (CBFs) that account for the varying inflation. Validate through simulations and real-world experiments.

Result: The approach enables fully actuated autonomous vessels to navigate through narrow spaces in real time, reduces conservativeness compared to fixed-ellipse representations, and helps resolve potential deadlocks while maintaining safety.

Conclusion: MPC combined with high-order CBFs and an adaptive obstacle-inflation scheme provides a practical, less conservative, real-time safe motion-planning solution for autonomous vessels in challenging environments; the framework shows promise for broader robot navigation tasks in constrained spaces.

Abstract: Safe motion planning is essential for autonomous vessel operations,
especially in challenging spaces such as narrow inland waterways. However,
conventional motion planning approaches are often computationally intensive or
overly conservative. This paper proposes a safe motion planning strategy
combining Model Predictive Control (MPC) and Control Barrier Functions (CBFs).
We introduce a time-varying inflated ellipse obstacle representation, where the
inflation radius is adjusted depending on the relative position and attitude
between the vessel and the obstacle. The proposed adaptive inflation reduces
the conservativeness of the controller compared to traditional fixed-ellipsoid
obstacle formulations. The MPC solution provides an approximate motion plan,
and high-order CBFs ensure the vessel's safety using the varying inflation
radius. Simulation and real-world experiments demonstrate that the proposed
strategy enables the fully-actuated autonomous robot vessel to navigate through
narrow spaces in real time and resolve potential deadlocks, all while ensuring
safety.

</details>


### [279] [A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots](https://arxiv.org/abs/2510.01381)
*Spencer Teetaert,Sven Lilge,Jessica Burgner-Kahrs,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: A probabilistic, continuous-time state estimation framework for continuum robots using Gaussian-process‚Äìbased factors in a factor-graph, enabling robust, sparsity-friendly estimation of pose, velocity, and strain with continuous-time interpolation and resilience to disturbances.


<details>
  <summary>Details</summary>
Motivation: Current state estimators for continuum robots rely on computationally heavy dynamics, crude shape models, or quasi-static assumptions, making them sensitive to unmodeled disturbances and data dropouts.

Method: Introduce continuous-time kinematic factors corrupted by a Gaussian process noise. Use a simple robot model with high-rate sensing; perform factor-graph optimization to estimate the mean and covariance of pose, velocity, and strain, with continuous-time interpolation in time or space. The interpolation scheme also allows incorporating measurements on states that are not explicitly estimated. The formulation yields sparsity, giving linear solve complexity with respect to time and interpolation queries.

Result: Validated on a continuum robot equipped with gyroscope and pose sensors, demonstrating adaptability to external forces and data dropouts, and providing interpretable mean/covariance estimates that can be interpolated in time/space.

Conclusion: The proposed GP-based continuous-time factor-graph framework offers robust, scalable, and interpolation-capable state estimation for continuum robots, resilient to disturbances and imperfect data, suitable for real-world deployment.

Abstract: State estimation techniques for continuum robots (CRs) typically involve
using computationally complex dynamic models, simplistic shape approximations,
or are limited to quasi-static methods. These limitations can be sensitive to
unmodelled disturbances acting on the robot. Inspired by a factor-graph
optimization paradigm, this work introduces a continuous-time stochastic state
estimation framework for continuum robots. We introduce factors based on
continuous-time kinematics that are corrupted by a white noise Gaussian process
(GP). By using a simple robot model paired with high-rate sensing, we show
adaptability to unmodelled external forces and data dropout. The result
contains an estimate of the mean and covariance for the robot's pose, velocity,
and strain, each of which can be interpolated continuously in time or space.
This same interpolation scheme can be used during estimation, allowing for
inclusion of measurements on states that are not explicitly estimated. Our
method's inherent sparsity leads to a linear solve complexity with respect to
time and interpolation queries in constant time. We demonstrate our method on a
CR with gyroscope and pose sensors, highlighting its versatility in real-world
systems.

</details>


### [280] [VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation](https://arxiv.org/abs/2510.01388)
*Arthur Zhang,Xiangyun Meng,Luca Calliari,Dong-Ki Kim,Shayegan Omidshafiei,Joydeep Biswas,Ali Agha,Amirreza Shaban*

Main category: cs.RO

TL;DR: VENTURA is a vision-language navigation system that generates a visual plan (path mask) via diffusion models and grounds it with a lightweight behavior-cloning policy to execute robust, context-aware robotic navigation guided by natural language.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between powerful vision-language models and robotics control, addressing misalignment in action spaces and pretraining objectives; enable fine-grained, context-aware navigation with scalable, self-supervised training.

Method: Finetune internet-pretrained image diffusion models to produce path masks in image space (visual plans) rather than low-level actions; learn a lightweight behavior-cloning policy to convert plans into executable trajectories; supervise on path masks derived from self-supervised tracking models paired with VLM-augmented captions, avoiding manual pixel-level annotation; evaluate on real-world tasks including object reaching, obstacle avoidance, and terrain preference.

Result: Outperforms state-of-the-art foundation-model baselines in real-world object-reaching, obstacle avoidance, and terrain preference tasks; 33% higher success rates and 54% fewer collisions across seen and unseen scenarios; demonstrates generalization to unseen combinations of distinct tasks (emergent compositionality).

Conclusion: Visual plans produced by diffusion models, when grounded by a lightweight policy, offer scalable, transferable navigation capable of following natural language and generalizing to new task combinations; the approach reduces data labeling needs and improves safety and performance in open-world settings.

Abstract: Robots must adapt to diverse human instructions and operate safely in
unstructured, open-world environments. Recent Vision-Language models (VLMs)
offer strong priors for grounding language and perception, but remain difficult
to steer for navigation due to differences in action spaces and pretraining
objectives that hamper transferability to robotics tasks. Towards addressing
this, we introduce VENTURA, a vision-language navigation system that finetunes
internet-pretrained image diffusion models for path planning. Instead of
directly predicting low-level actions, VENTURA generates a path mask (i.e. a
visual plan) in image space that captures fine-grained, context-aware
navigation behaviors. A lightweight behavior-cloning policy grounds these
visual plans into executable trajectories, yielding an interface that follows
natural language instructions to generate diverse robot behaviors. To scale
training, we supervise on path masks derived from self-supervised tracking
models paired with VLM-augmented captions, avoiding manual pixel-level
annotation or highly engineered data collection setups. In extensive real-world
evaluations, VENTURA outperforms state-of-the-art foundation model baselines on
object reaching, obstacle avoidance, and terrain preference tasks, improving
success rates by 33% and reducing collisions by 54% across both seen and unseen
scenarios. Notably, we find that VENTURA generalizes to unseen combinations of
distinct tasks, revealing emergent compositional capabilities. Videos, code,
and additional materials: https://venturapath.github.io

</details>


### [281] [INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models](https://arxiv.org/abs/2510.01389)
*Ulas Berk Karli,Ziyao Shangguan,Tesca FItzgerald*

Main category: cs.RO

TL;DR: INSIGHT uses token-level uncertainty signals from a VLA model to predict when to request human help. Temporal modeling with transformers outperforms static scores; strong supervision yields fine-grained triggers while weak supervision remains viable with aligned training/evaluation, across ID and OOD settings.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action models generalize well but lack introspective mechanisms to anticipate failures or solicit human assistance. There is a need for reliable, scalable triggers for human intervention and for enabling active learning and real-time error mitigation.

Method: From a pi_0-FAST base model, extract per-token entropy, log-probability, and Dirichlet-based estimates of aleatoric and epistemic uncertainty. Train compact transformer classifiers to map token-level uncertainty sequences to help-trigger signals. Compare strong vs weak supervision regimes and evaluate under in-distribution and out-of-distribution tasks, emphasizing the temporal evolution of uncertainty rather than static scores.

Result: Strong supervision captures fine-grained uncertainty dynamics for reliable help triggers; weak supervision, though noisier, remains competitive when training and evaluation are aligned, offering scalable annotation. Modeling temporal evolution of token-level uncertainty with transformers yields substantially higher predictive power than static sequence-level metrics. This constitutes the first systematic evaluation of uncertainty-based introspection in VLAs, enabling active learning and real-time selective human intervention.

Conclusion: Uncertainty-based introspection is viable for VLA systems, with clear trade-offs between supervision strength and scalability. The study points to future work in active learning and real-time error mitigation through selective human collaboration, leveraging temporal uncertainty dynamics for better help-trigger decisions.

Abstract: Recent Vision-Language-Action (VLA) models show strong generalization
capabilities, yet they lack introspective mechanisms for anticipating failures
and requesting help from a human supervisor. We present \textbf{INSIGHT}, a
learning framework for leveraging token-level uncertainty signals to predict
when a VLA should request help. Using $\pi_0$-FAST as the underlying model, we
extract per-token \emph{entropy}, \emph{log-probability}, and Dirichlet-based
estimates of \emph{aleatoric and epistemic uncertainty}, and train compact
transformer classifiers to map these sequences to help triggers. We explore
supervision regimes for strong or weak supervision, and extensively compare
them across in-distribution and out-of-distribution tasks. Our results show a
trade-off: strong labels enable models to capture fine-grained uncertainty
dynamics for reliable help detection, while weak labels, though noisier, still
support competitive introspection when training and evaluation are aligned,
offering a scalable path when dense annotation is impractical. Crucially, we
find that modeling the temporal evolution of token-level uncertainty signals
with transformers provides far greater predictive power than static
sequence-level scores. This study provides the first systematic evaluation of
uncertainty-based introspection in VLAs, opening future avenues for active
learning and for real-time error mitigation through selective human
intervention.

</details>


### [282] [Beyond Collision Cones: Dynamic Obstacle Avoidance for Nonholonomic Robots via Dynamic Parabolic Control Barrier Functions](https://arxiv.org/abs/2510.01402)
*Hun Kuk Park,Taekyung Kim,Dimitra Panagou*

Main category: cs.RO

TL;DR: Introduces Dynamic Parabolic Control Barrier Function (DPCBF) to relax safety constraints for nonholonomic robots in dynamic clutter. The parabola-based CBF adapts to obstacle distance and relative speed, improving QP feasibility and navigation success in dense scenarios (up to 100 dynamic obstacles) over collision-cone baselines, validated on a kinematic bicycle model.


<details>
  <summary>Details</summary>
Motivation: Collision-cone/velocity-obstacle constraints are conservative for nonholonomic robots, often causing CBF-QP infeasibility in dense, dynamic environments. There is a need for a safety boundary that adapts to motion context.

Method: Formulate a dynamic parabolic safe set; adapt parabola vertex and curvature as functions of distance to obstacle and magnitude of relative velocity; derive CBF conditions; prove CBF validity for a kinematic bicycle with input constraints; conduct extensive simulations comparing to baselines.

Result: Demonstrates higher navigation success rates and QP feasibility; handles dense environments with up to 100 dynamic obstacles; outperforms collision-cone-based methods which fail due to infeasibility in dense settings.

Conclusion: DPCBF provides a less conservative safety boundary enabling safer and more feasible control for nonholonomic robots in dynamic clutter; shows significant performance gains and broad potential applicability.

Abstract: Control Barrier Functions (CBFs) are a powerful tool for ensuring the safety
of autonomous systems, yet applying them to nonholonomic robots in cluttered,
dynamic environments remains an open challenge. State-of-the-art methods often
rely on collision-cone or velocity-obstacle constraints which, by only
considering the angle of the relative velocity, are inherently conservative and
can render the CBF-based quadratic program infeasible, particularly in dense
scenarios. To address this issue, we propose a Dynamic Parabolic Control
Barrier Function (DPCBF) that defines the safe set using a parabolic boundary.
The parabola's vertex and curvature dynamically adapt based on both the
distance to an obstacle and the magnitude of the relative velocity, creating a
less restrictive safety constraint. We prove that the proposed DPCBF is valid
for a kinematic bicycle model subject to input constraints. Extensive
comparative simulations demonstrate that our DPCBF-based controller
significantly enhances navigation success rates and QP feasibility compared to
baseline methods. Our approach successfully navigates through dense
environments with up to 100 dynamic obstacles, scenarios where collision
cone-based methods fail due to infeasibility.

</details>


### [283] [How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?](https://arxiv.org/abs/2510.01404)
*Lexi Foland,Thomas Cohn,Adam Wei,Nicholas Pfaff,Boyuan Chen,Russ Tedrake*

Main category: cs.RO

TL;DR: Diffusion policies learn only a coarse approximation of kinematic constraint manifolds; data size and quality influence learning; curvature shows no clear link to constraint satisfaction or task success; hardware validation supports findings.


<details>
  <summary>Details</summary>
Motivation: To understand whether diffusion-based policies truly learn and obey kinematic constraint manifolds, beyond just achieving task-level success, and to identify factors that affect this learning.

Method: Case study on a bimanual pick-and-place task that enforces a kinematic constraint. Systematically examined three factors‚Äîdataset size, dataset quality, and manifold curvature‚Äîthrough simulations and a hardware evaluation."

Result: Diffusion policies converge to a coarse representation of the constraint manifold. Reductions in dataset size and quality degrade learning of the constraint. Curvature of the manifold shows inconclusive correlations with constraint satisfaction and task success.

Conclusion: Learned constraint representations by diffusion policies are data-dependent and only approximate the true manifold; curvature effects are inconclusive. Hardware experiments validate the relevance of the findings for real-world robotic applications.

Abstract: Diffusion policies have shown impressive results in robot imitation learning,
even for tasks that require satisfaction of kinematic equality constraints.
However, task performance alone is not a reliable indicator of the policy's
ability to precisely learn constraints in the training data. To investigate, we
analyze how well diffusion policies discover these manifolds with a case study
on a bimanual pick-and-place task that encourages fulfillment of a kinematic
constraint for success. We study how three factors affect trained policies:
dataset size, dataset quality, and manifold curvature. Our experiments show
diffusion policies learn a coarse approximation of the constraint manifold with
learning affected negatively by decreases in both dataset size and quality. On
the other hand, the curvature of the constraint manifold showed inconclusive
correlations with both constraint satisfaction and task success. A hardware
evaluation verifies the applicability of our results in the real world. Project
website with additional results and visuals:
https://diffusion-learns-kinematic.github.io

</details>


### [284] [AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation](https://arxiv.org/abs/2510.01433)
*Anukriti Singh,Kasra Torshizi,Khuzema Habib,Kelin Yu,Ruohan Gao,Pratap Tokekar*

Main category: cs.RO

TL;DR: Efficient, affordance-guided keypoint policy AFFORD2ACT distills semantic 2D keypoints from a text prompt and a single image into a compact 38D state, enabling fast, data-efficient manipulation with strong generalization (82% success on unseen objects).


<details>
  <summary>Details</summary>
Motivation: Vision-based robot learning with dense inputs is computationally heavy and entangles background features. Existing keypoint methods either rely on manual heuristics or task-specific selection, hindering scalability and semantic understanding.

Method: A three-stage pipeline: (1) affordance filtering from a text prompt to select relevant capabilities, (2) construction of category-level keypoints, and (3) transformer-based policy learning with embedded gating to reason about the most relevant keypoints, yielding a 38-dimensional state; training time is about 15 minutes.

Result: In real-world manipulation tasks, AFFORD2ACT improves data efficiency and enables real-time operation without proprioception or dense representations, achieving an 82% success rate on unseen objects, novel categories, backgrounds, and distractors.

Conclusion: AFFORD2ACT provides a scalable, lightweight, and semantically grounded framework for manipulation policies by distilling keypoints from affordances, enabling robust generalization across objects and backgrounds.

Abstract: Vision-based robot learning often relies on dense image or point-cloud
inputs, which are computationally heavy and entangle irrelevant background
features. Existing keypoint-based approaches can focus on manipulation-centric
features and be lightweight, but either depend on manual heuristics or
task-coupled selection, limiting scalability and semantic understanding. To
address this, we propose AFFORD2ACT, an affordance-guided framework that
distills a minimal set of semantic 2D keypoints from a text prompt and a single
image. AFFORD2ACT follows a three-stage pipeline: affordance filtering,
category-level keypoint construction, and transformer-based policy learning
with embedded gating to reason about the most relevant keypoints, yielding a
compact 38-dimensional state policy that can be trained in 15 minutes, which
performs well in real-time without proprioception or dense representations.
Across diverse real-world manipulation tasks, AFFORD2ACT consistently improves
data efficiency, achieving an 82% success rate on unseen objects, novel
categories, backgrounds, and distractors.

</details>


### [285] [Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation](https://arxiv.org/abs/2510.01438)
*Minglun Wei,Xintong Yang,Yu-Kun Lai,Ze Ji*

Main category: cs.RO

TL;DR: A differentiable-physics, curriculum-guided trajectory optimization framework for powder transport in lab settings that improves stability and task success over a reinforcement learning baseline.


<details>
  <summary>Details</summary>
Motivation: Powders are notoriously hard to manipulate precisely in automated labs; achieving stable, accurate transport is essential for scalable powder-handling workflows.

Method: Integrates differentiable physics simulation to model granular dynamics, uses a low-dimensional skill-space parameterization to lower optimization complexity, and employs a curriculum-based strategy to progressively master long-horizon tasks, enabling end-to-end optimization of contact-rich robot trajectories.

Result: Experiments show higher task success rates and improved stability compared with a reinforcement learning baseline.

Conclusion: The proposed framework enables robust, efficient end-to-end optimization for granular powder transport in lab automation, illustrating effective combination of differentiable physics, skill-space reduction, and curriculum learning for complex contact-rich manipulation.

Abstract: Robotic automation is accelerating scientific discovery by reducing manual
effort in laboratory workflows. However, precise manipulation of powders
remains challenging, particularly in tasks such as transport that demand
accuracy and stability. We propose a trajectory optimisation framework for
powder transport in laboratory settings, which integrates differentiable
physics simulation for accurate modelling of granular dynamics, low-dimensional
skill-space parameterisation to reduce optimisation complexity, and a
curriculum-based strategy that progressively refines task competence over long
horizons. This formulation enables end-to-end optimisation of contact-rich
robot trajectories while maintaining stability and convergence efficiency.
Experimental results demonstrate that the proposed method achieves superior
task success rates and stability compared to the reinforcement learning
baseline.

</details>


### [286] [Touching the tumor boundary: A pilot study on ultrasound based virtual fixtures for breast-conserving surgery](https://arxiv.org/abs/2510.01452)
*Laura Connolly,Tamas Ungi,Adnan Munawar,Anton Deguet,Chris Yeung,Russell H. Taylor,Parvin Mousavi,Gabor Fichtinger Keyvan Hashtrudi-Zaad*

Main category: cs.RO

TL;DR: A haptic-guided cooperative robotic system using virtual fixtures improves tumor boundary localization in simulated breast-conserving surgery, reducing cognitive load and improving margins compared to non-haptic guidance.


<details>
  <summary>Details</summary>
Motivation: Difficulties in delineating mobile, non-palpable, and irregular breast tumors during breast-conserving surgery necessitate improved guidance to achieve clean surgical margins.

Method: Retrofit a small haptic robot with an electrocautery blade for cooperative control; use ultrasound and electromagnetic navigation to identify tumor boundaries; impose a forbidden-region virtual fixture when the tool hits the boundary; compare tumor resections in simulants with and without haptic guidance; assess results qualitatively and quantitatively.

Result: Virtual fixture guidance improves resection margins; users report lower mental demand, frustration, and perceived effort with haptic guidance; identification of unanticipated workflow impacts informing design/training.

Conclusion: Virtual fixtures appear beneficial for localizing tumor boundaries in simulated breast-conserving surgery; future work includes extensive user studies and system refinements to validate and optimize the guidance system.

Abstract: Purpose: Delineating tumor boundaries during breast-conserving surgery is
challenging as tumors are often highly mobile, non-palpable, and have
irregularly shaped borders. To address these challenges, we introduce a
cooperative robotic guidance system that applies haptic feedback for tumor
localization. In this pilot study, we aim to assess if and how this system can
be successfully integrated into breast cancer care.
  Methods: A small haptic robot is retrofitted with an electrocautery blade to
operate as a cooperatively controlled surgical tool. Ultrasound and
electromagnetic navigation are used to identify the tumor boundaries and
position. A forbidden region virtual fixture is imposed when the surgical tool
collides with the tumor boundary. We conducted a study where users were asked
to resect tumors from breast simulants both with and without the haptic
guidance. We then assess the results of these simulated resections both
qualitatively and quantitatively.
  Results: Virtual fixture guidance is shown to improve resection margins. On
average, users find the task to be less mentally demanding, frustrating, and
effort intensive when haptic feedback is available. We also discovered some
unanticipated impacts on surgical workflow that will guide design adjustments
and training protocol moving forward.
  Conclusion: Our results suggest that virtual fixtures can help localize tumor
boundaries in simulated breast-conserving surgery. Future work will include an
extensive user study to further validate these results and fine-tune our
guidance system.

</details>


### [287] [VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs](https://arxiv.org/abs/2510.01483)
*Mohamad Al Mdfaa,Svetlana Lukina,Timur Akhtyamov,Arthur Nigmatzyanov,Dmitrii Nalberskii,Sergey Zagoruyko,Gonzalo Ferrer*

Main category: cs.RO

TL;DR: VL-KnG builds a spatiotemporal knowledge-graph-based visual scene understanding system for robot navigation, enabling persistent object memory and explainable spatial reasoning; it introduces WalkieKnowledge benchmark and demonstrates real-time feasibility with competitive performance.


<details>
  <summary>Details</summary>
Motivation: To address persistent memory, limited spatial reasoning, and poor scalability of current VLM-based navigation systems; need for persistent scene graphs and efficient querying for real-time robotic tasks.

Method: Process video in chunks using VLMs to build a persistent, identity-maintaining spatiotemporal knowledge graph; enable queryable graph-based spatial reasoning; introduce WalkieKnowledge benchmark with ~200 questions over 8 trajectories (~100 min); validate on a differential-drive robot for localization, navigation, planning; compare against Gemini 2.5 Pro; report accuracy and success metrics.

Result: Achieves 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5 Pro; provides explainable reasoning via knowledge graph; computational efficiency suitable for real-time deployment; scalable across tasks; benchmark and dataset to be released.

Conclusion: VL-KnG offers a practical, scalable framework for vision-language navigation with persistent scene understanding and explainable reasoning, demonstrated on real robots and new benchmark; promises broad applicability and future release of code/dataset.

Abstract: Vision-language models (VLMs) have shown potential for robot navigation but
encounter fundamental limitations: they lack persistent scene memory, offer
limited spatial reasoning, and do not scale effectively with video duration for
real-time application. We present VL-KnG, a Visual Scene Understanding system
that tackles these challenges using spatiotemporal knowledge graph construction
and computationally efficient query processing for navigation goal
identification. Our approach processes video sequences in chunks utilizing
modern VLMs, creates persistent knowledge graphs that maintain object identity
over time, and enables explainable spatial reasoning through queryable graph
structures. We also introduce WalkieKnowledge, a new benchmark with about 200
manually annotated questions across 8 diverse trajectories spanning
approximately 100 minutes of video data, enabling fair comparison between
structured approaches and general-purpose VLMs. Real-world deployment on a
differential drive robot demonstrates practical applicability, with our method
achieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5
Pro performance while providing explainable reasoning supported by the
knowledge graph, computational efficiency for real-time deployment across
different tasks, such as localization, navigation and planning. Code and
dataset will be released after acceptance.

</details>


### [288] [Pose Estimation of a Thruster-Driven Bioinspired Multi-Link Robot](https://arxiv.org/abs/2510.01485)
*Nicholas B. Andrews,Yanhao Yang,Sofya Akhetova,Kristi A. Morgansen,Ross L. Hatton*

Main category: cs.RO

TL;DR: Pose/shape estimation for a free-floating, bioinspired multi-link robot with unactuated joints and per-link gyros using an unscented Kalman filter augmented with Gaussian-process residual learning; shows data-efficient multi-gait generalization and reliable state estimation in hardware and offline analysis.


<details>
  <summary>Details</summary>
Motivation: Address reliable pose estimation for underactuated, minimally sensed robots where joints are unactuated and sensing is sparse; explore how GP residual learning and gait diversity can improve estimation robustness and reduce training data needs.

Method: Hardware proof-of-concept platform with link-mounted thrusters and a single gyroscope per link. State estimation via an Unscented Kalman Filter augmented with Gaussian process residual learning to model non-zero-mean, non-Gaussian noise. Training includes a multi-gait dataset (forward, backward, left, right, turning) and comparison against a forward-gait-only dataset. Evaluation performed on a forward-gait test trajectory.

Result: The pose estimation is reliable; GP residual learning compensates for non-ideal noise, and the multi-gait-trained filter generalizes well to a forward gait similar to the larger forward-only dataset, indicating overlap in the gait input space and enabling reduced training data.

Conclusion: UKF with GP residual learning is effective for underactuated, minimally sensed bioinspired robots; gait diversity can improve generalization and data efficiency, suggesting practical benefits for controlling such systems and guiding future work to broaden gait coverage and sensing.

Abstract: This work demonstrates pose (position and shape) estimation for a
free-floating, bioinspired multi-link robot with unactuated joints,
link-mounted thrusters for control, and a single gyroscope per link, resulting
in an underactuated, minimally sensed platform. Through a proof-of-concept
hardware experiment and offline Kalman filter analysis, we show that the
robot's pose can be reliably estimated. State estimation is performed using an
unscented Kalman filter augmented with Gaussian process residual learning to
compensate for non-zero-mean, non-Gaussian noise. We further show that a filter
trained on a multi-gait dataset (forward, backward, left, right, and turning)
performs comparably to one trained on a larger forward-gait-only dataset when
both are evaluated on the same forward-gait test trajectory. These results
reveal overlap in the gait input space, which can be exploited to reduce
training data requirements while enhancing the filter's generalizability across
multiple gaits.

</details>


### [289] [Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments](https://arxiv.org/abs/2510.01519)
*Wei Han Chen,Yuchen Liu,Alexiy Buynitsky,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: Hierarchical planning that combines a sparse global graph with a neural-field local planner solving the Eikonal PDE to improve ANTFields, addressing spectral bias and forgetting for scalable, accurate navigation in large, unknown indoor environments.


<details>
  <summary>Details</summary>
Motivation: Traditional sampling-based methods struggle with resolution control and scalability; imitation learning requires large demonstration sets; Active Neural Time Fields (ANTFields) suffer from spectral bias and catastrophic forgetting, hindering performance in complex environments.

Method: A two-level approach: (1) high-level sparse graph captures global connectivity; (2) low-level neural-field planner uses a physics-informed Eikonal PDE solver to navigate local obstacles and produce a smooth cost landscape, mitigating spectral bias and fitting difficulties.

Result: Validated in large-scale environments, showing enhanced adaptability and precision over prior methods, with strong potential for online exploration, mapping, and real-world navigation.

Conclusion: A hierarchical, physics-informed neural-field framework improves robustness and scalability of ANTFields for complex indoor navigation, addressing key shortcomings and enabling more reliable online planning.

Abstract: Robot navigation in large, complex, and unknown indoor environments is a
challenging problem. The existing approaches, such as traditional
sampling-based methods, struggle with resolution control and scalability, while
imitation learning-based methods require a large amount of demonstration data.
Active Neural Time Fields (ANTFields) have recently emerged as a promising
solution by using local observations to learn cost-to-go functions without
relying on demonstrations. Despite their potential, these methods are hampered
by challenges such as spectral bias and catastrophic forgetting, which diminish
their effectiveness in complex scenarios. To address these issues, our approach
decomposes the planning problem into a hierarchical structure. At the high
level, a sparse graph captures the environment's global connectivity, while at
the low level, a planner based on neural fields navigates local obstacles by
solving the Eikonal PDE. This physics-informed strategy overcomes common
pitfalls like spectral bias and neural field fitting difficulties, resulting in
a smooth and precise representation of the cost landscape. We validate our
framework in large-scale environments, demonstrating its enhanced adaptability
and precision compared to previous methods, and highlighting its potential for
online exploration, mapping, and real-world navigation.

</details>


### [290] [Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion](https://arxiv.org/abs/2510.01592)
*Shun Niijima,Ryoichi Tsuzaki,Noriaki Takasugi,Masaya Kinoshita*

Main category: cs.RO

TL;DR: A GPU-accelerated voxel-based method combines vertex-connected component labeling with RANSAC plane detection and convex hulls to perform fast, real-time multi-plane segmentation from dense point clouds, enabling robust legged locomotion tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of online planar mapping: depth-only segmentation has poor temporal integration, height-map representations cannot capture overhangs, and voxel-plane segmentation has not been explored for real-time use in legged robotics.

Method: A framework that uses vertex-based connected component labeling, RANSAC-based plane detection, and convex hull computation, implemented on GPU to extract planar regions from high-resolution 3D voxel maps built from accumulated point clouds.

Result: Achieves fast and accurate 3D multi-plane segmentation at over 30 Hz update rate at 0.01 m resolution, demonstrated in both simulated environments and physical legged robots, enabling real-time use of detected planes in locomotion.

Conclusion: The proposed GPU-accelerated voxel mapping and plane detection pipeline enables robust real-time exploitation of 3D planar structures for legged locomotion, addressing prior trade-offs between accuracy and efficiency and expanding real-time 3D perception capabilities.

Abstract: This paper proposes a real-time multi-plane segmentation method based on
GPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion.
Existing online planar mapping approaches struggle to balance accuracy and
computational efficiency: direct depth image segmentation from specific sensors
suffers from poor temporal integration, height map-based methods cannot
represent complex 3D structures like overhangs, and voxel-based plane
segmentation remains unexplored for real-time applications. To address these
limitations, we develop a novel framework that integrates vertex-based
connected component labeling with random sample consensus based plane detection
and convex hull, leveraging GPU parallel computing to rapidly extract planar
regions from point clouds accumulated in high-resolution 3D voxel maps.
Experimental results demonstrate that the proposed method achieves fast and
accurate 3D multi-plane segmentation at over 30 Hz update rate even at a
resolution of 0.01 m, enabling the detected planes to be utilized in real time
for locomotion tasks. Furthermore, we validate the effectiveness of our
approach through experiments in both simulated environments and physical legged
robot platforms, confirming robust locomotion performance when considering 3D
planar structures.

</details>


### [291] [MiniBEE: A New Form Factor for Compact Bimanual Dexterity](https://arxiv.org/abs/2510.01603)
*Sharfin Islam,Zewen Chen,Zhanpeng He,Swapneel Bhatt,Andres Permuy,Brock Taylor,James Vickery,Pedro Piacenza,Cheng Zhang,Matei Ciocarlie*

Main category: cs.RO

TL;DR: A compact two-armed bimanual system (MiniBEE) with two 3-DOF arms coupled to preserve the relative gripper pose, guided by a kinematic dexterity metric to maximize dexterous workspace; supports wearable data collection and deployment on a standard robot arm; demonstrated via kinematic analysis, design optimization, and an end-to-end imitation-learning pipeline for real-world bimanual manipulation.


<details>
  <summary>Details</summary>
Motivation: Traditional bimanual manipulation relies on two full six- or seven-DOF arms, which adds complexity and underutilizes workspace. A compact, wearable, low-DOF core that preserves relative gripper positioning can expand dexterous capabilities while reducing mass and complexity.

Method: Introduce MiniBEE: two 3-DOF arms coupled into a kinematic chain that maintains full relative gripper positioning. Formulate a kinematic dexterity metric to enlarge the dexterous workspace. Perform kinematic analysis and design optimization to maximize dexterous range. Provide an end-to-end pipeline where wearable demonstrations train imitation-learning policies, and validate both wearable data collection and deployment on a standard robot arm.

Result: Development of a kinematic dexterity metric-guided design for maximizing dexterous range, along with kinematic analyses and optimization. Demonstration of an end-to-end pipeline where wearable demonstrations train imitation-learning policies that perform robust, real-world bimanual manipulation, and validation of the approach.

Conclusion: MiniBEE enables a compact, lightweight bimanual system that expands the dexterous workspace. The kinematic dexterity metric guides design decisions and allows extending dexterity across the robot‚Äôs workspace. The framework supports two modes‚Äîwearable data collection and deployment on standard robot arms‚Äîbroadening applicability for learning-based bimanual manipulation.

Abstract: Bimanual robot manipulators can achieve impressive dexterity, but typically
rely on two full six- or seven- degree-of-freedom arms so that paired grippers
can coordinate effectively. This traditional framework increases system
complexity while only exploiting a fraction of the overall workspace for
dexterous interaction. We introduce the MiniBEE (Miniature Bimanual
End-effector), a compact system in which two reduced-mobility arms (3+ DOF
each) are coupled into a kinematic chain that preserves full relative
positioning between grippers. To guide our design, we formulate a kinematic
dexterity metric that enlarges the dexterous workspace while keeping the
mechanism lightweight and wearable. The resulting system supports two
complementary modes: (i) wearable kinesthetic data collection with self-tracked
gripper poses, and (ii) deployment on a standard robot arm, extending dexterity
across its entire workspace. We present kinematic analysis and design
optimization methods for maximizing dexterous range, and demonstrate an
end-to-end pipeline in which wearable demonstrations train imitation learning
policies that perform robust, real-world bimanual manipulation.

</details>


### [292] [ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations](https://arxiv.org/abs/2510.01607)
*Qiyuan Zeng,Chengmeng Li,Jude St. John,Zhongyi Zhou,Junjie Wen,Guorui Feng,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: ActiveUMI enables portable, head-motion‚Äìaware data collection to train generalizable bimanual robotic policies, achieving 70% success on in-distribution tasks and 56% on novel objects/environments.


<details>
  <summary>Details</summary>
Motivation: To scale real-world robot learning for complex manipulation by bridging human demonstrations to robots and leveraging active perception (operator head motion) as a crucial signal.

Method: A portable VR teleoperation setup with sensorized controllers mapped to robot end-effectors; precise pose alignment; immersive 3D model rendering; self-contained wearable computer; calibration methods; records egocentric perception by tracking head movements via a head-mounted display during operation.

Result: Evaluated on six bimanual tasks; policies trained on ActiveUMI data achieved 70% success on in-distribution tasks and 56% on novel objects/environments, demonstrating strong generalization.

Conclusion: Portable data collection plus learned active perception provides an effective, scalable pathway to generalizable, capable real-world robot policies.

Abstract: We present ActiveUMI, a framework for a data collection system that transfers
in-the-wild human demonstrations to robots capable of complex bimanual
manipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized
controllers that mirror the robot's end-effectors, bridging human-robot
kinematics via precise pose alignment. To ensure mobility and data quality, we
introduce several key techniques, including immersive 3D model rendering, a
self-contained wearable computer, and efficient calibration methods.
ActiveUMI's defining feature is its capture of active, egocentric perception.
By recording an operator's deliberate head movements via a head-mounted
display, our system learns the crucial link between visual attention and
manipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies
trained exclusively on ActiveUMI data achieve an average success rate of 70\%
on in-distribution tasks and demonstrate strong generalization, retaining a
56\% success rate when tested on novel objects and in new environments. Our
results demonstrate that portable data collection systems, when coupled with
learned active perception, provide an effective and scalable pathway toward
creating generalizable and highly capable real-world robot policies.

</details>


### [293] [FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models](https://arxiv.org/abs/2510.01642)
*Zijun Lin,Jiafei Duan,Haoquan Fang,Dieter Fox,Ranjay Krishna,Cheston Tan,Bihan Wen*

Main category: cs.RO

TL;DR: FailSafe introduces a failure generation and recovery system for Vision-Language-Action models in robotic manipulation, enabling scalable creation of failure-action data, improving robustness and generalization, and boosting VLA performance by up to 22.6% on Maniskill tasks.


<details>
  <summary>Details</summary>
Motivation: Current robotic VLA datasets provide only ground-truth trajectories or textual explanations of failures, lacking actionable recovery data and robust failure-explanation utilities; robots need to anticipate and recover from unpredictable failures to be reliable.

Method: Propose FailSafe: a failure generation and recovery pipeline that automatically produces diverse failure cases paired with executable recovery actions. It can be applied to any manipulation task in any simulator. Fine-tune LLaVa-OneVision-7B to obtain FailSafe-VLM. Train on generated data; evaluate on various VLA models.

Result: FailSafe-VLM enables detection and recovery from potential failures and improves three state-of-the-art VLA models (pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across Maniskill tasks. Demonstrates generalization across spatial configurations, viewpoints, and embodiments.

Conclusion: FailSafe provides scalable failure-action data generation for robust robotic VLA systems, generalizes across settings, and should be released as open-source to support community adoption.

Abstract: Recent advances in robotic manipulation have integrated low-level robotic
control into Vision-Language Models (VLMs), extending them into
Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve
strong performance in downstream robotic applications, supported by large-scale
crowd-sourced robot training data, they still inevitably encounter failures
during execution. Enabling robots to reason about and recover from
unpredictable and abrupt failures remains a critical challenge. Existing
robotic manipulation datasets, collected in either simulation or the real
world, primarily provide only ground-truth trajectories, leaving robots unable
to recover once failures occur. Moreover, the few datasets that address failure
detection typically offer only textual explanations, which are difficult to
utilize directly in VLA models. To address this gap, we introduce FailSafe, a
novel failure generation and recovery system that automatically produces
diverse failure cases paired with executable recovery actions. FailSafe can be
seamlessly applied to any manipulation task in any simulator, enabling scalable
creation of failure-action data. To demonstrate its effectiveness, we fine-tune
LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results
show that FailSafe-VLM successfully helps robotic arm detect and recover from
potential failures, improving the performance of three state-of-the-art VLA
models pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several
tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different
spatial configurations, camera viewpoints, and robotic embodiments. We plan to
release the FailSafe code to the community.

</details>


### [294] [Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation](https://arxiv.org/abs/2510.01648)
*Seungwon Choi,Donggyu Park,Seo-Yeon Hwang,Tae-Wan Kim*

Main category: cs.RO

TL;DR: Online learning of measurement reliability for robust visual-inertial odometry by leveraging multi-view geometric consistency as self-supervision, improving accuracy and robustness in real time.


<details>
  <summary>Details</summary>
Motivation: Static, uniform uncertainty in VIO measurements fails to capture dynamic, real-world error characteristics; a data-driven approach is needed to adapt measurement weighting during optimization.

Method: A statistical framework that learns measurement reliability online from sensor data and optimization results, using multi-view geometric consistency as self-supervision to infer landmark uncertainty and adaptively weight visual measurements during optimization.

Result: Demonstrates significant accuracy improvements on EuRoC: ~24% reduction in translation error and ~42% reduction in rotation error compared to baselines with fixed uncertainty; operates in real time and enhances robustness.

Conclusion: Online learning of measurement reliability can substantially improve VIO performance, enabling adaptive weighting of measurements; code will be publicly available to support reproducibility.

Abstract: A fundamental challenge in robust visual-inertial odometry (VIO) is to
dynamically assess the reliability of sensor measurements. This assessment is
crucial for properly weighting the contribution of each measurement to the
state estimate. Conventional methods often simplify this by assuming a static,
uniform uncertainty for all measurements. This heuristic, however, may be
limited in its ability to capture the dynamic error characteristics inherent in
real-world data. To improve this limitation, we present a statistical framework
that learns measurement reliability assessment online, directly from sensor
data and optimization results. Our approach leverages multi-view geometric
consistency as a form of self-supervision. This enables the system to infer
landmark uncertainty and adaptively weight visual measurements during
optimization. We evaluated our method on the public EuRoC dataset,
demonstrating improvements in tracking accuracy with average reductions of
approximately 24\% in translation error and 42\% in rotation error compared to
baseline methods with fixed uncertainty parameters. The resulting framework
operates in real time while showing enhanced accuracy and robustness. To
facilitate reproducibility and encourage further research, the source code will
be made publicly available.

</details>


### [295] [Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation](https://arxiv.org/abs/2510.01661)
*Yifei Simon Shao,Yuchen Zheng,Sunan Sun,Pratik Chaudhari,Vijay Kumar,Nadia Figueroa*

Main category: cs.RO

TL;DR: SymSkill is a unified learning framework that merges imitation learning and task-and-motion planning to enable compositional generalization and real-time failure recovery in multi-step manipulation, by offline learning of predicates, operators, and skills from unlabeled, unsegmented demonstrations and runtime planning and recovery.


<details>
  <summary>Details</summary>
Motivation: Imitation learning struggles with compositional generalization because monolithic policies can't adapt to changing scenes; classical TAMP offers compositionality but suffers prohibitive runtime and real-time recovery challenges.

Method: Offline joint learning of predicates, operators, and skills from unlabeled/unsegmented demonstrations. At execution, a symbolic planner composes/reorders learned skills to meet symbolic goals; real-time motion and symbolic recovery via a compliant controller.

Result: In RoboCasa simulation, executes 12 single-step tasks with 85% success; can compose up to 6 skill recompositions for multi-step plans with robust failure recovery. On a real Franka robot, learns from 5 minutes of unlabeled play data to perform multiple tasks via goal specifications. Code and analyses available online.

Conclusion: SymSkill enables safe, uninterrupted execution under human/environmental disturbances by combining IL and TAMP for real-time compositional generalization and failure recovery in dynamic manipulation.

Abstract: Multi-step manipulation in dynamic environments remains challenging. Two
major families of methods fail in distinct ways: (i) imitation learning (IL) is
reactive but lacks compositional generalization, as monolithic policies do not
decide which skill to reuse when scenes change; (ii) classical task-and-motion
planning (TAMP) offers compositionality but has prohibitive planning latency,
preventing real-time failure recovery. We introduce SymSkill, a unified
learning framework that combines the benefits of IL and TAMP, allowing
compositional generalization and failure recovery in real-time. Offline,
SymSkill jointly learns predicates, operators, and skills directly from
unlabeled and unsegmented demonstrations. At execution time, upon specifying a
conjunction of one or more learned predicates, SymSkill uses a symbolic planner
to compose and reorder learned skills to achieve the symbolic goals, while
performing recovery at both the motion and symbolic levels in real time.
Coupled with a compliant controller, SymSkill enables safe and uninterrupted
execution under human and environmental disturbances. In RoboCasa simulation,
SymSkill can execute 12 single-step tasks with 85% success rate. Without
additional data, it composes these skills into multi-step plans requiring up to
6 skill recompositions, recovering robustly from execution failures. On a real
Franka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented
and unlabeled play data, is capable of performing multiple tasks simply by goal
specifications. The source code and additional analysis can be found on
https://sites.google.com/view/symskill.

</details>


### [296] [Geometric Backstepping Control of Omnidirectional Tiltrotors Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances](https://arxiv.org/abs/2510.01675)
*Jaewoo Lee,Dongjae Lee,Jinwoo Lee,Hyungyu Lee,Yeonjoon Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: Actuator-aware geometric backstepping control for a variable-tilt omnidirectional multirotor achieving exponential stability and robustness under actuator dynamics, outperforming a non-actuator-aware baseline in challenging maneuvers.


<details>
  <summary>Details</summary>
Motivation: Actuator dynamics introduce nonlinear effects, especially with variable tilt; existing actuator-aware controls rely on linear actuator-wrench relationships and large-scale approximations, limiting performance during aggressive maneuvers and disturbances.

Method: Exploit cascade structure between rigid-body dynamics and nonlinear actuator dynamics to design a backstepping controller; prove exponential stability; experimentally identify actuator model uncertainty and test robustness; compare to a baseline without actuator dynamics across fast translational tracking, rapid rotational tracking, and disturbance recovery.

Result: The proposed controller yields better tracking; baseline diverges/crashes under fastest translational and disturbance recovery; the proposed controller maintains stability and completes tasks.

Conclusion: Modeling actuator dynamics is essential for reliable control of variable-tilt multirotors; the proposed actuator-aware backstepping controller is robust to model uncertainty and enhances performance during aggressive maneuvers and disturbance rejection.

Abstract: This work presents a geometric backstepping controller for a variable-tilt
omnidirectional multirotor that explicitly accounts for both servo and rotor
dynamics. Considering actuator dynamics is essential for more effective and
reliable operation, particularly during aggressive flight maneuvers or recovery
from sudden disturbances. While prior studies have investigated actuator-aware
control for conventional and fixed-tilt multirotors, these approaches rely on
linear relationships between actuator input and wrench, which cannot capture
the nonlinearities induced by variable tilt angles. In this work, we exploit
the cascade structure between the rigid-body dynamics of the multirotor and its
nonlinear actuator dynamics to design the proposed backstepping controller and
establish exponential stability of the overall system. Furthermore, we reveal
parametric uncertainty in the actuator model through experiments, and we
demonstrate that the proposed controller remains robust against such
uncertainty. The controller was compared against a baseline that does not
account for actuator dynamics across three experimental scenarios: fast
translational tracking, rapid rotational tracking, and recovery from sudden
disturbance. The proposed method consistently achieved better tracking
performance, and notably, while the baseline diverged and crashed during the
fastest translational trajectory tracking and the recovery experiment, the
proposed controller maintained stability and successfully completed the tasks,
thereby demonstrating its effectiveness.

</details>


### [297] [PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization](https://arxiv.org/abs/2510.01708)
*Zixing Lei,Zibo Zhou,Sheng Yin,Yueru Chen,Qingyao Xu,Weixin Li,Yunhong Wang,Bowei Tang,Wei Jing,Siheng Chen*

Main category: cs.RO

TL;DR: PolySim is a multi-simulator training platform for humanoid whole-body control (WBC) that trains across heterogeneous simulators to reduce simulator inductive bias and the sim-to-real gap via dynamics-level domain randomization; it yields tighter bias bounds than single-simulator training, improves sim-to-sim performance (e.g., ~52.8-point gain over IsaacSim), and enables zero-shot real-world deployment on a Unitree G1; code will be released.


<details>
  <summary>Details</summary>
Motivation: The sim-to-real gap in humanoid WBC arises from simulator inductive biases and assumptions that differ across simulators and from reality. Training policies across a single simulator can overfit to its biases, limiting generalization. Multi-simulator training aiming to uncover dynamics that generalize beyond individual simulators can mitigate this gap.

Method: PolySim integrates multiple heterogeneous simulators and launches parallel environments from different engines within a single training run. This enables dynamics-level domain randomization by exposing the policy to diverse simulator dynamics, and it provides a theoretical bound showing a tighter upper bound on simulator inductive bias than single-simulator training.

Result: In experiments, PolySim substantially reduces motion-tracking error in sim-to-sim evaluations. On MuJoCo, it improves execution success by 52.8 over an IsaacSim baseline. It also enables zero-shot deployment on a real Unitree G1 without additional fine-tuning. The authors plan to release the code upon acceptance.

Conclusion: PolySim demonstrates that training across multiple simulators can tighten bounds on simulator inductive bias and improve cross-simulator and real-world transfer for WBC policies, enabling more robust sim-to-real transfer without per-simulator tuning.

Abstract: Humanoid whole-body control (WBC) policies trained in simulation often suffer
from the sim-to-real gap, which fundamentally arises from simulator inductive
bias, the inherent assumptions and limitations of any single simulator. These
biases lead to nontrivial discrepancies both across simulators and between
simulation and the real world. To mitigate the effect of simulator inductive
bias, the key idea is to train policies jointly across multiple simulators,
encouraging the learned controller to capture dynamics that generalize beyond
any single simulator's assumptions. We thus introduce PolySim, a WBC training
platform that integrates multiple heterogeneous simulators. PolySim can launch
parallel environments from different engines simultaneously within a single
training run, thereby realizing dynamics-level domain randomization.
Theoretically, we show that PolySim yields a tighter upper bound on simulator
inductive bias than single-simulator training. In experiments, PolySim
substantially reduces motion-tracking error in sim-to-sim evaluations; for
example, on MuJoCo, it improves execution success by 52.8 over an IsaacSim
baseline. PolySim further enables zero-shot deployment on a real Unitree G1
without additional fine-tuning, showing effective transfer from simulation to
the real world. We will release the PolySim code upon acceptance of this work.

</details>


### [298] [Contrastive Representation Regularization for Vision-Language-Action Models](https://arxiv.org/abs/2510.01711)
*Taeyoung Kim,Jimin Lee,Myungkyu Koo,Dongyoung Kim,Kyungmin Lee,Changyeon Kim,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: RS-CL is a lightweight state-aware contrastive loss that aligns VLA model representations with robot proprioceptive signals using relative state distances as soft supervision, improving manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: VLM-based representations in Vision-Language-Action (VLA) models often lack sensitivity to robotic signals; bridging this gap by coupling representations to proprioceptive states should improve control-relevant features.

Method: Introduce Robot State-aware Contrastive Loss (RS-CL) as a representation regularizer; use relative distances between robot states as soft supervision; complement the original action prediction objective; lightweight and fully compatible with standard VLA training pipelines.

Result: RS-CL substantially improves manipulation performance, e.g., RoboCasa-Kitchen pick-and-place accuracy improves from 30.8% to 41.5%; real-robot manipulation success rate rises from 45.0% to 58.3%.

Conclusion: RS-CL bridges the gap between VLM representations and robotic signals, delivering a simple, effective, and lightweight regularization that enhances control-relevant representation learning within standard VLA pipelines.

Abstract: Vision-Language-Action (VLA) models have shown its capabilities in robot
manipulation by leveraging rich representations from pre-trained
Vision-Language Models (VLMs). However, their representations arguably remain
suboptimal, lacking sensitivity to robotic signals such as control actions and
proprioceptive states. To address the issue, we introduce Robot State-aware
Contrastive Loss (RS-CL), a simple and effective representation regularization
for VLA models, designed to bridge the gap between VLM representations and
robotic signals. In particular, RS-CL aligns the representations more closely
with the robot's proprioceptive states, by using relative distances between the
states as soft supervision. Complementing the original action prediction
objective, RS-CL effectively enhances control-relevant representation learning,
while being lightweight and fully compatible with standard VLA training
pipeline. Our empirical results demonstrate that RS-CL substantially improves
the manipulation performance of state-of-the-art VLA models; it pushes the
prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,
through more accurate positioning during grasping and placing, and boosts
success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.

</details>


### [299] [Dual-Mode Magnetic Continuum Robot for Targeted Drug Delivery](https://arxiv.org/abs/2510.01761)
*Wendu Zhang,Heng Wang,Shuangyi Wang,Yuanrui Huang*

Main category: cs.RO

TL;DR: A compact magnetic continuum robot embeds magnets radially in the catheter wall to enable independent bending and torsion via a single external magnet, plus a dual-layer blockage for twist-activated drug release; validated by physics-based modeling, FE analysis, benchtop experiments, and an in-phantom end-to-end demonstration, indicating strong potential for site-specific therapies.


<details>
  <summary>Details</summary>
Motivation: Extend the deformation repertoire of axially magnetized magnetic continuum robots beyond bending-only motion, enabling torsion control and integrated drug delivery for site-specific therapies while maintaining a compact, cable-free form factor.

Method: Embed permanent magnets radially in the catheter wall; develop a physics-based actuation formulation; perform finite-element analysis to study decoupled bending and torsion; conduct benchtop experiments to validate mode decoupling under practical magnetic fields; implement a dual-layer blockage (outer grooves and inner plates) exploiting torsional shear for twist-activated drug release; test an in-phantom intervention with lumen following, bending to target, and twist-triggered release.

Result: Demonstrated decoupled bending and torsion control with a single external magnet; achieved twist-activated drug release using the dual-layer blockage; verified end-to-end functionality in an in-phantom lumen, indicating practical feasibility for targeted therapies with a compact, cable-free platform.

Conclusion: The proposed MCR platform offers versatile deformation and precise payload delivery, exhibiting strong potential for next-generation site-specific therapies and minimally invasive interventions, with a compact, cable-free design and integrated drug-release mechanism.

Abstract: Magnetic continuum robots (MCRs) enable minimally invasive navigation through
tortuous anatomical channels, yet axially magnetized designs have largely been
limited to bending-only motion. To expand deformation capabilities, this paper
presents a simple assembly that embeds permanent magnets radially within the
catheter wall, allowing a single externally steered permanent magnet to
independently induce either bending or torsion. A physics-based formulation
together with finite-element analysis establishes the actuation principles, and
benchtop experiments validate decoupled mode control under practical fields.
Building on this, a dual-layer blockage mechanism consisting of outer grooves
and inner plates leverages torsional shear to achieve on-demand drug release.
Finally, an in-phantom intervention experiment demonstrates end-to-end
operation: lumen following by bending for target approach, followed by
twist-activated release at the site. The resulting compact, cable-free platform
combines versatile deformation with precise payload delivery, indicating strong
potential for next-generation, site-specific therapies.

</details>


### [300] [An Anytime, Scalable and Complete Algorithm for Embedding a Manufacturing Procedure in a Smart Factory](https://arxiv.org/abs/2510.01770)
*Christopher Leet,Aidan Sciortino,Sven Koenig*

Main category: cs.RO

TL;DR: Introduces TS-ACES, a complete and scalable solver for the Smart Factory Embedding problem, enabling embedding in factories with over 100 machines.


<details>
  <summary>Details</summary>
Motivation: Embedding manufacturing procedures in smart factories requires assigning each process to a machine and specifying part transport between machines; existing SFE solvers scale only to a few dozen machines, but modern factories may have hundreds.

Method: Traffic System based Anytime Cyclic Embedding Solver (TS-ACES). It is a complete solver that uses a traffic-system viewpoint and Anytime/Cyclic embedding strategies to solve SFE at large scale.

Result: TS-ACES is complete and scalable to SFE instances based on real industrial scenarios with more than a hundred machines.

Conclusion: This work closes the scalability gap in SFE, enabling practical deployment of large-scale automated factories and broadening the applicability of smart factory embedding techniques.

Abstract: Modern automated factories increasingly run manufacturing procedures using a
matrix of programmable machines, such as 3D printers, interconnected by a
programmable transport system, such as a fleet of tabletop robots. To embed a
manufacturing procedure into a smart factory, an operator must: (a) assign each
of its processes to a machine and (b) specify how agents should transport parts
between machines. The problem of embedding a manufacturing process into a smart
factory is termed the Smart Factory Embedding (SFE) problem. State-of-the-art
SFE solvers can only scale to factories containing a couple dozen machines.
Modern smart factories, however, may contain hundreds of machines. We fill this
hole by introducing the first highly scalable solution to the SFE, TS-ACES, the
Traffic System based Anytime Cyclic Embedding Solver. We show that TS-ACES is
complete and can scale to SFE instances based on real industrial scenarios with
more than a hundred machines.

</details>


### [301] [Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2510.01795)
*Haibo Hu,Lianming Huang,Xinyu Wang,Yufei Cui,Nan Guan,Chun Jason Xue*

Main category: cs.RO

TL;DR: Nav-EE precomputes task-specific early-exit layers for vision-language models in autonomous driving and selects them online using navigation priors, reducing latency while maintaining full-inference-like accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision-language models offer unified perception and reasoning but incur high latency; early-exit helps but is task-specific and lacks generalization. Autonomous driving can foresee upcoming contexts (intersections, traffic signals), suggesting a navigation-guided exit strategy.

Method: Offline precomputation of task-specific exit layers; online dynamic application guided by navigation priors; evaluation on CODA, Waymo, BOSCH; real-vehicle integration with Autoware Universe.

Result: Matches full-inference accuracy while reducing latency by up to 63.9%; in-vehicle latency drops from 600 ms to 300 ms in complex scenarios; demonstrates practicality of navigation-guided exits with large models.

Conclusion: Coupling navigation foresight with early-exit provides a viable path to deploy large vision-language models in autonomous systems efficiently; code/data available at an anonymous repository.

Abstract: Vision-Language Models (VLMs) are increasingly applied in autonomous driving
for unified perception and reasoning, but high inference latency hinders
real-time deployment. Early-exit reduces latency by terminating inference at
intermediate layers, yet its task-dependent nature limits generalization across
diverse scenarios. We observe that this limitation aligns with autonomous
driving: navigation systems can anticipate upcoming contexts (e.g.,
intersections, traffic lights), indicating which tasks will be required. We
propose Nav-EE, a navigation-guided early-exit framework that precomputes
task-specific exit layers offline and dynamically applies them online based on
navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE
achieves accuracy comparable to full inference while reducing latency by up to
63.9%. Real-vehicle integration with Autoware Universe further demonstrates
reduced inference latency (600ms to 300ms), supporting faster decision-making
in complex scenarios. These results suggest that coupling navigation foresight
with early-exit offers a viable path toward efficient deployment of large
models in autonomous systems. Code and data are available at our anonymous
repository: https://anonymous.4open.science/r/Nav-EE-BBC4

</details>


### [302] [What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework](https://arxiv.org/abs/2510.01830)
*Hongze Wang,Boyang Sun,Jiaxu Xing,Fan Yang,Marco Hutter,Dhruv Shah,Davide Scaramuzza,Marc Pollefeys*

Main category: cs.RO

TL;DR: Large-scale empirical study shows perception quality and test-time strategies, not policy, primarily drive ObjectNav performance; proposes practical guidelines and a modular system that surpasses SotA; highlights a notable gap to human-level navigation.


<details>
  <summary>Details</summary>
Motivation: ObjectNav requires integrating semantic understanding, spatial reasoning, and long-horizon planning in unseen environments. There is no unifying analysis identifying which components truly impact performance, despite many RL design choices.

Method: Decompose modular RL ObjectNav systems into three components‚Äîperception, policy, and test-time enhancements‚Äîand conduct extensive controlled experiments with ablations to isolate each component's contribution. Compare against state-of-the-art methods and include a human baseline under identical conditions.

Result: Perception quality and test-time strategies are decisive drivers of performance; policy improvements with current methods yield only marginal gains. An enhanced modular system surpasses the state-of-the-art by 6.6% on SPL and by 2.7% in success rate. A human baseline under identical conditions achieves ~98% success, highlighting the gap between RL agents and humans.

Conclusion: The study provides principled design guidelines for ObjectNav, guiding future development and evaluation, and emphasizes prioritizing perceptual reliability and effective test-time strategies over marginal policy optimization.

Abstract: Object-Goal Navigation (ObjectNav) is a critical component toward deploying
mobile robots in everyday, uncontrolled environments such as homes, schools,
and workplaces. In this context, a robot must locate target objects in
previously unseen environments using only its onboard perception. Success
requires the integration of semantic understanding, spatial reasoning, and
long-horizon planning, which is a combination that remains extremely
challenging. While reinforcement learning (RL) has become the dominant
paradigm, progress has spanned a wide range of design choices, yet the field
still lacks a unifying analysis to determine which components truly drive
performance. In this work, we conduct a large-scale empirical study of modular
RL-based ObjectNav systems, decomposing them into three key components:
perception, policy, and test-time enhancement. Through extensive controlled
experiments, we isolate the contribution of each and uncover clear trends:
perception quality and test-time strategies are decisive drivers of
performance, whereas policy improvements with current methods yield only
marginal gains. Building on these insights, we propose practical design
guidelines and demonstrate an enhanced modular system that surpasses
State-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We
also introduce a human baseline under identical conditions, where experts
achieve an average 98% success, underscoring the gap between RL agents and
human-level navigation. Our study not only sets the SotA performance but also
provides principled guidance for future ObjectNav development and evaluation.

</details>


### [303] [Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots](https://arxiv.org/abs/2510.01843)
*Wanyue Li,Ji Ma,Minghao Lu,Peng Lu*

Main category: cs.RO

TL;DR: A drone-inspired spatial-temporal trajectory planner is adapted for a bipedal humanoid soccer robot to autonomously generate constrained foot trajectories for kicking, optimizing swing duration and achieving fast real-time planning.


<details>
  <summary>Details</summary>
Motivation: Current kicking control for humanoid robots struggles with stability during aggressive kicks and precise ball trajectory control. Existing MPC and RL approaches are limited, and simple leg-swing interpolation constrains environmental interaction. There is a need for fast, constrained, real-time foot trajectory planning that can mimic human kicking dynamics.

Method: Adapt spatial-temporal trajectory planning techniques from drone applications to a bipedal robot. The planner autonomously generates foot trajectories that meet target kicking position, velocity, and acceleration constraints while simultaneously optimizing the swing phase duration. Demonstrates planning times under 1 ms in simulation and hardware experiments.

Result: The optimized trajectories closely resemble human kicking with a backswing. The approach yields high reliability, achieving nearly 100% task completion when the soccer goal is within a -90¬∞ to 90¬∞ range, validating both efficiency and robustness.

Conclusion: Drone-inspired spatial-temporal planning effectively enhances kicking performance and environmental interaction for humanoid soccer, providing fast, constrained, real-time foot trajectory generation that complements or surpasses simple interpolation strategies.

Abstract: Humanoid robot soccer presents several challenges, particularly in
maintaining system stability during aggressive kicking motions while achieving
precise ball trajectory control. Current solutions, whether traditional
position-based control methods or reinforcement learning (RL) approaches,
exhibit significant limitations. Model predictive control (MPC) is a prevalent
approach for ordinary quadruped and biped robots. While MPC has demonstrated
advantages in legged robots, existing studies often oversimplify the leg swing
progress, relying merely on simple trajectory interpolation methods. This
severely constrains the foot's environmental interaction capability, hindering
tasks such as ball kicking. This study innovatively adapts the spatial-temporal
trajectory planning method, which has been successful in drone applications, to
bipedal robotic systems. The proposed approach autonomously generates foot
trajectories that satisfy constraints on target kicking position, velocity, and
acceleration while simultaneously optimizing swing phase duration. Experimental
results demonstrate that the optimized trajectories closely mimic human kicking
behavior, featuring a backswing motion. Simulation and hardware experiments
confirm the algorithm's efficiency, with trajectory planning times under 1 ms,
and its reliability, achieving nearly 100 % task completion accuracy when the
soccer goal is within the range of -90{\deg} to 90{\deg}.

</details>


### [304] [GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics](https://arxiv.org/abs/2510.01848)
*Diram Tabaa,Gianni Di Caro*

Main category: cs.RO

TL;DR: GreenhouseSplat presents a pipeline and dataset for photorealistic greenhouse assets using radiance-field methods (Gaussian splatting) built from cheap RGB images, integrated into a ROS-based simulator with camera and LiDAR, and demonstrated on 82 cucumber plants to support robotics evaluation, marking a step toward greenhouse-scale radiance-field simulation.


<details>
  <summary>Details</summary>
Motivation: Accurate, photorealistic greenhouse simulators are needed to bridge sim-to-real gaps in agricultural robotics. Existing assets are simplistic and limited in scale; radiance-field approaches show promise but have been confined to single plants or controlled conditions. A greenhouse-scale, photorealistic pipeline would enable robust evaluation and development.

Method: Capture inexpensive RGB images of cucumber plants in greenhouse rows; reconstruct radiance-field assets using Gaussian splatting; assemble assets into a ROS-based simulation environment with camera and LiDAR rendering; create a dataset of 82 plants across multiple row configurations; demonstrate tasks such as localization with fiducial markers.

Result: Photorealistic greenhouse assets and a ROS-integrated simulator derived from inexpensive RGB imagery; a dataset of 82 cucumber plants across several row configurations; demonstration of localization and robotics evaluation tasks in the simulated greenhouse.

Conclusion: GreenhouseSplat is the first step toward scalable, radiance-field-based greenhouse simulation, providing a foundation for future research in agricultural robotics and improving sim-to-real transfer.

Abstract: Simulating greenhouse environments is critical for developing and evaluating
robotic systems for agriculture, yet existing approaches rely on simplistic or
synthetic assets that limit simulation-to-real transfer. Recent advances in
radiance field methods, such as Gaussian splatting, enable photorealistic
reconstruction but have so far been restricted to individual plants or
controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a
framework and dataset for generating photorealistic greenhouse assets directly
from inexpensive RGB images. The resulting assets are integrated into a
ROS-based simulation with support for camera and LiDAR rendering, enabling
tasks such as localization with fiducial markers. We provide a dataset of 82
cucumber plants across multiple row configurations and demonstrate its utility
for robotics evaluation. GreenhouseSplat represents the first step toward
greenhouse-scale radiance-field simulation and offers a foundation for future
research in agricultural robotics.

</details>


### [305] [TACOS: Task Agnostic COordinator of a multi-drone System](https://arxiv.org/abs/2510.01869)
*Alessandro Nazzari,Roberto Rubinacci,Marco Lovera*

Main category: cs.RO

TL;DR: A language-model‚Äìdriven framework (TACOS) enables flexible, high-level control of multi-UAV systems via a unified three-component architecture (NL interface, coordinator, and execution agent) and is validated on real drones with an ablation study.


<details>
  <summary>Details</summary>
Motivation: To support varying levels of autonomy in multi-drone systems and reduce pilot workload by letting a single pilot delegate tasks through natural language, bridging semantic reasoning with real-time coordination.

Method: A three-part architecture: (1) a one-to-many natural language interface for user interaction; (2) an intelligent coordinator that converts user intent into structured task plans; (3) an autonomous agent that executes the plans by interfacing with a library of executable APIs. The system is demonstrated on real-world multi-drone scenarios and evaluated via ablation studies.

Result: Real-world demonstration of TACOS with ablation study showing the contribution of each module and validating the feasibility of language-guided multi-UAV coordination.

Conclusion: TACOS offers a practical, scalable framework for language-driven multi-UAV control, highlighting the role of LLMs in bridging human intent and autonomous robot actions; future work could address latency, safety, robustness, and broader API integration.

Abstract: When a single pilot is responsible for managing a multi-drone system, the
task demands varying levels of autonomy, from direct control of individual
UAVs, to group-level coordination, to fully autonomous swarm behaviors for
accomplishing high-level tasks. Enabling such flexible interaction requires a
framework that supports multiple modes of shared autonomy. As language models
continue to improve in reasoning and planning, they provide a natural
foundation for such systems, reducing pilot workload by enabling high-level
task delegation through intuitive, language-based interfaces. In this paper we
present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified
framework that enables high-level natural language control of multi-UAV systems
through Large Language Models (LLMs). TACOS integrates three key capabilities
into a single architecture: a one-to-many natural language interface for
intuitive user interaction, an intelligent coordinator for translating user
intent into structured task plans, and an autonomous agent that executes plans
interacting with the real-world. TACOS allows a LLM to interact with a library
of executable APIs, bridging semantic reasoning with real-time multi-robot
coordination. We demonstrate the system in real-world multi-drone system and
conduct an ablation study to assess the contribution of each module.

</details>


### [306] [SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot](https://arxiv.org/abs/2510.01984)
*Yue Wang*

Main category: cs.RO

TL;DR: A compact, open-source 3-DoF spine module (SPARC) for quadrupeds enabling programmable task-space impedance; validated with RNEA-based acceleration control and friction compensation; demonstrates linear stiffness 300‚Äì700 N/m with <1.5% error; suitable for spine compliance studies; hardware/firmware to be released.


<details>
  <summary>Details</summary>
Motivation: To enable systematic exploration of spine compliance in legged locomotion by providing a portable, torque-controlled spine module with tunable impedance.

Method: Hardware: 3 torque-controlled actuators, 1 kHz control board, protected power unit in 1.26 kg package; control algorithm: RNEA-based computed-acceleration controller with smooth Stribeck friction compensation to render spring-damper behavior without explicit inertia shaping; bench tests include quasi-static push-pull (stiffness 300‚Äì700 N/m, error ‚â§1.5%, R^2 ‚â• 0.992), dynamic displace-and-release showing mass-spring-damper responses across damping settings; task-space PD controller shows roughly linear stiffness with more variability and coupling; release with complete hardware/firmware resources.

Result: Achieved linear force-displacement characteristics with commanded horizontal stiffness spanning 300‚Äì700 N/m and relative error ‚â§1.5% (R^2 ‚â• 0.992); dynamic tests confirm mass-spring-damper behavior with interpretable phase deviations due to inertia and low-speed friction; PD controller offers linear stiffness but with variability and coupling sensitivity.

Conclusion: SPARC provides a portable, open-source platform for systematic studies of spine compliance in legged locomotion and will be released with complete hardware and firmware resources.

Abstract: We present SPARC, a compact, open-source 3-DoF sagittal-plane spine module
that combines revolute (pitch) and prismatic (axial) motion with programmable
task-space impedance for quadruped robots. The system integrates three
torque-controlled actuators, a custom 1 kHz control board, and a protected
power unit in a 1.26 kg package, enabling closed-loop stiffness and damping
shaping along x, z, and theta. We develop an RNEA-based computed-acceleration
controller with smooth Stribeck friction compensation to render spring-damper
behavior without explicit inertia shaping. Bench experiments validate the
approach. Quasi-static push-pull tests show linear force-displacement
characteristics with commanded horizontal stiffness spanning 300-700 N/m and <=
1.5% relative error (R^2 >= 0.992, narrow 95% CIs). Dynamic
displace-and-release trials confirm mass-spring-damper responses over multiple
damping settings, with small, interpretable phase deviations due to
configuration-dependent inertia and low-speed friction effects. A task-space PD
controller produces roughly linear stiffness but with greater variability and
coupling sensitivity. SPARC provides a portable platform for systematic studies
of spine compliance in legged locomotion and will be released with complete
hardware and firmware resources.

</details>


### [307] [Reducing Discomfort in Driving Simulators: Motion Cueing for Motion Sickness Mitigation](https://arxiv.org/abs/2510.01986)
*Varun Kotian,Vishrut Jain,Andrea Michelle Rios Lazcano,Daan Marinus Pool,Riender Happee,Barys Shyrokau*

Main category: cs.RO

TL;DR: MPC-based motion cueing reduces simulator-induced motion sickness by jointly optimizing specific force fidelity and sensory conflict, validated in human-in-the-loop hexapod driving experiments; sickness drops by ~50% without harming fidelity.


<details>
  <summary>Details</summary>
Motivation: To mitigate motion sickness in driving simulators caused by mismatches between downscaled motion and veridical visuals, enabling broader simulator use.

Method: Develop an MPC-based motion cueing algorithm that penalizes sensory conflict and specific force errors, jointly optimizing fidelity and comfort. Compare two MPC variants and baselines (adaptive washout, no motion) in a hexapod driving simulator with passive driving; evaluate sickness and fidelity.

Result: Experimental sickness aligned with the SVC model predictions. No-motion yields lowest sickness but lowest fidelity. The compromise MPC reduces sickness by over 50% relative to adaptive washout and the force-tracking variant, with no significant fidelity loss.

Conclusion: The proposed motion cueing approach advances the joint control of motion sickness and motion recreation, enabling broader and more comfortable use of driving simulators.

Abstract: Driving simulators are increasingly used in research and development.
However, simulators often cause motion sickness due to downscaled motion and
unscaled veridical visuals. In this paper, a motion cueing algorithm is
proposed that reduces motion sickness as predicted by the subjective vertical
conflict (SVC) model using model predictive control (MPC). Both sensory
conflict and specific force errors are penalised in the cost function, allowing
the algorithm to jointly optimise fidelity and comfort.
  Human-in-the-loop experiments were conducted to compare four simulator motion
settings: two variations of our MPC-based algorithm, one focused on pure
specific force tracking and the second compromising specific force tracking and
motion sickness minimisation, as well as reference adaptive washout and no
motion cases. The experiments were performed on a hexapod driving simulator
with participants exposed to passive driving.
  Experimental motion sickness results closely matched the sickness model
predictions. As predicted by the model, the no motion condition yielded the
lowest sickness levels. However, it was rated lowest in terms of fidelity. The
compromise solution reduced sickness by over 50% (average MISC level 3 to 1.5)
compared to adaptive washout and the algorithm focusing on specific force
tracking, without any significant reduction in fidelity rating.
  The proposed approach for developing MCA that takes into account both the
simulator dynamics and time evolution of motion sickness offers a significant
advancement in achieving an optimal control of motion sickness and specific
force recreation in driving simulators, supporting broader simulator use.

</details>


### [308] [EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2510.02080)
*Lingxiang Hu,Naima Ait Oufroukh,Fabien Bonardi,Raymond Ghandour*

Main category: cs.RO

TL;DR: EC3R-SLAM offers calibration-free monocular dense SLAM with a two-branch architecture: a tracking module that maintains a sparse feature map and a mapping module using a feed-forward 3D reconstruction model that also estimates intrinsics. It includes local and global loop closures for multi-view consistency, achieving competitive accuracy with lower latency and memory usage, and runs on resource-constrained hardware like laptops and Jetson platforms.


<details>
  <summary>Details</summary>
Motivation: To overcome the bottlenecks of monocular dense SLAM: high latency, large GPU memory consumption, and the need for camera calibration. The goal is a calibration-free, memory-efficient, low-latency framework that remains accurate in real-world robotics scenarios.

Method: A coupling of two modules: a tracking component preserving a sparse map of features and a mapping component based on a feed-forward 3D reconstruction model that jointly estimates camera intrinsics. The system also integrates both local and global loop closures to enforce multi-view consistency and improve robustness.

Result: Experimental results on multiple benchmarks show EC3R-SLAM is competitive with state-of-the-art methods while being faster and more memory-efficient. It also demonstrates viability on resource-constrained hardware (laptops and Jetson Orin NX), indicating potential for real-world robotics.

Conclusion: EC3R-SLAM demonstrates that a calibration-free monocular dense SLAM framework with a tight coupling between tracking and feed-forward mapping can achieve accurate localization and mapping with reduced latency and memory usage, suitable for deployment on edge devices in real-world applications.

Abstract: The application of monocular dense Simultaneous Localization and Mapping
(SLAM) is often hindered by high latency, large GPU memory consumption, and
reliance on camera calibration. To relax this constraint, we propose EC3R-SLAM,
a novel calibration-free monocular dense SLAM framework that jointly achieves
high localization and mapping accuracy, low latency, and low GPU memory
consumption. This enables the framework to achieve efficiency through the
coupling of a tracking module, which maintains a sparse map of feature points,
and a mapping module based on a feed-forward 3D reconstruction model that
simultaneously estimates camera intrinsics. In addition, both local and global
loop closures are incorporated to ensure mid-term and long-term data
association, enforcing multi-view consistency and thereby enhancing the overall
accuracy and robustness of the system. Experiments across multiple benchmarks
show that EC3R-SLAM achieves competitive performance compared to
state-of-the-art methods, while being faster and more memory-efficient.
Moreover, it runs effectively even on resource-constrained platforms such as
laptops and Jetson Orin NX, highlighting its potential for real-world robotics
applications.

</details>


### [309] [LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions](https://arxiv.org/abs/2510.02104)
*Yunhan Lin,Wenqi Wu,Zhijie Zhang,Huasong Min*

Main category: cs.RO

TL;DR: LangGrasp is a language-interactive grasping framework that uses fine-tuned LLMs to infer implicit intents and a 2D-guided point-cloud localization to enable fine-grained part-level manipulation, enabling robust grasping in unstructured environments.


<details>
  <summary>Details</summary>
Motivation: Existing language-driven grasping methods struggle with ambiguous instructions and implicit intents; there is a need to leverage large language models' commonsense understanding and perceptual abilities to clarify tasks and identify target objects or parts.

Method: Fine-tune LLMs to deduce implicit intents and clarify task requirements from linguistic instructions; introduce a point cloud localization module guided by 2D segmentation to achieve partial (part-level) localization; dynamically select grasping poses using environmental information.

Result: LangGrasp accurately resolves implicit intents, identifying critical unstated operations and target information, and selects optimal grasp poses for high-precision part-level manipulation, improving adaptability and task execution efficiency in unstructured environments.

Conclusion: By integrating LLM-based reasoning with vision-guided localization, LangGrasp enables robust, fine-grained language-driven robotic grasping; code is available at the linked GitHub repository.

Abstract: The existing language-driven grasping methods struggle to fully handle
ambiguous instructions containing implicit intents. To tackle this challenge,
we propose LangGrasp, a novel language-interactive robotic grasping framework.
The framework integrates fine-tuned large language models (LLMs) to leverage
their robust commonsense understanding and environmental perception
capabilities, thereby deducing implicit intents from linguistic instructions
and clarifying task requirements along with target manipulation objects.
Furthermore, our designed point cloud localization module, guided by 2D part
segmentation, enables partial point cloud localization in scenes, thereby
extending grasping operations from coarse-grained object-level to fine-grained
part-level manipulation. Experimental results show that the LangGrasp framework
accurately resolves implicit intents in ambiguous instructions, identifying
critical operations and target information that are unstated yet essential for
task completion. Additionally, it dynamically selects optimal grasping poses by
integrating environmental information. This enables high-precision grasping
from object-level to part-level manipulation, significantly enhancing the
adaptability and task execution efficiency of robots in unstructured
environments. More information and code are available here:
https://github.com/wu467/LangGrasp.

</details>


### [310] [Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control](https://arxiv.org/abs/2510.02129)
*Philip Reichenberg,Tim Laue*

Main category: cs.RO

TL;DR: The paper presents NAO stand-up motions for humanoid soccer, developed since 2019, addressing large joint-position errors with freeing motions and joint compensation, leading to higher stand-up success rates and adoption by other teams in the league, as evidenced by tournament video analyses.


<details>
  <summary>Details</summary>
Motivation: Ensure reliable stand-up in robot soccer to avoid being penalized or removed from play; improve game continuity by reducing fall-related failures.

Method: Develop stand-up motions for the NAO robot. To mitigate failed attempts caused by large joint-position errors, incorporate specialized motions to free stuck limbs (e.g., arms) and use compensation across other joints. The approach has been iteratively evaluated and expanded from 2019 through six years, with validation via analysis of videos from multiple tournaments.

Result: Significant increase in stand-up success rate; the motions are adopted by several other teams in the Standard Platform League, achieving similar success rates as evidenced by cross-tournament video analyses.

Conclusion: Robust, transferable stand-up routines can markedly improve game continuity in humanoid robot soccer; the authors‚Äô approach appears generalizable within the league, given its adoption by multiple teams and corroborating video analyses.

Abstract: Stand-up motions are an indispensable part of humanoid robot soccer. A robot
incapable of standing up by itself is removed from the game for some time. In
this paper, we present our stand-up motions for the NAO robot. Our approach
dates back to 2019 and has been evaluated and slightly expanded over the past
six years. We claim that the main reason for failed stand-up attempts are large
errors in the executed joint positions. By addressing such problems by either
executing special motions to free up stuck limbs such as the arms, or by
compensating large errors with other joints, we significantly increased the
overall success rate of our stand-up routine. The motions presented in this
paper are also used by several other teams in the Standard Platform League,
which thereby achieve similar success rates, as shown in an analysis of videos
from multiple tournaments.

</details>


### [311] [SCANS: A Soft Gripper with Curvature and Spectroscopy Sensors for In-Hand Material Differentiation](https://arxiv.org/abs/2510.02164)
*Nathaniel Hanson,Austin Allison,Charles DiMarzio,Ta≈ükƒ±n Padƒ±r,Kristen L. Dorsey*

Main category: cs.RO

TL;DR: Introduces SCANS, an electronics-free soft gripper that uses near-infrared spectral sensing for material discrimination during pre-touch and in-hand manipulation, with open hardware/resources.


<details>
  <summary>Details</summary>
Motivation: To extend soft robotics with an integrated, explainable spectral sensing modality capable of separating a wide range of materials, addressing limitations of purely tactile or visual sensing.

Method: Systematically studies soft substrate materials for spectral sensing; tests pre-touch and in-hand sensing on diverse object classes; applies linear discriminant analysis and spectral-angle metrics to evaluate separability; provides complete bill of materials, assembly guidelines, and processing code.

Result: Achieves statistically explainable separation across metal, wood, plastic, organic, paper, and foam; observes large spectral-angle differences between items; near-infrared bands are critical for differentiating visually similar objects.

Conclusion: SCANS broadens optics as a multifunctional sensor in soft robotics; open-source designs enable broader adoption and replication.

Abstract: We introduce the soft curvature and spectroscopy (SCANS) system: a versatile,
electronics-free, fluidically actuated soft manipulator capable of assessing
the spectral properties of objects either in hand or through pre-touch caging.
This platform offers a wider spectral sensing capability than previous soft
robotic counterparts. We perform a material analysis to explore optimal soft
substrates for spectral sensing, and evaluate both pre-touch and in-hand
performance. Experiments demonstrate explainable, statistical separation across
diverse object classes and sizes (metal, wood, plastic, organic, paper, foam),
with large spectral angle differences between items. Through linear
discriminant analysis, we show that sensitivity in the near-infrared
wavelengths is critical to distinguishing visually similar objects. These
capabilities advance the potential of optics as a multi-functional sensory
modality for soft robots. The complete parts list, assembly guidelines, and
processing code for the SCANS gripper are accessible at:
https://parses-lab.github.io/scans/.

</details>


### [312] [Product Digital Twin Supporting End-of-life Phase of Electric Vehicle Batteries Utilizing Product-Process-Resource Asset Network](https://arxiv.org/abs/2510.02167)
*Sara Strakosova,Petr Novak,Petr Kadera*

Main category: cs.RO

TL;DR: A digital twin-based Bi-Flow PAN (Bi-PAN) framework for end-of-life optimization of products, demonstrated on EV battery disassembly to enable flexible remanufacturing/recycling across battery types.


<details>
  <summary>Details</summary>
Motivation: End-of-life management requires efficient disassembly and data-sharing barriers hinder remanufacturing/recycling; a digital twin framework can model product-process-resource relationships across manufacturing and end-of-life phases to reduce ecological impact.

Method: Extend PAN to Bi-PAN to cover remanufacturing/recycling; leverage product-process-resource asset networks; implement using product digital twins; apply to a disassembly use-case of EV battery; enable flexible handling of diverse battery types.

Result: Feasibility demonstrated; the Bi-PAN framework can flexibly and efficiently solve disassembly challenges across battery types; shows potential to reduce ecological impact, though no quantitative metrics provided in abstract.

Conclusion: Bi-PAN extends PAN to encompass end-of-life stages, enabling integrated modeling of remanufacturing/recycling in digital twins; supports sustainability in circular economy by improving data sharing and optimization of disassembly processes.

Abstract: In the context of the circular economy, products in their end-of-life phase
should be either remanufactured or recycled. Both of these processes are
crucial for sustainability and environmental conservation. However,
manufacturers often do not support these processes enough by not sharing
relevant data. This paper proposes use of a digital twin technology, which is
capable to help optimizing the disassembly processes to reduce ecological
impact and enhance sustainability. The proposed approach is demonstrated
through a disassembly use-case of the product digital twin of an electric
vehicle battery. By utilizing product digital twins, challenges associated with
the disassembly of electric vehicle batteries can be solved flexibly and
efficiently for various battery types. As a backbone for the product digital
twin representation, the paper uses the paradigm of product-process-resource
asset networks (PAN). Such networks enable to model relevant relationships
across products, production resources, manufacturing processes, and specific
production operations that have to be done in the manufacturing phase of a
product. This paper introduces a Bi-Flow Product-Process-Resource Asset Network
(Bi-PAN) representation, which extends the PAN paradigm to cover not only the
manufacturing, but also the remanufacturing/recycling phase.

</details>


### [313] [DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis](https://arxiv.org/abs/2510.02178)
*Jialin Gao,Donghao Zhou,Mingjian Liang,Lihao Liu,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.RO

TL;DR: DisCo-Layout presents a disentangled, collaborative refinement framework for 3D indoor layout synthesis, improving realism and generalization via separate semantic and physical refinement tools and a multi-agent planner.


<details>
  <summary>Details</summary>
Motivation: Current 3D indoor layout methods struggle to generalize due to reliance on fixed datasets; LLM/VLM help with semantics but lack robust refinement; a system is needed to refine both semantic relationships and spatial feasibility.

Method: Independent refinement: Semantic Refinement Tool (SRT) corrects abstract object relationships; Physical Refinement Tool (PRT) uses a grid-matching algorithm to fix concrete spatial issues. Collaborative refinement: multi-agent framework with a planner for placement rules, a designer for initial layouts, and an evaluator for assessment; orchestrates SRT and PRT.

Result: Achieves state-of-the-art performance, producing realistic, coherent, and generalizable 3D indoor layouts; code will be released.

Conclusion: DisCo-Layout demonstrates effective disentanglement and coordination of semantic and physical refinement to improve layout realism and generalization, via a scalable multi-agent approach.

Abstract: 3D indoor layout synthesis is crucial for creating virtual environments.
Traditional methods struggle with generalization due to fixed datasets. While
recent LLM and VLM-based approaches offer improved semantic richness, they
often lack robust and flexible refinement, resulting in suboptimal layouts. We
develop DisCo-Layout, a novel framework that disentangles and coordinates
physical and semantic refinement. For independent refinement, our Semantic
Refinement Tool (SRT) corrects abstract object relationships, while the
Physical Refinement Tool (PRT) resolves concrete spatial issues via a
grid-matching algorithm. For collaborative refinement, a multi-agent framework
intelligently orchestrates these tools, featuring a planner for placement
rules, a designer for initial layouts, and an evaluator for assessment.
Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating
realistic, coherent, and generalizable 3D indoor layouts. Our code will be
publicly available.

</details>


### [314] [Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0](https://arxiv.org/abs/2510.02248)
*Yan Miao,Ege Yuceel,Georgios Fainekos,Bardh Hoxha,Hideki Okamoto,Sayan Mitra*

Main category: cs.RO

TL;DR: A single visual policy trained with Performance-Guided Refinement (PGR) in FalconGym 2.0 generalizes to unseen tracks and achieves strong sim-to-real transfer for aerial navigation.


<details>
  <summary>Details</summary>
Motivation: Current visual policies overfit to track geometry and degrade with track changes; a photorealistic, editable simulator is needed to diversify training and improve generalization, robustness, and real-world transfer.

Method: Develop FalconGym 2.0 (GSplat-based photorealistic simulation with an Edit API) to generate diverse static/dynamic tracks rapidly; apply Performance-Guided Refinement to focus training on challenging tracks and iteratively improve performance; evaluate on fixed-wing UAVs and quadrotors across varied dynamics/environments; compare to baselines; demonstrate zero-shot sim-to-real transfer to a quadrotor hardware.

Result: A single policy trained with PGR outperforms baselines in generalization and robustness; generalizes to three unseen tracks with 100% success without per-track retraining; maintains higher success under gate-pose perturbations; achieves 98.6% real-world gate success (69/70) over 30 trials across two 3-gate tracks and a moving gate.

Conclusion: PGR-enabled FalconGym 2.0 enables robust, generalizable visual policies for aerial navigation and practical sim-to-real transfer, reducing per-track retraining needs.

Abstract: Visual policy design is crucial for aerial navigation. However,
state-of-the-art visual policies often overfit to a single track and their
performance degrades when track geometry changes. We develop FalconGym 2.0, a
photorealistic simulation framework built on Gaussian Splatting (GSplat) with
an Edit API that programmatically generates diverse static and dynamic tracks
in milliseconds. Leveraging FalconGym 2.0's editability, we propose a
Performance-Guided Refinement (PGR) algorithm, which concentrates visual
policy's training on challenging tracks while iteratively improving its
performance. Across two case studies (fixed-wing UAVs and quadrotors) with
distinct dynamics and environments, we show that a single visual policy trained
with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in
generalization and robustness: it generalizes to three unseen tracks with 100%
success without per-track retraining and maintains higher success rates under
gate-pose perturbations. Finally, we demonstrate that the visual policy trained
with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a
quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30
trials spanning two three-gate tracks and a moving-gate track.

</details>


### [315] [Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking](https://arxiv.org/abs/2510.02252)
*Joao Pedro Araujo,Yanjie Ze,Pei Xu,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: A new retargeting method, General Motion Retargeting (GMR), improves motion-tracking fidelity and policy robustness for humanoid RL, approaching closed-source performance without extensive reward tuning.


<details>
  <summary>Details</summary>
Motivation: To bridge the embodiment gap between humans and humanoid robots, and to assess how retargeting quality impacts policy performance when reward tuning is minimized, addressing artifacts like foot sliding and self-penetration left in reference data.

Method: Introduce General Motion Retargeting (GMR) and compare it with two open-source retargeters (PHC and ProtoMotions) and a high-quality closed-source Unitree dataset. Use BeyondMimic to train policies, isolating retargeting effects without reward tuning. Evaluate on a diverse subset of the LAFAN1 dataset.

Result: GMR consistently outperforms open-source retargeters in tracking fidelity and faithfulness to source motion, achieving perceptual fidelity and policy success rates close to the closed-source baseline. Most motions can be tracked, but retargeting artifacts still reduce policy robustness for dynamic or long sequences.

Conclusion: Retargeting quality significantly impacts policy performance and generalization. GMR mitigates artifacts more effectively than existing open-source tools, reducing reliance on reward engineering and bringing open-source retargeting closer to closed-source standards for humanoid motion imitation.

Abstract: Humanoid motion tracking policies are central to building teleoperation
pipelines and hierarchical controllers, yet they face a fundamental challenge:
the embodiment gap between humans and humanoid robots. Current approaches
address this gap by retargeting human motion data to humanoid embodiments and
then training reinforcement learning (RL) policies to imitate these reference
trajectories. However, artifacts introduced during retargeting, such as foot
sliding, self-penetration, and physically infeasible motion are often left in
the reference trajectories for the RL policy to correct. While prior work has
demonstrated motion tracking abilities, they often require extensive reward
engineering and domain randomization to succeed. In this paper, we
systematically evaluate how retargeting quality affects policy performance when
excessive reward tuning is suppressed. To address issues that we identify with
existing retargeting methods, we propose a new retargeting method, General
Motion Retargeting (GMR). We evaluate GMR alongside two open-source
retargeters, PHC and ProtoMotions, as well as with a high-quality closed-source
dataset from Unitree. Using BeyondMimic for policy training, we isolate
retargeting effects without reward tuning. Our experiments on a diverse subset
of the LAFAN1 dataset reveal that while most motions can be tracked, artifacts
in retargeted data significantly reduce policy robustness, particularly for
dynamic or long sequences. GMR consistently outperforms existing open-source
methods in both tracking performance and faithfulness to the source motion,
achieving perceptual fidelity and policy success rates close to the
closed-source baseline. Website:
https://jaraujo98.github.io/retargeting_matters. Code:
https://github.com/YanjieZe/GMR.

</details>


### [316] [Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning](https://arxiv.org/abs/2510.02268)
*Tianchong Jiang,Jingtian Ji,Xiangshan Tan,Jiading Fang,Anand Bhattad,Vitor Guizilini,Matthew R. Walter*

Main category: cs.RO

TL;DR: Conditioning policies on camera extrinsics via Plucker embeddings improves view-invariant imitation learning and robustness to viewpoint shifts; introduces fixed vs randomized scene variants for evaluation; yields RGB-only robust control without depth; code released.


<details>
  <summary>Details</summary>
Motivation: To achieve true view-invariant imitation learning, policies should not rely on background cues to infer camera pose; embedding extrinsic information into policies can align actions across viewpoints.

Method: Encode per-pixel ray Plucker embeddings; condition policies such as ACT, Diffusion Policy, SmolVLA on extrinsics; create RoboSuite/ManiSkill tasks with fixed vs randomized scenes to test generalization; evaluate RGB-only control with and without depth.

Result: Extrinsic conditioning significantly improves cross-view generalization; without extrinsics policies exploit static backgrounds to infer pose, failing when scenes vary; extrinsics restore performance and enable robust RGB-only control.

Conclusion: Explicit extrinsic conditioning is a practical and effective strategy for view-invariant imitation learning; supports broader applicability and reduces reliance on background cues; release of tasks, demos, and code enhances reproducibility.

Abstract: We study view-invariant imitation learning by explicitly conditioning
policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we
show that conditioning on extrinsics significantly improves generalization
across viewpoints for standard behavior cloning policies, including ACT,
Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose. Our analysis reveals that policies without
extrinsics often infer camera pose using visual cues from static backgrounds in
fixed scenes; this shortcut collapses when workspace geometry or camera
placement shifts. Conditioning on extrinsics restores performance and yields
robust RGB-only control without depth. We release the tasks, demonstrations,
and code at https://ripl.github.io/know_your_camera/ .

</details>


### [317] [ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation](https://arxiv.org/abs/2510.02298)
*Wenye Yu,Jun Lv,Zixi Ying,Yang Jin,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: Proposes ARMADA, a multi-robot deployment framework with FLOAT for autonomous failure detection, enabling scalable, low-supervision imitation learning. It reduces human intervention while improving performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Imitation learning requires abundant in-domain data; collecting high-quality demonstrations is expensive and time-consuming. Human-in-the-loop systems currently need continuous supervision and struggle with data quality and scalability.

Method: Introduce ARMADA, a multi-robot deployment and adaptation system with human-in-the-loop shared control and an autonomous online failure detector FLOAT. FLOAT allows parallel rollout and only requests human input when needed, enabling efficient data acquisition and rapid adaptation.

Result: FLOAT achieves ~95% accuracy on average, >20% improvement over SOTA failure detection. ARMADA yields >4x higher success rate and >2x lower human intervention rate across multiple rollout/post-training rounds on four real-world tasks.

Conclusion: ARMADA enables scalable deployment and faster adaptation to new scenarios by reducing reliance on continuous human supervision and facilitating efficient in-domain data collection.

Abstract: Imitation learning has shown promise in learning from large-scale real-world
datasets. However, pretrained policies usually perform poorly without
sufficient in-domain data. Besides, human-collected demonstrations entail
substantial labour and tend to encompass mixed-quality data and redundant
information. As a workaround, human-in-the-loop systems gather domain-specific
data for policy post-training, and exploit closed-loop policy feedback to offer
informative guidance, but usually require full-time human surveillance during
policy rollout. In this work, we devise ARMADA, a multi-robot deployment and
adaptation system with human-in-the-loop shared control, featuring an
autonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA
enables paralleled policy rollout and requests human intervention only when
necessary, significantly reducing reliance on human supervision. Hence, ARMADA
enables efficient acquisition of in-domain data, and leads to more scalable
deployment and faster adaptation to new scenarios. We evaluate the performance
of ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on
average, surpassing prior state-of-the-art failure detection approaches by over
20%. Besides, ARMADA manifests more than 4$\times$ increase in success rate and
greater than 2$\times$ reduction in human intervention rate over multiple
rounds of policy rollout and post-training, compared to previous
human-in-the-loop learning methods.

</details>
