<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 62]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.LG](#cs.LG) [Total: 110]
- [cs.RO](#cs.RO) [Total: 20]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: Monocular 3D food reconstruction with learned scale estimation to produce true-to-scale models; achieves ~30% lower mean absolute volume error on two datasets; code provided.


<details>
  <summary>Details</summary>
Motivation: Accurate portion size estimation is essential for precision nutrition, but monocular 3D methods struggle with real-world scale; bridging 3D vision and digital health improves feasibility of volume estimation.

Method: Leverages visual features from large-scale pretrained models to estimate the scale of the reconstructed object from a single image, then applies this scale to convert single-view reconstructions into true-to-life models; extensive ablations on two public datasets validate improvements.

Result: Consistent performance gains across datasets with about a 30% reduction in mean absolute volume-estimation error compared with existing techniques; demonstrates practical, physically meaningful reconstructions.

Conclusion: The approach enables true-to-scale 3D reconstructions from monocular images, strengthening the utility of AI-based dietary assessment for precision nutrition; future work may address generalization and robustness across conditions.

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [2] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: Proposes DiSa, a saliency-aware foreground-background disentangled framework for open-vocabulary semantic segmentation, featuring SDM and HRM; shows state-of-the-art performance across six benchmarks.


<details>
  <summary>Details</summary>
Motivation: Open-vocabulary segmentation with VLMs is hampered by foreground bias and poor boundary localization, due to bias toward salient objects in VLM pretraining. A saliency-aware, disentangled approach aims to better model both foreground and background and improve spatial precision.

Method: Introduce Saliency-aware Disentanglement Module (SDM) to separately model foreground and background ensemble features in a divide-and-conquer fashion, leveraging saliency cues. Introduce Hierarchical Refinement Module (HRM) to exploit pixel-level spatial context and perform channel-wise feature refinement via multi-level updates. Framework DiSa for open-vocabulary segmentation using text labels with VLMs.

Result: Empirical evaluation on six benchmarks shows DiSa consistently outperforms state-of-the-art methods, indicating improved handling of foreground/background partitioning and boundary localization.

Conclusion: DiSa addresses foreground bias and limited spatial localization in VLM-based open-vocabulary segmentation by explicit saliency-guided disentanglement and multi-level refinement, achieving superior segmentation performance.

Abstract: Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

</details>


### [3] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

TL;DR: Incorporating sparsity into CLIP training yields interpretable yet high-performing representations, outperforming post-hoc sparse methods and preserving multimodal capabilities.


<details>
  <summary>Details</summary>
Motivation: CLIP's dense latent space is hard to interpret; the common belief that interpretability requires sacrificing accuracy motivates seeking an integrated, sparse representation during training that preserves multimodal functionality.

Method: Introduce a sparsity objective/regularization during CLIP training to produce sparse latent representations (Sparse CLIP). Compare against post-hoc Sparse Autoencoders (SAEs) on downstream tasks and multimodal capability; analyze training dynamics and semantic concept alignment; extend to a vision–language model trained on sparse CLIP representations for interpretable, vision-based steering.

Result: Sparse CLIP retains strong downstream performance, achieves superior interpretability relative to SAEs, and preserves multimodal capabilities. Multimodal sparse features enable straightforward semantic concept alignment and reveal cross-modal knowledge emergence. A proof-of-concept demonstrates interpretable, vision-based steering using sparse CLIP representations.

Conclusion: Interpretability and performance can be co-optimized by integrating sparsity into CLIP training, refuting the view that sparsity must harm accuracy; this provides a design principle for future multimodal models.

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

</details>


### [4] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: Introduces NucFuse: unified training and test sets for nuclei instance segmentation in H&E images, plus dataset ranking and external validation across CNN and CNN+Vision Transformer models.


<details>
  <summary>Details</summary>
Motivation: Dataset fragmentation and non-standardized annotations hinder fair cross-dataset evaluation and generalization; a unified benchmark is needed for reliable training, testing, and comparison.

Method: Conducted an extensive literature review to identify manually annotated public H&E nuclei datasets; standardized inputs/annotations; evaluated two state-of-the-art segmentation models (one CNN-based, one CNN+Vision Transformer hybrid) across datasets; ranked datasets by segmentation performance; proposed unified NucFuse-train and NucFuse-test; performed external validation; released implementation publicly.

Result: Created NucFuse-train and NucFuse-test; provided dataset rankings, comprehensive analyses, and a publicly available implementation to support fair cross-dataset benchmarking and improved model training.

Conclusion: A unified benchmark for nuclei instance segmentation on H&E images is established, enabling more reliable cross-dataset evaluation and potentially boosting generalization and benchmarking across datasets.

Abstract: Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

</details>


### [5] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: Proposes Structural Anchor Pruning (SAP) with OSR to prune middle-layer patches, achieving >90% index vector reduction with preserved retrieval fidelity in ViDoRe; challenges the focus on final-layer pruning.


<details>
  <summary>Details</summary>
Motivation: Reduce index size in visual document retrieval without retraining; address limitations of training-free pruning under heavy compression; understand layer-wise information distribution.

Method: Identify key visual patches from middle layers (SAP). Introduce Oracle Score Retention (OSR) to evaluate layer-wise retention of information; evaluate on ViDoRe benchmark.

Result: Index vectors reduced by >90%; retrieval fidelity retained; OSR shows semantic anchors persist in middle layers; demonstrates scalability for Visual RAG.

Conclusion: Middle-layer anchors are crucial; training-free pruning can be effective when targeting middle layers; OSR provides insight into information propagation in VDR models; potential to improve scalability.

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [6] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

TL;DR: This work analyzes diffusion-based large multimodal models (LLaDA-V) attention and proposes a middle-to-late-layer structured token pruning strategy to reduce FLOPs, achieving substantial efficiency with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Mitigate the high computational cost of diffusion-based multimodal models by understanding attention dynamics and pruning visual tokens without sacrificing accuracy.

Method: Perform attention analysis on LLaDA-V; design a structured token pruning method inspired by FastV that prunes a portion of visual tokens at designated layers, focusing on middle-to-late layers of the first denoising step; compare against shallow-layer pruning; evaluate across multiple benchmarks.

Result: Computational cost reduced by up to 65% while preserving about 95% of task performance on average.

Conclusion: First to investigate structured token pruning in diffusion-based multimodal models; vision-aware pruning in LLaDA-V can enable efficient inference without major losses, with pruning targeting middle-to-late first-denoising steps to align with delayed attention aggregation.

Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

</details>


### [7] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleStyle is a lightweight, image/video style-transfer model built on Qwen-Image-Edit. It uses a Curriculum Continual Learning regime over a hybrid dataset (curated clean triplets and synthetic noisy triplets) to achieve robust content preservation while enabling broad, unseen style transfer, with a video-to-video module for temporal consistency. Reported state-of-the-art on style similarity, content fidelity, and aesthetic quality; code and models released.


<details>
  <summary>Details</summary>
Motivation: Address entanglement of content and style representations in diffusion transformers for content-preserving style transfer. Seek robust content preservation, broad style customization, and unified image/video stylization. Build a high-quality, diverse style dataset and a training regime that generalizes to unseen styles without sacrificing content fidelity.

Method: TeleStyle is built on the Qwen-Image-Edit base. It leverages a curated dataset of distinct specific styles and synthesizes triplets using thousands of diverse, in-the-wild style categories. Training employs a Curriculum Continual Learning framework on a hybrid dataset of clean (curated) and noisy (synthetic) triplets. Adds a video-to-video stylization module to improve temporal consistency and visual quality for video. Evaluations cover style similarity, content consistency, and aesthetic quality. Code and pretrained models are released at the project repository.

Result: Reported state-of-the-art performance across three core metrics: style similarity, content consistency, and aesthetic quality for both image and video stylization. Demonstrates strong generalization to unseen styles while maintaining precise content fidelity. Lightweight design suggests practical efficiency.

Conclusion: TeleStyle demonstrates that a strong base model combined with curriculum-based training on a hybrid clean/noisy style triplet dataset can achieve robust, content-preserving style transfer across images and videos, with good temporal coherence and broad style generalization. The release of code and models facilitates adoption and further exploration.

Abstract: Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

</details>


### [8] [Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale](https://arxiv.org/abs/2601.20196)
*Brayden Hamilton,Tim Cashmore,Peter Driscoll,Trevor Gee,Henry Williams*

Main category: cs.CV

TL;DR: Automated biofouling severity classification on vessel hulls using computer vision (CV) models and zero-shot large language models (LLMs); CV excels at extreme LoF levels, LLMs provide competitive, training-free insights; hybrid methods that combine segmentation with LLM reasoning offer a scalable, interpretable path.


<details>
  <summary>Details</summary>
Motivation: Marine biofouling poses ecological, economic, and biosecurity risks; traditional diver-based surveys are hazardous and limited in scalability, necessitating scalable, interpretable automated assessment methods.

Method: Evaluate CNNs, transformer-based segmentation, and zero-shot LLMs on an expert-labelled New Zealand MPI dataset; apply structured prompts and retrieval-augmented reasoning for LLMs; analyze performance across Level of Fouling (LoF) categories and investigate data imbalance and framing effects.

Result: CV models show high accuracy for extreme LoF categories but struggle with intermediate levels due to dataset imbalance and image framing; LLMs achieve competitive performance without training and provide interpretable outputs; segmentation coverage can complement LLM reasoning; hybrid approaches yield scalable and interpretable biofouling assessment.

Conclusion: Hybrid methods that integrate segmentation coverage with LLM-based reasoning hold promise for scalable, interpretable biofouling assessment on vessel hulls.

Abstract: Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.

</details>


### [9] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: DenseGRPO introduces dense, step-wise rewards for each denoising step via an ODE-based reward model, and a reward-aware exploration scheme that adapts stochasticity in the SDE sampler; this resolves sparse-reward misalignment and improves flow-matching alignment and human preference alignment in text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Address the sparse terminal reward problem in GRPO-based flow matching for text-to-image generation by providing fine-grained feedback at intermediate denoising steps and aligning exploration with time-varying noise.

Method: Two components: (1) predict step-wise reward gains as dense rewards for each denoising step using an ODE-based reward model on intermediate clean images; (2) reveal mismatch between uniform exploration and time-varying noise, and propose a reward-aware scheme that adaptively adjusts timestep-specific stochasticity in the SDE sampler to calibrate the exploration space across timesteps.

Result: Extensive experiments on standard benchmarks demonstrate the effectiveness of DenseGRPO and highlight the critical role of valid dense rewards for effective flow-matching model alignment with human preferences.

Conclusion: DenseGRPO shows that providing dense, step-wise rewards and reward-aware exploration can improve the alignment between feedback signals and intermediate contributions in flow-matching models, leading to better human-preference alignment in text-to-image generation.

Abstract: Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

</details>


### [10] [Feature Projection Learning for Better Vision-Language Reasoning](https://arxiv.org/abs/2601.20224)
*Yi Zhang,Weicheng Lin,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: A simple, efficient method (FPL) to adapt CLIP to downstream tasks by projecting class prototypes into the image feature space and reconstructing feature maps; uses negative reconstruction error as the class score; combines this with CLIP predictions and reports SOTA performance.


<details>
  <summary>Details</summary>
Motivation: To adapt vision-language pre-trained models like CLIP to downstream tasks under limited supervision with high efficiency, addressing issues of limited performance, parameter bloat, and long training times.

Method: Introduce a projection model that maps class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error serves as the class score. Classification is effectively reframed as a feature projection problem, and final predictions combine the projection model output with the original CLIP output.

Result: Empirical evaluations show that FPL achieves superior accuracy, surpassing current state-of-the-art methods by a substantial margin.

Conclusion: FPL is a simple yet effective method for adapting CLIP to downstream tasks, offering strong performance gains with efficiency advantages.

Abstract: Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \textit{\textbf{F}eature \textbf{P}rojection \textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.

</details>


### [11] [Visual Prompt-Agnostic Evolution](https://arxiv.org/abs/2601.20232)
*Junze Wang,Lei Fan,Dezheng Zhang,Weipeng Jing,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: A prompt-evolution framework (PAE) for visual prompt tuning that uses frequency-guided prompt initialization, a shared Koopman operator for cross-layer coherence, and a Lyapunov-based regularizer to stabilize evolution; yields faster convergence and accuracy gains, while being prompt-agnostic and lightweight, and compatible with existing VPT variants without backbone changes.


<details>
  <summary>Details</summary>
Motivation: Visual Prompt Tuning often suffers with unstable training dynamics (gradient oscillations) and cross-layer mismatch due to divergent prompt updates across layers. A framework that explicitly models prompt dynamics and coordinates evolution is needed to speed up convergence and improve performance.

Method: 1) Frequency-domain initialization to align prompts with task-aware frequency shortcuts exploited by the backbone. 2) A shared Koopman operator enforcing a global linear transformation for coherent evolution across layers. 3) A Lyapunov-stability-inspired regularizer to constrain error amplification during evolution. 4) Integration with existing VPT variants without backbone modification or inference-time changes.

Result: Empirical evaluation shows ~1.41× faster convergence and 1–3% accuracy gains on 25 datasets across multiple downstream tasks. The method is prompt-agnostic, lightweight, and integrates with diverse VPT variants without backbone changes.

Conclusion: PAE effectively models prompt dynamics to stabilize and accelerate prompt tuning in ViTs. Its frequency-guided initialization, global evolutionary constraint via a Koopman operator, and Lyapunov-based regularization yield consistent improvements while remaining compatible with existing VPT setups.

Abstract: Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\mathtt{PAE}$ accelerates convergence with an average $1.41\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.

</details>


### [12] [BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning](https://arxiv.org/abs/2601.20246)
*Jan Niklas Kolf,Ozan Tezcan,Justin Theiss,Hyung Jun Kim,Wentao Bao,Bhargav Bhushanam,Khushi Gupta,Arun Kejariwal,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: BLenDeR is a diffusion-based sampling method that increases intra-class diversity in deep metric learning by applying union and intersection operations on denoising residuals, enabling controlled synthesis of diverse attribute combinations within each class; it yields consistent gains on standard DML benchmarks (e.g., +3.7% Recall@1 on CUB-200, +1.8% on Cars-196).


<details>
  <summary>Details</summary>
Motivation: To address limited intra-class diversity in synthetic data for deep metric learning. Existing generative augmentation methods often produce limited, uncontrolled diversity. BLenDeR introduces a controllable mechanism to synthesize diverse attribute combinations within a class by manipulating diffusion residuals using set-theory-inspired union and intersection operations.

Method: Utilizes diffusion model sampling with two residual-based operations: union, which aggregates attributes present across multiple prompts to promote diverse attributes; and intersection, which extracts the common direction via a principal component surrogate to stabilize variation. These operations act on denoising residuals to generate controlled, diverse attribute combinations within a class.

Result: BLenDeR consistently outperforms state-of-the-art baselines across multiple standard DML benchmarks and backbones, achieving a 3.7% Recall@1 improvement on CUB-200 and 1.8% on Cars-196 under standard experimental settings.

Conclusion: The proposed union/intersection framework on diffusion residuals provides controllable intra-class diversity for DML augmentation and yields significant performance gains, addressing limitations of prior generative approaches.

Abstract: The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.

</details>


### [13] [Reversible Efficient Diffusion for Image Fusion](https://arxiv.org/abs/2601.20260)
*Xingxin Xu,Bing Cao,DongDong Li,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: Explicitly supervised, reversible diffusion framework (RED) for multi-modal image fusion that preserves details and high visual fidelity while reducing computational cost by avoiding density estimation.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based fusion offers strong generative quality but suffers detail loss due to noise accumulation in the Markov process; fully supervised diffusion end-to-end is computationally intensive, hindering practical fusion tasks.

Method: Introduce Reversible Efficient Diffusion (RED): an explicitly supervised training framework that leverages diffusion models’ generative strength without estimating the data distribution, presumably employing a reversible diffusion process to enable efficient, end-to-end fusion.

Result: Aims to mitigate detail loss and inconsistency in fused images, achieving high visual fidelity with improved computational efficiency compared to conventional diffusion-based fusion methods.

Conclusion: RED provides a supervised diffusion-based approach for image fusion that reduces noise-induced degradation and efficiency bottlenecks, enabling practical end-to-end fusion without distribution estimation.

Abstract: Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.

</details>


### [14] [Hallucination Begins Where Saliency Drops](https://arxiv.org/abs/2601.20279)
*Xiaofeng Zhang,Yuanchao Zhu,Chaochen Gu,Xiaosong Yuan,Qiyan Zhao,Jiawei Cao,Feilong Tang,Sinan Fan,Yaomin Shen,Chen Shen,Hao Tang*

Main category: cs.CV

TL;DR: Introduces LVLMs-Saliency, a gradient-aware framework that combines attention with input gradients to diagnose hallucinations in LVLMs and proposes SGRS and LocoRE to mitigate them, achieving reduced hallucinations with preserved fluency and task performance; code available.


<details>
  <summary>Details</summary>
Motivation: Current hallucination detection in LVLMs relies solely on forward-pass attention patterns and ignores gradient-based signals that reveal how token influence propagates, leading to unreliable discrimination between hallucinated and grounded outputs; hallucinations correlate with memory breakdown when preceding tokens have low saliency for the next token.

Method: LVLMs-Saliency fuses attention weights with input gradients to quantify the visual grounding strength of each output token. The analysis uncovers that hallucinations often occur when prior tokens have low saliency to the next token. To mitigate this, two mechanisms are proposed: (1) Saliency-Guided Rejection Sampling (SGRS), which filters candidate tokens during autoregressive decoding by rejecting tokens whose saliency falls below a context-adaptive threshold; and (2) Local Coherence Reinforcement (LocoRE), a plug-and-play module that strengthens attention from the current token to its recent predecessors to counteract contextual forgetting.

Result: Extensive experiments across multiple LVLMs show that the proposed approach reduces hallucination rates significantly while preserving fluency and task performance.

Conclusion: LVLMs-Saliency provides an interpretable, robust solution for improving the reliability of LVLM outputs, with code available at the referenced GitHub repository.

Abstract: Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency

</details>


### [15] [A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency](https://arxiv.org/abs/2601.20284)
*Debopom Sutradhar,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: A source-free domain adaptation method using multiview augmentation and latent space consistency learns transferable representations from target domain with a ConvNeXt encoder; achieves improvements on Office datasets.


<details>
  <summary>Details</summary>
Motivation: DA methods often rely on access to source data and complex pseudo-labeling or adversarial training, which are costly. This work aims to learn domain-invariant features directly from the target domain without source data.

Method: Proposes a source-free DA framework that enforces consistency across multiple augmented views in the latent space. It uses a ConvNeXt-based encoder and a joint loss combining classification and consistency objectives, obviating source-target alignment or pseudo-label refinement.

Result: Average accuracies: Office-31 90.72%, Office-Home 84%, Office-Caltech 97.12%. Reported improvements over baselines: +1.23%, +7.26%, +1.77%.

Conclusion: Demonstrates the feasibility and competitiveness of target-only DA using multiview and latent-space consistency, reducing reliance on source data while achieving strong performance.

Abstract: Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.

</details>


### [16] [Artifact-Aware Evaluation for High-Quality Video Generation](https://arxiv.org/abs/2601.20297)
*Chen Zhu,Jiashu Zhu,Yanxun Li,Meiqi Wu,Bingze Song,Chubin Chen,Jiahong Wu,Xiangxiang Chu,Yangang Wang*

Main category: cs.CV

TL;DR: Proposes a fine-grained evaluation protocol for video generation artifacts along Appearance, Motion, and Camera; introduces GenVID (80k annotated videos) and DVAR (Dense Video Artifact Recognition) for per-frame artifact localization and categorization; shows improved artifact detection and practical content filtering.


<details>
  <summary>Details</summary>
Motivation: Current video-quality metrics are coarse and do not localize or categorize artifacts; a perception-aligned, fine-grained framework is needed to audit and regulate generative video systems.

Method: Develop a 3-axis artifact taxonomy (Appearance, Motion, Camera) with 10 categories; build GenVID, a large-scale 80k-video dataset annotated for these artifacts; train and evaluate DVAR, a dense recognition model for per-frame/artifact localization and categorization; demonstrate robustness across state-of-the-art generators.

Result: Extensive experiments show that DVAR achieves higher artifact detection accuracy across categories and enables effective filtering of low-quality content; GenVID provides a scalable benchmark for artifact localization and categorization in video generation.

Conclusion: The proposed evaluation protocol enables comprehensive, fine-grained auditing of generative videos, offering both a rich dataset and a practical recognition framework to assess and filter artifacts across Appearance, Motion, and Camera dimensions.

Abstract: With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.

</details>


### [17] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: A query-based multimodal end-to-end PnP framework Li-ViP3D++ with Query-Gated Deformable Fusion (QGDF) for fusing camera and LiDAR in query space; improves end-to-end perception, tracking, and forecasting; faster and with better detection quality on nuScenes.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of modular pipelines and suboptimal fuse-and-align steps by enabling fully differentiable, query-space fusion that leverages both visual and geometric cues without heuristic alignment biases.

Method: Introduce QGDF: (i) masked attention across multi-view cameras and feature levels to aggregate image evidence; (ii) differentiable BEV sampling with learned per-query offsets to extract LiDAR context; (iii) query-conditioned gating to weight visual vs. geometric cues per agent; joint optimization of detection, tracking, and multi-hypothesis trajectory forecasting in an end-to-end model.

Result: On nuScenes, Li-ViP3D++ achieves EPA 0.335 and mAP 0.502, with FP ratio 0.147, and faster inference (139.82 ms) compared to Li-ViP3D (145.91 ms).

Conclusion: Query-space, fully differentiable fusion of camera and LiDAR can improve robustness and end-to-end performance for perception and prediction in autonomous driving without sacrificing deployability.

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


### [18] [Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization](https://arxiv.org/abs/2601.20301)
*Jialuo He,Huangxun Chen*

Main category: cs.CV

TL;DR: Introduces Compression-aware ShArpness Minimization (C-SAM), which perturbs pruning masks during training to align sharpness minimization with structured pruning, improving robustness under pruning while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Pruning a SAM-trained model can degrade robustness because flatness in continuous space does not guarantee robustness to discrete pruning changes; prior approaches either prune and then train or apply SAM after pruning, which can be suboptimal due to architecture constraints.

Method: Shift sharpness minimization from parameter perturbations to mask perturbations by dynamically perturbing pruning masks during training, encouraging a flatter loss landscape with respect to model structure to jointly optimize compactness and input-robustness.

Result: Empirical evaluation on CelebA-HQ, Flowers-102, and CIFAR-10-C using ResNet-18, GoogLeNet, and MobileNet-V2 shows consistently higher certified robustness than strong baselines, with improvements up to 42%, while maintaining task accuracy comparable to unpruned models.

Conclusion: C-SAM enables concurrent optimization of model compactness and robustness by making pruning structure part of the sharpness-aware learning process, offering practical robustness gains for on-device DNN deployments.

Abstract: Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.

</details>


### [19] [Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy](https://arxiv.org/abs/2601.20302)
*Suresh Das,Siladittya Manna,Sayantari Ghosh*

Main category: cs.CV

TL;DR: Dual-domain learning using no-applicator (NA) CT data with a modest amount of with-applicator (WA) data improves bladder segmentation under covariate shift; 10–30% WA data can match WA-only models, with Dice up to 0.94 and IoU up to 0.92 across planes and architectures.


<details>
  <summary>Details</summary>
Motivation: Address covariate shift and data scarcity in medical image segmentation for brachytherapy by leveraging anatomically related but distribution-shifted data (NA vs WA CT scans) to improve robustness and generalizability.

Method: A dual-domain learning strategy that combines NA and WA CT data. Systematic experiments across axial, coronal, and sagittal planes using multiple architectures. Vary WA data proportion and evaluate segmentation with Dice and IoU metrics. Report that small WA data fractions significantly boost performance compared to NA alone and approach WA-only models.

Result: Incorporating a modest fraction (10–30%) of WA data into predominantly NA training yields segmentation performance comparable to WA-only models. Achieves Dice up to 0.94 and IoU up to 0.92, indicating effective domain adaptation and improved reliability for brachytherapy planning.

Conclusion: Integrating anatomically similar but distribution-shifted datasets mitigates covariate shift and data scarcity, enhancing deep-learning-based bladder segmentation for brachytherapy. The approach demonstrates the value of targeted data augmentation from related domains to boost performance with limited labeled WA data.

Abstract: Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.
  We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.
  The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.

</details>


### [20] [Physically Guided Visual Mass Estimation from a Single RGB Image](https://arxiv.org/abs/2601.20303)
*Sungjae Lee,Junhan Jeong,Yeonjoo Hong,Kwang In Kim*

Main category: cs.CV

TL;DR: Physically structured single-image mass estimation framework combining geometry, semantics, and appearance to infer mass via volume- and density-related latent factors; demonstrates superior performance on image2mass and ABO-500.


<details>
  <summary>Details</summary>
Motivation: Mass depends on both volume and material density, which are not directly observable from RGB imagery, making mass prediction ill-posed without physically meaningful constraints.

Method: From a single RGB image, estimate object-centric 3D geometry via monocular depth to inform volume; obtain coarse material semantics with a vision-language model to guide density reasoning; fuse geometry, semantics, and appearance with an instance-adaptive gating mechanism; predict two physically guided latent factors (volume- and density-related) using separate regression heads under mass-only supervision.

Result: The approach consistently outperforms state-of-the-art mass estimation methods on the image2mass and ABO-500 benchmarks.

Conclusion: Aligning visual cues with physical factors governing mass and explicitly modeling volume- and density-related latent factors improves single-image mass estimation, validating the benefit of multimodal and physically guided representations.

Abstract: Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.

</details>


### [21] [Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction](https://arxiv.org/abs/2601.20304)
*Genyuan Zhang,Zihao Wang,Zhifan Gao,Lei Xu,Zhen Zhou,Haijun Yu,Jianjia Zhang,Xiujian Liu,Weiwei Zhang,Shaoyu Wang,Huazhu Fu,Fenglin Liu,Weiwen Wu*

Main category: cs.CV

TL;DR: A Structure-constrained Language-informed Diffusion Model (SLDM) is proposed to enhance low-dose iodinated contrast-enhanced CT angiography by integrating structural priors, semantic supervision with spatial intelligence, and a subtraction angiography module, achieving improved structural fidelity and contrast.


<details>
  <summary>Details</summary>
Motivation: To reduce iodinated contrast media dose while preserving diagnostic accuracy, addressing issues of sensitivity/specificity degradation and risks (kidney injury, allergic reactions). Current methods struggle with incompletely paired data due to limited structural recognition.

Method: Introduce SLDM, a unified medical diffusion model that (1) extracts and uses structural priors to constrain inference for structural consistency, (2) applies a semantic supervision strategy with spatial intelligence to combine visual perception and spatial reasoning for accurate enhancement, and (3) adds a subtraction angiography enhancement module to boost contrast in the ICM region. The approach targets angiographic reconstruction from low-dose contrast-enhanced CT.

Result: Qualitative visual comparisons and quantitative metrics indicate the method effectively enhances low-dose ICM-CT angiography, improving angiographic reconstruction quality.

Conclusion: The framework achieves accurate enhancement and better contrast for the ICM region, enabling reliable low-dose CT angiography with iodinated contrast media.

Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.

</details>


### [22] [TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration](https://arxiv.org/abs/2601.20306)
*Yanjie Tu,Qingsen Yan,Axi Niu,Jiacong Tang*

Main category: cs.CV

TL;DR: Proposes Triple-Prior Guided Diffusion (TPGDiff) for unified image restoration, integrating degradation priors along the diffusion process, hierarchical priors (structural in shallow layers, semantic in deep layers), plus a degradation extractor and a distillation-based semantic extractor to enable adaptive, robust restoration across single- and multi-degradation tasks.


<details>
  <summary>Details</summary>
Motivation: All-in-one restoration aims to fix multiple degradation types with one model, but relying on priors often fails in severely degraded regions. Semantic guidance can help content generation but may blur spatial structure when injected into shallow layers. The work seeks hierarchical, complementary priors across diffusion stages to preserve structure while enabling realistic restoration.

Method: TPGDiff uses multi-source structural cues as shallow-layer priors to capture fine details; a distillation-driven semantic extractor provides robust semantic priors for deep layers; a degradation extractor learns degradation-aware priors to enable stage-adaptive diffusion control across timesteps. Degradation priors guide the diffusion trajectory throughout, enabling coordinated guidance.

Result: Extensive experiments on single- and multi-degradation benchmarks show superior performance and generalization of TPGDiff across diverse restoration scenarios.

Conclusion: A hierarchical, triple-prior framework with stage-aware diffusion control improves unified image restoration by leveraging structural, semantic, and degradation priors in a complementary fashion.

Abstract: All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.

</details>


### [23] [OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion](https://arxiv.org/abs/2601.20308)
*Shuoyan Wei,Feng Li,Chen Zhou,Runmin Cong,Yao Zhao,Huihui Bai*

Main category: cs.CV

TL;DR: OSDEnhancer: a one-step diffusion-based STVSR framework that uses temporal-spatial mixture-of-experts and a bidirectional deformable VAE decoder to achieve real-world high-fidelity and temporally coherent video upscaling.


<details>
  <summary>Details</summary>
Motivation: Tackle real-world STVSR under unknown degradations with a model that maintains spatial detail and temporal coherence while being computationally efficient, leveraging diffusion models for robust generation.

Method: 1) Initialize spatiotemporal structure with linear pre-interpolation; 2) train temporal refinement and spatial enhancement mixture of experts (TR-SE MoE) to learn specialized temporal and spatial representations; 3) employ an efficient one-step diffusion process for refinement; 4) use a bidirectional deformable VAE decoder for recurrent spatiotemporal aggregation and propagation across frames.

Result: Claims state-of-the-art performance and strong generalization to real-world degradations, demonstrated through experiments.

Conclusion: OSDEnhancer integrates one-step diffusion, MoE-based specialization, and a bidirectional deformable VAE to deliver real-world STVSR with high fidelity, temporal coherence, and efficiency.

Abstract: Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.

</details>


### [24] [CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.20318)
*Jiyuan Xu,Wenyu Zhang,Xin Jing,Shuai Chen,Shuai Zhang,Jiahao Nie*

Main category: cs.CV

TL;DR: CPiRi introduces a channel permutation invariant (CPI) framework for multivariate time series forecasting that decouples spatial (cross-channel) and temporal information. It uses a frozen temporal encoder and a learnable spatial module, with channel shuffling during training to enforce permutation invariance, and provides theoretical permutation equivariance with strong empirical results and inductive generalization to unseen channels.


<details>
  <summary>Details</summary>
Motivation: To overcome two main limitations: (i) channel-dependent models overfit to channel ordering and harm adaptability when channels are added/reordered, and (ii) channel-independent models ignore inter-channel dependencies, reducing performance. A CPI approach aims to capture cross-channel structure without fixed ordering.

Method: A spatio-temporal decoupled architecture: a frozen pretrained temporal encoder extracts temporal features; a lightweight spatial module learns content-driven inter-channel relations; permutation-invariant regularization via a channel shuffling training strategy enforces CPI. The framework is grounded theoretically via permutation equivariance in multivariate time series forecasting. Training and inference aim for robustness to channel permutations and efficient scaling; source code released.

Result: Achieves state-of-the-art performance on multiple benchmarks. Demonstrates stability under shuffled channel orders and strong inductive generalization to unseen channels, even when trained on only half of the channels. Maintains practical efficiency on large-scale datasets.

Conclusion: CPiRi provides a principled CPI framework for multivariate time series forecasting, backed by theoretical grounding and strong empirical performance, with good generalization to channel permutations and unseen channels and efficient scalability.

Abstract: Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose \textbf{CPiRi}, a \textbf{channel permutation invariant (CPI)} framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples \textbf{spatio-temporal decoupling architecture} with \textbf{permutation-invariant regularization training strategy}: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further \textbf{ground CPiRi in theory} by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong \textbf{inductive generalization} to unseen channels even when trained on \textbf{only half} of the channels, while maintaining \textbf{practical efficiency} on large-scale datasets. The source code is released at https://github.com/JasonStraka/CPiRi.

</details>


### [25] [GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction](https://arxiv.org/abs/2601.20331)
*Mai Su,Qihan Yu,Zhongtao Wang,Yilong Li,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.CV

TL;DR: This work improves 3D Gaussian Splatting surface reconstruction by introducing a Gaussian visibility-aware multi-view geometric consistency constraint and a progressive quadtree-calibrated monocular depth constraint, yielding better geometric accuracy on DTU and TNT datasets; code released.


<details>
  <summary>Details</summary>
Motivation: Accurate surface reconstruction with Gaussian splats is hindered by unreliable multi-view constraints under large geometric discrepancies and monocular priors with scale ambiguity and local inconsistencies.

Method: Propose two components: 1) Gaussian visibility-aware multi-view geometric consistency that aggregates visibility of shared Gaussian primitives across views; 2) progressive quadtree-calibrated monocular depth constraint with block-wise affine calibration from coarse to fine scales.

Result: Extensive experiments on DTU and TNT show consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods.

Conclusion: The proposed constraints yield more stable geometric supervision and better depth calibration, improving 3D Gaussian Splatting surface reconstruction; code available at GitHub.

Abstract: 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.

</details>


### [26] [Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining](https://arxiv.org/abs/2601.20333)
*Ali Zia,Usman Ali,Umer Ramzan,Abdul Rehman,Abdelwahed Khamis,Wei Xiang*

Main category: cs.CV

TL;DR: TopoOT: A topology-aware optimal transport framework for anomaly segmentation that uses multi-filtration persistence diagrams and test-time adaptation. It introduces Optimal Transport Chaining to align PDs across scales, producing geodesic stability scores that generate stability-based pseudo-labels to supervise a lightweight online head with OT-consistency and contrastive objectives, achieving state-of-the-art performance on 2D and 3D benchmarks (up to +24.1% mean F1 on 2D and +10.2% on 3D).


<details>
  <summary>Details</summary>
Motivation: Threshold-based binarisation can yield brittle masks under distribution shift. Topology-aware data analysis (via persistence diagrams and multi-filtration) offers global structural invariants that persist across scales, motivating a topology-guided, robust AS method.

Method: Integrate multi-filtration persistence diagrams with test-time adaptation using Optimal Transport Chaining to sequentially align PDs across thresholds/filtrations, producing geodesic stability scores. Use these stability-aware pseudo-labels to supervise a lightweight head trained online with OT-consistency and contrastive objectives for robust adaptation under domain shift.

Result: State-of-the-art performance on standard 2D and 3D anomaly detection benchmarks, with improvements of up to 24.1 percentage points in mean F1 on 2D datasets and 10.2 percentage points on 3D benchmarks relative to competitive methods.

Conclusion: TopoOT demonstrates that topology-informed OT alignment of persistence diagrams across filtrations can provide stable, scale-aware anomaly segmentation under distribution shift, enabling effective test-time adaptation with a lightweight online head.

Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

</details>


### [27] [PalmBridge: A Plug-and-Play Feature Alignment Framework for Open-Set Palmprint Verification](https://arxiv.org/abs/2601.20351)
*Chenke Zhang,Ziyuan Yang,Licheng Yan,Shuyi Li,Andrew Beng Jin Teoh,Bob Zhang,Yi Zhang*

Main category: cs.CV

TL;DR: PalmBridge is a plug-and-play feature-space alignment framework for open-set palmprint verification that uses learned representative vectors to map and blend features, improving domain generalization with minimal runtime cost.


<details>
  <summary>Details</summary>
Motivation: Real-world palmprint systems suffer from feature distribution shifts due to heterogeneous deployment; data augmentation alone often fails to bridge domain gaps, especially in open-set verification.

Method: Learn a compact dictionary of representative feature vectors from training data. During enrollment and verification, map each feature to its nearest representative under minimum distance, blend the mapped vector with the original, and train the backbone jointly with task supervision, a feature-consistency objective, and an orthogonality regularization term. Analyze mappings via assignment consistency and collision rate.

Result: Consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization across multiple palmprint datasets and backbone architectures, with negligible to modest runtime overhead.

Conclusion: Feature-space alignment via vector quantization provides robust, domain-invariant embeddings for open-set palmprint verification, achieving better generalization with limited computational cost.

Abstract: Palmprint recognition is widely used in biometric systems, yet real-world performance often degrades due to feature distribution shifts caused by heterogeneous deployment conditions. Most deep palmprint models assume a closed and stationary distribution, leading to overfitting to dataset-specific textures rather than learning domain-invariant representations. Although data augmentation is commonly used to mitigate this issue, it assumes augmented samples can approximate the target deployment distribution, an assumption that often fails under significant domain mismatch. To address this limitation, we propose PalmBridge, a plug-and-play feature-space alignment framework for open-set palmprint verification based on vector quantization. Rather than relying solely on data-level augmentation, PalmBridge learns a compact set of representative vectors directly from training features. During enrollment and verification, each feature vector is mapped to its nearest representative vector under a minimum-distance criterion, and the mapped vector is then blended with the original vector. This design suppresses nuisance variation induced by domain shifts while retaining discriminative identity cues. The representative vectors are jointly optimized with the backbone network using task supervision, a feature-consistency objective, and an orthogonality regularization term to form a stable and well-structured shared embedding space. Furthermore, we analyze feature-to-representative mappings via assignment consistency and collision rate to assess model's sensitivity to blending weights. Experiments on multiple palmprint datasets and backbone architectures show that PalmBridge consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization with negligible to modest runtime overhead.

</details>


### [28] [Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models](https://arxiv.org/abs/2601.20354)
*Zengbin Wang,Xuecai Hu,Yong Wang,Feng Xiong,Man Zhang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: A new benchmark, SpatialGenEval, systematically evaluates spatial intelligence in text-to-image (T2I) models using 1,230 dense prompts across 25 real-world scenes with 10 spatial sub-domains and 10 QA pairs each; and SpatialT2I, a dataset of 15,400 T2I pairs with rewritten prompts preserving information density. Evaluations on 21 SOTA models identify higher-order spatial reasoning as a major bottleneck; fine-tuning gives consistent gains on base models, supporting a data-centric path to spatial intelligence in T2I.


<details>
  <summary>Details</summary>
Motivation: Current T2I benchmarks rely on short or sparse prompts, failing to probe spatial perception, reasoning, and interactions; a systematic, information-dense evaluation is needed to diagnose and improve spatial understanding in T2I models.

Method: Design SpatialGenEval with large-scale, information-dense prompts (1,230 prompts across 25 real scenes), covering 10 spatial sub-domains and 10 QA pairs per prompt; evaluate 21 SOTA models. Build SpatialT2I with 15,400 T2I pairs and rewritten prompts to preserve density and ensure image consistency. Fine-tune representative foundation models (Stable Diffusion-XL, Uniworld-V1, OmniGen2) and report gains.

Result: Findings show higher-order spatial reasoning remains a bottleneck; fine-tuning yields gains of +4.2% (Stable Diffusion-XL), +5.7% (Uniworld-V1), +4.4% (OmniGen2) on spatial tasks, with more realistic spatial relations, validating a data-centric approach for spatial intelligence.

Conclusion: A dense-prompt, data-centric evaluation framework (SpatialGenEval/SpatialT2I) advances understanding and improvement of spatial abilities in T2I models, guiding future research toward richer data and evaluation protocols.

Abstract: Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.

</details>


### [29] [CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization](https://arxiv.org/abs/2601.20355)
*Yue Liang,Jiatong Du,Ziyi Yang,Yanjun Huang,Hong Chen*

Main category: cs.CV

TL;DR: CURVE integrates probabilistic uncertainty modeling with uncertainty-guided regularization to debias scene graphs, using prototype-conditioned debiasing to extract invariant interactions and enforce a sparse, domain-stable topology; evaluated on zero-shot transfer and low-data sim-to-real adaptation.


<details>
  <summary>Details</summary>
Motivation: Scene graphs often overfit to spurious correlations, harming out-of-distribution generalization; a causal, uncertainty-aware approach is needed to learn domain-stable relational structures.

Method: A causality-inspired framework that combines variational uncertainty modeling with uncertainty-guided structural regularization. It uses prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variation, promoting a sparse, domain-stable topology.

Result: Empirical results show CURVE learns domain-stable sparse topologies and yields reliable uncertainty estimates, supporting risk prediction under distribution shifts, demonstrated in zero-shot transfer and low-data sim-to-real adaptation.

Conclusion: CURVE offers a causality-inspired solution to debias scene graphs, improving OOD generalization by enforcing sparse, domain-stable topologies and providing uncertainty estimates for risk assessment under distribution shifts.

Abstract: Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.

</details>


### [30] [RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching](https://arxiv.org/abs/2601.20364)
*Zhen Liu,Diedong Feng,Hai Jiang,Liaoyuan Zeng,Hao Wang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: Proposes RAW-Flow, a deterministic latent transport framework for RGB-to-RAW reconstruction using flow matching to map RGB to RAW in latent space, with cross-scale guidance and a dual-domain latent autoencoder, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: RGB-to-RAW reconstruction is ill-posed and challenging due to information loss in quantized RGB and inverse ISP; existing regression-based methods yield detail inconsistency and color deviation; a generative latent-transport approach can better preserve structure and color.

Method: Introduces RAW-Flow, a deterministic latent transport approach using flow matching to learn a vector field in latent space to bridge RGB and RAW. Includes a cross-scale context guidance module injecting hierarchical RGB features into flow estimation. Employs a dual-domain latent autoencoder with a feature alignment constraint to encode RGB and RAW inputs and stabilize training, enabling accurate reconstruction.

Result: Extensive experiments show RAW-Flow surpasses state-of-the-art methods both quantitatively and visually.

Conclusion: Recasting RGB-to-RAW as deterministic latent transport via flow matching, augmented with cross-scale guidance and dual-domain autoencoding, yields superior reconstruction quality and stability compared to prior regression-based methods.

Abstract: RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.

</details>


### [31] [Dual-Modality IoT Framework for Integrated Access Control and Environmental Safety Monitoring with Real-Time Cloud Analytics](https://arxiv.org/abs/2601.20366)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib,Nihal Das Ankur,Anish Giri*

Main category: cs.CV

TL;DR: Dual-modality IoT framework integrating RFID access control with multi-sensor environmental safety monitoring using ESP32, cloud logging, and offline caching; two subsystems enable unified, cost-effective security and safety management.


<details>
  <summary>Details</summary>
Motivation: Overcome inefficiencies and complexity of siloed security and environmental safety systems in smart infrastructure; deliver faster responses and lower total cost.

Method: Two coordinated subsystems: Subsystem 1 RFID authentication with servo-controlled gate and Google Sheets logging; Subsystem 2 environmental safety with flame detection, water flow measurement, LCD status display, and personnel identification. Edge processing on ESP32; unified cloud architecture; local caching for offline operation; 45-day evaluation; cost analysis.

Result: RFID authentication accuracy 99.2% with 0.82 s avg response; flame detection reliability 98.5% within 5 m; cloud data logging success 99.8%; operation maintained during network disruptions via local caching; total implementation cost 5,400 BDT (~$48) with 82% cost reduction vs commercial solutions.

Conclusion: Demonstrates a practical, scalable framework for integrating security and safety in smart infrastructure; shows that careful architecture and component optimization yield professional-grade performance at low cost; broad applicability.

Abstract: The integration of physical security systems with environmental safety monitoring represents a critical advancement in smart infrastructure management. Traditional approaches maintain these systems as independent silos, creating operational inefficiencies, delayed emergency responses, and increased management complexity. This paper presents a comprehensive dual-modality Internet of Things framework that seamlessly integrates RFID-based access control with multi-sensor environmental safety monitoring through a unified cloud architecture. The system comprises two coordinated subsystems: Subsystem 1 implements RFID authentication with servo-actuated gate control and real-time Google Sheets logging, while Subsystem 2 provides comprehensive safety monitoring incorporating flame detection, water flow measurement, LCD status display, and personnel identification. Both subsystems utilize ESP32 microcontrollers for edge processing and wireless connectivity. Experimental evaluation over 45 days demonstrates exceptional performance metrics: 99.2\% RFID authentication accuracy with 0.82-second average response time, 98.5\% flame detection reliability within 5-meter range, and 99.8\% cloud data logging success rate. The system maintains operational integrity during network disruptions through intelligent local caching mechanisms and achieves total implementation cost of 5,400 BDT (approximately \$48), representing an 82\% reduction compared to commercial integrated solutions. This research establishes a practical framework for synergistic security-safety integration, demonstrating that professional-grade performance can be achieved through careful architectural design and component optimization while maintaining exceptional cost-effectiveness and accessibility for diverse application scenarios.

</details>


### [32] [RepSFNet : A Single Fusion Network with Structural Reparameterization for Crowd Counting](https://arxiv.org/abs/2601.20369)
*Mas Nurul Achmadiah,Chi-Chia Sun,Wen-Kai Kuo,Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: RepSFNet is a lightweight crowd counting network using a RepLK-ViT backbone with large reparameterized kernels, fused multi-scale features via ASPP+CAN, and a Concatenate Fusion module to preserve resolution. It forgoes attention and multi-branch designs, trained with MSE plus Optimal Transport loss, achieving competitive accuracy with up to 34% lower latency on ShanghaiTech, NWPU, and UCF-QNRF datasets, suitable for real-time edge deployment.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of variable-density crowd counting—scale variation, occlusions, and high computational cost—by designing a lightweight, real-time architecture that maintains accuracy without attention or multi-branch overhead.

Method: Propose RepSFNet with a RepLK-ViT backbone featuring large reparameterized kernels for efficient multi-scale feature extraction. Integrate a Feature Fusion module that combines Atrous Spatial Pyramid Pooling (ASPP) and Context-Aware Network (CAN) for robust, density-adaptive context. Use a Concatenate Fusion module to preserve spatial resolution in density maps. Train with a joint objective of Mean Squared Error (MSE) and Optimal Transport (OT) loss. Evaluate on ShanghaiTech, NWPU, and UCF-QNRF datasets.

Result: Demonstrates competitive counting accuracy while significantly reducing inference latency by up to 34% compared to recent state-of-the-art methods.

Conclusion: RepSFNet achieves efficient real-time crowd counting with strong multi-scale and density-aware context modeling, while maintaining a compact, low-complexity design suitable for edge computing.

Abstract: Crowd counting remains challenging in variable-density scenes due to scale variations, occlusions, and the high computational cost of existing models. To address these issues, we propose RepSFNet (Reparameterized Single Fusion Network), a lightweight architecture designed for accurate and real-time crowd estimation. RepSFNet leverages a RepLK-ViT backbone with large reparameterized kernels for efficient multi-scale feature extraction. It further integrates a Feature Fusion module combining Atrous Spatial Pyramid Pooling (ASPP) and Context-Aware Network (CAN) to achieve robust, density-adaptive context modeling. A Concatenate Fusion module is employed to preserve spatial resolution and generate high-quality density maps. By avoiding attention mechanisms and multi-branch designs, RepSFNet significantly reduces parameters and computational complexity. The training objective combines Mean Squared Error and Optimal Transport loss to improve both count accuracy and spatial distribution alignment. Experiments conducted on ShanghaiTech, NWPU, and UCF-QNRF datasets demonstrate that RepSFNet achieves competitive accuracy while reducing inference latency by up to 34 percent compared to recent state-of-the-art methods, making it suitable for real-time and low-power edge computing applications.

</details>


### [33] [HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation](https://arxiv.org/abs/2601.20383)
*Mengge Liu,Yan Di,Gu Wang,Yun Qu,Dekai Zhu,Yanyan Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: HINT introduces the first autoregressive diffusion-based framework for multi-human motion generation with hierarchical interaction modeling. It uses a canonical latent space to decouple local motion semantics from inter-person interactions and a sliding-window online generation strategy to handle variable agent counts and long sequences, achieving competitive results and state-of-the-art FID on InterHuman.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of prior offline methods that assume fixed-length motions and fixed numbers of agents, and to improve handling of long or variable text and varying agent counts through autoregressive generation with structured interaction modeling.

Method: Autoregressive diffusion framework (HINT) with hierarchical interaction modeling. It employs a disentangled motion representation in a canonical latent space that decouples local motion from inter-person interactions, enabling adaptation to varying numbers of participants. A sliding-window strategy enables online generation by aggregating local within-window conditions and global cross-window history and text guidance.

Result: HINT matches strong offline models and surpasses autoregressive baselines. On the InterHuman benchmark, it achieves an FID of 3.100, significantly better than the previous state-of-the-art 5.154.

Conclusion: HINT is the first autoregressive framework for multi-human motion generation with hierarchical interactions, scalable to variable agent counts and long sequences. The sliding-window approach yields efficient online generation while preserving long-horizon coherence and aligns well with text guidance.

Abstract: Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.

</details>


### [34] [Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models](https://arxiv.org/abs/2601.20419)
*Yuhao Sun,Chengyi Cai,Jiacheng Zhang,Zesheng Ye,Xingliang Yuan,Feng Liu*

Main category: cs.CV

TL;DR: BiFTA improves fine-grained text-visual alignment by removing redundancy in both image patches (via IoU-based view refinement) and text descriptions (via cosine similarity-based pruning), leading to superior zero-shot CLIP performance on 6 benchmarks for ViT- and ResNet-backbones.


<details>
  <summary>Details</summary>
Motivation: Redundant visual patches and text descriptions weaken the discriminative power of text-visual alignment; pruning redundancy should yield more distinctive samples and diverse prompts, improving zero-shot generalization.

Method: BiFTA introduces two refinemet steps: (1) View refinement selects and preserves distinctive image patches by removing highly overlapping patches using Intersection over Union (IoU) filters; (2) Description refinement prunes highly similar textual descriptions based on pairwise cosine similarity to enhance description diversity before alignment.

Result: BiFTA achieves superior zero-shot performance across 6 benchmark datasets for both ViT-based and ResNet-based CLIP architectures, validating the benefit of redundancy removal in visual-text alignment.

Conclusion: Redundancy reduction in both visual and textual modalities improves zero-shot CLIP performance, indicating the value of BiFTA for fine-grained text-visual alignment and suggesting potential applicability to other multimodal alignment tasks.

Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.

</details>


### [35] [Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance](https://arxiv.org/abs/2601.20425)
*Chenliang Zhou,Fangcheng Zhong,Weihao Xia,Albert Miao,Canberk Baykal,Cengiz Oztireli*

Main category: cs.CV

TL;DR: A four-diffusion-model framework for 3D point cloud generation that enforces symmetry and part priors via coordinated global, symmetry, part, and assembly latents, with a central global latent for coherence.


<details>
  <summary>Details</summary>
Motivation: Tackle limitations of holistic shape generation by incorporating explicit priors for symmetry and semantic parts to improve controllability, coherence, and output quality.

Method: Four coordinated diffusion models learn distributions over (1) global shape latents, (2) symmetries, (3) semantic parts, and (4) their spatial assembly; a central global latent enforces cross-part coherence; disentangled pipeline enables targeted part manipulation while preserving global consistency.

Result: Claims state-of-the-art performance and novelty as the first 3D point cloud generation framework to fully integrate and enforce both symmetry and part priors throughout the generative process.

Conclusion: The approach offers interpretable, controllable generation with guaranteed symmetry and coherent part placement, potentially advancing 3D point cloud synthesis; evaluation details and generalization potential warrant scrutiny.

Abstract: We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.

</details>


### [36] [Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding](https://arxiv.org/abs/2601.20430)
*Kun Yin,Yunfei Wu,Bing Liu,Zhongpeng Cai,Xiaotian Li,Huang Chen,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Qianyu Li,Antai Guo,Yanzhen Liao,Yanqiu Qu,Haodong Lin,Chengxu He,Shuangyin Liu*

Main category: cs.CV

TL;DR: Youtu-Parsing is a high-performance, versatile document parsing model that combines a ViT-based visual encoder with a prompt-guided LLM for layout analysis and region-prompted decoding, employing token and query parallelism to achieve substantial speedups, state-of-the-art results on OmniDocBench and olmOCR-bench, and robustness across multilingual, handwritten, and diverse document elements.


<details>
  <summary>Details</summary>
Motivation: The need for efficient, scalable document understanding across diverse content types (text, formulas, tables, charts, seals, hierarchical structures) with robustness to rare characters, multilingual text, and handwriting, addressing the bottleneck of slow autoregressive decoding.

Method: A decoupled architecture with a native Vision Transformer (ViT) visual encoder using dynamic-resolution features, combined with a prompt-guided Youtu-LLM-2B for layout analysis. Introduces two parallel decoding strategies: token parallelism (up to 64 candidate tokens per step with verification) and region-based query parallelism (up to five bounding boxes processed simultaneously) to accelerate decoding while preserving output quality.

Result: 5–11x speedup over traditional autoregressive decoding; additional ~2x acceleration from query parallelism; strong performance on diverse document elements and robustness to rare characters, multilingual and handwritten content; achieves state-of-the-art on OmniDocBench and olmOCR-bench benchmarks.

Conclusion: Youtu-Parsing offers a decoupled, feature-reusable framework that combines high-throughput decoding with accurate content extraction, delivering practical benefits for large-scale document intelligence applications, especially in highly structured domains like tables, while maintaining robustness across languages and writing styles.

Abstract: This paper presents Youtu-Parsing, an efficient and versatile document parsing model designed for high-performance content extraction. The architecture employs a native Vision Transformer (ViT) featuring a dynamic-resolution visual encoder to extract shared document features, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. Leveraging this decoupled and feature-reusable framework, we introduce a high-parallelism decoding strategy comprising two core components: token parallelism and query parallelism. The token parallelism strategy concurrently generates up to 64 candidate tokens per inference step, which are subsequently validated through a verification mechanism. This approach yields a 5--11x speedup over traditional autoregressive decoding and is particularly well-suited for highly structured scenarios, such as table recognition. To further exploit the advantages of region-prompted decoding, the query parallelism strategy enables simultaneous content prediction for multiple bounding boxes (up to five), providing an additional 2x acceleration while maintaining output quality equivalent to standard decoding. Youtu-Parsing encompasses a diverse range of document elements, including text, formulas, tables, charts, seals, and hierarchical structures. Furthermore, the model exhibits strong robustness when handling rare characters, multilingual text, and handwritten content. Extensive evaluations demonstrate that Youtu-Parsing achieves state-of-the-art (SOTA) performance on both the OmniDocBench and olmOCR-bench benchmarks. Overall, Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications.

</details>


### [37] [MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models](https://arxiv.org/abs/2601.20433)
*Wenbo Xu,Wei Lu,Xiangyang Luo,Jiantao Zhou*

Main category: cs.CV

TL;DR: MARE: Multimodal alignment and reinforcement for explainable Deepfake detection using vision-language models; employs RLHF-guided, text-spatial reasoning and a forgery disentanglement module to achieve state-of-the-art accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection requires higher accuracy and explaining model decisions. Existing work mostly uses classification or spatial localization and may not capture cross-modal cues; rapid advances in generative models demand more robust, explainable approaches.

Method: Integrates vision-language models with reinforcement learning from human feedback (RLHF) to produce text-spatially aligned reasoning content; introduces a forgery disentanglement module to extract intrinsic forgery traces from high-level facial semantics; designs comprehensive reward functions to incentivize human-preferred explanations.

Result: Extensive evaluations on the reasoning content generated by MARE show quantitative and qualitative results achieving state-of-the-art performance in accuracy and reliability.

Conclusion: MARE advances deepfake detection by combining VLMs, RLHF-driven explainable reasoning, and forgery disentanglement to enhance accuracy, reliability, and interpretability.

Abstract: Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.

</details>


### [38] [Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection](https://arxiv.org/abs/2601.20461)
*Yanzhu Liu,Xiao Liu,Yuexuan Wang,Mondal Soumik*

Main category: cs.CV

TL;DR: Contamination-based detector to generalize across unseen AI image generators by modifying real images with each generator’s final component; taxonomy of 21 generators; strong cross-generator performance with minimal data.


<details>
  <summary>Details</summary>
Motivation: Address poor generalization of deepfake detectors to images from unseen generators despite shared final architectural components.

Method: Identify the final component common to image generators; contaminate real images by inserting this final component; train a detector to distinguish contaminated from pristine images; create a taxonomy of 21 generators by their final components; fine-tune a detector (based on the DINOv3 backbone) using only 100 samples per taxonomy category.

Result: Detector achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.

Conclusion: Contamination-based training combined with a final-component taxonomy yields strong cross-generator generalization with minimal per-category data; supports robustness of detection across diverse generation paradigms.

Abstract: With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to "contaminate" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.

</details>


### [39] [Efficient Autoregressive Video Diffusion with Dummy Head](https://arxiv.org/abs/2601.20499)
*Hang Guo,Zhaoyang Jia,Jiahao Li,Bin Li,Yuanhao Cai,Jiangshan Wang,Yawei Li,Yan Lu*

Main category: cs.CV

TL;DR: Dummy Forcing reduces redundant context usage in autoregressive video diffusion models by classifying attention heads and compressing KV caches, enabling up to 2x speedup with minimal quality loss (0.5%).


<details>
  <summary>Details</summary>
Motivation: Attention-based video diffusion models waste computation when many heads attend mainly to the current frame; discarding KV caches from such heads yields little performance loss, indicating room to optimize memory and context flow without retraining.

Method: Introduce Dummy Forcing to control head-level context access; employ heterogeneous memory allocation and dynamic head programming to classify heads by type; apply context packing to compress caches further; all without additional training.

Result: Empirical results show up to 2.0× speedup over baseline, achieving 24.3 FPS for video generation with less than 0.5% quality drop.

Conclusion: Selective, dynamic management of attention heads and cache compression can substantially accelerate autoregressive video diffusion while preserving quality, suggesting broader applicability to transformer-based generative models and cache-conscious inference.

Abstract: The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.

</details>


### [40] [Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI](https://arxiv.org/abs/2601.20503)
*Jesse Phitidis,Alison Q. Smithard,William N. Whiteley,Joanna M. Wardlaw,Miguel O. Bernabeu,Maria Valdés Hernández*

Main category: cs.CV

TL;DR: Six-strategy evaluation for joint WMH/ISL segmentation using partially labelled data; pseudolabels yield the best performance.


<details>
  <summary>Details</summary>
Motivation: WMH and ISL co-occur and are visually confounded on FLAIR; fully labelled data are scarce; leveraging partial labels can improve segmentation models.

Method: Aggregate 2052 MRI volumes from private and public sources with varying labeling; evaluate six training strategies for a combined WMH/ISL segmentation model; compare against fully labelled baselines; key finding: pseudolabels performed best.

Result: Several strategies effectively utilize partially labelled data; pseudolabeling provides the strongest improvement in segmentation performance for WMH and ISL.

Conclusion: Partially labelled data, particularly when using pseudolabels, is a viable path to robust joint WMH/ISL segmentation and may ease data annotation burden.

Abstract: White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.

</details>


### [41] [Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V](https://arxiv.org/abs/2601.20504)
*Meiqi Wu,Bingze Song,Ruimin Lin,Chen Zhu,Xiaokun Feng,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: Introduces Latent Temporal Discrepancy (LTD), a motion-prior-based loss weighting for diffusion video generation that emphasizes high-latent-frame-discrepancy regions to improve motion fidelity; reports 3.31% gains on VBench and 3.58% on VMBench.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for video often optimize with a static loss, which fails to capture dynamic motion; noise degrades temporal coherence and complicates learning high-frequency dynamics.

Method: Define LTD as a motion prior that quantifies frame-to-frame variation in latent space and uses it to assign larger loss penalties to regions with higher discrepancy, while keeping regular optimization for stable regions; integrate LTD into the training objective to stabilize learning and enhance dynamic reconstruction.

Result: Empirical evaluation on VBench and VMBench shows consistent improvements, with 3.31% and 3.58% gains over strong baselines, indicating improved motion quality and ability to handle dynamic changes.

Conclusion: LTD provides an effective motion-aware loss weighting mechanism that improves motion fidelity in video generation and could generalize to other motion-centric diffusion tasks.

Abstract: Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.

</details>


### [42] [Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits](https://arxiv.org/abs/2601.20511)
*Zelong Sun,Jiahui Wu,Ying Ba,Dong Jing,Zhiwu Lu*

Main category: cs.CV

TL;DR: Introduces Portrait Collection Generation (PCG): editing a reference portrait via natural language to create coherent portrait collections. Presents CHEESE, a large-scale PCG dataset (24k collections, 573k samples) built with an LLM-based pipeline and inversion-based verification, and SCheese, a text-guided generation framework with adaptive feature fusion and ConsistencyNet to preserve identity and fine details. Experiments show CHEESE's effectiveness and SCheese achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: People want intuitive, high-quality tools to generate diverse portrait collections with complex attribute edits (pose, layout, viewpoint) while preserving identity and details (clothes, accessories). Current methods struggle with multi-attribute edits and maintaining fidelity across a collection.

Method: CHEESE dataset construction via a Large Vision-Language Model (LVLM) pipeline and inversion-based verification to produce 24K portrait collections (573K samples) with high-quality modification texts. SCheese framework combines text-guided generation with hierarchical identity and detail preservation: adaptive feature fusion to maintain identity consistency, and a ConsistencyNet to inject fine-grained features for detail consistency.

Result: CHEESE effectively enables PCG as a new task, providing a large benchmark for future work. SCheese achieves state-of-the-art performance on PCG tasks, with ablation/experiments validating the benefits of adaptive fusion and ConsistencyNet for identity and detail preservation.

Conclusion: The work establishes PCG as a viable task, providing CHEESE as a large, high-quality dataset and SCheese as a strong baseline framework. This setup addresses the dual challenges of multi-attribute editing and high-fidelity detail preservation, advancing portrait generation research and enabling richer, coherent portrait collections.

Abstract: As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.

</details>


### [43] [Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective](https://arxiv.org/abs/2601.20520)
*Qiyan Zhao,Xiaofeng Zhang,Shuochen Chang,Qianyu Chen,Xiaosong Yuan,Xuhang Chen,Luoqi Liu,Jiajun Zhang,Xu-Yao Zhang,Da-Han Wang*

Main category: cs.CV

TL;DR: Cache-accelerated diffusion models in dMLLMs induce repetition (Repeat Curse); CoTA mitigates it by reengineering context-token attention and adding a penalty to uncertain context-token confidence, improving generation quality.


<details>
  <summary>Details</summary>
Motivation: Investigate why caching causes repetition and understand information-flow dynamics in dMLLMs—context tokens as anchors; entropy convergence across layers; disruptions cause repetition.

Method: Introduce CoTA: plug-and-play adjustments to attention on context tokens to preserve information-flow patterns; add a penalty term to the decoding confidence score to discourage outputs driven by uncertain context tokens.

Result: Empirical evaluation shows CoTA reduces repetition and yields consistent performance gains on general tasks; code provided.

Conclusion: CoTA effectively mitigates the Repeat Curse by stabilizing information flow and calibrating confidence, suggesting practical utility for dMLLMs.

Abstract: Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA

</details>


### [44] [AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors](https://arxiv.org/abs/2601.20524)
*Matic Fučka,Vitjan Zavrtanik,Danijel Skočaj*

Main category: cs.CV

TL;DR: AnomalyVFM turns any pretrained vision foundation model into a strong zero-shot anomaly detector by combining a three-stage synthetic data generation scheme with parameter-efficient adaptation (low-rank adapters and confidence-weighted pixel loss), achieving state-of-the-art AUROC on 9 datasets with RADIO as backbone.


<details>
  <summary>Details</summary>
Motivation: Address two practical limitations of VFMs in zero-shot anomaly detection: limited diversity in auxiliary anomaly data and shallow adaptation strategies.

Method: Three-stage synthetic data generation for anomalies, plus a parameter-efficient adaptation using low-rank feature adapters and a confidence-weighted pixel loss to tune the VFM without full fine-tuning.

Result: On RADIO backbone, AnomalyVFM attains average image-level AUROC of 94.1% across 9 diverse datasets, outperforming prior methods by 3.3 percentage points.

Conclusion: VFMs can be effectively repurposed for zero-shot anomaly detection via the proposed synthetic data generation and lightweight adaptation approach, delivering substantial gains without in-domain training data.

Abstract: Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/

</details>


### [45] [IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework](https://arxiv.org/abs/2601.20526)
*Shaokun Wang,Yifan Yu,Yuhang He,Weili Guan,Yihong Gong*

Main category: cs.CV

TL;DR: IOTA combines a data-driven Black Box and a knowledge-driven White Box to improve PET-style adaptation; corrective knowledge is extracted from the White Box, verbalized as prompts, and used to select prompts for the Black Box, achieving superior performance on 12 image classification benchmarks in few-shot and easy-to-hard settings.


<details>
  <summary>Details</summary>
Motivation: Existing PET methods treat the pre-trained model as an opaque black box, underutilizing inherent prior knowledge. There is a need to integrate interpretable, knowledge-based cues with data-driven tuning to enhance downstream adaptation.

Method: A White Box module derives corrective knowledge by contrasting incorrect predictions with correct cognition, verbalizes it into interpretable human prompts, and uses a corrective knowledge-guided prompt selection strategy to steer the Black Box module. The two modules are jointly trained to leverage both knowledge- and data-driven signals.

Result: Empirical evaluation on 12 image classification benchmarks shows that corrective knowledge improves performance, with the proposed framework outperforming state-of-the-art PET methods in few-shot and easy-to-hard adaptation scenarios.

Conclusion: Integrating a knowledge-driven White Box with a data-driven Black Box for prompt learning yields effective downstream adaptation and gains over existing PET approaches.

Abstract: Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.

</details>


### [46] [Advancing Open-source World Models](https://arxiv.org/abs/2601.20540)
*Robbyant Team,Zelin Gao,Qiuyu Wang,Yanhong Zeng,Jiapeng Zhu,Ka Leong Cheng,Yixuan Li,Hanlin Wang,Yinghao Xu,Shuailei Ma,Yihang Chen,Jie Liu,Yansong Cheng,Yao Yao,Jiayi Zhu,Yihao Meng,Kecheng Zheng,Qingyan Bai,Jingye Chen,Zehong Shen,Yue Yu,Xing Zhu,Yujun Shen,Hao Ouyang*

Main category: cs.CV

TL;DR: Open-source world simulator LingBot-World offers high-fidelity dynamics across diverse domains, a minute-level horizon with long-term memory, and sub-second latency at 16fps, released publicly to bridge open-source and closed-source gaps and enable applications in content creation, gaming, and robotics.


<details>
  <summary>Details</summary>
Motivation: Fill the gap between open-source and closed-source world models by providing a flexible, multi-domain, temporally consistent simulator with real-time interactivity for practical applications.

Method: A video-generation–based world model capable of rendering realistic, scientific, and cartoon-style environments; supports minute-level temporal horizon with maintained context (long-term memory) and real-time interactivity (latency <1s at 16fps). Public release of code and model.

Result: Presentation of a public release with claimed capabilities; no empirical results reported in the abstract; emphasizes potential impact on content creation, gaming, and robot learning.

Conclusion: The authors advocate for wider community access to reduce the open-vs-closed source divide and to accelerate practical deployments of advanced world models across domains.

Abstract: We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as "long-term memory". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.

</details>


### [47] [DeepSeek-OCR 2: Visual Causal Flow](https://arxiv.org/abs/2601.20552)
*Haoran Wei,Yaofeng Sun,Yukun Li*

Main category: cs.CV

TL;DR: Proposes DeepSeek-OCR-2: an encoder with causal reasoning (DeepEncoder V2) to dynamically reorder visual tokens before LLM interpretation, using two cascaded 1D causal reasoning to enable 2D reasoning; code is publicly available.


<details>
  <summary>Details</summary>
Motivation: Address the rigid raster-scan token processing and fixed positional encodings in vision-language models, aiming to mimic human semantically-driven, causally-informed visual scanning, especially for complex layouts.

Method: Introduce DeepEncoder V2 that reorders visual tokens prior to LLM input; employ two cascaded 1D causal reasoning modules to approximate 2D reasoning and enable dynamic token sequencing based on image semantics.

Result: The abstract presents a conceptual framework without reported empirical results; emphasizes architectural proposal and cognitive-inspired approach; code and model weights are publicly accessible.

Conclusion: If validated, the approach could enable genuine 2D reasoning in vision-language systems; provides an open-source implementation for community evaluation.

Abstract: We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.

</details>


### [48] [DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression](https://arxiv.org/abs/2601.20564)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: cs.CV

TL;DR: Real-time diffusion-based perceptual neural video compression (DiffVC-RT) with efficiency-focused architecture, explicit/implicit temporal consistency, and asynchronous decoding, reporting strong perceptual bitrate efficiency and real-time 720p performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based neural video compression suffers high computational cost, information loss, latency, and temporal flicker; a real-time, artifact-controlled approach is needed for practical deployment.

Method: 1) Efficient architectural design via module replacements and pruning to cut compute while preserving information. 2) Explicit and implicit consistency modeling: a zero-cost Online Temporal Shift Module within the U-Net plus hybrid implicit consistency constraints to reduce temporal flicker. 3) Asynchronous and parallel decoding with mixed half precision, enabling latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design.

Result: Reported 80.1% LPIPS-based bitrate savings over VTM-17.0 on a HEVC dataset, with real-time encoding/decoding at 206 fps / 30 fps for 720p on an NVIDIA H800 GPU.

Conclusion: DiffVC-RT demonstrates the feasibility of real-time diffusion-based perceptual NVC with substantial perceptual efficiency gains and practical decoding speeds, marking a significant step toward deployable diffusion-based video codecs. Further validation across content types, resolutions, and longer sequences is needed.

Abstract: The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.

</details>


### [49] [StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval](https://arxiv.org/abs/2601.20597)
*Shaokun Wang,Weili Guan,Jizhou Han,Jianlong Wu,Yupeng Hu,Liqiang Nie*

Main category: cs.CV

TL;DR: StructAlign uses a simplex ETF prior to align text and video representations for continual text-to-video retrieval, plus a Cross-modal Relation Preserving loss to curb intra-modal drift, yielding state-of-the-art continual retrieval performance.


<details>
  <summary>Details</summary>
Motivation: To mitigate catastrophic forgetting in continual multimodal retrieval by addressing both intra-modal feature drift and cross-modal misalignment through a unified geometric framework and relational supervision.

Method: Adopt a simplex Equiangular Tight Frame (ETF) geometry as a unified prior to reduce modality misalignment. Introduce a cross-modal ETF alignment loss that aligns text and video features to category-level ETF prototypes, encouraging a simplex ETF-like structure. Add a Cross-modal Relation Preserving loss that leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. Train jointly to address non-cooperative drift across modalities and intra-modal drift.

Result: Empirical evaluation on benchmark datasets shows StructAlign consistently outperforms state-of-the-art continual retrieval approaches.

Conclusion: StructAlign effectively mitigates both non-cooperative cross-modal drift and intra-modal drift in CTVR, alleviating catastrophic forgetting and improving retrieval performance.

Abstract: Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.

</details>


### [50] [Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?](https://arxiv.org/abs/2601.20598)
*Lakshman Balasubramanian*

Main category: cs.CV

TL;DR: Across 11 models and 9 datasets, supervised ReID models excel within their trained domain but fail to generalize cross-domain; language-aligned/foundation models show strong cross-domain robustness, suggesting richer, transferable representations improve generalization. Code and data are available for replication.


<details>
  <summary>Details</summary>
Motivation: Cross-domain generalization is a central challenge in Person Re-Identification. The study assesses the robustness of state-of-the-art models and the potential of foundation/language-aligned models to improve generalization.

Method: Empirical analysis comparing three training paradigms—supervised, self-supervised, and language-aligned models—across 11 models and 9 datasets. Evaluates cross-domain performance and specifically examines a foundation model (SigLIP2).

Result: Supervised models dominate within-domain accuracy but deteriorate in cross-domain settings. Language-aligned models exhibit notable cross-domain robustness despite not being explicitly trained for ReID. The foundation model SigLIP2 shows potential for generalization, though weaknesses remain for both supervised and foundational approaches.

Conclusion: Language-aligned/foundation representations can enhance cross-domain ReID generalization, mitigating some weaknesses of purely supervised approaches. Further work should address remaining gaps in cross-domain transfer and establish comprehensive benchmarks; the authors provide code/data resources for replication.

Abstract: Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.

</details>


### [51] [CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification](https://arxiv.org/abs/2601.20601)
*Zhuonan Wang,Wenjie Yan,Wenqiao Zhang,Xiaohui Song,Jian Ma,Ke Yao,Yibo Yu,Beng Chin Ooi*

Main category: cs.CV

TL;DR: CLEAR-Mamba is a hypernetwork-conditioned, reliability-aware extension of MedMamba for cross-domain ophthalmic angiography (FFA/ICGA). It introduces HaC for adaptive parameter generation and RaP for reliability-focused training, paired with a large-scale multi-modality dataset, achieving improved generalization and reliability in multi-disease classification.


<details>
  <summary>Details</summary>
Motivation: Overcome single-modality limitations, significant inter-device variability, and the need for high-confidence predictions in ophthalmic angiography by improving cross-domain adaptability and model reliability.

Method: Architectural: HaC—a hypernetwork-based adaptive conditioning layer that generates model parameters from input feature distributions. Training: RaP—a reliability-aware prediction scheme based on evidential uncertainty learning to emphasize low-confidence samples. Dataset: large-scale FFA/ICGA angiography data spanning multiple retinal diseases. Evaluation: comparisons against baselines including MedMamba across multi-disease classification and reliability metrics.

Result: CLEAR-Mamba consistently outperforms baselines (including the original MedMamba) across evaluated metrics, with notable gains in multi-disease classification performance and reliability-aware predictions.

Conclusion: The approach provides a generalizable and reliable solution for modality-specific medical image classification, aided by a substantial ophthalmic angiography dataset; useful for CAD workflows across FFA and ICGA modalities.

Abstract: Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.

</details>


### [52] [GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2601.20618)
*Shuguang Zhang,Junhong Lian,Guoxin Yu,Baoxun Xu,Xiang Ao*

Main category: cs.CV

TL;DR: GDCNet uses stable descriptive captions from Multimodal LLMs as anchors to compute cross-modal discrepancies (semantic, sentiment, visual-textual fidelity) and fuses these with visual/textual features via a gating mechanism for robust multimodal sarcasm detection, achieving state-of-the-art performance on MMSD2.0.


<details>
  <summary>Details</summary>
Motivation: Existing cross-modal misalignment cues may fail when image and text are loosely related; generative sarcasm cues from LLMs can be noisy. The authors propose stable semantic anchors via objective captions to measure true discrepancies and improve robustness.

Method: GDCNet: generate objective captions with Multimodal LLMs; compute semantic and sentiment discrepancies between the captions and the original text; measure visual-textual fidelity; fuse discrepancy features with visual and textual representations through a gated module to balance modality contributions.

Result: Extensive experiments on MSD benchmarks show superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.

Conclusion: Using anchor captions and discrepancy-based features effectively addresses limitations of prior cross-modal methods and LLM-generated cues, yielding robust multimodal sarcasm detection and SOTA performance.

Abstract: Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.

</details>


### [53] [OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks](https://arxiv.org/abs/2601.20650)
*Jing Wu,Daphne Barretto,Yiye Chen,Nicholas Gydé,Yanan Jian,Yuhang He,Vibhav Vineet*

Main category: cs.CV

TL;DR: OS-Marathon provides a benchmark of 242 long-horizon, repetitive tasks across two domains for evaluating state-of-the-art CUAs and introduces a few-shot demonstration method to learn the underlying workflow logic for scalable execution on larger datasets.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of evaluation benchmarks for long-horizon, repetitive professional tasks that are tedious for humans but amenable to automation via structured workflows.

Method: Assembles OS-Marathon with 242 tasks across two domains; proposes a cost-effective, few-shot demonstration protocol to teach agents the underlying workflow logic so they can generalize to larger unseen data collections.

Result: Extensive experiments reveal the inherent challenges of long-horizon tasks and demonstrate the effectiveness of the proposed few-shot workflow-learning approach in enabling agents to perform similar workflows on larger datasets.

Conclusion: OS-Marathon establishes a new benchmark for long-horizon repetitive tasks and shows that condensed, few-shot demonstrations can effectively transfer workflow knowledge to scalable CUAs, though challenges remain.

Abstract: Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.

</details>


### [54] [ProSkill: Segment-Level Skill Assessment in Procedural Videos](https://arxiv.org/abs/2601.20661)
*Michele Mazzamuto,Daniele Di Mauro,Gianpiero Francesca,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: ProSkill is the first large-scale benchmark for action-level skill assessment in procedural videos, combining absolute skill annotations with pairwise comparisons using a Swiss tournament and Elo rating, enabling benchmarking of both ranking-based and pairwise skill methods.


<details>
  <summary>Details</summary>
Motivation: There is a need for objective, scalable skill assessment in complex procedural tasks. Current datasets are sports-focused, small in scope, and provide limited labeling (binary or pairwise). ProSkill fills this gap by enabling absolute skill rankings in procedural contexts.

Method: A novel annotation protocol uses Swiss tournament-based pairwise comparisons to collect data, which are then aggregated into continuous global skill scores via an Elo-based rating system. The dataset provides both absolute skill annotations and pairwise comparisons.

Result: Benchmark experiments show suboptimal performance of state-of-the-art skill assessment algorithms on ProSkill, highlighting the dataset's difficulty and its value for driving advances in procedural skill evaluation.

Conclusion: ProSkill addresses a key gap in skill assessment research, offering a scalable, annotated benchmark for procedural tasks and enabling meaningful cross-method evaluation; data and code are publicly available.

Abstract: Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/

</details>


### [55] [bi-modal textual prompt learning for vision-language models in remote sensing](https://arxiv.org/abs/2601.20675)
*Pankhi Kashyap,Mainak Singha,Biplab Banerjee*

Main category: cs.CV

TL;DR: BiMoRS introduces a lightweight bi-modal prompt learning approach for remote sensing (RS) that keeps the CLIP backbone frozen. It leverages a frozen image-captioning model (BLIP-2) to generate textual summaries from RS images, tokenizes captions with BERT, fuses them with CLIP visual features, and uses a cross-attention module to produce contextualized prompts for downstream tasks. This method improves domain generalization on four RS datasets, achieving up to 2% average gains over strong baselines.


<details>
  <summary>Details</summary>
Motivation: RS imagery poses challenges for prompt-based VLM adaptation due to multi-label scenes, high intra-class variability, and diverse resolutions, which hinder generalization and semantic cue identification. A bi-modal prompt that leverages textual cues from captions could guide prompts toward more robust semantic cues without retraining the backbone.

Method: 1) Freeze CLIP image encoder and evaluation backbone. 2) Use a frozen captioning model (BLIP-2) to generate textual summaries from remote sensing images. 3) Tokenize captions with a BERT tokenizer. 4) Fuse textual features with high-level CLIP visual features. 5) Apply a lightweight cross-attention module to condition a learnable query prompt on the fused representation, producing contextualized prompts. 6) Use prompts with the frozen CLIP to perform downstream recognition tasks. 7) Evaluate on RS datasets under domain generalization settings.

Result: BiMoRS yields consistent improvements over strong baselines across four RS datasets and three DG tasks, with average gains up to about 2%. The approach preserves the CLIP backbone while leveraging textual cues to form contextualized prompts.

Conclusion: A compact bi-modal prompt learning framework, BiMoRS, effectively enhances domain generalization for RS tasks by extracting textual summaries via a captioning model, fusing them with visual features, and conditioning prompts through cross-attention, while maintaining a frozen CLIP backbone.

Abstract: Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.

</details>


### [56] [Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework](https://arxiv.org/abs/2601.20689)
*Xinyue Li,Zhichao Zhang,Zhiming Xu,Shubo Xu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: LEAF distills perceptual quality priors from an MLLM teacher into a lightweight regressor to calibrate MOS with minimal supervision, reducing annotation effort while preserving MOS-aligned correlations.


<details>
  <summary>Details</summary>
Motivation: The bottleneck in MLLM-based IQA is MOS scale calibration rather than the perceptual capacity of models. Large-scale models are costly to adapt and rely on substantial MOS annotations.

Method: A teacher model provides dense supervision via point-wise judgments and pair-wise preferences, with a reliability estimate. A lightweight student regressor learns the teacher's quality perception through joint distillation and is calibrated on a small MOS subset to align with human annotations.

Result: Experiments on user-generated and AI-generated IQA benchmarks show significant annotation reduction while maintaining strong MOS-aligned correlations.

Conclusion: A label-efficient IQA framework is practical under limited annotation budgets; distillation transfers perceptual priors effectively and enables MOS calibration with minimal human data.

Abstract: Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.

</details>


### [57] [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/abs/2601.20705)
*Zhuang Yu,Lei Shen,Jing Zhao,Shiliang Sun*

Main category: cs.CV

TL;DR: LEMON is a new lecture-based multimodal benchmark for STEM videos, targeting long-horizon reasoning and cross-modal understanding with 2,277 video segments and 4,181 QA pairs across 5 disciplines; it reveals large performance gaps for current MLLMs, including GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs underperform on long-form, knowledge-intensive, temporally structured educational content. A benchmark that captures video-audio-text integration, temporal structure, and pedagogical context is needed to drive progress.

Method: Construct LEMON from STEM lecture videos (5 disciplines, 29 courses), producing 4,181 QA pairs (3,413 MC, 768 open-ended) across six major tasks and twelve subtasks, with explicit temporal and pedagogical structure and multi-turn questioning; evaluate state-of-the-art MLLMs on these tasks.

Result: Significant performance gaps across tasks; even strong models like GPT-4o struggle with temporal reasoning and instructional prediction.

Conclusion: LEMON offers an extensible, challenging benchmark to advance multimodal perception, reasoning, and generation in long-form instructional content.

Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.

</details>


### [58] [Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification](https://arxiv.org/abs/2601.20742)
*Xin Jin,Jinming Liu,Yuntao Wei,Junyan Lin,Zhicheng Wang,Jianguo Huang,Xudong Yang,Yanxiao Liu,Wenjun Zeng*

Main category: cs.CV

TL;DR: A comprehensive overview unifying visual coding and vision token technology under an optimization-centric framework, proposing a bidirectional bridge, with experimental validation in task-oriented tokens, and a roadmap toward a standard-token technology akin to traditional codecs for intelligent tasks.


<details>
  <summary>Details</summary>
Motivation: Bridge longstanding visual coding and modern vision-token approaches, clarify how compression efficiency trades off with model performance, and anticipate the next generation of visual codecs/tokens.

Method: Proposes a unified optimization formulation that expresses both visual coding and vision token strategies; synthesizes insights from both, derives implications, and conducts experiments illustrating token developments in MLLMs, AIGC, and embodied AI.

Result: A unified view that connects two dominant approaches, offers theoretical and practical insights, and demonstrates promising potential of task-oriented token approaches in real-world multimodal systems.

Conclusion: A unified token-based paradigm could achieve high-efficiency compression across diverse intelligent tasks; a path toward standardizing a general token technology is envisioned, similar to codecs like H.264/265 for efficient intelligent-task processing.

Abstract: "Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.

</details>


### [59] [FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models](https://arxiv.org/abs/2601.20791)
*Haonan Zhong,Wei Song,Tingxu Han,Maurice Pagnucco,Jingling Xue,Yang Song*

Main category: cs.CV

TL;DR: Proposes FairT2V, a training-free debiasing framework for text-to-video that neutralizes encoder-induced gender bias via anchor-based spherical geodesic transformations on prompt embeddings, applying debiasing early to preserve temporal coherence; introduces a video-level fairness evaluation using VideoLLM reasoning plus human verification; shows reduced bias with minimal quality loss on Open-Sora.


<details>
  <summary>Details</summary>
Motivation: Text-to-video models exhibit encoder-induced gender bias due to implicit gender associations in pretrained text encoders. Finetuning or retraining is costly; a training-free solution is desirable to neutralize bias without harming semantics or temporal coherence.

Method: Bias analysis and quantification with a gender-leaning score. Debias prompt embeddings using anchor-based spherical geodesic transformations to neutralize gender associations while preserving semantics. Debiasing is applied only during early identity-forming steps via a dynamic denoising schedule to maintain temporal coherence. Proposes a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification.

Result: FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality on the Open-Sora model. The approach achieves debiasing without fine-tuning and preserves temporal coherence through a selective, early-stage application.

Conclusion: Training-free debiasing of T2V models is feasible and effective for mitigating encoder-induced gender bias. The paper provides a practical evaluation protocol and highlights the importance of early-stage debiasing to preserve temporal coherence while achieving fairness.

Abstract: Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.

</details>


### [60] [Open-Vocabulary Functional 3D Human-Scene Interaction Generation](https://arxiv.org/abs/2601.20835)
*Jie Liu,Yu Sun,Alpar Cseke,Yao Feng,Nicolas Heron,Michael J. Black,Yan Zhang*

Main category: cs.CV

TL;DR: A training-free framework (FunHSI) that achieves functionally correct 3D human-scene interactions from open prompts by reasoning about object functionality, reconstructing functional elements, and using vision-language guidance plus stage-wise pose optimization.


<details>
  <summary>Details</summary>
Motivation: Current methods lack explicit reasoning about object functionality and contact dynamics, leading to implausible interactions. There is a need for functionally aware, open-vocabulary interaction generation in 3D for embodied AI, robotics, and content creation.

Method: FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstructs their 3D geometry, and builds a contact graph to model interactions. It uses vision-language models to synthesize a human performing the task and estimates 3D body/hand poses, followed by stage-wise optimization to ensure physical plausibility and functional correctness, all without training.

Result: FunHSI yields more plausible functionally correct interactions (e.g., sitting on a sofa, adjusting room temperature) and demonstrates consistency across diverse indoor/outdoor scenes with physically plausible poses and functionally correct contact patterns.

Conclusion: FunHSI enables functionally correct 3D human-scene interactions from open prompts without training, enabling rich, functionally-aware interactions for varied environments and tasks.

Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

</details>


### [61] [A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion](https://arxiv.org/abs/2601.20847)
*Willams de Lima Costa,Thifany Ketuli Silva de Souza,Jonas Ferreira Silva,Carlos Gabriel Bezerra Pereira,Bruno Reis Vila Nova,Leonardo Silvino Brito,Rafael Raider Leoni,Juliano Silva Filho,Valter Ferreira,Sibele Miguel Soares Neto,Samantha Uehara,Daniel Giacometti Amaral,João Marcelo Teixeira,Veronica Teichrieb,Cristiano Coelho de Araújo*

Main category: cs.CV

TL;DR: Multimodal RGB-IMU road surface classification with cross-attention and adaptive gating; introduces ROAD dataset (real multimodal, vision-only, synthetic) to improve robustness and generalization; shows gains on PVS and ROAD benchmarks and resilience in adverse conditions.


<details>
  <summary>Details</summary>
Motivation: Existing road surface classification methods struggle to generalize due to single-modality sensing and datasets lacking environmental diversity. A multimodal, adaptive framework can improve robustness under domain shifts.

Method: A lightweight bidirectional cross-attention module fuses RGB and inertial data, followed by an adaptive gating layer that reweights modality contributions under domain shifts. The ROAD dataset comprises three subsets: real-world synchronized RGB-IMU with diverse conditions, a large vision-only set for robustness under adverse illumination and heterogeneous capture setups, and a synthetic subset for studying out-of-distribution generalization.

Result: Empirical improvements: +1.4 pp on the PVS benchmark and +11.6 pp on the multimodal ROAD subset, with consistently higher F1-scores on minority classes. Demonstrates stable performance under nighttime, heavy rain, and surface transitions.

Conclusion: Integrating affordable camera and IMU sensors with multimodal attention yields a scalable, robust road surface understanding framework. ROAD provides a diverse benchmark to assess generalization in variable environments and under cost constraints.

Abstract: Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.

</details>


### [62] [FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models](https://arxiv.org/abs/2601.20857)
*Hongyu Zhou,Zisen Shao,Sheng Miao,Pan Wang,Dongfeng Bai,Bingbing Liu,Yiyi Liao*

Main category: cs.CV

TL;DR: A fine-tuning-free method, FreeFix, enhances extrapolated rendering for NeRF/3D Gaussian Splatting using pretrained image diffusion models via an interleaved 2D-3D refinement and a per-pixel confidence mask to selectively improve uncertain regions, achieving strong multi-frame consistency and competitive fidelity without overfitting.


<details>
  <summary>Details</summary>
Motivation: Current neural rendering methods require dense inputs and degrade at extrapolated views. Diffusion-guided supervision resolves fidelity vs generalization trade-offs, but fine-tuning diffusion models risks overfitting and fine-tuning-free methods may underperform on fidelity. A solution that leverages diffusion models without tuning could maintain generalization while improving extrapolation fidelity.

Method: Introduce FreeFix with an interleaved 2D-3D refinement strategy that uses pretrained image diffusion models for refinement without relying on costly video diffusion models. Develop a per-pixel confidence mask to identify uncertain regions for targeted refinement, enabling selective improvement during rendering.

Result: Experiments across multiple datasets show improved multi-frame consistency and performance comparable to or surpassing fine-tuning-based methods, while preserving strong generalization.

Conclusion: FreeFix demonstrates that fine-tuning-free diffusion-guided refinement can push fidelity in extrapolated views without sacrificing generalization, offering a cost-effective alternative to diffusion model fine-tuning and enabling reinforced consistency in neural rendering.

Abstract: Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [NeuroAI and Beyond](https://arxiv.org/abs/2601.19955)
*Jean-Marc Fellous,Gert Cauwenberghs,Cornelia Fermüller,Yulia Sandamisrkaya,Terrence Sejnowski*

Main category: cs.AI

TL;DR: NeuroAI is proposed as a neuroscience-informed AI paradigm emerging from a 2025 workshop, outlining synergistic areas (embodiment, language and communication, robotics, learning in humans and machines, neuromorphic engineering), potential future avenues, and SWOT analyses; argues for formal development of NeuroAI.


<details>
  <summary>Details</summary>
Motivation: To bridge neuroscience and AI more tightly, leveraging neural principles to improve AI efficiency and generalization, while using AI as a tool to test and understand biological neural computations.

Method: Synthesis of workshop discussions, a review of progress in the highlighted subareas, inclusion of personal statements from leading researchers, and two SWOT analyses by researchers and trainees appended to the report.

Result: Provides a stock-taking map of where neuroscience–AI integration stands, identifies promising future avenues, and advocates for the creation of NeuroAI as a formal interdisciplinary field.

Conclusion: NeuroAI has the potential to expand AI capabilities and simultaneously advance neuroscience; calls for coordinated, interdisciplinary efforts to advance the field, with critical evaluation via SWOT analyses to address risks and opportunities.

Abstract: Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.

</details>


### [64] [Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning](https://arxiv.org/abs/2601.20014)
*Shuhui Qu*

Main category: cs.AI

TL;DR: SQ-BCP tackles planning under partial observability by explicitly tracking precondition status and validating plans with a categorical verifier, combining self-queries and bridging actions in bidirectional search to ensure hard-constraint compatibility during inference-time planning.


<details>
  <summary>Details</summary>
Motivation: To reduce resource-violations and hallucinations when LLM-driven plans are induced under partial observability, by enforcing explicit precondition handling and formal verification during planning.

Method: Introduce Self-Querying Bidirectional Categorical Planning (SQ-BCP) that annotates preconditions with Sat/Viol/Unk, uses targeted self-queries or bridging actions to resolve Unk, performs bidirectional search, and uses a pullback-based categorical verifier as a certificate of goal compatibility. Distance-based scores are used only for ranking/pruning. Theoretical guarantees: if verifier succeeds and hard constraints pass deterministic checks, plans are goal-compatible; under bounded branching/depth, an accepting plan is found if one exists.

Result: Empirical evaluation on WikiHow and RecipeNLG with withheld preconditions shows SQ-BCP reduces resource-violation rates to 14.9% and 5.8% respectively, compared with 26.0% and 15.7% by the best baseline, while maintaining competitive reference quality.

Conclusion: SQ-BCP provides a principled planning framework under partial observability that ensures compatibility with hard constraints via a categorical verifier and targeted self-query mechanisms, yielding substantial reductions in resource violations without sacrificing plan quality.

Abstract: Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\texttt{Sat}/\texttt{Viol}/\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \textbf{14.9\%} and \textbf{5.8\%} (vs.\ \textbf{26.0\%} and \textbf{15.7\%} for the best baseline), while maintaining competitive reference quality.

</details>


### [65] [Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints](https://arxiv.org/abs/2601.20021)
*Shuhui Qu*

Main category: cs.AI

TL;DR: FCP introduces fuzzy, category-theoretic planning by annotating actions with degrees of applicability, composing plan quality with a Lukasiewicz t-norm, and preserving crisp executability checks via pullbacks; grounded through an LLM and evaluated on PDDL3 and RecipeNLG-Subs, achieving higher success and fewer violations while staying competitive with standard planners.


<details>
  <summary>Details</summary>
Motivation: Natural-language planning often involves vague predicates with graded satisfaction. Existing categorical planners mix crisp applicability with thresholding, losing nuanced quality signals and hindering multi-step degradation tracking. There is a need to integrate graded applicability, language grounding, and structural guarantees.

Method: Annotate each morphism with a degree in [0,1]; compose plan quality via a Lukasiewicz t-norm; maintain crisp pullback-based executability checks. Ground graded applicability using an LLM with k-sample median aggregation; enable meeting-in-the-middle search via residuum-based backward requirements.

Result: Empirical evaluation on public PDDL3 preference/oversubscription benchmarks and RecipeNLG-Subs shows FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct baselines; performance remains competitive with classical PDDL3 planners.

Conclusion: FCP provides a graded, compositional planning framework that preserves feasibility checks and ties language-based graded applicability to formal planning, improving NL planning under vague predicates while maintaining competitive performance.

Abstract: Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.

</details>


### [66] [Insight Agents: An LLM-Based Multi-Agent System for Data Insights](https://arxiv.org/abs/2601.20048)
*Jincheng Bai,Zhenyu Zhang,Jennifer Zhang,Zhihuai Zhu*

Main category: cs.AI

TL;DR: Proposes Insight Agents (IA), a hierarchical multi-agent LLM-backed system for e-commerce data insights, achieving 90% accuracy and sub-15s latency in Amazon US deployment.


<details>
  <summary>Details</summary>
Motivation: E-commerce sellers struggle to discover and utilize available programs/tools and to understand and leverage rich data from various tools; IA aims to provide personalized data and business insights to accelerate decision-making and act as a force multiplier.

Method: Plan-and-execute end-to-end agentic architecture with a hierarchical two-worker, one-manager multi-agent system. Manager uses out-of-domain detection via a lightweight encoder-decoder and routing via a BERT-based classifier. Workers perform data presentation and insight generation with strategic API data-model planning, granularity decomposition of queries, and dynamic domain knowledge injection.

Result: IA has been launched for Amazon sellers in the US with high accuracy (90% in human evaluation) and P90 latency below 15 seconds.

Conclusion: The paper presents a novel, effective LLM-backed end-to-end agentic system designed for comprehensive coverage, high accuracy, and low latency, showing potential as a scalable tool for seller data insights and faster decision-making.

Abstract: Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.

</details>


### [67] [Should I Have Expressed a Different Intent? Counterfactual Generation for LLM-Based Autonomous Control](https://arxiv.org/abs/2601.20090)
*Amirmohammad Farzaneh,Salvatore D'Oro,Osvaldo Simeone*

Main category: cs.AI

TL;DR: A conformal counterfactual generation framework for LLM-driven control using SCM and test-time scaling to produce calibrated counterfactual outcome sets with high coverage, evaluated on wireless network control and outperforming naive re-execution baselines.


<details>
  <summary>Details</summary>
Motivation: Provide reliable, calibrated counterfactual reasoning for agentic LLM controllers to reason about alternate intents and outcomes after observing results, addressing uncertainty in plan execution.

Method: Model the closed-loop user-agent-environment as a structural causal model; use probabilistic abduction to generate multiple counterfactuals at test time; offline calibration to enable conformal prediction and produce counterfactual sets with high probability coverage; call this CCG.

Result: Empirical evaluation on a wireless network control use case shows significant gains over naive re-execution baselines, in terms of reliability/coverage or efficiency.

Conclusion: CCG offers guaranteed-calibration counterfactual reasoning for LLM-driven control, enabling robust evaluation of alternative intents and facilitating safer, more reliable agent autonomy.

Abstract: Large language model (LLM)-powered agents can translate high-level user intents into plans and actions in an environment. Yet after observing an outcome, users may wonder: What if I had phrased my intent differently? We introduce a framework that enables such counterfactual reasoning in agentic LLM-driven control scenarios, while providing formal reliability guarantees. Our approach models the closed-loop interaction between a user, an LLM-based agent, and an environment as a structural causal model (SCM), and leverages test-time scaling to generate multiple candidate counterfactual outcomes via probabilistic abduction. Through an offline calibration phase, the proposed conformal counterfactual generation (CCG) yields sets of counterfactual outcomes that are guaranteed to contain the true counterfactual outcome with high probability. We showcase the performance of CCG on a wireless network control use case, demonstrating significant advantages compared to naive re-execution baselines.

</details>


### [68] [Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis](https://arxiv.org/abs/2601.20206)
*Zixuan Xiao,Chunguang Hu,Jun Ma*

Main category: cs.AI

TL;DR: A multi-modal LLM agent framework for urban park development monitoring with data alignment and a domain-specific toolkit to enable robust multi-modal analysis, outperforming vanilla GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Urban park development monitoring requires high-level intelligent, multi-modal analysis; traditional remote sensing change detection is limited and lacks flexibility for diverse urban planning scenarios.

Method: Proposes a multi-modal LLM agent framework with horizontal and vertical data alignment for consistent multi-modal data tracking, and a domain-specific toolkit to mitigate LLM hallucinations; enables robust multi-modal information fusion and analysis.

Result: Claims robust multi-modal fusion and analysis and scalability, outperforming vanilla GPT-4o and other agents; however the abstract does not provide empirical results.

Conclusion: Provides a framework that addresses the demand for flexible, trustworthy urban planning analysis by integrating alignment mechanisms and domain-specific tooling to enhance multi-modal monitoring of urban parks.

Abstract: As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.

</details>


### [69] [Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2601.20221)
*Hang Zhang,Ruheng Wang,Yuelyu Ji,Mingu Kwak,Xizhi Wu,Chenyu Li,Li Zhang,Wenqi Shi,Yifan Peng,Yanshan Wang*

Main category: cs.AI

TL;DR: Agentic medical reasoning verifier that iteratively retrieves external medical evidence to justify reasoning, achieving higher accuracy and lower sampling cost.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of explicit justification in scalar reward models and enable adaptive knowledge access during verification for medical reasoning.

Method: Train verifiers to perform tool-augmented, iterative retrieval during evaluation using reinforcement learning with trace-level supervision and an adaptive curriculum that shifts the training data distribution.

Result: On four medical reasoning benchmarks, MedQA accuracy improves by 23.5% and MedXpertQA by 32.0% relative to the base generator; sampling budget reduced by 8x compared to prior reward-model baselines.

Conclusion: Grounding verification in dynamically retrieved evidence offers a principled path to more reliable medical reasoning systems.

Abstract: Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\method$ demonstrates an $\mathbf{8\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.

</details>


### [70] [Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models](https://arxiv.org/abs/2601.20305)
*Zhenchen Tang,Songlin Yang,Zichuan Wang,Bo Peng,Yang Li,Beibei Dong,Jing Dong*

Main category: cs.AI

TL;DR: Introduces Endogenous Reprompting via SEER to bridge a cognitive gap in Unified Multimodal Models by turning latent evaluation into explicit self-descriptor prompts during generation. Uses a two-stage endogenous loop (RLVR and RLMT) trained on a compact proxy task with 300 samples (Visual Instruction Elaboration). Claims improvements in evaluation accuracy, reprompting efficiency, and generation quality while preserving multimodal capabilities.


<details>
  <summary>Details</summary>
Motivation: UMMs excel at understanding but often fail to use that understanding to guide generation; this cognitive gap is the lack of explicit generative reasoning. The work proposes endogenous reprompting to make the model generate self-aligned descriptors during generation, enabling better alignment between understanding and output.

Method: Two-stage endogenous loop: (1) RLVR (Reinforcement Learning with Verifiable Rewards) uses curriculum learning to elicit a high-fidelity endogenous reward signal from the model's latent evaluation ability; (2) RLMT (Reinforcement Learning with Model-rewarded Thinking) uses that reward to optimize the generative reasoning policy. Training relies on 300 samples from a compact proxy task, Visual Instruction Elaboration, to instantiate the loop. The mechanism converts internal understanding into explicit self-descriptors (reprompting) during generation.

Result: SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, while maintaining general multimodal capabilities.

Conclusion: Endogenous prompts and the SEER RL framework can effectively bridge the cognitive gap by transforming latent evaluation into an explicit, self-generated reasoning process, achieving stronger generation guidance without sacrificing multimodal performance. Potential limitations include dependence on the proxy task quality and the reward signal reliability, as well as computational overhead and generalization across diverse multimodal tasks.

Abstract: Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model's understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model's latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities.

</details>


### [71] [ECG-Agent: On-Device Tool-Calling Agent for ECG Multi-Turn Dialogue](https://arxiv.org/abs/2601.20323)
*Hyunseung Chung,Jungwoo Oh,Daeun Kyung,Jiho Kim,Yeonsu Kwon,Min-Gyu Kim,Edward Choi*

Main category: cs.AI

TL;DR: Introduces ECG-Agent, an LLM-based tool-calling agent for multi-turn ECG dialogue, and ECG-MTD, a dataset of realistic user–assistant ECG dialogues. Demonstrates on-device ECG agents achieving competitive performance with larger models, improving response accuracy and tool-calling in realistic, multi-turn scenarios.


<details>
  <summary>Details</summary>
Motivation: To overcome seven gaps in existing Multimodal LLMs for ECG: (1) lack of multi-turn conversational ability, (2) on-device efficiency constraints, (3) inadequate understanding of ECG measurements (e.g., PQRST intervals), and (4) limited practical deployment in real-world settings.

Method: Develop a family of ECG-Agents of varying sizes with tool-calling capabilities; construct ECG-MTD containing diverse lead configurations and realistic user–assistant multi-turn dialogues; evaluate agents on response accuracy, tool-calling reliability, and hallucination propensity across on-device and larger variants.

Result: ECG-Agents outperform baseline ECG-LLMs in response accuracy. On-device agents achieve comparable performance to larger agents across multiple evaluation facets, including response accuracy, tool-calling ability, and hallucination control.

Conclusion: LLM-based tool-calling architectures enable effective, resource-efficient multi-turn ECG dialogue and on-device deployment, with demonstrated viability for real-world ECG tasks such as classification, report generation, and QA.

Abstract: Recent advances in Multimodal Large Language Models have rapidly expanded to electrocardiograms, focusing on classification, report generation, and single-turn QA tasks. However, these models fall short in real-world scenarios, lacking multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements such as the PQRST intervals. To address these limitations, we introduce ECG-Agent, the first LLM-based tool-calling agent for multi-turn ECG dialogue. To facilitate its development and evaluation, we also present ECG-Multi-Turn-Dialogue (ECG-MTD) dataset, a collection of realistic user-assistant multi-turn dialogues for diverse ECG lead configurations. We develop ECG-Agents in various sizes, from on-device capable to larger agents. Experimental results show that ECG-Agents outperform baseline ECG-LLMs in response accuracy. Furthermore, on-device agents achieve comparable performance to larger agents in various evaluations that assess response accuracy, tool-calling ability, and hallucinations, demonstrating their viability for real-world applications.

</details>


### [72] [AMA: Adaptive Memory via Multi-Agent Collaboration](https://arxiv.org/abs/2601.20352)
*Weiquan Huang,Zixuan Wang,Hehai Lin,Sudong Wang,Bo Xu,Qian Li,Beier Zhu,Linyi Yang,Chengwei Qin*

Main category: cs.AI

TL;DR: AMA introduces a hierarchical, multi-agent memory system for LLMs with adaptive retrieval granularity, improving retrieval accuracy and long-term memory consistency while greatly reducing token usage.


<details>
  <summary>Details</summary>
Motivation: Existing memory approaches suffer from rigid retrieval granularity, accumulation-heavy maintenance, and coarse updates, causing misalignment with task demands and accumulation of inconsistencies. There is a need for memory that adapts to task complexity and maintains long-term consistency.

Method: AMA uses a Constructor and Retriever to build multi-granularity memory and route queries adaptively. The Judge verifies relevance and consistency of retrieved content and invokes iterative retrieval if needed. The Refresher enforces memory consistency by performing targeted updates or removing outdated entries.

Result: On challenging long-context benchmarks, AMA outperforms state-of-the-art baselines and reduces token consumption by about 80% compared with full-context methods, demonstrating improved retrieval precision and long-term memory consistency.

Conclusion: AMA demonstrates that coordinated multi-agent memory across granularities can achieve efficient, accurate, and consistent long-term reasoning, with strong empirical gains and potential for broader adoption and future enhancements.

Abstract: The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.

</details>


### [73] [Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution](https://arxiv.org/abs/2601.20379)
*Zhengbo Jiao,Hongyu Xian,Qinglong Wang,Yunpu Ma,Zhebo Wang,Zifan Zhang,Dezhang Kong,Meng Han*

Main category: cs.AI

TL;DR: Policy of Thoughts (PoT) reframes reasoning as an online, within-instance optimization that learns from failed attempts. It uses diverse candidate generation and GRPO to update a transient LoRA adapter based on execution feedback, enabling dynamic, instance-specific refinement of reasoning priors. Demonstrates strong performance gains for a 4B model on LiveCodeBench (~49.7% accuracy), surpassing larger models.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from unstable long-horizon reasoning due to a frozen policy; current test-time scaling treats feedback as external without internalizing it. Inspired by Popper’s conjectures and refutations, the work argues for real-time policy evolution through learning from failures to improve reasoning.

Method: Generate diverse candidate solutions via an efficient exploration mechanism; apply Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter using execution feedback; a closed-loop, instance-specific refinement of the model's reasoning priors.

Result: PoT yields substantial performance gains: a 4B model attains 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50x smaller.

Conclusion: A closed-loop, within-instance optimization framework effectively improves long-horizon reasoning in LLMs by continuously updating a reasoning policy in response to feedback, enabling smaller models to compete with larger ones.

Abstract: Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of "conjectures and refutations," we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.

</details>


### [74] [OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution](https://arxiv.org/abs/2601.20380)
*Le Zhang,Yixiong Xiao,Xinjiang Lu,Jingjia Cao,Yusai Zhao,Jingbo Zhou,Lang An,Zikan Feng,Wanxiang Sha,Yu Shi,Congxi Xiao,Jian Xiong,Yankai Zhang,Hua Wu,Haifeng Wang*

Main category: cs.AI

TL;DR: OmegaUse is a general-purpose GUI agent for autonomous task execution across mobile and desktop platforms, built with a novel data synthesis workflow and a two-stage training regimen on a Mixture-of-Experts backbone, achieving competitive and state-of-the-art results on GUI benchmarks and cross-terminal OS benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address core bottlenecks in GUI agent development: high-quality data and effective training methods, enabling robust cross-platform (phone and computer) task automation.

Method: Data construction uses curated open-source datasets and an automated synthesis framework combining bottom-up autonomous exploration with top-down taxonomy-guided generation. Training employs a two-stage strategy: Supervised Fine-Tuning (SFT) to establish interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. The model uses a Mixture-of-Experts backbone. Evaluation is conducted via OS-Nav, with ChiM-Nav (Android mobile) and Ubu-Nav (Ubuntu desktop) as benchmarks.

Result: OmegaUse achieves state-of-the-art 96.3% on ScreenSpot-V2 and 79.1% step success on AndroidControl. In OS-Nav, it attains 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav, indicating strong cross-terminal GUI agent performance.

Conclusion: The work demonstrates that a carefully engineered data pipeline and a decoupled training paradigm, underpinned by a MoE backbone, yield strong cross-platform GUI agent capabilities, validated by competitive results on established GUI benchmarks and OS-specific tasks.

Abstract: Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.

</details>


### [75] [CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning](https://arxiv.org/abs/2601.20467)
*Zhenxuan Fan,Jie Cao,Yang Dai,Zheqi Lv,Wenqiao Zhang,Zhongle Xie,Peng LU,Beng Chin Ooi*

Main category: cs.AI

TL;DR: CtrlCoT introduces dual-granularity CoT compression combining semantic abstraction and token-level pruning to reduce tokens while preserving reasoning accuracy; achieves significant token savings and accuracy gains on MATH-500 with Qwen2.5-7B-Instruct; code released.


<details>
  <summary>Details</summary>
Motivation: CoT prompts yield high latency and memory cost due to verbose traces. Existing methods are either semantically conservative (shortening CoTs) or aggressively prune tokens (risking task cues). Combining semantic-level abstraction and token pruning is difficult due to sequential dependency, task-agnostic pruning, and distribution mismatch.

Method: Three components: (1) Hierarchical Reasoning Abstraction—generate CoTs at multiple semantic granularities; (2) Logic-Preserving Distillation—train a logic-aware pruner to retain indispensable cues (e.g., numbers and operators) across pruning ratios; (3) Distribution-Alignment Generation—align compressed traces with fluent inference-time reasoning styles to avoid fragmentation.

Result: On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT achieves 30.7% token reduction and 7.6 percentage-point higher accuracy than the strongest baseline.

Conclusion: CtrlCoT effectively harmonizes semantic abstraction and token pruning to achieve more efficient and reliable CoT reasoning; code is publicly available.

Abstract: Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.

</details>


### [76] [Normative Equivalence in human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups](https://arxiv.org/abs/2601.20487)
*Nico Mutzner,Taha Yasseri,Heiko Rauhut*

Main category: cs.AI

TL;DR: In a four-player online Public Goods Game, three humans and one bot (framed as either human or AI and following one of three strategies) showed cooperative behavior driven by reciprocity and inertia, with no difference between human and AI labels. This yields normative equivalence: cooperative norms generalize to mixed human-AI groups and are robust to partner identity, extending to follow-up tasks like Prisoner’s Dilemma.


<details>
  <summary>Details</summary>
Motivation: Address how AI agents influence the emergence and maintenance of cooperative social norms in small groups, beyond existing dyadic studies, by testing whether AI labeling and agent strategy affect group cooperation and normative perceptions.

Method: Repeated four-player Public Goods Game in an online experiment with 236 participants. Each group had three human players and one bot. Bots were framed as either human or AI and implemented one of three strategies: unconditional cooperation, conditional cooperation, or free-riding. Measures included cooperation rates, reciprocal dynamics, behavioral inertia, norm persistence (via a follow-up Prisoner's Dilemma), and participants’ normative perceptions. Random assignment of bot framing and strategy.

Result: Cooperation was driven by reciprocal group dynamics and behavioral inertia, with identical effects across conditions. No significant differences in cooperation between human vs. AI labeling. No differences in norm persistence in the follow-up Prisoner’s Dilemma or in perceived norms. Participants’ behavior followed the same normative logic regardless of partner identity.

Conclusion: Normative mechanisms that sustain cooperation extend to mixed human-AI groups; cooperative norms appear flexible enough to include artificial agents, supporting normative equivalence between human-only and mixed groups.

Abstract: The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.

</details>


### [77] [PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs](https://arxiv.org/abs/2601.20539)
*Oguzhan Gungordu,Siheng Xiong,Faramarz Fekri*

Main category: cs.AI

TL;DR: A multi-agent, stateful planning framework (PathWise) for automated heuristic design in COPs, using an entailment graph and world-model reasoning to speed up convergence and generalize across LLMs.


<details>
  <summary>Details</summary>
Motivation: Current automated heuristic design (AHD) methods rely on fixed evolutionary rules and static prompts, leading to myopic search, redundant evaluations, and poor reuse of information across generations.

Method: Introduce PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs. It uses an entailment graph as a compact memory of the search trajectory. Three agents operate sequentially: a policy agent plans evolutionary actions, a world-model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps.

Result: Empirical evaluation across diverse COPs shows PathWise converges faster to higher-quality heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.

Conclusion: PathWise shifts AHD from trial-and-error evolution to state-aware planning through reasoning, enabling faster convergence, better generalization, and scalability in heuristic design for COPs.

Abstract: Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.

</details>


### [78] [Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function](https://arxiv.org/abs/2601.20554)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: ICVaR-based risk-sensitive planning for POMDPs; provides finite-time policy evaluation guarantees independent of action-space size; extends Sparse Sampling, PFT-DPW, and POMCPOW to optimize ICVaR instead of expected return; introduces an alpha risk parameter; empirical results show reduced tail risk compared to risk-neutral planners.


<details>
  <summary>Details</summary>
Motivation: Standard planning under partial observability optimizes expected return, but omits tail-risk considerations. Dynamic risk measures like Iterated Conditional Value-at-R risk (ICVaR) enable risk-averse planning with theoretical guarantees. The work aims to provide scalable, finite-time techniques and to adapt popular online planners to ICVaR.

Method: Develops a finite-time policy evaluation algorithm for ICVaR whose performance bounds do not depend on action-space cardinality. Extends three online planning algorithms—Sparse Sampling, PFT-DPW, and POMCPOW—to optimize the ICVaR objective. Introduces risk parameter α (α=1 recovers the risk-neutral objective; α<1 induces risk aversion). For ICVaR Sparse Sampling, derives finite-time guarantees and an ICVaR-tailored exploration strategy.

Result: The modified planners optimize the ICVaR objective and demonstrate lower tail risk in benchmark POMDP domains compared to their risk-neutral counterparts. α<1 yields increased risk aversion. Theoretical guarantees accompany the practical planning algorithms.

Conclusion: ICVaR-based planning provides a scalable framework for risk-sensitive decision making under partial observability, combining finite-time guarantees with practical performance improvements in tail risk; it can augment existing online planning pipelines with risk-aware behavior.

Abstract: We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $α$, where $α= 1$ recovers standard expectation-based planning and $α< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.

</details>


### [79] [Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies](https://arxiv.org/abs/2601.20604)
*Gray Cox*

Main category: cs.AI

TL;DR: A four-role multi-model dialogue framework to empirically test AI alignment strategies inspired by Peace Studies; uses Claude, Gemini, GPT-4o across six conditions; 72 turns; shows capacity for dialogical reasoning and emergent VCW insights; highlights model-specific concerns; offers replicable stress-testing method with limitations.


<details>
  <summary>Details</summary>
Motivation: Reframe AI alignment as a relational problem solvable via dialogical reasoning; provide empirically testable framework to evaluate alignment proposals before deployment; leverage peace negotiation concepts to expose trade-offs and governance issues.

Method: Assign four roles (Proposer, Responder, Monitor, Translator) to multiple LLMs across six conditions; conduct structured dialogue totaling 72 turns and ~576k chars; analyze cross-architecture patterns and emergent insights; use Claude, Gemini, GPT-4o; identify foregrounded concerns per model.

Result: AI systems can engage with Peace Studies concepts, surface complementary objections, and generate novel syntheses like VCW as a transitional framework; cross-architecture patterns show role-specific focal points (verification, bias/scalability, implementation barriers).

Conclusion: Provides replicable stress-testing methods for alignment proposals; offers preliminary evidence of dia-logical reasoning capacity in LLMs; limitations include emphasis on process over foundational AI nature and limited generalizability; future work via human-AI hybrid protocols and extended dialogue studies.

Abstract: This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.
  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.
  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of "VCW as transitional framework." Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.
  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.

</details>


### [80] [Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation](https://arxiv.org/abs/2601.20614)
*Yanqi Dai,Yuxiang Ji,Xiao Zhang,Yong Wang,Xiangxiang Chu,Zhiwu Lu*

Main category: cs.AI

TL;DR: MathForge introduces DGPO and MQR to target harder questions in RL-based mathematical reasoning, addressing update imbalance in GRPO and limited intrinsic difficulty in data augmentation; shows strong gains across math reasoning tasks; code/data released.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR with GRPO tends to underutilize harder questions due to implicit update imbalance, and augmentation mainly rephrases questions without increasing intrinsic difficulty. This limits capability growth in mathematical reasoning.

Method: DGPO (difficulty-balanced group policy optimization) uses difficulty-aware group advantage estimation and question-level weighting to emphasize harder questions. MQR (multi-aspect question reformulation) reformulates questions across multiple aspects to increase intrinsic difficulty while preserving the gold answer. MathForge creates a loop: MQR expands the data frontier, DGPO learns from augmented data.

Result: Extensive experiments show MathForge significantly outperforms existing methods on various mathematical reasoning tasks.

Conclusion: MathForge synergistically combines harder-question data augmentation with a difficulty-aware learning algorithm, yielding improved performance and providing code and augmented data for reproducibility.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.

</details>


### [81] [MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents](https://arxiv.org/abs/2601.20831)
*Vishnu Sashank Dorbala,Dinesh Manocha*

Main category: cs.AI

TL;DR: MemCtrl introduces a trainable memory head μ for MLLMs to prune memory online in embodied agents, yielding substantial improvements on EmbodiedBench (avg ~16%, some subsets >20%) and showing effectiveness for long/complex instructions.


<details>
  <summary>Details</summary>
Motivation: Embodied agents face strict memory and compute constraints; large context windows require memory management; existing retrieval-based memory systems treat memory as offline storage, which is ill-suited for online decision making.

Method: MemCtrl augments Multimodal LLMs with a trainable memory head μ that gates which observations or reflections are retained, updated, or discarded during exploration. μ is trained in two ways: offline via an expert and online via reinforcement learning. Evaluation uses two low-performing MLLMs augmented with MemCtrl on multiple EmbodiedBench subsets.

Result: μ-augmented MLLMs show significant improvement in embodied task completion: about 16% average improvement, over 20% on specific instruction subsets. Qualitative analysis indicates μ yields memory fragments that support long and complex instruction types.

Conclusion: Online memory pruning with a trainable gate in MLLMs effectively improves embodied agent performance under memory constraints, suggesting μ as a viable memory management module and motivating further exploration of online, adaptive memory pruning.

Abstract: Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.

</details>


### [82] [Investigating the Development of Task-Oriented Communication in Vision-Language Models](https://arxiv.org/abs/2601.20641)
*Boaz Carmeli,Orr Paradise,Shafi Goldwasser,Yonatan Belinkov,Ron Meir*

Main category: cs.AI

TL;DR: LLM-based agents can develop task-oriented communication protocols in referential games that are efficient for task goals yet can be covert and hard to interpret; this duality presents both promise and risk, with referential games serving as a useful testbed for further study.


<details>
  <summary>Details</summary>
Motivation: To determine whether LLM-based agents can produce task-oriented communication protocols that depart from natural language, and to assess their efficiency and interpretability in collaborative reasoning tasks.

Method: A referential-game framework using vision-language model (VLM) agents who communicate to solve tasks, enabling controlled, measurable comparison across language variants and protocols.

Result: VLMs can develop effective, task-adapted communication patterns and, in some cases, covert protocols that are difficult for humans and external agents to interpret. Additionally, spontaneous coordination arises between similar models without explicit shared protocols.

Conclusion: Task-oriented communication holds promise for efficient collaboration but raises transparency and control concerns. Referential games are a valuable, tractable testbed for studying these phenomena and guiding future work.

Abstract: We investigate whether \emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.

</details>


### [83] [Implementing Metric Temporal Answer Set Programming](https://arxiv.org/abs/2601.20735)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Susana Hahn,Javier Romero,Torsten Schaub*

Main category: cs.AI

TL;DR: Proposes a scalable Metric ASP approach by externalizing timing via difference constraints, decoupling time granularity from reasoning.


<details>
  <summary>Details</summary>
Motivation: Expressing quantitative temporal constraints (durations, deadlines) in ASP typically suffers from grounding bottlenecks when time is fine-grained; a scalable approach is needed.

Method: Extend ASP with difference constraints (a simplified linear constraint form) to handle time externally, thereby decoupling metric ASP from time granularity.

Result: The approach achieves time-precision agnosticism: the solution is effectively unaffected by the granularity of time and the grounding bottleneck is mitigated, yielding scalable metric reasoning.

Conclusion: Externalizing time with difference constraints enables scalable metric ASP that remains robust to time precision, mitigating grounding-related scalability issues.

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.

</details>


### [84] [Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)](https://arxiv.org/abs/2601.20843)
*Saurav Prateek*

Main category: cs.AI

TL;DR: Deep Researcher uses sequential plan refinement via reflection and a candidates crossover search to outperform parallel scaling on a 100-task DeepResearch Bench, achieving 46.21 with Gemini 2.5 Pro, surpassing several competitors and supporting sequential scaling's superiority.


<details>
  <summary>Details</summary>
Motivation: Mitigate knowledge silos and inefficiencies of parallel scaling in automated research agents by maintaining a centralized Global Research Context and enabling runtime plan adaptation.

Method: Introduce Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm; maintain Global Research Context; perform multiple LLM runs with varied prompts/parameters; synthesize findings; produce One Shot Report Generation; evaluate on DeepResearch Bench.

Result: Achieves 46.21 score, surpassing Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher, Grok Deeper Search; slightly better than previous Static DRA.

Conclusion: Sequential scaling with reflection and crossover exploration yields superior performance and higher fact density for long-form research reports; results support the proposed paradigm; further validation on diverse tasks recommended.

Abstract: This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from siloed knowledge. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density. Powered by the Gemini 2.5 Pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher and Grok Deeper Search present on the DeepResearch Bench actively running leaderboard. This performance marginally exceeds our previous work, Static DRA, and reinforces the finding that sequential scaling consistently outperforms the parallel self consistency paradigm.

</details>


### [85] [SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models](https://arxiv.org/abs/2601.20856)
*Sebastiano Monti,Carlo Nicolini,Gianni Pellegrini,Jacopo Staiano,Bruno Lepri*

Main category: cs.AI

TL;DR: A study evaluates long-horizon planning in Large Reasoning Models (LRMs) using a Sokoban-based benchmark; finds planning performance degrades when solutions require more than ~25 moves; external PDDL-based tooling offers modest gains, implying architectural limits to forward planning.


<details>
  <summary>Details</summary>
Motivation: To systematically assess LRMs' long-horizon planning and reasoning capabilities, addressing gaps in understanding of forward planning limits and whether scaling or tooling can overcome them.

Method: Introduce a Sokoban-based benchmark designed to isolate long-horizon planning from state persistence; evaluate state-of-the-art LRMs; augment evaluation with PDDL parsing/validation/solving tools to test impact of external planning components.

Result: Consistent degradation in planning performance beyond 25 moves; PDDL tooling provides modest improvements; results suggest intrinsic architectural limitations not easily overcome by test-time scaling.

Conclusion: LRMs exhibit fundamental constraints in forward planning for long-horizon tasks; external planning tools help but do not fully overcome these limits, indicating the need for architectural innovations to achieve robust long-horizon planning.

Abstract: Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [86] [Computational aspects of disks enclosing many points](https://arxiv.org/abs/2601.20036)
*Prosenjit Bose,Guillermo Esteban,Tyler Tuttle*

Main category: cs.CG

TL;DR: Algorithms for finding a pair of points whose containing disks are dense with respect to the set S. Main results: (i) a randomized O(n log n) algorithm in general position yielding a pair with disk-depth c = 1/2 − sqrt((1+2α)/12) for any 0 < α < 1; (ii) a deterministic O(n^2) algorithm in general position with a fixed improved constant c = 1/2 − 1/√12 ≈ 0.211; (iii) the second algorithm serves as a subroutine to maximize the number of points inside any disk that contains the pair in O(n^2 log n); (iv) when S is in convex position, a linear-time algorithm yields a pair through which any disk contains at least n/3 points; (v) a linear-time diametral-disk variant also achieves at least n/3 points; (vi) a generalization to bichromatic colorings; (vii) extensions to S inside a simple polygon P with geodesic disks replacing Euclidean disks.



<details>
  <summary>Details</summary>
Motivation: To study robust notions of centrality/depth for planar point sets via disks determined by point pairs, and to develop efficient algorithms for finding pairs that force disks to contain many points. The work also investigates natural variants (convex position, diametral disks, bichromatic data) and extensions to geodesic disks within polygons.

Method: (i) Randomized O(n log n) approach for general-position data with a tunable parameter α that affects the guaranteed density c; (ii) a deterministic O(n^2) procedure that fixes a stronger constant c; (iii) use of the latter as a subroutine to maximize the contained-point count in O(n^2 log n); (iv) exploitation of convex-position structure to achieve linear-time guarantees for c ≥ n/3; (v) a linear-time diametral-disk variant achieving the same bound; (vi) extension to two-color (bichromatic) data; (vii) extension to points inside a simple polygon using geodesic disks instead of Euclidean disks.

Result: Formal guarantees: c values as above; time complexities: O(n log n) expected, O(n^2), and linear-time variants; extended results include O(n^2 log n) subroutine for maximizing containment, bichromatic generalization, and geodesic-disk extensions within polygons.

Conclusion: The paper establishes several efficient algorithms achieving nontrivial disk-depth guarantees for pairs of points. Convex-position yields stronger linear-time results with a guaranteed n/3 bound, and the diameter-focused variant attains the same. The results extend to colored data and to geodesic disks in polygonal domains, indicating broad applicability and potential for further refinement, including removing general-position assumptions and tightening constants.

Abstract: Let $S$ be a set of $n$ points in the plane. We present several different algorithms for finding a pair of points in $S$ such that any disk that contains that pair must contain at least $cn$ points of $S$, for some constant $c>0$. The first is a randomized algorithm that finds a pair in $O(n\log n)$ expected time for points in general position, and $c = 1/2-\sqrt{(1+2α)/12}$, for any $0<α<1$. The second algorithm, also for points in general position, takes quadratic time, but the constant $c$ is improved to $1/2-1/{\sqrt{12}} \approx 1/4.7$. The second algorithm can also be used as a subroutine to find the pair that maximizes the number of points inside any disk that contains the pair, in $O(n^2\log n)$ time. We also consider variants of the problem. When the set $S$ is in convex position, we present an algorithm that finds in linear time a pair of points such that any disk through them contains at least $n/3$ points of $ S $. For the variant where we are only interested in finding a pair such that the diametral disk of that pair contains many points, we also have a linear-time algorithm that finds a disk with at least $n/3$ points of $S$. Finally, we present a generalization of the first two algorithms to the case where the set $S$ of points is coloured using two colours. We also consider adapting these algorithms to solve the same problems when $S$ is a set of points inside of a simple polygon $P$, with the notion of a disk replaced by that of a geodesic disk.

</details>


### [87] [How many times can two minimum spanning trees cross?](https://arxiv.org/abs/2601.20060)
*Todor Antić,Morteza Saghafian,Maria Saumell,Felix Schröder,Josef Tkadlec,Pavel Valtr*

Main category: cs.CG

TL;DR: The paper studies the maximum number of crossings between the MSTs of two color classes in a two-colored point set, proving a linear upper bound for generic sets, linear lower bounds in dense/convex cases, and linear expected crossings in a random model.


<details>
  <summary>Details</summary>
Motivation: Investigate how color-induced MSTs interact and cross, contributing to geometric crossing theory and network design under color constraints.

Method: Define crossAB(R,B) as the number of crossings between MSTs of R and B; cross(P) = max over bipartitions P=R∪B of crossAB(R,B). Prove a linear upper bound for generic P. Provide linear lower bounds for dense or convex-position P. In a random model (points uniform in unit square, colors uniform), show E[crossAB(R,B)] is linear in n.

Result: cross(P) = O(n) for generic P; linear lower bounds in dense/convex-position cases; in the random model, E[crossAB(R,B)] = Θ(n).

Conclusion: The bicolored MST crossing number grows linearly with the number of points across studied regimes, indicating limited interaction between color classes. The results open questions on exact constants, tightness of bounds, and extensions to more colors or higher dimensions.

Abstract: Let $P$ be a generic set of $n$ points in the plane, and let $P=R\cup B$ be a coloring of $P$ in two colors. We are interested in the number of crossings between the minimum spanning trees (MSTs) of $R$ and $B$, denoted by $\crossAB(R,B)$. We define the \emph{bicolored MST crossing number} of $P$, denoted by $\cross(P)$, as $\cross(P) = \max_{P= R\cup B}(\crossAB(R,B))$. We prove a linear upper bound for $\cross(P)$ when $P$ is generic. If $P$ is dense or in convex position, we provide linear lower bounds. Lastly, if $P$ is chosen uniformly at random from the unit square and is colored uniformly at random, we prove that the expected value of $\crossAB(R,B)$ is linear.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [88] [Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data](https://arxiv.org/abs/2601.19936)
*Minseo Kwak,Jaehyung Kim*

Main category: cs.LG

TL;DR: Gap-K% introduces a gradient-informed data-detection score for LLM pretraining data, using the log-probability gap between top-1 and target tokens with a sliding window; it achieves state-of-the-art detection on WikiMIA and MIMIR across model sizes and input lengths.


<details>
  <summary>Details</summary>
Motivation: Opacity of pretraining corpora raises significant privacy and copyright concerns. Existing methods rely on token likelihoods but often ignore the divergence from the model's top-1 prediction and local correlations between adjacent tokens.

Method: Analyze the next-token prediction objective; observe that discrepancies between the model's top-1 prediction and the target token induce strong gradient signals that are penalized during training. Gap-K% computes the log-probability gap between the top-1 predicted token and the target token, and uses a sliding window to capture local token correlations and mitigate token-level fluctuations.

Result: Empirical evaluation on WikiMIA and MIMIR benchmarks shows Gap-K% achieves state-of-the-art performance, consistently outperforming prior baselines across various model sizes and input lengths.

Conclusion: Gap-K% provides a robust, dynamics-aware approach to pretraining data detection, leveraging gradient signals and local token correlations to improve detection accuracy across model scales.

Abstract: The opacity of massive pretraining corpora in Large Language Models (LLMs) raises significant privacy and copyright concerns, making pretraining data detection a critical challenge. Existing state-of-the-art methods typically rely on token likelihoods, yet they often overlook the divergence from the model's top-1 prediction and local correlation between adjacent tokens. In this work, we propose Gap-K%, a novel pretraining data detection method grounded in the optimization dynamics of LLM pretraining. By analyzing the next-token prediction objective, we observe that discrepancies between the model's top-1 prediction and the target token induce strong gradient signals, which are explicitly penalized during training. Motivated by this, Gap-K% leverages the log probability gap between the top-1 predicted token and the target token, incorporating a sliding window strategy to capture local correlations and mitigate token-level fluctuations. Extensive experiments on the WikiMIA and MIMIR benchmarks demonstrate that Gap-K% achieves state-of-the-art performance, consistently outperforming prior baselines across various model sizes and input lengths.

</details>


### [89] [DecHW: Heterogeneous Decentralized Federated Learning Exploiting Second-Order Information](https://arxiv.org/abs/2601.19938)
*Adnan Ahmad,Chiara Boldrini,Lorenzo Valerio,Andrea Passarella,Marco Conti*

Main category: cs.LG

TL;DR: Robust aggregation for Decentralized Federated Learning (DFL) using second-order information to weight neighborhood updates, addressing data and model heterogeneity for better convergence and reduced communication in CV tasks.


<details>
  <summary>Details</summary>
Motivation: Non-IID data and heterogeneous local initializations across devices cause slow convergence in DFL. There is a need to account for parameter-level uncertainty (credence) to robustly combine neighbor updates.

Method: Compute consensus weights by approximating second-order information (local curvature) of each device's model on its own data. Use these weights to scale neighbor updates before aggregating into a global neighborhood representation, enabling robust, parameter-aware aggregation.

Result: Experimental results on computer vision tasks show improved generalization of local models and reduced communication costs, demonstrating the method's robustness to heterogeneity.

Conclusion: The proposed second-order-informed aggregation enhances convergence and generalization in DFL under data/model heterogeneity while maintaining communication efficiency.

Abstract: Decentralized Federated Learning (DFL) is a serverless collaborative machine learning paradigm where devices collaborate directly with neighbouring devices to exchange model information for learning a generalized model. However, variations in individual experiences and different levels of device interactions lead to data and model initialization heterogeneities across devices. Such heterogeneities leave variations in local model parameters across devices that leads to slower convergence. This paper tackles the data and model heterogeneity by explicitly addressing the parameter level varying evidential credence across local models. A novel aggregation approach is introduced that captures these parameter variations in local models and performs robust aggregation of neighbourhood local updates. Specifically, consensus weights are generated via approximation of second-order information of local models on their local datasets. These weights are utilized to scale neighbourhood updates before aggregating them into global neighbourhood representation. In extensive experiments with computer vision tasks, the proposed approach shows strong generalizability of local models at reduced communication costs.

</details>


### [90] [oculomix: Hierarchical Sampling for Retinal-Based Systemic Disease Prediction](https://arxiv.org/abs/2601.19939)
*Hyunmin Kim,Yukun Zhou,Rahul A. Jonas,Lie Ju,Sunjin Hwang,Pearse A. Keane,Siegfried K. Wagner*

Main category: cs.LG

TL;DR: A hierarchical mixed-sample augmentation method (Oculomix) for oculomics that uses exam- and patient-level priors to constrain mixes, improving five-year MACE AUROC by up to 3% over image-level Mixup/CutMix in ViT models on the Alzeye dataset.


<details>
  <summary>Details</summary>
Motivation: Standard image-level augmentations (e.g., CutMix, MixUp) disrupt patient-specific attributes (comorbidities, clinical factors). There is a need to preserve patient-level and exam-level information while leveraging mixed-sample augmentation for retinal imaging in predicting systemic diseases.

Method: Introduce hierarchical sampling with two clinical priors: exam level (images from the same patient at the same time point share attributes) and patient level (images from the same patient across time points follow a soft temporal trend). Constrain the mixing space to patient and exam levels to preserve attributes and exploit hierarchical relationships. Validate with Vision Transformer (ViT) models for five-year MACE prediction on a large ethnically diverse cohort (Alzeye).

Result: Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% AUROC in predicting five-year risk of major adverse cardiovascular events, demonstrating the effectiveness of incorporating clinical priors and hierarchical structure in mixed-sample augmentations for oculomics.

Conclusion: Incorporating exam- and patient-level priors into mixed-sample augmentations improves predictive performance for oculomics tasks, highlighting the importance of aligning data augmentation with meaningful clinical hierarchies.

Abstract: Oculomics - the concept of predicting systemic diseases, such as cardiovascular disease and dementia, through retinal imaging - has advanced rapidly due to the data efficiency of transformer-based foundation models like RETFound. Image-level mixed sample data augmentations, such as CutMix and MixUp, are frequently used for training transformers, yet these techniques perturb patient-specific attributes, such as medical comorbidity and clinical factors, since they only account for images and labels. To address this limitation, we propose a hierarchical sampling strategy, Oculomix, for mixed sample augmentations. Our method is based on two clinical priors. First (exam level), images acquired from the same patient at the same time point share the same attributes. Second (patient level), images acquired from the same patient at different time points have a soft temporal trend, as morbidity generally increases over time. Guided by these priors, our method constrains the mixing space to the patient and exam levels to better preserve patient-specific characteristics and leverages their hierarchical relationships. The proposed method is validated using ViT models on a five-year prediction of major adverse cardiovascular events (MACE) in a large ethnically diverse population (Alzeye). We show that Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% in AUROC, demonstrating the necessity and value of the proposed method in oculomics.

</details>


### [91] [Continuous-Flow Data-Rate-Aware CNN Inference on FPGA](https://arxiv.org/abs/2601.19940)
*Tobias Habermann,Michael Mecik,Zhenyu Wang,César David Vera,Martin Kumm,Mario Garrido*

Main category: cs.LG

TL;DR: Data-rate-aware continuous-flow CNN accelerator for FPGA that interleaves low-rate data and shares hardware units to sustain near-100% utilization, achieving high-throughput MobileNet on a single FPGA.


<details>
  <summary>Details</summary>
Motivation: Fully parallel CNN accelerators suffer underutilization due to data-rate reductions from pooling and strided convolutions; efficient FPGA mapping requires maintaining high data throughput and unit utilization.

Method: Perform data-flow analysis of CNNs and design a data-rate-aware continuous-flow architecture. Key ideas include interleaving low data-rate outputs with high-rate streams, sharing hardware units, and choosing the right degree of parallelism to preserve throughput akin to a fully parallel design.

Result: Significant reduction in arithmetic logic is achieved; high hardware utilization close to 100% is attained; complex CNNs such as MobileNet can be mapped to a single FPGA with high throughput.

Conclusion: Data-rate-aware, continuous-flow CNN architectures effectively address data-rate bottlenecks in FPGA accelerators, enabling efficient implementation of modern CNNs on single devices and guiding design choices for throughput and resource utilization.

Abstract: Among hardware accelerators for deep-learning inference, data flow implementations offer low latency and high throughput capabilities. In these architectures, each neuron is mapped to a dedicated hardware unit, making them well-suited for field-programmable gate array (FPGA) implementation. Previous unrolled implementations mostly focus on fully connected networks because of their simplicity, although it is well known that convolutional neural networks (CNNs) require fewer computations for the same accuracy. When observing the data flow in CNNs, pooling layers and convolutional layers with a stride larger than one, the number of data at their output is reduced with respect to their input. This data reduction strongly affects the data rate in a fully parallel implementation, making hardware units heavily underutilized unless it is handled properly. This work addresses this issue by analyzing the data flow of CNNs and presents a novel approach to designing data-rate-aware, continuous-flow CNN architectures. The proposed approach ensures a high hardware utilization close to 100% by interleaving low data rate signals and sharing hardware units, as well as using the right parallelization to achieve the throughput of a fully parallel implementation. The results show that a significant amount of the arithmetic logic can be saved, which allows implementing complex CNNs like MobileNet on a single FPGA with high throughput.

</details>


### [92] [Latent Object Permanence: Topological Phase Transitions, Free-Energy Principles, and Renormalization Group Flows in Deep Transformer Manifolds](https://arxiv.org/abs/2601.19942)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.LG

TL;DR: The paper analyzes how multi-step reasoning emerges in large Transformer LMs by viewing hidden-state trajectories as flows on a geometric manifold, using layerwise covariance spectra to detect a phase-transition-like drop in effective dimensionality, and introducing Transient Class Objects (TCOs) and a renormalization-like forward pass. It provides theoretical conditions linking logical separability to spectral decay and validates predictions across open-weight models.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which deep Transformers develop stable, reusable, concept-like representations and multi-step reasoning, by connecting geometric, spectral, and statistical-physics perspectives.

Method: Model the forward pass as a discrete coarse-graining map on a hidden-state manifold; analyze layerwise covariance C^(ℓ)=E[h^(ℓ) h^(ℓ)ᵀ] and its spectrum; track deviations from a random-matrix bulk; define an order parameter Ω(h)=1−||h||₁/(√d ||h||₂) to measure sparsity/localization; identify a critical depth γ_c≈0.42; formalize formation of stable 'concept basins' as fixed points of a renormalization-like dynamics; introduce Transient Class Objects (TCOs) as transient, reusable object-like structures; validate via layerwise probes on open-weight model families.

Result: Across scales (1.5B–30B), a sharp reduction in effective dimensionality with a phase-transition-like behavior is observed; Ω(h) exhibits a discontinuity near γ_c≈0.42 in large models; spectral tail collapse and formation of TCOs; a forward-pass coarse-graining perspective and theoretical links between logical separability and spectral decay; empirical validation through layerwise probes on multiple model families.

Conclusion: The low-entropy regime features stable concept basins and reusable representations arising from a renormalization-like dynamic; TCOs capture transient, object-like structures in representation space; the approach provides a unified geometric-statistical-physics framework for understanding emergent multi-step reasoning in large Transformers and yields testable predictions across model scales.

Abstract: We study the emergence of multi-step reasoning in deep Transformer language models through a geometric and statistical-physics lens. Treating the hidden-state trajectory as a flow on an implicit Riemannian manifold, we analyze the layerwise covariance spectrum of activations, where $C^{(\ell)}=\mathbb{E}[h^{(\ell)}h^{(\ell)\top}]$, and track deviations from a random-matrix bulk. Across model scales (1.5B--30B), we observe a sharp reduction in effective dimensionality consistent with a phase transition: an order parameter based on sparsity/localization, $Ω(h)=1-\|h\|_1/(\sqrt{d}\|h\|_2)$, exhibits a discontinuity near a critical normalized depth $γ_c\approx 0.42$ in sufficiently large models. We formalize the forward pass as a discrete coarse-graining map and relate the appearance of stable "concept basins" to fixed points of this renormalization-like dynamics. The resulting low-entropy regime is characterized by a spectral tail collapse and by the formation of transient, reusable object-like structures in representation space, which we call Transient Class Objects (TCOs). We provide theoretical conditions connecting logical separability to spectral decay and validate the predicted signatures with layerwise probes on multiple open-weight model families.

</details>


### [93] [Emergent Specialization in Learner Populations: Competition as the Source of Diversity](https://arxiv.org/abs/2601.19943)
*Yuhao Li*

Main category: cs.LG

TL;DR: Emergent specialization arises from competition without explicit incentives. The NichePopulation algorithm (competitive exclusion + niche affinity tracking) yields high specialization across six real-world domains, outperforming MARL baselines and homogeneous baselines with large effect sizes.


<details>
  <summary>Details</summary>
Motivation: Address how populations of learners can spontaneously develop coordinated, diverse behaviors without explicit communication or diversity incentives, aligning with ecological niche theory.

Method: Introduce NichePopulation: a simple mechanism combining competitive exclusion with niche affinity tracking. Evaluated across six real-world domains (cryptocurrency trading, commodity prices, weather forecasting, solar irradiance, urban traffic, air quality). Uses Specialization Index (SI) as primary metric; includes experiments at lambda=0 to test emergent specialization; comparisons against MARL baselines (QMIX, MAPPO, IQL).

Result: Mean Specialization Index (SI) of 0.75 with Cohen's d > 20. At lambda=0, SI > 0.30 indicating emergent specialization without niche bonus. Diverse populations outperform homogeneous baselines by +26.5%. Outperforms MARL baselines by 4.3x and is 4x faster.

Conclusion: Competition suffices to induce emergent specialization in learner populations, consistent with ecological niche theory. NichePopulation provides a simple, scalable mechanism achieving robust division of labor across varied real-world tasks.

Abstract: How can populations of learners develop coordinated, diverse behaviors without explicit communication or diversity incentives? We demonstrate that competition alone is sufficient to induce emergent specialization -- learners spontaneously partition into specialists for different environmental regimes through competitive dynamics, consistent with ecological niche theory. We introduce the NichePopulation algorithm, a simple mechanism combining competitive exclusion with niche affinity tracking. Validated across six real-world domains (cryptocurrency trading, commodity prices, weather forecasting, solar irradiance, urban traffic, and air quality), our approach achieves a mean Specialization Index of 0.75 with effect sizes of Cohen's d > 20. Key findings: (1) At lambda=0 (no niche bonus), learners still achieve SI > 0.30, proving specialization is genuinely emergent; (2) Diverse populations outperform homogeneous baselines by +26.5% through method-level division of labor; (3) Our approach outperforms MARL baselines (QMIX, MAPPO, IQL) by 4.3x while being 4x faster.

</details>


### [94] [Classifier Calibration at Scale: An Empirical Study of Model-Agnostic Post-Hoc Methods](https://arxiv.org/abs/2601.19944)
*Valery Manokhin,Daniel Grønhaug*

Main category: cs.LG

TL;DR: Venn-Abers calibration yields the strongest log-loss improvements across diverse tabular models; Beta calibration also helps; Platt scaling and isotonic regression can degrade scoring; no method uniformly dominates; accuracy gains are minimal.


<details>
  <summary>Details</summary>
Motivation: Assess the effectiveness of model-agnostic, post-hoc calibration methods (notably conformal and Venn-based) on real i.i.d. tabular data to provide distribution-free validity and improve probabilistic predictions.

Method: Benchmark 21 classifiers (linear, SVM, CatBoost, XGBoost, LightGBM, tabular neural/foundation models) on binary tasks from TabArena-v0.1. Use randomized stratified 5-fold CV with a held-out test fold. Train five calibrators on a separate calibration split and apply to test predictions. Evaluate with log-loss, Brier score, Spiegelhalter's Z, ECE, ECI, plus AUC-ROC and accuracy.

Result: Across tasks and architectures, Venn-Abers predictors yield the largest average log-loss reductions; Beta calibration also effective; Platt scaling weaker and sometimes harmful; isotonic regression can degrade proper scoring for strong models; accuracy largely preserved; no method uniformly dominates; expected accuracy gain ~0.008%.

Conclusion: Calibration methods yield dataset/architecture-dependent gains; distribution-free validity from conformal/Venn approaches remains valuable; practical gains vary; prefer Venn-Abers or Beta calibration; use caution with Platt and isotonic regression.

Abstract: We study model-agnostic post-hoc calibration methods intended to improve probabilistic predictions in supervised binary classification on real i.i.d. tabular data, with particular emphasis on conformal and Venn-based approaches that provide distribution-free validity guarantees under exchangeability. We benchmark 21 widely used classifiers, including linear models, SVMs, tree ensembles (CatBoost, XGBoost, LightGBM), and modern tabular neural and foundation models, on binary tasks from the TabArena-v0.1 suite using randomized, stratified five-fold cross-validation with a held-out test fold. Five calibrators; Isotonic regression, Platt scaling, Beta calibration, Venn-Abers predictors, and Pearsonify are trained on a separate calibration split and applied to test predictions. Calibration is evaluated using proper scoring rules (log-loss and Brier score) and diagnostic measures (Spiegelhalter's Z, ECE, and ECI), alongside discrimination (AUC-ROC) and standard classification metrics. Across tasks and architectures, Venn-Abers predictors achieve the largest average reductions in log-loss, followed closely by Beta calibration, while Platt scaling exhibits weaker and less consistent effects. Beta calibration improves log-loss most frequently across tasks, whereas Venn-Abers displays fewer instances of extreme degradation and slightly more instances of extreme improvement. Importantly, we find that commonly used calibration procedures, most notably Platt scaling and isotonic regression, can systematically degrade proper scoring performance for strong modern tabular models. Overall classification performance is often preserved, but calibration effects vary substantially across datasets and architectures, and no method dominates uniformly. In expectation, all methods except Pearsonify slightly increase accuracy, but the effect is marginal, with the largest expected gain about 0.008%.

</details>


### [95] [NCSAM Noise-Compensated Sharpness-Aware Minimization for Noisy Label Learning](https://arxiv.org/abs/2601.19947)
*Jiayu Xu,Junbiao Pang*

Main category: cs.LG

TL;DR: Link between loss landscape flatness and label noise; proposes Noise-Compensated Sharpness-aware Minimization (NCSAM); shows noise can improve generalization when simulated; strong empirical results across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address real-world noisy labels by exploring theoretical connection between loss flatness and noise; move beyond label correction to optimization-based robustness.

Method: Theoretical analysis establishing relationship; introduce NCSAM that augments Sharpness-Aware Minimization with noise compensation; relies on perturbations of the loss landscape to mitigate noise damage.

Result: Testing accuracy behavior parallels clean datasets; empirical results show NCSAM outperforms state-of-the-art on diverse tasks.

Conclusion: Noise-Compensated SAM harnesses label noise under controlled perturbations to improve generalization and robustness; flatness considerations are central to learning with noisy labels.

Abstract: Learning from Noisy Labels (LNL) presents a fundamental challenge in deep learning, as real-world datasets often contain erroneous or corrupted annotations, \textit{e.g.}, data crawled from Web. Current research focuses on sophisticated label correction mechanisms. In contrast, this paper adopts a novel perspective by establishing a theoretical analysis the relationship between flatness of the loss landscape and the presence of label noise. In this paper, we theoretically demonstrate that carefully simulated label noise synergistically enhances both the generalization performance and robustness of label noises. Consequently, we propose Noise-Compensated Sharpness-aware Minimization (NCSAM) to leverage the perturbation of Sharpness-Aware Minimization (SAM) to remedy the damage of label noises. Our analysis reveals that the testing accuracy exhibits a similar behavior that has been observed on the noise-clear dataset. Extensive experimental results on multiple benchmark datasets demonstrate the consistent superiority of the proposed method over existing state-of-the-art approaches on diverse tasks.

</details>


### [96] [Probabilistic Sensing: Intelligence in Data Sampling](https://arxiv.org/abs/2601.19953)
*Ibrahim Albulushi,Saleh Bunaiyan,Suraj S. Cheema,Hesham ElSawy,Feras Al-Dirini*

Main category: cs.LG

TL;DR: Probabilistic sensing paradigm using a p-neuron for real-time selective sampling in seismic data, enabling lossless data acquisition with major energy/time savings.


<details>
  <summary>Details</summary>
Motivation: Extend sensor intelligence into data acquisition to improve energy efficiency while avoiding information loss; deterministic sampling can miss data.

Method: Develop a probabilistic neuron (p-neuron) driven by an analog feature extraction circuit, providing microsecond response and enabling probabilistic data sampling; validated on active seismic survey data.

Result: Lossless probabilistic data acquisition; normalized mean squared error 0.41%; 93% saving in active operation time and in the number of generated samples.

Conclusion: Enables real-time autonomous activation of data sampling with substantial energy efficiency gains while preserving data integrity.

Abstract: Extending the intelligence of sensors to the data-acquisition process - deciding whether to sample or not - can result in transformative energy-efficiency gains. However, making such a decision in a deterministic manner involves risk of losing information. Here we present a sensing paradigm that enables making such a decision in a probabilistic manner. The paradigm takes inspiration from the autonomous nervous system and employs a probabilistic neuron (p-neuron) driven by an analog feature extraction circuit. The response time of the system is on the order of microseconds, over-coming the sub-sampling-rate response time limit and enabling real-time intelligent autonomous activation of data-sampling. Validation experiments on active seismic survey data demonstrate lossless probabilistic data acquisition, with a normalized mean squared error of 0.41%, and 93% saving in the active operation time of the system and the number of generated samples.

</details>


### [97] [MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference](https://arxiv.org/abs/2601.19961)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Ruijia Wu,Li YanTao,Qiang Hui,Yuren You,Ting Lu,Chao Tan,Shaoan Zhao,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.LG

TL;DR: MeanCache is a training-free caching framework for Flow Matching inference that uses interval average velocities derived from cached Jacobian–vector products (JVPs) to mitigate local error accumulation, plus a trajectory-stability scheduling strategy via Peak-Suppressed Shortest Path under budget constraints. It yields substantial speedups (4.12x, 4.56x, 3.59x on FLUX.1, Qwen-Image, HunyuanVideo) while outperforming caching baselines in generation quality.


<details>
  <summary>Details</summary>
Motivation: To reduce error accumulation and instability in velocity-based caches for Flow Matching under high acceleration, enabling stable and efficient inference for large-scale generative models.

Method: Construct interval average velocities from cached instantaneous velocities using Jacobian–vector products (JVPs). Introduce a trajectory-stability scheduling strategy that selects cache points by solving a Peak-Suppressed Shortest Path under a budget, optimizing cache timing and JVP reuse stability.

Result: Significant inference speedups on three benchmarks (FLUX.1: 4.12x, Qwen-Image: 4.56x, HunyuanVideo: 3.59x) with generation quality consistently surpassing state-of-the-art caching baselines.

Conclusion: A simple yet effective stability-driven acceleration approach for Flow Matching inference, offering a new perspective on cache design and potentially enabling broader adoption in commercial-scale generative models.

Abstract: We present MeanCache, a training-free caching framework for efficient Flow Matching inference. Existing caching methods reduce redundant computation but typically rely on instantaneous velocity information (e.g., feature caching), which often leads to severe trajectory deviations and error accumulation under high acceleration ratios. MeanCache introduces an average-velocity perspective: by leveraging cached Jacobian--vector products (JVP) to construct interval average velocities from instantaneous velocities, it effectively mitigates local error accumulation. To further improve cache timing and JVP reuse stability, we develop a trajectory-stability scheduling strategy as a practical tool, employing a Peak-Suppressed Shortest Path under budget constraints to determine the schedule. Experiments on FLUX.1, Qwen-Image, and HunyuanVideo demonstrate that MeanCache achieves 4.12X and 4.56X and 3.59X acceleration, respectively, while consistently outperforming state-of-the-art caching baselines in generation quality. We believe this simple yet effective approach provides a new perspective for Flow Matching inference and will inspire further exploration of stability-driven acceleration in commercial-scale generative models.

</details>


### [98] [Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment](https://arxiv.org/abs/2601.19963)
*Canyang Zhao,Bolin Peng,J. Patrick Mayo,Ce Ju,Bing Liu*

Main category: cs.LG

TL;DR: TCLA uses a task-conditioned latent alignment within an autoencoder to transfer cross-session neural decoding from a rich source session to data-sparse target sessions, improving generalization.


<details>
  <summary>Details</summary>
Motivation: Cross-session nonstationarity in invasive BCIs leads to poor generalization; limited target data makes retraining hard; need robust transfer learning.

Method: Train autoencoder on source session to extract low-dimensional neural dynamics. For target sessions with limited data, perform a task-conditioned alignment of target latent representations to the source latent space, enabling transfer of learned dynamics.

Result: Outperforms baselines trained only on target data across macaque motor and oculomotor datasets. Notably, increases in R^2 for y-coordinate velocity decoding up to 0.386 in motor data; demonstrates robustness across decoding settings.

Conclusion: Task-conditioned latent alignment enables effective cross-session transfer with limited data, improving the robustness of neural decoding in invasive BCIs.

Abstract: Cross-session nonstationarity in neural activity recorded by implanted electrodes is a major challenge for invasive Brain-computer interfaces (BCIs), as decoders trained on data from one session often fail to generalize to subsequent sessions. This issue is further exacerbated in practice, as retraining or adapting decoders becomes particularly challenging when only limited data are available from a new session. To address this challenge, we propose a Task-Conditioned Latent Alignment framework (TCLA) for cross-session neural decoding. Building upon an autoencoder architecture, TCLA first learns a low-dimensional representation of neural dynamics from a source session with sufficient data. For target sessions with limited data, TCLA then aligns target latent representations to the source in a task-conditioned manner, enabling effective transfer of learned neural dynamics. We evaluate TCLA on the macaque motor and oculomotor center-out dataset. Compared to baseline methods trained solely on target-session data, TCLA consistently improves decoding performance across datasets and decoding settings, with gains in the coefficient of determination of up to 0.386 for y coordinate velocity decoding in a motor dataset. These results suggest that TCLA provides an effective strategy for transferring knowledge from source to target sessions, enabling more robust neural decoding under conditions with limited data.

</details>


### [99] [Modeling Cascaded Delay Feedback for Online Net Conversion Rate Prediction: Benchmark, Insights and Solutions](https://arxiv.org/abs/2601.19965)
*Mingxuan Luo,Guipeng Xv,Sishuo Chen,Xinyu Li,Li Zhang,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Bo Zheng,Chen Lin*

Main category: cs.LG

TL;DR: Introduces TESLA, a cascaded CVR–refundNet NetCVR modeling framework, and CASCADE dataset for online continuous NetCVR; reports consistent improvements over state-of-the-art on NetCVR prediction.


<details>
  <summary>Details</summary>
Motivation: CVR ignores refunds and thus misrepresents user satisfaction and business value; NetCVR is a more faithful metric but is difficult due to cascaded delayed feedback and lack of open large-scale datasets and online training schemes.

Method: CASCADE is a large-scale Taobao-derived NetCVR dataset enabling online continuous modeling. TESLA is a continuous NetCVR modeling framework with a CVR–refund-rate cascaded architecture, stage-wise debiasing, and a delay-time-aware ranking loss, designed to handle cascaded delays and non-stationarity.

Result: TESLA achieves absolute improvements of 12.41% in RI-AUC and 14.94% in RI-PRAUC for NetCVR prediction on CASCADE compared with baselines.

Conclusion: The work introduces a new dataset and a novel modeling framework that outperform existing methods on NetCVR prediction, with publicly available code and data.

Abstract: In industrial recommender systems, conversion rate (CVR) is widely used for traffic allocation, but it fails to fully reflect recommendation effectiveness because it ignores refund behavior. To better capture true user satisfaction and business value, net conversion rate (NetCVR), defined as the probability that a clicked item is purchased and not refunded, has been proposed.Unlike CVR, NetCVR prediction involves a more complex multi-stage cascaded delayed feedback process. The two cascaded delays from click to conversion and from conversion to refund have opposite effects, making traditional CVR modeling methods inapplicable. Moreover, the lack of open-source datasets and online continuous training schemes further hinders progress in this area.To address these challenges, we introduce CASCADE (Cascaded Sequences of Conversion and Delayed Refund), the first large-scale open dataset derived from the Taobao app for online continuous NetCVR prediction. Through an in-depth analysis of CASCADE, we identify three key insights: (1) NetCVR exhibits strong temporal dynamics, necessitating online continuous modeling; (2) cascaded modeling of CVR and refund rate outperforms direct NetCVR modeling; and (3) delay time, which correlates with both CVR and refund rate, is an important feature for NetCVR prediction.Based on these insights, we propose TESLA, a continuous NetCVR modeling framework featuring a CVR-refund-rate cascaded architecture, stage-wise debiasing, and a delay-time-aware ranking loss. Extensive experiments demonstrate that TESLA consistently outperforms state-of-the-art methods on CASCADE, achieving absolute improvements of 12.41 percent in RI-AUC and 14.94 percent in RI-PRAUC on NetCVR prediction. The code and dataset are publicly available at https://github.com/alimama-tech/NetCVR.

</details>


### [100] [Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers](https://arxiv.org/abs/2601.19967)
*Jinlin Liu,Wei Chen,Xiaojin Zhang*

Main category: cs.LG

TL;DR: Introduces PIL, a fast perturbation method for creating unlearnable data using linear surrogates; achieves performance on par with neural-based methods with much lower compute; reveals linearization as a key mechanism and analyzes partial perturbation.


<details>
  <summary>Details</summary>
Motivation: Protect web-sourced data from unauthorized use by making it unlearnable, while addressing the heavy computational costs of existing surrogate-based perturbations that rely on deep networks.

Method: Perturbation-Induced Linearization (PIL) uses linear surrogate models to generate perturbations, leveraging induced linearization in deep networks; demonstrates computational efficiency and competitive effectiveness; analyzes practitioner-controlled partial perturbation.

Result: PIL achieves comparable or better effectiveness than existing surrogate-based methods with a dramatic reduction in computation time; provides empirical and theoretical support for the linearization mechanism; analyzes perturbation behavior under percentage-based partial perturbation.

Conclusion: PIL offers a practical data-protection approach and contributes to understanding unlearnable examples by showing that inducing linearization is a core factor; includes analysis of partial perturbation properties.

Abstract: Collecting web data to train deep models has become increasingly common, raising concerns about unauthorized data usage. To mitigate this issue, unlearnable examples introduce imperceptible perturbations into data, preventing models from learning effectively. However, existing methods typically rely on deep neural networks as surrogate models for perturbation generation, resulting in significant computational costs. In this work, we propose Perturbation-Induced Linearization (PIL), a computationally efficient yet effective method that generates perturbations using only linear surrogate models. PIL achieves comparable or better performance than existing surrogate-based methods while reducing computational time dramatically. We further reveal a key mechanism underlying unlearnable examples: inducing linearization to deep models, which explains why PIL can achieve competitive results in a very short time. Beyond this, we provide an analysis about the property of unlearnable examples under percentage-based partial perturbation. Our work not only provides a practical approach for data protection but also offers insights into what makes unlearnable examples effective.

</details>


### [101] [BayPrAnoMeta: Bayesian Proto-MAML for Few-Shot Industrial Image Anomaly Detection](https://arxiv.org/abs/2601.19992)
*Soham Sarkar,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: BayPrAnoMeta: a Bayesian generalization of Proto-MAML for few-shot industrial anomaly detection, replacing fixed class prototypes with probabilistic normality models (NIW prior) to yield a Student-t predictive score; adds federated meta-learning with supervised contrastive regularization; achieves AUROC gains on MVTec AD in few-shot settings.


<details>
  <summary>Details</summary>
Motivation: Address extreme class imbalance and scarcity of labeled defects in industrial anomaly detection, especially in few-shot scenarios, where deterministic prototypes in Proto-MAML may be brittle and lack uncertainty handling.

Method: Model normal support embeddings with a Normal-Inverse-Wishart prior to obtain a Bayesian posterior predictive (Student-t) distribution; perform inner-loop adaptation via Bayesian posterior predictive likelihood; extend to federated meta-learning across heterogeneous clients with supervised contrastive regularization; prove convergence to stationary points of the nonconvex objective.

Result: On MVTec AD, BayPrAnoMeta achieves consistent and significant AUROC improvements over MAML, Proto-MAML, and PatchCore-based methods in few-shot anomaly detection settings.

Conclusion: Incorporating probabilistic normality models and Bayesian adaptation improves robustness and performance for few-shot industrial anomaly detection, and the federated extension with contrastive regularization broadens applicability across heterogeneous clients.

Abstract: Industrial image anomaly detection is a challenging problem owing to extreme class imbalance and the scarcity of labeled defective samples, particularly in few-shot settings. We propose BayPrAnoMeta, a Bayesian generalization of Proto-MAML for few-shot industrial image anomaly detection. Unlike existing Proto-MAML approaches that rely on deterministic class prototypes and distance-based adaptation, BayPrAnoMeta replaces prototypes with task-specific probabilistic normality models and performs inner-loop adaptation via a Bayesian posterior predictive likelihood. We model normal support embeddings with a Normal-Inverse-Wishart (NIW) prior, producing a Student-$t$ predictive distribution that enables uncertainty-aware, heavy-tailed anomaly scoring and is essential for robustness in extreme few-shot settings. We further extend BayPrAnoMeta to a federated meta-learning framework with supervised contrastive regularization for heterogeneous industrial clients and prove convergence to stationary points of the resulting nonconvex objective. Experiments on the MVTec AD benchmark demonstrate consistent and significant AUROC improvements over MAML, Proto-MAML, and PatchCore-based methods in few-shot anomaly detection settings.

</details>


### [102] [Decomposing multimodal embedding spaces with group-sparse autoencoders](https://arxiv.org/abs/2601.20028)
*Chiraag Kaushik,Davis Barch,Andrea Fanelli*

Main category: cs.LG

TL;DR: SAEs often yield split (unimodal) dictionaries in multimodal embeddings; the paper proposes cross-modal random masking with group-sparse regularization to learn a truly multimodal dictionary, improving alignment, reducing dead neurons, and enhancing interpretability for CLIP/CLAP embeddings.


<details>
  <summary>Details</summary>
Motivation: To achieve interpretable, cross-modality-aligned decompositions of multimodal embeddings (e.g., CLIP/CLAP) using sparse autoencoders, addressing the problem that standard SAEs produce split dictionaries that fail to align concepts across modalities.

Method: The authors argue about the existence of a non-split dictionary with better alignment; they then introduce cross-modal random masking and group-sparse regularization in SAEs. They apply the method to CLIP (image/text) and CLAP (audio/text) embeddings and compare to standard SAEs.

Result: Their method yields a more multimodal dictionary, fewer dead neurons, and improved semanticity of features, leading to better cross-modal alignment; these improvements translate into enhanced interpretability and control for cross-modal tasks.

Conclusion: Cross-modal masking plus group sparsity enables SAEs to decompose multimodal embeddings into aligned, interpretable features, mitigating split-dictionary issues and benefiting downstream cross-modal applications.

Abstract: The Linear Representation Hypothesis asserts that the embeddings learned by neural networks can be understood as linear combinations of features corresponding to high-level concepts. Based on this ansatz, sparse autoencoders (SAEs) have recently become a popular method for decomposing embeddings into a sparse combination of linear directions, which have been shown empirically to often correspond to human-interpretable semantics. However, recent attempts to apply SAEs to multimodal embedding spaces (such as the popular CLIP embeddings for image/text data) have found that SAEs often learn "split dictionaries", where most of the learned sparse features are essentially unimodal, active only for data of a single modality. In this work, we study how to effectively adapt SAEs for the setting of multimodal embeddings while ensuring multimodal alignment. We first argue that the existence of a split dictionary decomposition on an aligned embedding space implies the existence of a non-split dictionary with improved modality alignment. Then, we propose a new SAE-based approach to multimodal embedding decomposition using cross-modal random masking and group-sparse regularization. We apply our method to popular embeddings for image/text (CLIP) and audio/text (CLAP) data and show that, compared to standard SAEs, our approach learns a more multimodal dictionary while reducing the number of dead neurons and improving feature semanticity. We finally demonstrate how this improvement in alignment of concepts between modalities can enable improvements in the interpretability and control of cross-modal tasks.

</details>


### [103] [Structural Compositional Function Networks: Interpretable Functional Compositions for Tabular Discovery](https://arxiv.org/abs/2601.20037)
*Fang Li*

Main category: cs.LG

TL;DR: Proposes StructuralCFN, a relation-aware neural architecture for tabular data that learns feature representations as differentiable compositions with adaptive gating, enabling domain priors integration and intrinsic symbolic interpretability. Demonstrates statistically significant gains on 18 benchmarks via 10-fold CV, with a compact parameter footprint (300–2,500 params) and symbolic expressions of the data laws.


<details>
  <summary>Details</summary>
Motivation: Address the gap where traditional DL underperforms gradient-boosted trees on tabular data and lacks interpretability. Standard NNs treat features independently, ignoring manifold structure and relationships that encode the data-generating process. There is a need for models that exploit relational dependencies and yield human-readable explanations.

Method: Structural Compositional Function Networks (StructuralCFN) impose a relation-aware inductive bias through a differentiable structural prior. Each feature is modeled as a composition of related features via differentiable adaptive gating, allowing discovery of activation physics (e.g., attention-like filtering vs. inhibitory polarity). The framework supports Structured Knowledge Integration by injecting domain-specific relational priors into the architecture. The model emphasizes intrinsic symbolic interpretability by recovering governing laws as readable mathematical expressions and achieves a compact parameter footprint (300–2,500 parameters).

Result: Evaluated with 10-fold cross-validation on 18 benchmarks; reported statistically significant improvements (p<0.05) on scientific and clinical datasets (e.g., Blood Transfusion, Ozone, WDBC).

Conclusion: StructuralCFN delivers improved predictive performance with interpretable, compact models for tabular data, leveraging relational priors to recover symbolic representations of the data manifold.

Abstract: Despite the ubiquity of tabular data in high-stakes domains, traditional deep learning architectures often struggle to match the performance of gradient-boosted decision trees while maintaining scientific interpretability. Standard neural networks typically treat features as independent entities, failing to exploit the inherent manifold structural dependencies that define tabular distributions. We propose Structural Compositional Function Networks (StructuralCFN), a novel architecture that imposes a Relation-Aware Inductive Bias via a differentiable structural prior. StructuralCFN explicitly models each feature as a mathematical composition of its counterparts through Differentiable Adaptive Gating, which automatically discovers the optimal activation physics (e.g., attention-style filtering vs. inhibitory polarity) for each relationship. Our framework enables Structured Knowledge Integration, allowing domain-specific relational priors to be injected directly into the architecture to guide discovery. We evaluate StructuralCFN across a rigorous 10-fold cross-validation suite on 18 benchmarks, demonstrating statistically significant improvements (p < 0.05) on scientific and clinical datasets (e.g., Blood Transfusion, Ozone, WDBC). Furthermore, StructuralCFN provides Intrinsic Symbolic Interpretability: it recovers the governing "laws" of the data manifold as human-readable mathematical expressions while maintaining a compact parameter footprint (300--2,500 parameters) that is over an order of magnitude (10x--20x) smaller than standard deep baselines.

</details>


### [104] [CiMRAG: Cim-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs](https://arxiv.org/abs/2601.20041)
*Shih-Hsuan Chiu,Ming-Syan Chen*

Main category: cs.LG

TL;DR: TONEL proposes a noise-aware embedding learning framework to enable robust Retrieval-Augmented Generation on edge devices with Computing-in-Memory (CiM), improving accuracy under environmental noise and enabling domain adaptability.


<details>
  <summary>Details</summary>
Motivation: Edge RAG with CiM faces rapid profile data growth and data movement bottlenecks; environmental noise in CiM degrades retrieval accuracy; there is a need for robust, domain-adaptive edge solutions across multiple domains (e.g., travel, medicine, law).

Method: Introduces a noise-aware projection model that learns task-specific embeddings compatible with CiM hardware constraints to sustain accurate retrieval under noisy conditions in RAG.

Result: Empirical evaluation on personalization benchmarks shows TONEL outperforms strong baselines, particularly in task-specific noisy scenarios, demonstrating practicality and effectiveness.

Conclusion:  TONEL advances noise-robust, domain-adaptive embedding learning for CiM-backed edge RAG, enabling practical improvements for personalized assistants in multi-domain, noisy environments.

Abstract: Personalized virtual assistants powered by large language models (LLMs) on edge devices are attracting growing attention, with Retrieval-Augmented Generation (RAG) emerging as a key method for personalization by retrieving relevant profile data and generating tailored responses. However, deploying RAG on edge devices faces efficiency hurdles due to the rapid growth of profile data, such as user-LLM interactions and recent updates. While Computing-in-Memory (CiM) architectures mitigate this bottleneck by eliminating data movement between memory and processing units via in-situ operations, they are susceptible to environmental noise that can degrade retrieval precision. This poses a critical issue in dynamic, multi-domain edge-based scenarios (e.g., travel, medicine, and law) where both accuracy and adaptability are paramount. To address these challenges, we propose Task-Oriented Noise-resilient Embedding Learning (TONEL), a framework that improves noise robustness and domain adaptability for RAG in noisy edge environments. TONEL employs a noise-aware projection model to learn task-specific embeddings compatible with CiM hardware constraints, enabling accurate retrieval under noisy conditions. Extensive experiments conducted on personalization benchmarks demonstrate the effectiveness and practicality of our methods relative to strong baselines, especially in task-specific noisy scenarios.

</details>


### [105] [Regime-Adaptive Bayesian Optimization via Dirichlet Process Mixtures of Gaussian Processes](https://arxiv.org/abs/2601.20043)
*Yan Zhang,Xuefeng Liu,Sipeng Chen,Sascha Ranftl,Chong Liu,Shibo Li*

Main category: cs.LG

TL;DR: RAMBO: a Dirichlet Process Mixture of Gaussian Processes for multi-regime Bayesian optimization; discovers latent regimes, each with its own GP and hyperparameters; uses collapsed Gibbs sampling; adaptive concentration scheduling; acquisition functions separate intra- and inter-regime uncertainty; validated on synthetic benchmarks and real-world multi-regime problems.


<details>
  <summary>Details</summary>
Motivation: Standard BO assumes uniform smoothness across the search space, an assumption violated in multi-regime landscapes (e.g., distinct energy basins, heterogeneous molecular scaffolds). This leads to over-smoothing or miscalibrated uncertainty.

Method: Dirichlet Process Mixture of Gaussian Processes with latent regime assignments; locally optimized hyperparameters per regime; collapsed Gibbs sampling that marginalizes latent functions for efficient inference; adaptive concentration parameter scheduling for coarse-to-fine regime discovery; acquisition functions decompose total uncertainty into intra-regime and inter-regime components.

Result: Empirical evaluation on synthetic benchmarks and real-world tasks (molecular conformer optimization, virtual screening for drug discovery, fusion reactor design) shows consistent improvements over state-of-the-art baselines on multi-regime objectives.

Conclusion: RAMBO enables automatic regime discovery and robust optimization in heterogeneous landscapes by decomposing uncertainty and allowing regime-specific modeling; demonstrates broad applicability and performance gains, with typical computational considerations tied to DP-GP inference.

Abstract: Standard Bayesian Optimization (BO) assumes uniform smoothness across the search space an assumption violated in multi-regime problems such as molecular conformation search through distinct energy basins or drug discovery across heterogeneous molecular scaffolds. A single GP either oversmooths sharp transitions or hallucinates noise in smooth regions, yielding miscalibrated uncertainty. We propose RAMBO, a Dirichlet Process Mixture of Gaussian Processes that automatically discovers latent regimes during optimization, each modeled by an independent GP with locally-optimized hyperparameters. We derive collapsed Gibbs sampling that analytically marginalizes latent functions for efficient inference, and introduce adaptive concentration parameter scheduling for coarse-to-fine regime discovery. Our acquisition functions decompose uncertainty into intra-regime and inter-regime components. Experiments on synthetic benchmarks and real-world applications, including molecular conformer optimization, virtual screening for drug discovery, and fusion reactor design, demonstrate consistent improvements over state-of-the-art baselines on multi-regime objectives.

</details>


### [106] [Externally Validated Longitudinal GRU Model for Visit-Level 180-Day Mortality Risk in Metastatic Castration-Resistant Prostate Cancer](https://arxiv.org/abs/2601.20046)
*Javier Mencia-Ledo,Mohammad Noaeen,Zahra Shakeri*

Main category: cs.LG

TL;DR: Longitudinal, visit-level 180-day mortality risk modeling in metastatic castration-resistant prostate cancer (mCRPC); GRU and RSF show strong initial discrimination, external validation demonstrates good calibration and potential for proactive care planning.


<details>
  <summary>Details</summary>
Motivation: Address the need for short-horizon mortality risk prediction in mCRPC using longitudinal clinical data to support proactive care planning and resource allocation.

Method: Two Phase III cohorts (n=526 and n=640). Labeling: only visits with observable 180-day outcomes; right-censor excluded. Compared five architectures: LSTM, GRU, Cox Proportional Hazards, Random Survival Forest (RSF), and Logistic Regression. For each dataset, selected the smallest risk threshold achieving an 85% sensitivity floor. Assessed discrimination (C-index), calibration (slope and intercept), and PR-AUC; external validation performed. Clinical impact metrics included median time-in-warning (true positives vs false positives) and alerts per 100 patient-visits. Permutation importance identified feature associations. 

Result: GRU and RSF demonstrated high discriminative performance early (C-index ≈ 0.87). In external validation, the GRU achieved better calibration (slope = 0.93, intercept = 0.07) and a PR-AUC of 0.87. Clinical impact analysis reported a median time-in-warning of 151.0 days for true positives (vs. 59.0 days for false positives) and 18.3 alerts per 100 patient-visits. Permutation importance indicated BMI and systolic blood pressure as the strongest associations, consistent with frailty and hemodynamic instability. The study suggests longitudinal routine clinical markers can estimate short-horizon mortality risk in mCRPC and support proactive care planning over several months.

Conclusion: Longitudinal clinical data-enable short-horizon mortality risk estimation in mCRPC with GRU/RSF models showing robust external validation; findings support proactive care planning and monitoring using routinely collected patient data.

Abstract: Metastatic castration-resistant prostate cancer (mCRPC) is a highly aggressive disease with poor prognosis and heterogeneous treatment response. In this work, we developed and externally validated a visit-level 180-day mortality risk model using longitudinal data from two Phase III cohorts (n=526 and n=640). Only visits with observable 180-day outcomes were labeled; right-censored cases were excluded from analysis. We compared five candidate architectures: Long Short-Term Memory, Gated Recurrent Unit (GRU), Cox Proportional Hazards, Random Survival Forest (RSF), and Logistic Regression. For each dataset, we selected the smallest risk-threshold that achieved an 85% sensitivity floor. The GRU and RSF models showed high discrimination capabilities initially (C-index: 87% for both). In external validation, the GRU obtained a higher calibration (slope: 0.93; intercept: 0.07) and achieved an PR-AUC of 0.87. Clinical impact analysis showed a median time-in-warning of 151.0 days for true positives (59.0 days for false positives) and 18.3 alerts per 100 patient-visits. Given late-stage frailty or cachexia and hemodynamic instability, permutation importance ranked BMI and systolic blood pressure as the strongest associations. These results suggest that longitudinal routine clinical markers can estimate short-horizon mortality risk in mCRPC and support proactive care planning over a multi-month window.

</details>


### [107] [Domain Expansion: A Latent Space Construction Framework for Multi-Task Learning](https://arxiv.org/abs/2601.20069)
*Chi-Yao Huang,Khoa Vo,Aayush Atul Verma,Duo Lu,Yezhou Yang*

Main category: cs.LG

TL;DR: Domain Expansion uses an orthogonal pooling mechanism to allocate each objective to a distinct, orthogonal latent subspace, mitigating gradient conflicts and yielding an interpretable, compositional latent space for multi-task learning.


<details>
  <summary>Details</summary>
Motivation: Multi-task learning often suffers from conflicting gradients that corrupt shared representations, leading to latent representation collapse and suboptimal performance across tasks.

Method: Introduce an orthogonal pooling mechanism to construct a latent space where each objective is assigned to a mutually orthogonal subspace, within a unified framework called Domain Expansion; validate on diverse benchmarks (ShapeNet, MPIIGaze, Rotated MNIST) with tasks combining classification, pose, and gaze estimation.

Result: The approach prevents latent collapse, improves task synergy, and yields an explicit, interpretable latent space that enables direct manipulation of concepts.

Conclusion: Orthogonalizing latent subspaces via Domain Expansion provides a principled solution to gradient conflict in multi-task learning and produces a compositional latent representation with enhanced interpretability and controllability.

Abstract: Training a single network with multiple objectives often leads to conflicting gradients that degrade shared representations, forcing them into a compromised state that is suboptimal for any single task--a problem we term latent representation collapse. We introduce Domain Expansion, a framework that prevents these conflicts by restructuring the latent space itself. Our framework uses a novel orthogonal pooling mechanism to construct a latent space where each objective is assigned to a mutually orthogonal subspace. We validate our approach across diverse benchmarks--including ShapeNet, MPIIGaze, and Rotated MNIST--on challenging multi-objective problems combining classification with pose and gaze estimation. Our experiments demonstrate that this structure not only prevents collapse but also yields an explicit, interpretable, and compositional latent space where concepts can be directly manipulated.

</details>


### [108] [Techno-economic optimization of a heat-pipe microreactor, part II: multi-objective optimization analysis](https://arxiv.org/abs/2601.20079)
*Paul Seurin,Dean Price*

Main category: cs.LG

TL;DR: Extends LCOE-driven optimization of heat-pipe microreactors to multi-objective Pareto optimization using PEARL; balances minimizing rod-integrated peaking factor (FΔh) and LCOE under safety constraints across three cost scenarios, revealing design strategies and surrogate-model limitations.


<details>
  <summary>Details</summary>
Motivation: Remotely deployed, transportable nuclear power requires balancing safety/performance with economics. Building on a surrogate RL framework to capture trade-offs beyond single-objective cost minimization.

Method: PEARL-based multi-objective reinforcement-learning optimization with surrogate models. Objectives: minimize FΔh and LCOE under safety/operational constraints. Design variables include geometry (solid moderator radius, pin pitch, drum coating angle, fuel height) and materials. Evaluate three cost scenarios with varying reflector and fuel costs.

Result: FΔh can be lowered by reducing solid moderator radius, pin pitch, and drum coating angle while increasing fuel height. Across all cost scenarios, four strategies consistently optimize LCOE: (1) minimize axial reflector contribution when costly, (2) reduce dependence on control drums, (3) substitute expensive TRISO fuel with axial-reflector material priced at graphite level, (4) maximize fuel burnup. PEARL shows promise in navigating trade-offs, but surrogate predictions diverge from full-order simulations, indicating room for improvement via constraint relaxation and better surrogates.

Conclusion: PEARL is a promising tool for multi-objective design optimization of heat-pipe microreactors, capable of revealing robust design strategies across scenarios. However, current surrogate-model limitations and the need for improved constraint handling warrant further work and validation against high-fidelity simulations.

Abstract: Heat-pipe microreactors (HPMRs) are compact and transportable nuclear power systems exhibiting inherent safety, well-suited for deployment in remote regions where access is limited and reliance on costly fossil fuels is prevalent. In prior work, we developed a design optimization framework that incorporates techno-economic considerations through surrogate modeling and reinforcement learning (RL)-based optimization, focusing solely on minimizing the levelized cost of electricity (LCOE) by using a bottom-up cost estimation approach. In this study, we extend that framework to a multi-objective optimization that uses the Pareto Envelope Augmented with Reinforcement Learning (PEARL) algorithm. The objectives include minimizing both the rod-integrated peaking factor ($F_{Δh}$) and LCOE -- subject to safety and operational constraints. We evaluate three cost scenarios: (1) a high-cost axial and drum reflectors, (2) a low-cost axial reflector, and (3) low-cost axial and drum reflectors. Our findings indicate that reducing the solid moderator radius, pin pitch, and drum coating angle -- all while increasing the fuel height -- effectively lowers $F_{Δh}$. Across all three scenarios, four key strategies consistently emerged for optimizing LCOE: (1) minimizing the axial reflector contribution when costly, (2) reducing control drum reliance, (3) substituting expensive tri-structural isotropic (TRISO) fuel with axial reflector material priced at the level of graphite, and (4) maximizing fuel burnup. While PEARL demonstrates promise in navigating trade-offs across diverse design scenarios, discrepancies between surrogate model predictions and full-order simulations remain. Further improvements are anticipated through constraint relaxation and surrogate development, constituting an ongoing area of investigation.

</details>


### [109] [Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery](https://arxiv.org/abs/2601.20088)
*Meng Xin,Sweta Priyadarshi,Jingyu Xin,Bilal Kartal,Aditya Vavre,Asma Kuriparambil Thekkumpate,Zijia Chen,Ameya Sunil Mahabaleshwarkar,Ido Shahaf,Akhiad Bercovich,Kinjal Patel,Suguna Varshini Velury,Chenjie Luo,Zhiyu Cheng,Jenny Chen,Chen-Han Yu,Wei Ping,Oleg Rybakov,Nima Tajbakhsh,Oluwatobi Olabiyi,Dusan Stosic,Di Wu,Song Han,Eric Chung,Sharath Turuvekere Sreenivas,Bryan Catanzaro,Yoshi Suhara,Tijmen Blankevoort,Huizi Mao*

Main category: cs.LG

TL;DR: Introduces quantization-aware distillation (QAD) to recover accuracy of quantized LLMs/VLMs (NVFP4) via KL distillation from a full-precision teacher; robust to multi-stage post-training and data quality, outperforming QAT in these pipelines; validated on multiple Nemotron variants and Llama Nemotron Super, achieving near-BF16 accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of maintaining accuracy when quantizing large models with low precision (NVFP4), especially in complex post-training pipelines where QAT is unstable and data quality may be limited.

Method: Use a KL-divergence distillation loss from a full-precision teacher to a quantized student during quantization-aware distillation (QAD); apply in post-training learning pipelines (SFT, RL, and model merging) with minimal reliance on full training data.

Result: Empirically recover accuracy to near BF16 across several post-trained models including AceReason Nemotron, Nemotron 3 Nano, Nemotron Nano V2, Nemotron Nano V2 VL, and Llama Nemotron Super v1.

Conclusion: QAD provides stable, data-efficient accuracy recovery for NVFP4 quantization in modern LLM/VLM pipelines and offers a practical alternative to conventional QAT in multi-stage post-training contexts.

Abstract: This technical report presents quantization-aware distillation (QAD) and our best practices for recovering accuracy of NVFP4-quantized large language models (LLMs) and vision-language models (VLMs). QAD distills a full-precision teacher model into a quantized student model using a KL divergence loss. While applying distillation to quantized models is not a new idea, we observe key advantages of QAD for today's LLMs: 1. It shows remarkable effectiveness and stability for models trained through multi-stage post-training pipelines, including supervised fine-tuning (SFT), reinforcement learning (RL), and model merging, where traditional quantization-aware training (QAT) suffers from engineering complexity and training instability; 2. It is robust to data quality and coverage, enabling accuracy recovery without full training data. We evaluate QAD across multiple post-trained models including AceReason Nemotron, Nemotron 3 Nano, Nemotron Nano V2, Nemotron Nano V2 VL (VLM), and Llama Nemotron Super v1, showing consistent recovery to near-BF16 accuracy.

</details>


### [110] [In-Context Reinforcement Learning From Suboptimal Historical Data](https://arxiv.org/abs/2601.20116)
*Juncheng Dong,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: Proposes Decision Importance Transformer (DIT) for in-context reinforcement learning with offline suboptimal data. It first learns a transformer value function for advantage estimation and then trains a transformer policy via a weighted likelihood loss, using the value to reweight actions. Empirically, DIT outperforms baselines on bandits and MDPs when offline data is suboptimal.


<details>
  <summary>Details</summary>
Motivation: To overcome suboptimality when training autoregressive policies from offline trajectories. Standard autoregressive training reduces to imitation learning in the presence of suboptimal data, so DIT introduces an in-context actor-critic-style approach to reweight actions using a transformer-based value function.

Method: 1) Train a transformer-based value function to estimate the advantage of behavior policies from suboptimal trajectories. 2) Train a transformer-based policy with a weighted maximum likelihood objective, where weights are derived from the estimated advantages to guide the policy toward optimal behavior.

Result: Extensive experiments on both bandit and MDP settings show that DIT achieves superior performance, particularly when offline data is suboptimal.

Conclusion: DIT demonstrates that in-context reinforcement learning with a transformer can emulate actor-critic learning, effectively leveraging suboptimal offline data to produce strong policies across tasks.

Abstract: Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.

</details>


### [111] [A Reinforcement Learning Based Universal Sequence Design for Polar Codes](https://arxiv.org/abs/2601.20118)
*David Kin Wai Ho,Arman Fazeli,Mohamad M. Mansour,Louay M. A. Jalloul*

Main category: cs.LG

TL;DR: An RL-based universal sequence design for Polar codes that scales to N up to 2048, achieving competitive performance with the 5G NR sequence across all (N,K) and up to 0.2 dB gain over the beta-expansion baseline at N=2048, aided by physics-informed constraints using the universal partial order of Polar codes, limited lookahead via the weak long-term influence of decisions, and joint multi-configuration optimization.


<details>
  <summary>Details</summary>
Motivation: To enable scalable, adaptable Polar-code sequence design for 6G that remains competitive across diverse channel conditions and decoding strategies and is suitable for standardization.

Method: A reinforcement learning–based universal sequence design framework. It incorporates physics-informed learning by leveraging the universal partial order of Polar codes, exploits the weak long-term influence of decisions to limit lookahead, and performs joint optimization across multiple configurations to improve learning efficiency; designed to scale to code lengths up to 2048.

Result: The method achieves competitive performance relative to the 5G NR sequence across all (N,K) configurations supported in 5G, and yields up to a 0.2 dB gain over the beta-expansion baseline at N=2048.

Conclusion: The work demonstrates scalable, adaptable Polar-code sequence design via learning, with key elements enabling scale (physics-informed constraints, bounded lookahead, multi-configuration optimization) and potential applicability to standardization workflows.

Abstract: To advance Polar code design for 6G applications, we develop a reinforcement learning-based universal sequence design framework that is extensible and adaptable to diverse channel conditions and decoding strategies. Crucially, our method scales to code lengths up to $2048$, making it suitable for use in standardization. Across all $(N,K)$ configurations supported in 5G, our approach achieves competitive performance relative to the NR sequence adopted in 5G and yields up to a 0.2 dB gain over the beta-expansion baseline at $N=2048$. We further highlight the key elements that enabled learning at scale: (i) incorporation of physical law constrained learning grounded in the universal partial order property of Polar codes, (ii) exploitation of the weak long term influence of decisions to limit lookahead evaluation, and (iii) joint multi-configuration optimization to increase learning efficiency.

</details>


### [112] [Going NUTS with ADVI: Exploring various Bayesian Inference techniques with Facebook Prophet](https://arxiv.org/abs/2601.20120)
*Jovan Krajevski,Biljana Tojtovska Ribarski*

Main category: cs.LG

TL;DR: PyMC reimplementation of Facebook Prophet enables alternative Bayesian inference (MCMC, MAP, VI) and systematic comparison on time-series forecasting, with focus on sampling, convergence, metrics, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Prophet's default inference methods and flexible API constrain experimentation with custom Bayesian models; a PyMC-based implementation provides extensibility to test different inference techniques.

Method: Reimplement Prophet in PyMC, integrate full Bayesian inference (MCMC via NUTS, MAP optimization, Variational Inference), apply to time-series forecasting tasks, and analyze sampling, convergence diagnostics, forecasting metrics, computational efficiency, and potential issues.

Result: Provides a framework and empirical discussion of how different Bayesian inference methods perform in Prophet-like models, highlighting practical considerations and potential pitfalls; sets stage for future improvements.

Conclusion: A PyMC-based Prophet extends flexibility for Bayesian experimentation, enabling systematic comparison of inference techniques and highlighting areas for refinement and future work.

Abstract: Since its introduction, Facebook Prophet has attracted positive attention from both classical statisticians and the Bayesian statistics community. The model provides two built-in inference methods: maximum a posteriori estimation using the L-BFGS-B algorithm, and Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS). While exploring various time-series forecasting problems using Bayesian inference with Prophet, we encountered limitations stemming from the inability to apply alternative inference techniques beyond those provided by default. Additionally, the fluent API design of Facebook Prophet proved insufficiently flexible for implementing our custom modeling ideas. To address these shortcomings, we developed a complete reimplementation of the Prophet model in PyMC, which enables us to extend the base model and evaluate and compare multiple Bayesian inference methods. In this paper, we present our PyMC-based implementation and analyze in detail the implementation of different Bayesian inference techniques. We consider full MCMC techniques, MAP estimation and Variational inference techniques on a time-series forecasting problem. We discuss in details the sampling approach, convergence diagnostics, forecasting metrics as well as their computational efficiency and detect possible issues which will be addressed in our future work.

</details>


### [113] [Membership Inference Attacks Against Fine-tuned Diffusion Language Models](https://arxiv.org/abs/2601.20125)
*Yuetian Chen,Kaiyuan Zhang,Yuntao Du,Edoardo Stoppa,Charles Fleming,Ashish Kundu,Bruno Ribeiro,Ninghui Li*

Main category: cs.LG

TL;DR: Introduces SAMA, a robust subset-aggregated membership attack against Diffusion Language Models (DLMs) exploiting multiple mask configurations; achieves significant improvements over baselines in AUC, especially at low FPR, revealing new privacy vulnerabilities in DLMs.


<details>
  <summary>Details</summary>
Motivation: Assess whether the bidirectional, mask-based inference in DLMs introduces new privacy leakage pathways not captured by attacks on autoregressive models, and quantify the effectiveness of such attacks.

Method: Propose SAMA: sample masked subsets across progressive densities, apply sign-based statistics resilient to heavy-tailed noise, and use inverse-weighted aggregation to prioritize signals from sparser masks, converting sparse memorization signals into a robust voting mechanism.

Result: Empirical evaluation on nine datasets showing SAMA attains ~30% relative AUC improvement over the best baseline, with up to 8x gains at low false positive rates.

Conclusion: DLMs exhibit significant, previously underappreciated privacy vulnerabilities; defenses tailored to the diffusion, mask-based setting are needed to protect against such subset-based attacks.

Abstract: Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the first systematic investigation of MIA vulnerabilities in DLMs. Unlike the autoregressive models' single fixed prediction pattern, DLMs' multiple maskable configurations exponentially increase attack opportunities. This ability to probe many independent masks dramatically improves detection chances. To exploit this, we introduce SAMA (Subset-Aggregated Membership Attack), which addresses the sparse signal challenge through robust aggregation. SAMA samples masked subsets across progressive densities and applies sign-based statistics that remain effective despite heavy-tailed noise. Through inverse-weighted aggregation prioritizing sparse masks' cleaner signals, SAMA transforms sparse memorization detection into a robust voting mechanism. Experiments on nine datasets show SAMA achieves 30% relative AUC improvement over the best baseline, with up to 8 times improvement at low false positive rates. These findings reveal significant, previously unknown vulnerabilities in DLMs, necessitating the development of tailored privacy defenses.

</details>


### [114] [Scaling Next-Brain-Token Prediction for MEG](https://arxiv.org/abs/2601.20138)
*Richard Csaky*

Main category: cs.LG

TL;DR: A large autoregressive model for source-space MEG enabling long-context generation across datasets, using a vector-quantizer and a Qwen2.5-VL backbone; evaluated with three long-horizon tests and cross-dataset validation; code released.


<details>
  <summary>Details</summary>
Motivation: Address long-context modeling and cross-dataset generalization in MEG source-space signals, and provide robust evaluation for generated brain token sequences across scanners/datasets.

Method: Preprocess multichannel MEG with a SEANet-style vector quantizer to produce a flattened token stream; train a from-scratch Qwen2.5-VL backbone to predict next brain token; generate minutes of MEG from up to one minute context; train on CamCAN and Omega; test on held-out MOUS; three tests: on-manifold stability (generated drift vs real distribution), conditional specificity with prompt-swap controls using neurophysiological metrics.

Result: Generations remain relatively stable over long rollouts; trajectories closer to correct continuation than swapped controls; cross-dataset generalization observed on held-out MOUS; code provided.

Conclusion: Demonstrates feasibility of long-context MEG generation and cross-dataset transfer; establishes evaluation framework for generated neurophysiological data; enables scalable generation of MEG with preserved structure.

Abstract: We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce three task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: https://github.com/ricsinaruto/brain-gen.

</details>


### [115] [Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning](https://arxiv.org/abs/2601.20154)
*Bo Dai,Na Li,Dale Schuurmans*

Main category: cs.LG

TL;DR: A spectral-analysis-based unified framework for self-supervised representation learning, linking diverse objectives to spectral properties to enable principled understanding and efficient algorithm design.


<details>
  <summary>Details</summary>
Motivation: SSL has many objectives and lacking a unifying theoretical foundation, hindering principled analysis, comparison, and practical deployment.

Method: Analyse representation sufficiency from a spectral perspective, uncover the spectral essence of existing SSL algorithms, and formulate a unified framework for understanding and developing SSL methods.

Result: Identifies a spectral basis for the success of SSL methods and provides a unified framework that clarifies connections among objectives, guiding the design of efficient, principled algorithms.

Conclusion: A principled, spectral foundation for representation learning that supports theoretical understanding and practical algorithm development in SSL.

Abstract: Self-supervised learning (SSL) have improved empirical performance by unleashing the power of unlabeled data for practical applications. Specifically, SSL extracts the representation from massive unlabeled data, which will be transferred to a plenty of down streaming tasks with limited data. The significant improvement on diverse applications of representation learning has attracted increasing attention, resulting in a variety of dramatically different self-supervised learning objectives for representation extraction, with an assortment of learning procedures, but the lack of a clear and unified understanding. Such an absence hampers the ongoing development of representation learning, leaving a theoretical understanding missing, principles for efficient algorithm design unclear, and the use of representation learning methods in practice unjustified. The urgency for a unified framework is further motivated by the rapid growth in representation learning methods. In this paper, we are therefore compelled to develop a principled foundation of representation learning. We first theoretically investigate the sufficiency of the representation from a spectral representation view, which reveals the spectral essence of the existing successful SSL algorithms and paves the path to a unified framework for understanding and analysis. Such a framework work also inspires the development of more efficient and easy-to-use representation learning algorithms with principled way in real-world applications.

</details>


### [116] [PASS: Ambiguity Guided Subsets for Scalable Classical and Quantum Constrained Clustering](https://arxiv.org/abs/2601.20157)
*Pedro Chumpitaz-Flores,My Duong,Ying Mao,Kaixun Hua*

Main category: cs.LG

TL;DR: PASS is a scalable pairwise-constraint clustering framework that uses pseudo-points for must-link constraints and two selectors to build a high-information subset, achieving competitive SSE with lower cost than exact/penalty methods.


<details>
  <summary>Details</summary>
Motivation: To address scalability and performance challenges in pairwise-constrained clustering, especially in niche domains like quantum or quantum-hybrid clustering, while maintaining ML/CL constraint satisfaction.

Method: COLLAPSE must-link constraints into pseudo-points; employ two selectors: (1) constraint-aware margin rule that gathers near-boundary points and all detected cannot-link violations; (2) information-geometric rule that scores points via Fisher-Rao distance derived from soft assignment posteriors and selects the highest-information subset under a budget.

Result: Empirically competitive SSE across diverse benchmarks at substantially lower computational cost than exact or penalty-based approaches; effective in regimes where prior methods fail.

Conclusion: PASS offers a scalable, constraint-respecting clustering framework that maintains ML/CL satisfaction while delivering high-quality partitions at reduced cost, broadening applicability to domains with limited data or expensive labeling.

Abstract: Pairwise-constrained clustering augments unsupervised partitioning with side information by enforcing must-link (ML) and cannot-link (CL) constraints between specific samples, yielding labelings that respect known affinities and separations. However, ML and CL constraints add an extra layer of complexity to the clustering problem, with current methods struggling in data scalability, especially in niche applications like quantum or quantum-hybrid clustering. We propose PASS, a pairwise-constraints and ambiguity-driven subset selection framework that preserves ML and CL constraints satisfaction while allowing scalable, high-quality clustering solution. PASS collapses ML constraints into pseudo-points and offers two selectors: a constraint-aware margin rule that collects near-boundary points and all detected CL violations, and an information-geometric rule that scores points via a Fisher-Rao distance derived from soft assignment posteriors, then selects the highest-information subset under a simple budget. Across diverse benchmarks, PASS attains competitive SSE at substantially lower cost than exact or penalty-based methods, and remains effective in regimes where prior approaches fail.

</details>


### [117] [What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering](https://arxiv.org/abs/2601.20164)
*Jim Maar,Denis Paperno,Callum Stuart McDougall,Neel Nanda*

Main category: cs.LG

TL;DR: Proposes a lightweight, scalable method to study implicit planning in LLMs by steering end-of-line cues to influence intermediate tokens, showing planning signals across models from about 1B params in rhyme and QA tasks.


<details>
  <summary>Details</summary>
Motivation: Fill the gap for simple, scalable techniques to detect and study implicit planning in language models, with implications for safety and controllability.

Method: Apply minimal perturbation: channel a directional vector at the end of a preceding line to steer the generation of intermediate tokens toward a predicted rhyme or answer; test on rhyme poetry generation and question answering across multiple models; assess universality and parameter threshold.

Result: End-of-line steering can bias intermediate token choices, shaping the eventual rhyme (e.g., a specific suffix) or the final answer (e.g., 'whale'); implicit planning is observed across models, including smaller ones (~1B parameters); methodology scales across models.

Conclusion: Implicit planning appears to be a universal mechanism in LLMs, not confined to large models; the proposed method offers a direct, scalable approach for evaluating planning abilities, with broader implications for AI safety and control.

Abstract: Prior work suggests that language models, while trained on next token prediction, show implicit planning behavior: they may select the next token in preparation to a predicted future token, such as a likely rhyming word, as supported by a prior qualitative study of Claude 3.5 Haiku using a cross-layer transcoder. We propose much simpler techniques for assessing implicit planning in language models. With case studies on rhyme poetry generation and question answering, we demonstrate that our methodology easily scales to many models. Across models, we find that the generated rhyme (e.g. "-ight") or answer to a question ("whale") can be manipulated by steering at the end of the preceding line with a vector, affecting the generation of intermediate tokens leading up to the rhyme or answer word. We show that implicit planning is a universal mechanism, present in smaller models than previously thought, starting from 1B parameters. Our methodology offers a widely applicable direct way to study implicit planning abilities of LLMs. More broadly, understanding planning abilities of language models can inform decisions in AI safety and control.

</details>


### [118] [Local Duality for Sparse Support Vector Machines](https://arxiv.org/abs/2601.20170)
*Penghe Zhang,Naihua Xiu,Houduo Qi*

Main category: cs.LG

TL;DR: A theoretical framework for sparse SVM (SSVM) with an l0 penalty is developed, establishing local duality with 0/1-loss SVM, linking to hinge/ramp SVMs, and yielding practical guidance and convergence results supported by experiments.


<details>
  <summary>Details</summary>
Motivation: To justify SSVM by connecting its l0-based dual to well-studied SVM losses, addressing the lack of theoretical grounding when introducing a cardinality penalty into the SVM dual.

Method: Develop a local duality theory for the l0-penalized SVM, prove equivalence to the dual of the 0/1-loss SVM, establish a local representer theorem, analyze how local SSVM solutions relate to hSVM and rSVM, and derive convergence relations between global hSVM solutions and local 0/1 SVM solutions; supplement with numerical experiments on real datasets.

Result: SSVM is shown to be exactly the dual of the 0/1-loss SVM; a linear representer theorem holds for local solutions; hyperparameter guidance for hSVM and rSVM is obtained from SSVM's local solution; under certain conditions, sequences of global hSVM solutions converge to a local 0/1 SVM solution, and a local 0/1 SVM minimizer is also a local minimizer of rSVM; empirical tests indicate potential advantages of SSVM with locally favorable solutions.

Conclusion: The l0-based SSVM provides a principled local-duality framework that bridges hSVM, rSVM, and 0/1-loss SVM, explains observed empirical benefits, and offers practical guidance for hyperparameter tuning and solution quality.

Abstract: Due to the rise of cardinality minimization in optimization, sparse support vector machines (SSVMs) have attracted much attention lately and show certain empirical advantages over convex SVMs. A common way to derive an SSVM is to add a cardinality function such as $\ell_0$-norm to the dual problem of a convex SVM. However, this process lacks theoretical justification. This paper fills the gap by developing a local duality theory for such an SSVM formulation and exploring its relationship with the hinge-loss SVM (hSVM) and the ramp-loss SVM (rSVM). In particular, we prove that the derived SSVM is exactly the dual problem of the 0/1-loss SVM, and the linear representer theorem holds for their local solutions. The local solution of SSVM also provides guidelines on selecting hyperparameters of hSVM and rSVM. {Under specific conditions, we show that a sequence of global solutions of hSVM converges to a local solution of 0/1-loss SVM. Moreover, a local minimizer of 0/1-loss SVM is a local minimizer of rSVM.} This explains why a local solution induced by SSVM outperforms hSVM and rSVM in the prior empirical study. We further conduct numerical tests on real datasets and demonstrate potential advantages of SSVM by working with locally nice solutions proposed in this paper.

</details>


### [119] [Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence Functions Reveal About Robust Generalization](https://arxiv.org/abs/2601.20172)
*James Amarel,Robyn Miller,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Alexei Skurikhin,Earl Lawrence,Gerd J. Kunde*

Main category: cs.LG

TL;DR: A gradient-based, orbit-wise diagnostic quantifies how neural PDE emulators internalize physical symmetries by measuring metric-weighted loss-gradient overlap along symmetry group orbits, linking learning dynamics to symmetry-based generalization.


<details>
  <summary>Details</summary>
Motivation: To move beyond forward-pass symmetry checks and directly diagnose whether training dynamics align with symmetry-related configurations, thereby understanding how learned solvers generalize across symmetry transformations.

Method: Define an influence-based diagnostic as the metric-weighted overlap of loss gradients evaluated along group orbits; analyze local loss-landscape geometry and gradient coherence to assess coupling between learning dynamics and symmetry-related states; apply to autoregressive fluid-flow emulators.

Result: Orbit-wise gradient coherence correlates with generalization over symmetry transformations and signals when training lands in a symmetry-compatible basin; establishes a practical tool to test whether a surrogate model has internalized the symmetry structure of the target operator.

Conclusion: Provides a novel, quantitative mechanism to evaluate symmetry internalization in surrogate models of PDE solution operators, enabling diagnosis and potentially guiding symmetry-aware training and model selection.

Abstract: We study how neural emulators of partial differential equation solution operators internalize physical symmetries by introducing an influence-based diagnostic that measures the propagation of parameter updates between symmetry-related states, defined as the metric-weighted overlap of loss gradients evaluated along group orbits. This quantity probes the local geometry of the learned loss landscape and goes beyond forward-pass equivariance tests by directly assessing whether learning dynamics couple physically equivalent configurations. Applying our diagnostic to autoregressive fluid flow emulators, we show that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects a symmetry compatible basin. The result is a novel technique for evaluating if surrogate models have internalized symmetry properties of the known solution operator.

</details>


### [120] [MAPLE: Self-supervised Learning-Enhanced Nonlinear Dimensionality Reduction for Visual Analysis](https://arxiv.org/abs/2601.20173)
*Zeyang Huang,Takanori Fujiwara,Angelos Chatzimparmpas,Wandrille Duchemin,Andreas Kerren*

Main category: cs.LG

TL;DR: MAPLE improves UMAP by using self-supervised MMCRs to better capture curved, high-variance manifolds, yielding clearer clusters with similar compute cost.


<details>
  <summary>Details</summary>
Motivation: To enhance nonlinear manifold modeling in dimensionality reduction, addressing UMAP limitations on complex, high-variance manifolds common in biology and image data.

Method: Introduce MAPLE with maximum manifold capacity representations (MMCRs) and a self-supervised learning framework to compress intra-cluster variances and amplify inter-cluster differences, thereby better encoding low-dimensional manifold geometry.

Result: Qualitative and quantitative evaluations show clearer cluster separations and finer subcluster resolution compared to UMAP, while maintaining comparable computational cost.

Conclusion: MAPLE provides improved manifold modeling and clustering resolution over UMAP on high-dimensional, curved manifolds, with similar runtime efficiency.

Abstract: We present a new nonlinear dimensionality reduction method, MAPLE, that enhances UMAP by improving manifold modeling. MAPLE employs a self-supervised learning approach to more efficiently encode low-dimensional manifold geometry. Central to this approach are maximum manifold capacity representations (MMCRs), which help untangle complex manifolds by compressing variances among locally similar data points while amplifying variance among dissimilar data points. This design is particularly effective for high-dimensional data with substantial intra-cluster variance and curved manifold structures, such as biological or image data. Our qualitative and quantitative evaluations demonstrate that MAPLE can produce clearer visual cluster separations and finer subcluster resolution than UMAP while maintaining comparable computational cost.

</details>


### [121] [NeuraLSP: An Efficient and Rigorous Neural Left Singular Subspace Preconditioner for Conjugate Gradient Methods](https://arxiv.org/abs/2601.20174)
*Alexander Benanti,Xi Han,Hong Qin*

Main category: cs.LG

TL;DR: NeuraLSP introduces a neural preconditioner that encodes spectral information from the near-nullspace of the system matrix into a fixed low-rank operator, yielding robust convergence and up to 53% speedups in PDE solvers, with theoretical guarantees and broad empirical validation.


<details>
  <summary>Details</summary>
Motivation: Existing graph-aggregation preconditioners suffer from rank inflation and suboptimal convergence when converting PDE discretizations into graphs; a loss that directly leverages the left singular subspace of near-nullspace vectors can produce a more effective, spectrally faithful preconditioner.

Method: Develop NeuraLSP: a neural preconditioner paired with a novel loss metric based on left singular vectors of the system matrix's near-nullspace; compress spectral information into a fixed low-rank operator; provide theoretical guarantees for the loss and empirical robustness.

Result: Demonstrates up to 53% speedup over baseline solvers across diverse PDE families; shows robustness to rank inflation; validates theoretical guarantees via experiments.

Conclusion: The approach combines spectral-aware neural preconditioning with low-rank compression to achieve robust, efficient PDE solves, addressing limitations of graph-based methods and offering broad applicability.

Abstract: Numerical techniques for solving partial differential equations (PDEs) are integral for many fields across science and engineering. Such techniques usually involve solving large, sparse linear systems, where preconditioning methods are critical. In recent years, neural methods, particularly graph neural networks (GNNs), have demonstrated their potential through accelerated convergence. Nonetheless, to extract connective structures, existing techniques aggregate discretized system matrices into graphs, and suffer from rank inflation and a suboptimal convergence rate. In this paper, we articulate NeuraLSP, a novel neural preconditioner combined with a novel loss metric that leverages the left singular subspace of the system matrix's near-nullspace vectors. By compressing spectral information into a fixed low-rank operator, our method exhibits both theoretical guarantees and empirical robustness to rank inflation, affording up to a 53% speedup. Besides the theoretical guarantees for our newly-formulated loss function, our comprehensive experimental results across diverse families of PDEs also substantiate the aforementioned theoretical advances.

</details>


### [122] [Causal-Driven Feature Evaluation for Cross-Domain Image Classification](https://arxiv.org/abs/2601.20176)
*Chen Cheng,Ang Li*

Main category: cs.LG

TL;DR: A causal framework for OOD classification that assesses representations by their necessity and sufficiency across domains using a segment-level evaluation, yielding improved OOD performance on multi-domain benchmarks.


<details>
  <summary>Details</summary>
Motivation: Invariance across domains does not guarantee causal relevance for prediction. OOD generalization requires representations that are causally effective under distribution shifts, motivating a causal perspective beyond domain-invariant features.

Method: Introduce a segment-level causal evaluation framework that directly measures the necessity and sufficiency of learned representations for prediction across domains, providing a faithful assessment of causal effectiveness under distribution shift.

Result: Experiments on multi-domain benchmarks show consistent improvements in OOD performance, especially under challenging domain shifts, supporting the value of causal evaluation for robust generalization.

Conclusion: Causal evaluation offers a more faithful criterion than invariance for robust OOD generalization; segment-level measurement of causal effectiveness enhances reliability under distribution shifts.

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in real-world classification, where test distributions often differ substantially from training data. Most existing approaches pursue domain-invariant representations, implicitly assuming that invariance implies reliability. However, features that are invariant across domains are not necessarily causally effective for prediction.
  In this work, we revisit OOD classification from a causal perspective and propose to evaluate learned representations based on their necessity and sufficiency under distribution shift. We introduce an explicit segment-level framework that directly measures causal effectiveness across domains, providing a more faithful criterion than invariance alone.
  Experiments on multi-domain benchmarks demonstrate consistent improvements in OOD performance, particularly under challenging domain shifts, highlighting the value of causal evaluation for robust generalization.

</details>


### [123] [On the Computational Complexity of Performative Prediction](https://arxiv.org/abs/2601.20180)
*Ioannis Anagnostides,Rohan Chauhan,Ioannis Panageas,Tuomas Sandholm,Jingming Yan*

Main category: cs.LG

TL;DR: PPAD-hardness phase transition for performative prediction; computing an ε-performatively stable point is PPAD-complete even at ρ≈1; results extend to convex domains and show PLS-hardness for strategic local optimum.


<details>
  <summary>Details</summary>
Motivation: Investigate computational limits of performative prediction where deployment shifts the underlying data distribution, focusing on the hard regime ρ>1 and near the critical boundary ρ=1, to characterize tractability vs intractability and connect to equilibria and variational-inequality notions.

Method: Provide complexity-theoretic reductions establishing PPAD-hardness for ε-performatively stable points, extend hardness to general convex domains, and show PLS-hardness for strategic local optima; analyze a simple quadratic-loss setting with linear distribution shifts as a concrete instance.

Result: Hardness results: computing ε-performatively stable points is PPAD-complete (and hence as hard as Nash equilibria in general-sum games); this hardness persists even when ρ=1+O(ε) and in a quadratic-loss, linear-shift setting; extended to general convex domains; strategic local optimum computing is PLS-hard.

Conclusion: There is a sharp tractable-vs-intractable phase transition in performative prediction. In the strong-responding regime (ρ≥1), efficient computation of performatively stable points corresponds to solving PPAD-complete problems (and PLS-hard problems for strategic local optima), linking performative prediction to Nash equilibria and variational-inequality hardness and signaling substantial computational barriers for algorithm design in these settings.

Abstract: Performative prediction captures the phenomenon where deploying a predictive model shifts the underlying data distribution. While simple retraining dynamics are known to converge linearly when the performative effects are weak ($ρ< 1$), the complexity in the regime $ρ> 1$ was hitherto open. In this paper, we establish a sharp phase transition: computing an $ε$-performatively stable point is PPAD-complete -- and thus polynomial-time equivalent to Nash equilibria in general-sum games -- even when $ρ= 1 + O(ε)$. This intractability persists even in the ostensibly simple setting with a quadratic loss function and linear distribution shifts. One of our key technical contributions is to extend this PPAD-hardness result to general convex domains, which is of broader interest in the complexity of variational inequalities. Finally, we address the special case of strategic classification, showing that computing a strategic local optimum is PLS-hard.

</details>


### [124] [Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery](https://arxiv.org/abs/2601.20193)
*Zhipeng Zhang,Wenting Ma,Kai Li,Meng Guo,Lei Yang,Wei Yu,Hongji Cui,Yichen Zhang,Mo Zhang,Jinzhe Lin,Zhenjie Yao*

Main category: cs.LG

TL;DR: A meta-cognitive RL framework introduces a meta-trust variable driven by Value Prediction Error Stability (VPES) to regulate learning under reward corruption, enabling fail-safe regulation and trust recovery.


<details>
  <summary>Details</summary>
Motivation: Robust RL often ignores the reliability of its own learning process, leading to overconservative behavior or catastrophic failures as uncertainty accumulates. There is a need for internal reliability signals to guide learning dynamics.

Method: Implement a meta-trust variable influenced by VPES that modulates learning updates via fail-safe regulation and a recovery mechanism to gradually restore trust after disturbances.

Result: On continuous-control tasks with corrupted rewards, the approach achieves higher average returns and significantly reduces late-stage training failures compared with strong robustness baselines.

Conclusion: Incorporating meta-cognitive signals (meta-trust via VPES) improves robustness and stability of RL by monitoring and regulating the learning process itself, enabling recovery after perturbations.

Abstract: Robust reinforcement learning methods typically focus on suppressing unreliable experiences or corrupted rewards, but they lack the ability to reason about the reliability of their own learning process. As a result, such methods often either overreact to noise by becoming overly conservative or fail catastrophically when uncertainty accumulates.
  In this work, we propose a meta-cognitive reinforcement learning framework that enables an agent to assess, regulate, and recover its learning behavior based on internally estimated reliability signals. The proposed method introduces a meta-trust variable driven by Value Prediction Error Stability (VPES), which modulates learning dynamics via fail-safe regulation and gradual trust recovery.
  Experiments on continuous-control benchmarks with reward corruption demonstrate that recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.

</details>


### [125] [DeRaDiff: Denoising Time Realignment of Diffusion Models](https://arxiv.org/abs/2601.20198)
*Ratnavibusena Don Shahain Manujith,Yang Zhang,Teoh Tze Tzun,Kenji Kawaguchi*

Main category: cs.LG

TL;DR: DeRaDiff enables on-the-fly decoding-time realignment to emulate diffusion models trained at different KL regularization strengths, avoiding expensive training sweeps.


<details>
  <summary>Details</summary>
Motivation: Choosing the KL regularization strength in diffusion-model alignment is hard: too strong leads to under-alignment; too weak risks reward hacking; conventional approaches require expensive multi-strength alignment sweeps.

Method: Denoising time realignment: after one alignment, replace the reverse-step posterior with a geometric mixture of the aligned and reference posteriors, yielding a closed-form update under common schedulers; uses a single tunable lambda to control the mix; no additional training; extends decoding-time realignment from language to diffusion models.

Result: Empirically, DeRaDiff closely approximates models aligned at various regularization strengths across text-image alignment and image-quality metrics, while substantially reducing computational costs by eliminating alignment sweeps.

Conclusion: DeRaDiff provides an efficient, practical decoding-time tool to search for the optimal regularization strength during sampling, enabling diffusion-model alignment with human preferences without retraining.

Abstract: Recent advances align diffusion models with human preferences to increase aesthetic appeal and mitigate artifacts and biases. Such methods aim to maximize a conditional output distribution aligned with higher rewards whilst not drifting far from a pretrained prior. This is commonly enforced by KL (Kullback Leibler) regularization. As such, a central issue still remains: how does one choose the right regularization strength? Too high of a strength leads to limited alignment and too low of a strength leads to "reward hacking". This renders the task of choosing the correct regularization strength highly non-trivial. Existing approaches sweep over this hyperparameter by aligning a pretrained model at multiple regularization strengths and then choose the best strength. Unfortunately, this is prohibitively expensive. We introduce DeRaDiff, a denoising time realignment procedure that, after aligning a pretrained model once, modulates the regularization strength during sampling to emulate models trained at other regularization strengths without any additional training or finetuning. Extending decoding-time realignment from language to diffusion models, DeRaDiff operates over iterative predictions of continuous latents by replacing the reverse step reference distribution by a geometric mixture of an aligned and reference posterior, thus giving rise to a closed form update under common schedulers and a single tunable parameter, lambda, for on the fly control. Our experiments show that across multiple text image alignment and image-quality metrics, our method consistently provides a strong approximation for models aligned entirely from scratch at different regularization strengths. Thus, our method yields an efficient way to search for the optimal strength, eliminating the need for expensive alignment sweeps and thereby substantially reducing computational costs.

</details>


### [126] [Minimum-Cost Network Flow with Dual Predictions](https://arxiv.org/abs/2601.20203)
*Zhiyang Chen,Hailong Yao,Xia Yin*

Main category: cs.LG

TL;DR: First min-cost network flow algorithm augmented with a dual prediction via epsilon-relaxation; provides bounds tied to infinity-norm error and PAC-learning; shows 12.74x and 1.64x speedups in two applications.


<details>
  <summary>Details</summary>
Motivation: Show how machine-learned predictions can provably improve classic optimization algorithms, extending to minimum-cost flow with robustness and sample complexity guarantees.

Method: Augment the epsilon-relaxation algorithm for minimum-cost flow with a dual prediction. Derive time complexity bounds as a function of the infinity-norm prediction error (consistent and robust). Establish PAC-learning sample complexity for the prediction. Evaluate with two prediction schemes: a fixed prediction and a feature-based neural network to infer the prediction.

Result: Theoretical analysis yields time complexity bounds tied to the prediction error; PAC-learning guarantees for the predictor. Empirical results show substantial speedups: 12.74x and 1.64x on traffic networks and chip routing respectively.

Conclusion: Demonstrates that dual-prediction augmentation can significantly accelerate minimum-cost flow algorithms with rigorous learning-theoretic and optimization guarantees; supports practical deployment with two prediction schemes across two applications.

Abstract: Recent work has shown that machine-learned predictions can provably improve the performance of classic algorithms. In this work, we propose the first minimum-cost network flow algorithm augmented with a dual prediction. Our method is based on a classic minimum-cost flow algorithm, namely $\varepsilon$-relaxation. We provide time complexity bounds in terms of the infinity norm prediction error, which is both consistent and robust. We also prove sample complexity bounds for PAC-learning the prediction. We empirically validate our theoretical results on two applications of minimum-cost flow, i.e., traffic networks and chip escape routing, in which we learn a fixed prediction, and a feature-based neural network model to infer the prediction, respectively. Experimental results illustrate $12.74\times$ and $1.64\times$ average speedup on two applications.

</details>


### [127] [Hyperparameter Transfer with Mixture-of-Expert Layers](https://arxiv.org/abs/2601.20205)
*Tianze Jiang,Blake Bordelon,Cengiz Pehlevan,Boris Hanin*

Main category: cs.LG

TL;DR: A new parameterization for transformer MoE models with a DMFT-backed justification improves hyperparameter transfer across scales (width, depth, number/size of experts) and enables efficient scaling by reusing hyperparameters from small models on larger ones.


<details>
  <summary>Details</summary>
Motivation: MoE layers add router weights and dimensions that complicate HP tuning; scalable, reliable HP settings are needed when scaling width, depth, and experts.

Method: Introduce a parameterization for MoE-enabled transformers; support with dynamical mean-field theory analysis; empirical evaluation across model variants at fixed token budgets; transfer of HPs from small to large models and longer token horizons.

Result: Demonstrates reliable HP transfer across models from 51M to 2B parameters; HPs from small models perform well on larger models with longer horizons; overall improved scalability.

Conclusion: The proposed parameterization enables robust, scalable training of MoE transformers with reduced HP tuning overhead and better transferability of HPs across model scales.

Abstract: Mixture-of-Experts (MoE) layers have emerged as an important tool in scaling up modern neural networks by decoupling total trainable parameters from activated parameters in the forward pass for each token. However, sparse MoEs add complexity to training due to (i) new trainable parameters (router weights) that, like all other parameter groups, require hyperparameter (HP) tuning; (ii) new architecture scale dimensions (number of and size of experts) that must be chosen and potentially taken large. To make HP selection cheap and reliable, we propose a new parameterization for transformer models with MoE layers when scaling model width, depth, number of experts, and expert (hidden) size. Our parameterization is justified by a novel dynamical mean-field theory (DMFT) analysis. When varying different model dimensions trained at a fixed token budget, we find empirically that our parameterization enables reliable HP transfer across models from 51M to over 2B total parameters. We further take HPs identified from sweeping small models on a short token horizon to train larger models on longer horizons and report performant model behaviors.

</details>


### [128] [Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning](https://arxiv.org/abs/2601.20209)
*Jinyang Wu,Shuo Yang,Changpeng Yang,Yuhao Shen,Shuai Zhang,Zhengqi Wen,Jianhua Tao*

Main category: cs.LG

TL;DR: Spark enables adaptive, state-aware branching to efficiently explore long-horizon tasks by focusing exploration on critical decision points, improving sample efficiency and generalization while reducing wasted computation.


<details>
  <summary>Details</summary>
Motivation: Long-horizon reinforcement learning with large language models suffers from scarce high-quality trajectories and excessive computation wasted on trivial steps. Existing methods scale rollout sizes and allocate resources uniformly, which does not guarantee sample quality.

Method: Introduce Spark: Strategic Policy-Aware exploration via Key-state dynamic branching. Branch selectively at critical decision states using intrinsic decision signals; adaptive branching to probe promising trajectories; reduces dependence on human priors; enables autonomous expansion of exploration.

Result: Empirical results across diverse tasks (e.g., embodied planning) show Spark achieves higher success rates with significantly fewer training samples and robust generalization to unseen scenarios.

Conclusion: Adaptive, key-state branching yields more sample-efficient exploration and stronger generalization for long-horizon RL with LLM agents, by prioritizing sampling quality over blind coverage.

Abstract: Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \textbf{Spark} (\textbf{S}trategic \textbf{P}olicy-\textbf{A}ware explo\textbf{R}ation via \textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.

</details>


### [129] [ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance](https://arxiv.org/abs/2601.20227)
*Zichao Yu,Ming Li,Wenyi Zhang,Difan Zou,Weiguo Gao*

Main category: cs.LG

TL;DR: ProFlow introduces a two-step proximal guidance framework for zero-shot physics-consistent sampling: (1) a proximal projection onto the intersection of physical and observational constraints, and (2) interpolation back to the fixed generative prior, enabling inference from sparse data without retraining. It yields improved physical fidelity and distributional statistics versus diffusion- and flow-based baselines.


<details>
  <summary>Details</summary>
Motivation: Enforce hard physical constraints in inverse problems with sparse observations using a fixed pre-trained generative prior, avoiding task-specific retraining.

Method: Two-step scheme: (1) terminal optimization/proximal minimization to enforce physics-data consistency; (2) interpolation to map the refined state back to the generative trajectory, preserving the learned flow prior. Interpretable as local MAP updates in a Bayesian view.

Result: Benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations show ProFlow achieves superior physical and observational consistency and more accurate distributional statistics compared to state-of-the-art diffusion- and flow-based baselines.

Conclusion: ProFlow enables zero-shot physics-consistent sampling with a fixed prior, offering a principled, efficient approach that generalizes across diverse PDEs and outperforms strong baselines.

Abstract: Inferring physical fields from sparse observations while strictly satisfying partial differential equations (PDEs) is a fundamental challenge in computational physics. Recently, deep generative models offer powerful data-driven priors for such inverse problems, yet existing methods struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. Consequently, there is a critical need for a sampling mechanism that can reconcile strict physical consistency and observational fidelity with the statistical structure of the pre-trained prior. To this end, we present ProFlow, a proximal guidance framework for zero-shot physics-consistent sampling, defined as inferring solutions from sparse observations using a fixed generative prior without task-specific retraining. The algorithm employs a rigorous two-step scheme that alternates between: (\romannumeral1) a terminal optimization step, which projects the flow prediction onto the intersection of the physically and observationally consistent sets via proximal minimization; and (\romannumeral2) an interpolation step, which maps the refined state back to the generative trajectory to maintain consistency with the learned flow probability path. This procedure admits a Bayesian interpretation as a sequence of local maximum a posteriori (MAP) updates. Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations demonstrate that ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics, compared to state-of-the-art diffusion- and flow-based baselines.

</details>


### [130] [An Accounting Identity for Algorithmic Fairness](https://arxiv.org/abs/2601.20217)
*Hadi Elzayn,Jacob Goldin*

Main category: cs.LG

TL;DR: An accounting identity links a model’s accuracy with fairness criteria: for globally calibrated models, miscalibration within groups plus cross-group error imbalance sum to a total unfairness budget; in binary outcomes this budget equals MSE times the difference in group outcome prevalence. The framework encompasses impossibility results, clarifies tradeoffs, and treats accuracy and fairness as complementary; evidenced by benchmarks and extended to non-binary outcomes where extra outcome information relaxes incompatibilities.


<details>
  <summary>Details</summary>
Motivation: Provide a unified, quantitative framework to relate accuracy and fairness, quantify unavoidable fairness violations, and guide design choices for interventions.

Method: Derive an accounting identity that decomposes total unfairness into within-group miscalibration and across-group error imbalance for globally calibrated models; define the total unfairness budget; specialize to binary outcomes with a budget proportional to MSE and class-prevalence differences; analyze the extension to non-binary outcomes; test via experiments on benchmark datasets; discuss implications for common fairness interventions.

Result: The identity shows that standard impossibility results are special cases and reveals inherent tradeoffs when fairness criteria are not simultaneously satisfied. Empirically, increasing accuracy reduces the total unfairness budget, while interventions that reduce one fairness violation can increase others or reduce accuracy by enlarging the budget. The theory is validated on benchmark data and extended to non-binary outcomes with conditions under which binary-like impossibilities extend to regression tasks.

Conclusion: Accuracy and fairness are best viewed as complementary in binary prediction tasks; the accounting framework clarifies tradeoffs and guides intervention design. With non-binary outcomes, additional information can relax incompatibilities; the paper also identifies when binary impossibility results extend to regression tasks.

Abstract: We derive an accounting identity for predictive models that links accuracy with common fairness criteria. The identity shows that for globally calibrated models, the weighted sums of miscalibration within groups and error imbalance across groups is equal to a "total unfairness budget." For binary outcomes, this budget is the model's mean-squared error times the difference in group prevalence across outcome classes. The identity nests standard impossibility results as special cases, while also describing inherent tradeoffs when one or more fairness measures are not perfectly satisfied. The results suggest that accuracy and fairness are best viewed as complements in binary prediction tasks: increasing accuracy necessarily shrinks the total unfairness budget and vice-versa. Experiments on benchmark data confirm the theory and show that many fairness interventions largely substitute between fairness violations, and when they reduce accuracy they tend to expand the total unfairness budget. The results extend naturally to prediction tasks with non-binary outcomes, illustrating how additional outcome information can relax fairness incompatibilities and identifying conditions under which the binary-style impossibility does and does not extend to regression tasks.

</details>


### [131] [Certificate-Guided Pruning for Stochastic Lipschitz Optimization](https://arxiv.org/abs/2601.20231)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: Certificate-Guided Pruning (CGP) for noisy Lipschitz black-box optimization maintains an active set of candidate-optimal points via confidence-adjusted envelopes; outside-A_t points are certifiably suboptimal with high probability. It achieves a sample complexity of ~ε^{-(2+α)} under a margin condition with near-optimality dimension α. Variants include CGP-Adaptive (online Lipschitz constant estimation), CGP-TR (trust-region scaling to high dimensions), and CGP-Hybrid (GP refinement with detected local smoothness). Empirical results (12 benchmarks, d up to 100) show competitive performance and principled stopping criteria via certificate volume.


<details>
  <summary>Details</summary>
Motivation: Existing adaptive discretization methods in noisy Lipschitz black-box optimization implicitly avoid suboptimal regions but lack explicit certificates of optimality or measurable progress guarantees. There is a need for rigorous stopping criteria and provable guarantees on convergence toward the optimum under evaluation noise.

Method: Construct confidence-adjusted Lipschitz envelopes to bound the objective over the search space. Maintain an active set A_t of points that could be optimal; certify suboptimality of points outside A_t with high probability. Leverage a margin condition with near-optimality dimension α to prove that Vol(A_t) shrinks at a controlled rate, yielding the sample complexity ~ Õ(ε^{-(2+α)}. Extend with: (i) CGP-Adaptive – online learning of the Lipschitz constant L with O(log T) overhead; (ii) CGP-TR – trust regions for scalability to d>50 with local certificates; (iii) CGP-Hybrid – switch to GP-based refinement when local smoothness is detected.

Result: Theoretical guarantee: Vol(A_t) shrinks and sample complexity scales as Õ(ε^{-(2+α)}) under the margin condition. Empirical validation on 12 benchmarks with dimensions 2–100 showing CGP variants match or exceed strong baselines, with principled stopping criteria via certificate volume.

Conclusion: CGP provides explicit, probabilistic optimality certificates in black-box optimization under noise, enabling measurable progress guarantees and stopping criteria. The proposed extensions address online model learning and high-dimensional scalability, and GP refinement offers a hybrid approach when local structure is detected.

Abstract: We study black-box optimization of Lipschitz functions under noisy evaluations. Existing adaptive discretization methods implicitly avoid suboptimal regions but do not provide explicit certificates of optimality or measurable progress guarantees. We introduce \textbf{Certificate-Guided Pruning (CGP)}, which maintains an explicit \emph{active set} $A_t$ of potentially optimal points via confidence-adjusted Lipschitz envelopes. Any point outside $A_t$ is certifiably suboptimal with high probability, and under a margin condition with near-optimality dimension $α$, we prove $\Vol(A_t)$ shrinks at a controlled rate yielding sample complexity $\tildeO(\varepsilon^{-(2+α)})$. We develop three extensions: CGP-Adaptive learns $L$ online with $O(\log T)$ overhead; CGP-TR scales to $d > 50$ via trust regions with local certificates; and CGP-Hybrid switches to GP refinement when local smoothness is detected. Experiments on 12 benchmarks ($d \in [2, 100]$) show CGP variants match or exceed strong baselines while providing principled stopping criteria via certificate volume.

</details>


### [132] [Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization](https://arxiv.org/abs/2601.20226)
*Julian Gutierrez,Redouane Silvente*

Main category: cs.LG

TL;DR: Dual-framework approach for EPEX SPOT day-ahead forecasting and storage optimization: a fast parametric model for hourly curves and a generative model for 24-hour order-level scenarios, enabling revenue-optimal storage and revealing price-compression effects.


<details>
  <summary>Details</summary>
Motivation: Improve forecast accuracy for hourly supply/demand curves and enable storage optimization under uncertainty in day-ahead electricity markets by combining a fast operational model with a richer, scenario-based analysis.

Method: 1) Parametric fast model using a low-dimensional, grid-robust representation with minimum/maximum volumes and a Chebyshev polynomial for the elastic segment. 2) Generative models that learn the joint distribution of 24-hour order-level submissions given weather and fuel variables, generating synthetic daily buy/sell orders, which are aggregated to obtain hourly curves, followed by optimization of a price-making storage strategy and analysis of revenue distributions.

Result: Parametric model supports daily operation with low error and interpretability; generative models provide comprehensive analysis (less suited for daily operation). Storage optimization yields revenue distributions and demonstrates price compression: lower peaks, higher off-peak levels, diminishing returns as capacity expands.

Conclusion: A two-tier framework integrating fast, interpretable operational forecasting with richer scenario-based analysis to inform storage decisions and reveal price dynamics in the EPEX SPOT day-ahead market.

Abstract: We present two machine learning frameworks for forecasting aggregated curves and optimizing storage in the EPEX SPOT day-ahead market. First, a fast parametric model forecasts hourly demand and supply curves in a low-dimensional and grid-robust representation, with minimum and maximum volumes combined with a Chebyshev polynomial for the elastic segment. The model enables daily use with low error and clear interpretability. Second, for a more comprehensive analysis, though less suited to daily operation, we employ generative models that learn the joint distribution of 24-hour order-level submissions given weather and fuel variables. These models generate synthetic daily scenarios of individual buy and sell orders, which, once aggregated, yield hourly supply and demand curves. Based on these forecasts, we optimize a price-making storage strategy, quantify revenue distributions, and highlight the price-compression effect with lower peaks, higher off-peak levels, and diminishing returns as capacity expands.

</details>


### [133] [Order-Optimal Sample Complexity of Rectified Flows](https://arxiv.org/abs/2601.20250)
*Hari Krishna Sahoo,Mudit Gaur,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Rectified flows constrain transport trajectories to be linear from base to data, enabling fast sampling and order-optimal sample complexity. They improve over flow matching and match mean-estimation rates, often generating high-quality samples with a single Euler step.


<details>
  <summary>Details</summary>
Motivation: To accelerate sampling in flow-based generative models by enforcing linear transport paths, and to provide theoretical guarantees that explain empirical performance.

Method: Impose a rectified flow structure with linear transport from base to data; train the model using a squared loss along linear paths; analyze under standard neural network and data-distribution assumptions using localized Rademacher complexity.

Result: Achieves sample complexity ~ O~(ε^-2); improves upon the O(ε^-4) bound for flow matching; matches the optimal rate for mean estimation; often a single Euler step suffices for high-quality generation; provides theoretical justification for empirical performance.

Conclusion: Rectified flows offer a theoretically grounded acceleration for flow-based models, achieving order-optimal sample complexity due to the restricted hypothesis class and linear-trajectory structure.

Abstract: Recently, flow-based generative models have shown superior efficiency compared to diffusion models. In this paper, we study rectified flow models, which constrain transport trajectories to be linear from the base distribution to the data distribution. This structural restriction greatly accelerates sampling, often enabling high-quality generation with a single Euler step. Under standard assumptions on the neural network classes used to parameterize the velocity field and data distribution, we prove that rectified flows achieve sample complexity $\tilde{O}(\varepsilon^{-2})$. This improves on the best known $O(\varepsilon^{-4})$ bounds for flow matching model and matches the optimal rate for mean estimation. Our analysis exploits the particular structure of rectified flows: because the model is trained with a squared loss along linear paths, the associated hypothesis class admits a sharply controlled localized Rademacher complexity. This yields the improved, order-optimal sample complexity and provides a theoretical explanation for the strong empirical performance of rectified flow models.

</details>


### [134] [Robust SDE Parameter Estimation Under Missing Time Information Setting](https://arxiv.org/abs/2601.20268)
*Long Van Tran,Truyen Tran,Phuoc Nguyen*

Main category: cs.LG

TL;DR: A framework that recovers temporal order and estimates SDE parameters when timestamps are corrupted or missing, using a score-matching criterion based on forward/backward process asymmetry, followed by sorting and maximum-likelihood parameter estimation; validated on synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: Parameter estimation for SDEs requires accurate temporal ordering; when order is corrupted, missing, or privacy-preserved, existing methods fail. There is a need to establish conditions for recoverability of order and to jointly reconstruct order with parameter estimation.

Method: Derive a score-matching criterion exploiting asymmetries between forward and backward diffusion processes to infer the correct temporal order between observation pairs. Use the inferred pairwise orderings to assemble a total order via sorting. Estimate SDE parameters from the reconstructed sequence using maximum likelihood.

Result: Extensive experiments on synthetic and real-world datasets demonstrate effectiveness in recovering the temporal order and accurately estimating SDE parameters under missing or hidden ordering, thereby widening applicability to privacy-sensitive settings.

Conclusion: The proposed framework enables robust SDE parameter estimation when temporal information is incomplete, by jointly recovering order and fitting parameters; it broadens applicability to privacy-preserving data and related scenarios.

Abstract: Recent advances in stochastic differential equations (SDEs) have enabled robust modeling of real-world dynamical processes across diverse domains, such as finance, health, and systems biology. However, parameter estimation for SDEs typically relies on accurately timestamped observational sequences. When temporal ordering information is corrupted, missing, or deliberately hidden (e.g., for privacy), existing estimation methods often fail. In this paper, we investigate the conditions under which temporal order can be recovered and introduce a novel framework that simultaneously reconstructs temporal information and estimates SDE parameters. Our approach exploits asymmetries between forward and backward processes, deriving a score-matching criterion to infer the correct temporal order between pairs of observations. We then recover the total order via a sorting procedure and estimate SDE parameters from the reconstructed sequence using maximum likelihood. Finally, we conduct extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness of our method, extending parameter estimation to settings with missing temporal order and broadening applicability in sensitive domains.

</details>


### [135] [The Forecast After the Forecast: A Post-Processing Shift in Time Series](https://arxiv.org/abs/2601.20280)
*Daojun Liang,Qi Li,Yinglong Wang,Jing Chen,Hu Zhang,Xiaoxiao Cui,Qizheng Wang,Shuo Li*

Main category: cs.LG

TL;DR: A lightweight, architecture-agnostic post-processing module called δ-Adapter boosts deployed time series forecasters by input nudging and output residual correction, with optional feature selection and probabilistic calibration, achieving improved accuracy and calibration without retraining.


<details>
  <summary>Details</summary>
Motivation: Forecasting models are approaching diminishing returns; there is a critical yet underexplored need to improve accuracy and uncertainty without retraining deployed backbones (the last-mile gap).

Method: δ-Adapter learns tiny, bounded modules at two interfaces: input nudging (soft edits to covariates) and output residual correction. It provides local descent guarantees, O(δ) drift bounds, and compositional stability for combined adapters. It can also learn a sparse, horizon-aware mask over inputs for feature selection (improving interpretability). Additionally, it includes distribution-calibration components (Quantile Calibrator and Conformal Corrector) to deliver calibrated, personalized prediction intervals with finite-sample coverage.

Result: Empirical evaluations across diverse backbones and datasets show δ-Adapter improves accuracy and calibration with negligible compute and no interface changes.

Conclusion: δ-Adapter offers a practical, interpretable, and scalable post-processing approach to boost time-series forecasting without retraining, with benefits in accuracy, calibration, and feature interpretability.

Abstract: Time series forecasting has long been dominated by advances in model architecture, with recent progress driven by deep learning and hybrid statistical techniques. However, as forecasting models approach diminishing returns in accuracy, a critical yet underexplored opportunity emerges: the strategic use of post-processing. In this paper, we address the last-mile gap in time-series forecasting, which is to improve accuracy and uncertainty without retraining or modifying a deployed backbone. We propose $δ$-Adapter, a lightweight, architecture-agnostic way to boost deployed time series forecasters without retraining. $δ$-Adapter learns tiny, bounded modules at two interfaces: input nudging (soft edits to covariates) and output residual correction. We provide local descent guarantees, $O(δ)$ drift bounds, and compositional stability for combined adapters. Meanwhile, it can act as a feature selector by learning a sparse, horizon-aware mask over inputs to select important features, thereby improving interpretability. In addition, it can also be used as a distribution calibrator to measure uncertainty. Thus, we introduce a Quantile Calibrator and a Conformal Corrector that together deliver calibrated, personalized intervals with finite-sample coverage. Our experiments across diverse backbones and datasets show that $δ$-Adapter improves accuracy and calibration with negligible compute and no interface changes.

</details>


### [136] [Cheap2Rich: A Multi-Fidelity Framework for Data Assimilation and System Identification of Multiscale Physics -- Rotating Detonation Engines](https://arxiv.org/abs/2601.20295)
*Yuxuan Bao,Jan Zajac,Megan Powers,Venkat Raman,J. Nathan Kutz*

Main category: cs.LG

TL;DR: Multi-scale data assimilation method Cheap2Rich combines a fast low-fidelity prior with learned discrepancy corrections to reconstruct high-fidelity states from sparse sensor data, demonstrated on rotating detonation engines; offers interpretable discrepancy dynamics and a general framework for rapid design and real-time monitoring.


<details>
  <summary>Details</summary>
Motivation: Bridge sim2real gap in engineering-scale multi-scale problems where reduced-order models miss important dynamics; enable rapid exploration and control with interpretable corrections.

Method: Integrates a fast low-fidelity prior with data-driven discrepancy corrections; reconstructs high-fidelity states from sparse measurements via data assimilation; isolates injector-driven discrepancy dynamics; provides an interpretable discrepancy term; code available at GitHub.

Result: High-fidelity RDE states reconstructed from sparse measurements; physically meaningful discrepancy dynamics identified; demonstrates general multi-fidelity data assimilation and system identification framework; supports rapid design exploration and real-time monitoring and control.

Conclusion: The work provides a general, interpretable multi-fidelity data assimilation framework for complex multi-scale systems, enabling efficient design-space exploration and real-time operation, with a clear separation of physically meaningful discrepancies.

Abstract: Bridging the sim2real gap between computationally inexpensive models and complex physical systems remains a central challenge in machine learning applications to engineering problems, particularly in multi-scale settings where reduced-order models typically capture only dominant dynamics. In this work, we present Cheap2Rich, a multi-scale data assimilation framework that reconstructs high-fidelity state spaces from sparse sensor histories by combining a fast low-fidelity prior with learned, interpretable discrepancy corrections. We demonstrate the performance on rotating detonation engines (RDEs), a challenging class of systems that couple detonation-front propagation with injector-driven unsteadiness, mixing, and stiff chemistry across disparate scales. Our approach successfully reconstructs high-fidelity RDE states from sparse measurements while isolating physically meaningful discrepancy dynamics associated with injector-driven effects. The results highlight a general multi-fidelity framework for data assimilation and system identification in complex multi-scale systems, enabling rapid design exploration and real-time monitoring and control while providing interpretable discrepancy dynamics. Code for this project is is available at: github.com/kro0l1k/Cheap2Rich.

</details>


### [137] [HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH](https://arxiv.org/abs/2601.20255)
*Yueyang Wang,Jiawei Fu,Baolong Bi,Xili Wang,Xiaoqing Liu*

Main category: cs.LG

TL;DR: Introduces HE-SNR, a mid-training metric based on the Entropy Compression Hypothesis to better predict SWE performance, addressing perplexity issues and long-context effects; validated on MoE LLMs with 32K/128K contexts, showing improved robustness and predictive power.


<details>
  <summary>Details</summary>
Motivation: Perplexity and standard metrics fail to reliably predict downstream software-engineering performance during mid-training due to long-context effects and misalignment with SWE tasks; a data-filtering step and a theory-grounded metric are needed to guide mid-training.

Method: Propose a data filtering strategy; formalize the Entropy Compression Hypothesis; define HE-SNR (High-Entropy Signal-to-Noise Ratio) as the evaluation metric; validate on industrial-scale Mixture-of-Experts models across 32K and 128K context windows.

Result: HE-SNR exhibits superior robustness and predictive power for downstream SWE performance compared to perplexity, with consistent results across different context windows and model scales.

Conclusion: Provides a theoretical foundation and practical tools to optimize LLM latent potential in complex software-engineering domains, informing mid-training objectives and SWE benchmarking.

Abstract: SWE-bench has emerged as the premier benchmark for evaluating Large Language Models on complex software engineering tasks. While these capabilities are fundamentally acquired during the mid-training phase and subsequently elicited during Supervised Fine-Tuning (SFT), there remains a critical deficit in metrics capable of guiding mid-training effectively. Standard metrics such as Perplexity (PPL) are compromised by the "Long-Context Tax" and exhibit weak correlation with downstream SWE performance. In this paper, we bridge this gap by first introducing a rigorous data filtering strategy. Crucially, we propose the Entropy Compression Hypothesis, redefining intelligence not by scalar Top-1 compression, but by the capacity to structure uncertainty into Entropy-Compressed States of low orders ("reasonable hesitation"). Grounded in this fine-grained entropy analysis, we formulate a novel metric, HE-SNR (High-Entropy Signal-to-Noise Ratio). Validated on industrial-scale Mixture-of-Experts (MoE) models across varying context windows (32K/128K), our approach demonstrates superior robustness and predictive power. This work provides both the theoretical foundation and practical tools for optimizing the latent potential of LLMs in complex engineering domains.

</details>


### [138] [Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction](https://arxiv.org/abs/2601.20299)
*Tianyi Alex Qiu,Micah Carroll,Cameron Allen*

Main category: cs.LG

TL;DR: Peer prediction-based evaluation and post-training for LLMs uses mutual predictability to reward truthful/informative answers without ground-truth labels, resisting deception and enabling effective evaluation across large models under weak supervision.


<details>
  <summary>Details</summary>
Motivation: Evaluation and post-training of LLMs rely on supervision; for frontier models, strong supervision is often unavailable, and ground-truth signals can be exploited by deceptive models; a robust, weak-supervision-friendly evaluation method is needed.

Method: Apply peer prediction, a mechanism-design approach, to model evaluation and post-training. Use mutual predictability to reward truthful responses without ground-truth labels; design incentives to discourage deception; provide theoretical guarantees; validate empirically on models up to 405B params; show that training an 8B model with peer-prediction rewards recovers most of the truthfulness drop caused by malicious finetuning, even when the reward is produced by a tiny 0.135B LM.

Result: The method demonstrates deception resistance with theoretical guarantees and empirical validation; the 8B model recovers most truthfulness losses due to prior malicious finetuning; reward signals can be supplied by very small LMs; in evaluation, peer prediction exhibits inverse scaling: resilience to deception increases as the capability gap between experts and participants widens, enabling reliable evaluation of strong models under weak supervision; LLM-as-a-Judge performs poorly when facing deceptive models much larger than the judge (5–20x), whereas peer prediction thrives with large size gaps (even >100x).

Conclusion: Peer prediction provides a robust, weak-supervision-friendly framework for evaluating and post-training LLMs, mitigating deception risks and enabling reliable assessment of frontier models where ground-truth supervision is scarce.

Abstract: The evaluation and post-training of large language models (LLMs) rely on supervision, but strong supervision for difficult tasks is often unavailable, especially when evaluating frontier models. In such cases, models are demonstrated to exploit evaluations built on such imperfect supervision, leading to deceptive results. However, underutilized in LLM research, a wealth of mechanism design research focuses on game-theoretic incentive compatibility, i.e., eliciting honest and informative answers with weak supervision. Drawing from this literature, we introduce the peer prediction method for model evaluation and post-training. It rewards honest and informative answers over deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. We demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and empirical validation on models with up to 405B parameters. We show that training an 8B model with peer prediction-based reward recovers most of the drop in truthfulness due to prior malicious finetuning, even when the reward is produced by a 0.135B language model with no finetuning. On the evaluation front, in contrast to LLM-as-a-Judge which requires strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception is strengthened as the capability gap between the experts and participants widens, enabling reliable evaluation of strong models with weak supervision. In particular, LLM-as-a-Judge become worse than random guess when facing deceptive models 5-20x the judge's size, while peer prediction thrives when such gaps are large, including in cases with over 100x size difference.

</details>


### [139] [C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding](https://arxiv.org/abs/2601.20257)
*Jinren Ding,Xuejian Xu,Shen Jiang,Zhitong Hao,Jinhui Yang,Peng Jiang*

Main category: cs.LG

TL;DR: Proposes C2 to enhance Decision Transformer (DT) for auto-bidding by adding a Cross Learning Block (CLB) with cross-attention and a Constraint-aware Loss (CL) for selective learning under budget/ CPA constraints; yields up to 3.23% improvements on AuctionNet and ablation confirms synergetic gains.


<details>
  <summary>Details</summary>
Motivation: DT struggles with limited cross-sequence interdependencies among state, action, and RTG and tends to learn both optimal and suboptimal behaviors indiscriminately. The work aims to strengthen cross-sequence modeling and enforce constraint-driven selectivity to improve auto-bidding performance.

Method: Introduce a Cross Learning Block (CLB) based on cross-attention to enhance inter-sequence correlation among state, action, and RTG; design a Constraint-aware Loss (CL) that incorporates budget and CPA constraints to bias learning toward optimal trajectories.

Result: Offline evaluations on AuctionNet show consistent performance gains (up to 3.23% over state-of-the-art GAVE) across various budget settings; ablation studies validate the complementary synergy of CLB and CL; code released at provided URL.

Conclusion: C2 effectively enhances DT for auto-bidding by combining improved cross-sequence modeling with constraint-guided optimization; CLB and CL provide complementary gains and outperform prior methods.

Abstract: Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: https://github.com/Dingjinren/C2.

</details>


### [140] [Memory Retrieval in Transformers: Insights from The Encoding Specificity Principle](https://arxiv.org/abs/2601.20282)
*Viet Hung Dinh,Ming Ding,Youyang Qu,Kanchana Thilakarathna*

Main category: cs.LG

TL;DR: Attention in transformer LLMs may instantiate memory via cue-based retrieval: keywords act as retrieval cues and specific attention neurons encode them, enabling extraction for tasks like unlearning.


<details>
  <summary>Details</summary>
Motivation: Growing regulatory pressure demands transparency, accountability, and privacy-preserving unlearning in LLMs; yet the role of attention as a memory mechanism is underexplored; cross-disciplinary insights from psychology guide the hypothesis.

Method: Analyze attention layers to map queries/keys/values to a retrieval-like process; identify neurons that selectively encode context-defining keywords; extract these keywords from neurons; demonstrate potential utility for unlearning.

Result: Converging evidence for the keywords-as-cues hypothesis; isolation of keyword-encoding neurons within attention layers; extracted keywords can be retrieved and repurposed for downstream tasks such as unlearning.

Conclusion: Supports view that attention mediates context-based memory retrieval in LLMs; keywords-as-cues provide a tractable, interpretable mechanism; opens avenues for transparent memory control and privacy-preserving unlearning.

Abstract: While explainable artificial intelligence (XAI) for large language models (LLMs) remains an evolving field with many unresolved questions, increasing regulatory pressures have spurred interest in its role in ensuring transparency, accountability, and privacy-preserving machine unlearning. Despite recent advances in XAI have provided some insights, the specific role of attention layers in transformer based LLMs remains underexplored. This study investigates the memory mechanisms instantiated by attention layers, drawing on prior research in psychology and computational psycholinguistics that links Transformer attention to cue based retrieval in human memory. In this view, queries encode the retrieval context, keys index candidate memory traces, attention weights quantify cue trace similarity, and values carry the encoded content, jointly enabling the construction of a context representation that precedes and facilitates memory retrieval. Guided by the Encoding Specificity Principle, we hypothesize that the cues used in the initial stage of retrieval are instantiated as keywords. We provide converging evidence for this keywords-as-cues hypothesis. In addition, we isolate neurons within attention layers whose activations selectively encode and facilitate the retrieval of context-defining keywords. Consequently, these keywords can be extracted from identified neurons and further contribute to downstream applications such as unlearning.

</details>


### [141] [A Learning-based Framework for Spatial Impulse Response Compensation in 3D Photoacoustic Computed Tomography](https://arxiv.org/abs/2601.20291)
*Kaiyi Yang,Seonyeong Park,Gangwon Jeong,Hsuan-Kai Huang,Alexander A. Oraevsky,Umberto Villa,Mark A. Anastasio*

Main category: cs.LG

TL;DR: A data-domain, learned SIR compensation framework for 3D PACT that maps SIR-corrupted measurements to ideal point-like transducer measurements, enabling fast analytic reconstructions; compares U-Net and physics-inspired Deconv-Net; validated in simulations and in-vivo breast data with improved resolution and robustness; first 3D demonstration.


<details>
  <summary>Details</summary>
Motivation: SIRs of large-area ultrasound transducers degrade spatial resolution in PACT and analytic reconstructions that neglect SIRs are fast but inaccurate. Although optimization-based methods can model SIRs, they are computationally intensive, especially in 3D. A fast yet accurate 3D PACT reconstruction requires effective data-domain compensation.

Method: Propose a learned, data-domain SIR compensation: train networks to map SIR-corrupted measurements to compensated data equivalent to ideal point-like transducers. Compare two architectures: a U-Net and a physics-inspired Deconv-Net. Use a fast, analytical data-generation pipeline to synthesize training pairs. After compensation, apply a fast analytic reconstruction that assumes ideal transducers.

Result: Simulated studies show improved spatial resolution and robustness to noise, object complexity, and sound-speed heterogeneity. In-vivo breast imaging demonstrates recovery of fine structures obscured by SIR artifacts. Both variants yield effective compensation, with the Deconv-Net leveraging physics priors.

Conclusion: This work demonstrates, for the first time in 3D PACT, that learned SIR compensation in the data domain can enable accurate, rapid reconstructions by allowing simple analytic methods to be used with compensated data.

Abstract: Photoacoustic computed tomography (PACT) is a promising imaging modality that combines the advantages of optical contrast with ultrasound detection. Utilizing ultrasound transducers with larger surface areas can improve detection sensitivity. However, when computationally efficient analytic reconstruction methods that neglect the spatial impulse responses (SIRs) of the transducer are employed, the spatial resolution of the reconstructed images will be compromised. Although optimization-based reconstruction methods can explicitly account for SIR effects, their computational cost is generally high, particularly in three-dimensional (3D) applications. To address the need for accurate but rapid 3D PACT image reconstruction, this study presents a framework for establishing a learned SIR compensation method that operates in the data domain. The learned compensation method maps SIR-corrupted PACT measurement data to compensated data that would have been recorded by idealized point-like transducers. Subsequently, the compensated data can be used with a computationally efficient reconstruction method that neglects SIR effects. Two variants of the learned compensation model are investigated that employ a U-Net model and a specifically designed, physics-inspired model, referred to as Deconv-Net. A fast and analytical training data generation procedure is also a component of the presented framework. The framework is rigorously validated in virtual imaging studies, demonstrating resolution improvement and robustness to noise variations, object complexity, and sound speed heterogeneity. When applied to in-vivo breast imaging data, the learned compensation models revealed fine structures that had been obscured by SIR-induced artifacts. To our knowledge, this is the first demonstration of learned SIR compensation in 3D PACT imaging.

</details>


### [142] [Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku](https://arxiv.org/abs/2601.20363)
*Mariia Drozdova*

Main category: cs.LG

TL;DR: Continuous-time generative models (flow-based and score-based) can represent highly constrained discrete distributions like Sudoku, with stochastic sampling outperforming deterministic flows; DDPM-style sampling yields high validity; the approach can be used as a probabilistic Sudoku solver via guided sampling, though not as sample-efficient as specialized solvers.


<details>
  <summary>Details</summary>
Motivation: To assess whether standard continuous-time generative models can assign non-zero probability mass to globally constrained combinatorial structures. Sudoku is used as a controlled testbed by viewing completed grids as a subset of a continuous relaxation, enabling evaluation of different sampling schemes and potential for guided constraint satisfaction.

Method: Train flow-matching and score-based models along a Gaussian probability path. Compare deterministic (ODE) sampling, stochastic (SDE) sampling, and DDPM-style discretizations derived from the same continuous-time training. Evaluate unconditional generation and guided generation by clamping clues and stopping when all constraints are satisfied.

Result: Unconditionally, stochastic sampling substantially outperforms deterministic flows; score-based samplers are the most reliable among continuous-time methods; DDPM-style ancestral sampling achieves the highest validity overall. The models can be repurposed for guided generation by iterative sampling under clamped clues to solve Sudoku probabilistically.

Conclusion: Classic diffusion/flow formulations can assign non-zero probability to globally constrained combinatorial structures and enable constraint satisfaction via stochastic search, though they remain less sample-efficient than classical solvers and discrete-geometry-aware diffusion methods.

Abstract: Can standard continuous-time generative models represent distributions whose support is an extremely sparse, globally constrained discrete set? We study this question using completed Sudoku grids as a controlled testbed, treating them as a subset of a continuous relaxation space. We train flow-matching and score-based models along a Gaussian probability path and compare deterministic (ODE) sampling, stochastic (SDE) sampling, and DDPM-style discretizations derived from the same continuous-time training. Unconditionally, stochastic sampling substantially outperforms deterministic flows; score-based samplers are the most reliable among continuous-time methods, and DDPM-style ancestral sampling achieves the highest validity overall. We further show that the same models can be repurposed for guided generation: by repeatedly sampling completions under clamped clues and stopping when constraints are satisfied, the model acts as a probabilistic Sudoku solver. Although far less sample-efficient than classical solvers and discrete-geometry-aware diffusion methods, these experiments demonstrate that classic diffusion/flow formulations can assign non-zero probability mass to globally constrained combinatorial structures and can be used for constraint satisfaction via stochastic search.

</details>


### [143] [LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning](https://arxiv.org/abs/2601.20375)
*Wei Huang,Anda Cheng,Yinggui Wang,Lei Wang,Tao Wei*

Main category: cs.LG

TL;DR: LLM-AutoDP automatically learns data-processing pipelines for LLM fine-tuning using LLM agents, with three efficiency techniques; yields large gains vs unprocessed data and competitive gains vs AutoML baselines; accelerates search up to 10x.


<details>
  <summary>Details</summary>
Motivation: Data quality in domain-specific fine-tuning data is often poor; manual DP is costly and privacy-sensitive; automating DP with minimal data exposure is desirable.

Method: LLMs act as agents to generate and refine multiple DP strategies via iterative in-context learning, guided by feedback and comparative evaluation; introduces Distribution Preserving Sampling, Processing Target Selection using a classifier, and Cache-and-Reuse to accelerate search.

Result: Models trained on DP-processed data outperform those trained on raw data by large margins; ~80% win rate vs unprocessed; ~65% win vs AutoML baselines; search-time reduced up to 10x.

Conclusion: LLM-AutoDP is effective and efficient for automated data processing; shows promise for privacy-preserving, low-labor DP in domain-specific fine-tuning; further work could explore robustness and different domains.

Abstract: Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.

</details>


### [144] [Delayed Feedback Modeling for Post-Click Gross Merchandise Volume Prediction: Benchmark, Insights and Approaches](https://arxiv.org/abs/2601.20307)
*Xinyu Li,Sishuo Chen,Guipeng Xv,Li Zhang,Mingxuan Luo,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Chen Lin*

Main category: cs.LG

TL;DR: Introduces TRACE benchmark for GMV delayed feedback and READER model with repurchase-aware routing and dynamic calibration, achieving 2.19% accuracy gain on TRACE.


<details>
  <summary>Details</summary>
Motivation: GMV prediction requires modeling delayed feedback for a continuous target; existing CVR-focused delayed feedback studies are insufficient; GMV labels evolve quickly and repurchase distributions differ, necessitating online, repurchase-aware modeling.

Method: Proposes TRACE benchmark with complete online transaction sequences; READER model uses a router to selectively activate expert parameters based on repurchase predictions; includes dual-branch predictor architecture and dynamic regression target calibration to mitigate incomplete labels.

Result: READER outperforms baselines on TRACE, with 2.19% improvement in accuracy; code and data released.

Conclusion: Highlights a new direction for online delayed feedback modeling in GMV prediction; TRACE toolkit will facilitate future research and industry application.

Abstract: The prediction objectives of online advertisement ranking models are evolving from probabilistic metrics like conversion rate (CVR) to numerical business metrics like post-click gross merchandise volume (GMV). Unlike the well-studied delayed feedback problem in CVR prediction, delayed feedback modeling for GMV prediction remains unexplored and poses greater challenges, as GMV is a continuous target, and a single click can lead to multiple purchases that cumulatively form the label. To bridge the research gap, we establish TRACE, a GMV prediction benchmark containing complete transaction sequences rising from each user click, which supports delayed feedback modeling in an online streaming manner. Our analysis and exploratory experiments on TRACE reveal two key insights: (1) the rapid evolution of the GMV label distribution necessitates modeling delayed feedback under online streaming training; (2) the label distribution of repurchase samples substantially differs from that of single-purchase samples, highlighting the need for separate modeling. Motivated by these findings, we propose RepurchasE-Aware Dual-branch prEdictoR (READER), a novel GMV modeling paradigm that selectively activates expert parameters according to repurchase predictions produced by a router. Moreover, READER dynamically calibrates the regression target to mitigate under-estimation caused by incomplete labels. Experimental results show that READER yields superior performance on TRACE over baselines, achieving a 2.19% improvement in terms of accuracy. We believe that our study will open up a new avenue for studying online delayed feedback modeling for GMV prediction, and our TRACE benchmark with the gathered insights will facilitate future research and application in this promising direction. Our code and dataset are available at https://github.com/alimama-tech/OnlineGMV .

</details>


### [145] [FedRD: Reducing Divergences for Generalized Federated Learning via Heterogeneity-aware Parameter Guidance](https://arxiv.org/abs/2601.20397)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Mingjin Zhang*

Main category: cs.LG

TL;DR: FedRD is a heterogeneity-aware federated learning algorithm that addresses optimization divergence and performance divergence in heterogeneous FL by using parameter-guided global generalization aggregation and local debiased classification to improve generalization to unseen clients.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous federated learning (HFL) struggles to generalize models to unseen clients due to data heterogeneity, leading to optimization divergence and performance divergence; solving these issues is crucial for robust cross-client generalization.

Method: FedRD proposes two core components: (1) parameter-guided global generalization aggregation to align globally but account for heterogeneity, and (2) local debiased classification to reduce local bias and divergences, aiming for an optimal global model for both participating and unseen clients.

Result: Experiments on public multi-domain datasets show substantial performance gains of FedRD over competing baselines in addressing generalization under heterogeneity.

Conclusion: FedRD effectively mitigates optimization and performance divergences in heterogeneous federated learning, improving generalization to unseen clients and achieving a stronger global model.

Abstract: Heterogeneous federated learning (HFL) aims to ensure effective and privacy-preserving collaboration among different entities. As newly joined clients require significant adjustments and additional training to align with the existing system, the problem of generalizing federated learning models to unseen clients under heterogeneous data has become progressively crucial. Consequently, we highlight two unsolved challenging issues in federated domain generalization: Optimization Divergence and Performance Divergence. To tackle the above challenges, we propose FedRD, a novel heterogeneity-aware federated learning algorithm that collaboratively utilizes parameter-guided global generalization aggregation and local debiased classification to reduce divergences, aiming to obtain an optimal global model for participating and unseen clients. Extensive experiments on public multi-domain datasets demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [146] [Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching](https://arxiv.org/abs/2601.20332)
*Fengrui Zuo,Zhiwei Ke,Yiming Liu,Wenqi Lou,Chao Wang,Xvehai Zhou*

Main category: cs.LG

TL;DR: Window-based token pruning and caching for diffusion language models enables efficient, pretrained-inference with sliding local windows; achieves up to 99x speedup while largely preserving quality.


<details>
  <summary>Details</summary>
Motivation: DLMs require full-sequence attention at each denoising step, causing substantial redundant computation. Block-wise diffusion exists but often needs retraining and constrained update orders, hindering use with pretrained models. Observed locality in inference suggests pruning by window and caching can accelerate decoding.

Method: Maintain a sliding local computation window during denoising. Partition undecoded tokens into: (i) active tokens computed online, (ii) buffer tokens whose KV states are cached and refreshed periodically, (iii) far-field tokens pruned outside the window. Compute only on active and buffer tokens within the window; omit far-field tokens. Reuse intermediate representations across steps except for a brief post-decode transient. Implementation referenced as Window-Diffusion; code at GitHub.

Result: Experiments on LLaDA and Dream show up to 99× inference speedup under matched compute budgets, with largely preserved generation performance.

Conclusion: Locality in DLM inference enables windowed pruning and caching to accelerate pretrained diffusion models without retraining, offering a practical, scalable speedup with minimal quality loss.

Abstract: Diffusion language models (DLMs) generate text through iterative denoising, but inference requires full-sequence attention at every iteration, resulting in substantial redundant computation on masked tokens. Block-wise diffusion can reduce this cost, yet it typically relies on retraining and constrained update orders, limiting its direct applicability to pretrained DLMs. Our token-level analysis reveals pronounced structural locality in DLM inference. Decoding is driven by a small set of prefix-localized active tokens; the influence of distant undecoded context diminishes rapidly, and decoded tokens exhibit stage-wise temporal stability, enabling reuse of intermediate representations except for a brief post-decode transient. Motivated by these observations, we propose \textbf{\placeholder}\footnote{The source code is available at https://github.com/vhicrgit/Window-Diffusion.}, a window-based token pruning and caching method for inference. We maintain a local computation window that slides rightward as denoising progresses, and partition undecoded tokens into: (i) \textit{active tokens} that are computed online, (ii) \textit{buffer tokens} whose KV states are cached and periodically refreshed, and (iii) \textit{far-field tokens} that are pruned outside the window. Computation is restricted to active and buffer tokens within the window, while far-field tokens are omitted at each stage. Experiments on LLaDA and Dream show that, under matched compute budgets, our method achieves up to $99\times$ inference speedup while largely preserving generation performance.

</details>


### [147] [TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs](https://arxiv.org/abs/2601.20357)
*Minjae Lee,Wonjun Kang,Byeongkeun Ahn,Christian Classen,Kevin Galim,Seunghyuk Oh,Minghao Yan,Hyung Il Koo,Kangwook Lee*

Main category: cs.LG

TL;DR: TABED is a training-free, plug-and-play method for speculative decoding in LVLMs that dynamically ensembles multiple draft tokens at test time. It leverages deviations from past ground truths to adapt batching, improving robustness and speed over standard methods.


<details>
  <summary>Details</summary>
Motivation: Speculative decoding (SD) is effective for speedups in LLMs but has not been thoroughly explored for Large Vision-Language Models (LVLMs). LVLMs present image+text prompts and show scenario-specific performance fluctuations; there is a need for adaptive, ensemble approaches to stabilize and accelerate inference.

Method: Test-time Adaptive Batched Ensemble Drafting (TABED) ensembles multiple drafts generated via batch inference. It uses deviations from past ground truths observed in the SD setting to adaptively select/weight drafts, with parameter sharing to keep costs negligible. It is training-free and designed for plug-and-play integration with advanced verification and alternative drafting methods.

Result: TABED yields an average robust walltime speedup of 1.74x compared to autoregressive decoding and about 5% improvement over single drafting methods. The approach is training-free and maintains negligible ensembling costs due to parameter sharing; it also supports plug-and-play enhancements.

Conclusion: TABED provides a practical, training-free pathway to accelerate and stabilize inference in LVLMs via adaptive, test-time ensembling of drafts. Its plug-and-play nature and reported gains make it a promising direction for LVLM efficiency, with potential for further verification-method integration.

Abstract: Speculative decoding (SD) has proven effective for accelerating LLM inference by quickly generating draft tokens and verifying them in parallel. However, SD remains largely unexplored for Large Vision-Language Models (LVLMs), which extend LLMs to process both image and text prompts. To address this gap, we benchmark existing inference methods with small draft models on 11 datasets across diverse input scenarios and observe scenario-specific performance fluctuations. Motivated by these findings, we propose Test-time Adaptive Batched Ensemble Drafting (TABED), which dynamically ensembles multiple drafts obtained via batch inference by leveraging deviations from past ground truths available in the SD setting. The dynamic ensemble method achieves an average robust walltime speedup of 1.74x over autoregressive decoding and a 5% improvement over single drafting methods, while remaining training-free and keeping ensembling costs negligible through parameter sharing. With its plug-and-play compatibility, we further enhance TABED by integrating advanced verification and alternative drafting methods. Code and custom-trained models are available at https://github.com/furiosa-ai/TABED.

</details>


### [148] [Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations](https://arxiv.org/abs/2601.20449)
*Fatima Ezzeddine,Obaida Ammar,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: A model-agnostic reinforcement learning approach to generate fair counterfactual explanations (CFs) for ML decisions, enforcing individual and group fairness while preserving CF quality; introduces hybrid fairness and extends fairness metrics; evaluated on three datasets.


<details>
  <summary>Details</summary>
Motivation: XAI requires transparent and fair recourse; CFs should be similar for similar individuals and equitable across protected groups; current CF methods may be unfair or treat fairness as orthogonal.

Method: Formulates CF generation as an optimization with fairness constraints; uses a model-agnostic reinforcement learning framework to produce CFs satisfying both individual- and group-level fairness; extends metrics such as equal choice of recourse and equal effectiveness; evaluation on three benchmark datasets.

Result: Shows the approach achieves both individual and group fairness without compromising CF quality (proximity and plausibility); quantifies the cost of fairness across levels; discusses hybrid fairness implications.

Conclusion: Highlights hybrid fairness as a meaningful concept for XAI and CFs; advocates broader discussion and future work on balancing fairness and recourse quality across domains.

Abstract: Explainable Artificial Intelligence (XAI) is becoming increasingly essential for enhancing the transparency of machine learning (ML) models. Among the various XAI techniques, counterfactual explanations (CFs) hold a pivotal role due to their ability to illustrate how changes in input features can alter an ML model's decision, thereby offering actionable recourse to users. Ensuring that individuals with comparable attributes and those belonging to different protected groups (e.g., demographic) receive similar and actionable recourse options is essential for trustworthy and fair decision-making. In this work, we address this challenge directly by focusing on the generation of fair CFs. Specifically, we start by defining and formulating fairness at: 1) individual fairness, ensuring that similar individuals receive similar CFs, 2) group fairness, ensuring equitable CFs across different protected groups and 3) hybrid fairness, which accounts for both individual and broader group-level fairness. We formulate the problem as an optimization task and propose a novel model-agnostic, reinforcement learning based approach to generate CFs that satisfy fairness constraints at both the individual and group levels, two objectives that are usually treated as orthogonal. As fairness metrics, we extend existing metrics commonly used for auditing ML models, such as equal choice of recourse and equal effectiveness across individuals and groups. We evaluate our approach on three benchmark datasets, showing that it effectively ensures individual and group fairness while preserving the quality of the generated CFs in terms of proximity and plausibility, and quantify the cost of fairness in the different levels separately. Our work opens a broader discussion on hybrid fairness and its role and implications for XAI and beyond CFs.

</details>


### [149] [TINNs: Time-Induced Neural Networks for Solving Time-Dependent PDEs](https://arxiv.org/abs/2601.20361)
*Chen-Yang Dai,Che-Chia Chang,Te-Sheng Lin,Ming-Chih Lai,Chieh-Hsin Lai*

Main category: cs.LG

TL;DR: TINNs parameterize network weights as a function of time, enabling evolving spatial representations in PINNs; solved as nonlinear least-squares with Levenberg–Marquardt, achieving up to 4× accuracy and 10× faster convergence.


<details>
  <summary>Details</summary>
Motivation: Standard space–time PINNs fix weights shared across all times, forcing the same features to capture distinct, time-evolving dynamics, which degrades accuracy and can destabilize training when enforcing PDE, boundary, and initial constraints.

Method: Introduce Time-Induced Neural Networks (TINNs) where network weights are a learned function of time. This yields a time-conditioned, evolving spatial representation. Formulate training as a nonlinear least-squares problem over the PDE residuals and boundary/initial constraints, solved efficiently with Levenberg–Marquardt.

Result: Experiments on various time-dependent PDEs show up to 4× improvements in accuracy and up to 10× faster convergence compared with PINNs and strong baselines.

Conclusion: Time-varying weight parameterization yields more faithful temporal representations and stable, efficient training for time-dependent PDEs; LM-based optimization is effective for the resulting nonlinear least-squares formulation, making TINNs a robust alternative to PINNs.

Abstract: Physics-informed neural networks (PINNs) solve time-dependent partial differential equations (PDEs) by learning a mesh-free, differentiable solution that can be evaluated anywhere in space and time. However, standard space--time PINNs take time as an input but reuse a single network with shared weights across all times, forcing the same features to represent markedly different dynamics. This coupling degrades accuracy and can destabilize training when enforcing PDE, boundary, and initial constraints jointly. We propose Time-Induced Neural Networks (TINNs), a novel architecture that parameterizes the network weights as a learned function of time, allowing the effective spatial representation to evolve over time while maintaining shared structure. The resulting formulation naturally yields a nonlinear least-squares problem, which we optimize efficiently using a Levenberg--Marquardt method. Experiments on various time-dependent PDEs show up to $4\times$ improved accuracy and $10\times$ faster convergence compared to PINNs and strong baselines.

</details>


### [150] [CCMamba: Selective State-Space Models for Higher-Order Graph Learning on Combinatorial Complexes](https://arxiv.org/abs/2601.20518)
*Jiawen Chen,Qi Shao,Mingtong Zhou,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: CCMamba is a unified framework for learning on combinatorial complexes using rank-aware state-space message passing to achieve linear-time, attention-free higher-order reasoning; it matches or surpasses baselines with expressivity up to 1-WL on graph, hypergraph, and simplicial benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome the quadratic complexity and limited expressivity of attention-based local message passing in higher-order combinatorial structures, enabling scalable, degree-aware information aggregation across multi-rank incidence relations.

Method: Reformulate message passing as selective state-space modeling. Organize multi-rank incidence relations into structured sequences processed by rank-aware state-space models, enabling adaptive, directional, and long-range propagation in linear time without self-attention. Prove the expressive power upper-bound of the message passing is the 1-Weisfeiler-Lehman test.

Result: Empirical evaluations on graph, hypergraph, and simplicial benchmarks show CCMamba consistently outperforms existing methods, with improved scalability and robustness to depth.

Conclusion:  CCMamba introduces the first unified mamba-based neural framework for combinatorial complexes, delivering linear-time, high-expressivity (1-WL) message passing and strong empirical performance across order-agnostic benchmarks.

Abstract: Topological deep learning has emerged for modeling higher-order relational structures beyond pairwise interactions that standard graph neural networks fail to capture. Although combinatorial complexes offer a unified topological framework, most existing topological deep learning methods rely on local message passing via attention mechanisms, which incur quadratic complexity and remain low-dimensional, limiting scalability and rank-aware information aggregation in higher-order complexes.We propose Combinatorial Complex Mamba (CCMamba), the first unified mamba-based neural framework for learning on combinatorial complexes. CCMamba reformulates message passing as a selective state-space modeling problem by organizing multi-rank incidence relations into structured sequences processed by rank-aware state-space models. This enables adaptive, directional, and long range information propagation in linear time without self attention. We further establish the theoretical analysis that the expressive power upper-bound of CCMamba message passing is the 1-Weisfeiler-Lehman test. Experiments on graph, hypergraph, and simplicial benchmarks demonstrate that CCMamba consistently outperforms existing methods while exhibiting improved scalability and robustness to depth.

</details>


### [151] [Unsupervised Ensemble Learning Through Deep Energy-based Models](https://arxiv.org/abs/2601.20556)
*Ariel Maymon,Yanir Buznah,Uri Shaham*

Main category: cs.LG

TL;DR: Unsupervised ensemble learning using a deep energy-based meta-learner that leverages only base predictions, with theoretical guarantees under conditional independence, showing superior performance in diverse ensemble scenarios including mixture-of-experts setups.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of fusing multiple learners without access to ground truth labels or problem-specific data, especially when evaluating individual models is difficult or privacy concerns exist.

Method: Train a deep energy-based model that acts as a meta-learner, using only the predictions of individual learners as input. The approach does not use labeled data, learner features, or problem-specific information and aims to capture complex dependencies among base learners. Theoretical guarantees are provided under a conditional independence assumption.

Result: Empirical results indicate superior performance over baselines across diverse ensemble settings, including mixture of experts, on standard ensemble datasets and curated tests designed to probe fusion of expertise.

Conclusion: Unsupervised ensemble learning with a deep energy-based meta-learner demonstrates strong potential for leveraging collective intelligence in data-scarce or privacy-sensitive environments, warranting further exploration and extension.

Abstract: Unsupervised ensemble learning emerged to address the challenge of combining multiple learners' predictions without access to ground truth labels or additional data. This paradigm is crucial in scenarios where evaluating individual classifier performance or understanding their strengths is challenging due to limited information. We propose a novel deep energy-based method for constructing an accurate meta-learner using only the predictions of individual learners, potentially capable of capturing complex dependence structures between them. Our approach requires no labeled data, learner features, or problem-specific information, and has theoretical guarantees for when learners are conditionally independent. We demonstrate superior performance across diverse ensemble scenarios, including challenging mixture of experts settings. Our experiments span standard ensemble datasets and curated datasets designed to test how the model fuses expertise from multiple sources. These results highlight the potential of unsupervised ensemble learning to harness collective intelligence, especially in data-scarce or privacy-sensitive environments.

</details>


### [152] [Robust Distributed Learning under Resource Constraints: Decentralized Quantile Estimation via (Asynchronous) ADMM](https://arxiv.org/abs/2601.20571)
*Anna van Elst,Igor Colin,Stephan Clémençon*

Main category: cs.LG

TL;DR: AsylADMM: a memory-light asynchronous gossip algorithm for decentralized median/quantile estimation on resource-constrained edge devices, with only two variables per node and strong empirical/theoretical results for trimming and robust statistics.


<details>
  <summary>Details</summary>
Motivation: Address the need for robust, communication-efficient decentralized learning on edge devices with limited memory, where existing ADMM-based median estimators require memory scaling with node degree and asynchronous methods struggle with robustness.

Method: Introduce AsylADMM, a gossip-based decentralized algorithm achieving median/quantile estimation with only two variables per node, designed for asynchronous updates. Provide a synchronous variant with theoretical guarantees, empirical evidence of fast convergence for the asynchronous version, and demonstrate support for quantile-based trimming, geometric median estimation, and depth-based trimming. Include a novel Markov-chain-based analysis of rank-based trimming.

Result: The synchronous variant is theoretically guaranteed; the asynchronous algorithm converges quickly in practice; quantile-based trimming outperforms rank-based methods empirically; the paper provides a new theoretical analysis of rank-based trimming via Markov chains.

Conclusion: AsylADMM offers a memory-efficient, robust decentralized estimation framework suitable for edge settings, enabling versatile trimming and robust statistics with both theoretical guarantees and strong empirical performance.

Abstract: Specifications for decentralized learning on resource-constrained edge devices require algorithms that are communication-efficient, robust to data corruption, and lightweight in memory usage. While state-of-the-art gossip-based methods satisfy the first requirement, achieving robustness remains challenging. Asynchronous decentralized ADMM-based methods have been explored for estimating the median, a statistical centrality measure that is notoriously more robust than the mean. However, existing approaches require memory that scales with node degree, making them impractical when memory is limited. In this paper, we propose AsylADMM, a novel gossip algorithm for decentralized median and quantile estimation, primarily designed for asynchronous updates and requiring only two variables per node. We analyze a synchronous variant of AsylADMM to establish theoretical guarantees and empirically demonstrate fast convergence for the asynchronous algorithm. We then show that our algorithm enables quantile-based trimming, geometric median estimation, and depth-based trimming, with quantile-based trimming empirically outperforming existing rank-based methods. Finally, we provide a novel theoretical analysis of rank-based trimming via Markov chain theory.

</details>


### [153] [Ranking-aware Reinforcement Learning for Ordinal Ranking](https://arxiv.org/abs/2601.20585)
*Aiming Hao,Chen Zhu,Jiashu Zhu,Jiahong Wu,Xiangxiang Chu*

Main category: cs.LG

TL;DR: A unified RL framework (RARL) that couples regression with Learning-to-Rank through a ranking-aware reward and noise-based exploration (RMO), achieving improved regression and ranking performance on three benchmarks.


<details>
  <summary>Details</summary>
Motivation: Ordinal regression and ranking involve inherent ordinal dependencies that traditional methods struggle to capture; RL offers sequential decision modeling but requires a unified objective and effective exploration.

Method: Proposes Ranking-Aware Reinforcement Learning (RARL) with a unified objective that integrates regression and L2R training signals via a ranking-aware verifiable reward; optimizes policy through reinforcement learning; introduces Response Mutation Operations (RMO) to inject controlled noise for better exploration and to avoid saddle points.

Result: Extensive experiments on three benchmarks demonstrate the effectiveness of RARL, showing improved alignment between regression accuracy and ranking performance.

Conclusion: RARL with RMO provides a principled framework for jointly optimizing regression and ranking tasks, with enhanced exploration leading to more robust training and better overall performance.

Abstract: Ordinal regression and ranking are challenging due to inherent ordinal dependencies that conventional methods struggle to model. We propose Ranking-Aware Reinforcement Learning (RARL), a novel RL framework that explicitly learns these relationships. At its core, RARL features a unified objective that synergistically integrates regression and Learning-to-Rank (L2R), enabling mutual improvement between the two tasks. This is driven by a ranking-aware verifiable reward that jointly assesses regression precision and ranking accuracy, facilitating direct model updates via policy optimization. To further enhance training, we introduce Response Mutation Operations (RMO), which inject controlled noise to improve exploration and prevent stagnation at saddle points. The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks.

</details>


### [154] [ScatterFusion: A Hierarchical Scattering Transform Framework for Enhanced Time Series Forecasting](https://arxiv.org/abs/2601.20401)
*Wei Li*

Main category: cs.LG

TL;DR: ScatterFusion is a multi-component time-series forecasting framework that combines scattering transforms with hierarchical attention to model multi-scale patterns, featuring HSTM, SAFE, MRTA, and a TSR-guided loss; it reports improved accuracy on seven benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Time-series data exhibit complex, multi-scale temporal dependencies and varying patterns across scales; robust, scalable forecasting requires invariant feature extraction, dynamic scale weighting, and structure-aware supervision.

Method: Four components: (1) Hierarchical Scattering Transform Module (HSTM) for multi-scale invariant feature extraction; (2) Scale-Adaptive Feature Enhancement (SAFE) to dynamically adjust scale importance; (3) Multi-Resolution Temporal Attention (MRTA) to capture dependencies at different horizons; (4) TSR-decomposition-guided structure-aware loss to regularize learning.

Result: Extensive experiments on seven benchmark datasets show ScatterFusion achieving significant reductions in common forecasting error metrics across multiple prediction horizons compared to baseline methods.

Conclusion: Integrating scattering-based representation with hierarchical attention and TSR-guided supervision yields robust, scalable time-series forecasting with strong generalization across diverse datasets.

Abstract: Time series forecasting presents significant challenges due to the complex temporal dependencies at multiple time scales. This paper introduces ScatterFusion, a novel framework that synergistically integrates scattering transforms with hierarchical attention mechanisms for robust time series forecasting. Our approach comprises four key components: (1) a Hierarchical Scattering Transform Module (HSTM) that extracts multi-scale invariant features capturing both local and global patterns; (2) a Scale-Adaptive Feature Enhancement (SAFE) module that dynamically adjusts feature importance across different scales; (3) a Multi-Resolution Temporal Attention (MRTA) mechanism that learns dependencies at varying time horizons; and (4) a Trend-Seasonal-Residual (TSR) decomposition-guided structure-aware loss function. Extensive experiments on seven benchmark datasets demonstrate that ScatterFusion outperforms other common methods, achieving significant reductions in error metrics across various prediction horizons.

</details>


### [155] [AWGformer: Adaptive Wavelet-Guided Transformer for Multi-Resolution Time Series Forecasting](https://arxiv.org/abs/2601.20409)
*Wei Li*

Main category: cs.LG

TL;DR: AWGformer combines adaptive wavelet decomposition with cross-scale attention for multivariate time series forecasting, achieving strong multi-scale and non-stationary performance with theoretical convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Time series contain patterns at multiple temporal scales and frequencies, especially in multivariate, non-stationary data. Standard models struggle to efficiently capture cross-scale interactions and frequency content.

Method: Four components: (1) Adaptive Wavelet Decomposition Module (AWDM) selects wavelet bases and decomposition levels adaptively; (2) Cross-Scale Feature Fusion (CSFF) learns coupling matrices to fuse features across frequency bands; (3) Frequency-Aware Multi-Head Attention (FAMA) weights attention heads by frequency selectivity; (4) Hierarchical Prediction Network (HPN) produces multi-resolution forecasts before reconstruction.

Result: Extensive experiments on benchmark datasets show significant average improvements over state-of-the-art methods, particularly for multi-scale and non-stationary series, with convergence guarantees and a solid link to classical signal processing principles.

Conclusion: The wavelet-guided attention framework delivers practical forecasting gains, aligns with signal processing theory, and offers an efficient, multi-scale approach for multivariate time series forecasting.

Abstract: Time series forecasting requires capturing patterns across multiple temporal scales while maintaining computational efficiency. This paper introduces AWGformer, a novel architecture that integrates adaptive wavelet decomposition with cross-scale attention mechanisms for enhanced multi-variate time series prediction. Our approach comprises: (1) an Adaptive Wavelet Decomposition Module (AWDM) that dynamically selects optimal wavelet bases and decomposition levels based on signal characteristics; (2) a Cross-Scale Feature Fusion (CSFF) mechanism that captures interactions between different frequency bands through learnable coupling matrices; (3) a Frequency-Aware Multi-Head Attention (FAMA) module that weights attention heads according to their frequency selectivity; (4) a Hierarchical Prediction Network (HPN) that generates forecasts at multiple resolutions before reconstruction. Extensive experiments on benchmark datasets demonstrate that AWGformer achieves significant average improvements over state-of-the-art methods, with particular effectiveness on multi-scale and non-stationary time series. Theoretical analysis provides convergence guarantees and establishes the connection between our wavelet-guided attention and classical signal processing principles.

</details>


### [156] [Regularized Gradient Temporal-Difference Learning](https://arxiv.org/abs/2601.20599)
*Hyunjun Na,Donghwan Lee*

Main category: cs.LG

TL;DR: Regularized GTD (R-GTD) stabilizes off-policy evaluation with function approximation by regularizing MSPBE, ensuring a unique solution and convergence even when the feature interaction matrix is singular; supported by theory and experiments.


<details>
  <summary>Details</summary>
Motivation: Off-policy policy evaluation with function approximation can suffer when the feature interaction matrix (FIM) is singular, causing instability and degraded performance in standard GTD algorithms.

Method: Reformulate MSPBE minimization with regularization to derive a regularized GTD algorithm (R-GTD) that guarantees convergence to a unique solution under singular FIM; provide theoretical convergence guarantees and explicit error bounds; validate with empirical experiments.

Result: R-GTD converges to a unique solution even when FIM is singular; explicit error bounds are established; empirical experiments demonstrate improved stability and performance compared to non-regularized GTD variants.

Conclusion: Regularized MSPBE optimization yields a robust GTD framework (R-GTD) with guaranteed convergence and measurable error bounds, effectively addressing instability due to singular FIM in off-policy evaluation.

Abstract: Gradient temporal-difference (GTD) learning algorithms are widely used for off-policy policy evaluation with function approximation. However, existing convergence analyses rely on the restrictive assumption that the so-called feature interaction matrix (FIM) is nonsingular. In practice, the FIM can become singular and leads to instability or degraded performance. In this paper, we propose a regularized optimization objective by reformulating the mean-square projected Bellman error (MSPBE) minimization. This formulation naturally yields a regularized GTD algorithms, referred to as R-GTD, which guarantees convergence to a unique solution even when the FIM is singular. We establish theoretical convergence guarantees and explicit error bounds for the proposed method, and validate its effectiveness through empirical experiments.

</details>


### [157] [Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs](https://arxiv.org/abs/2601.20420)
*Yuhang Liu,Erdun Gao,Dong Gong,Anton van den Hengel,Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: Proposes Concept Component Analysis (ConCA) for unsupervised extraction of human-interpretable concepts from LLMs by unmixing representations into log-posteriors; sparse ConCA variants aim to address ill-posedness and show advantages over Sparse Autoencoders.


<details>
  <summary>Details</summary>
Motivation: SAEs lack formal grounding linking LLM representations to human concepts, causing design/evaluation challenges; need a principled latent-variable framing to recover concept log-posteriors.

Method: Model LLM activations as linear mixtures of per-concept log-posteriors; perform unsupervised linear unmixing to recover concept components; introduce sparse ConCA with sparsity prior; implement 12 variants.

Result: Across multiple LLMs, ConCA extracts meaningful concepts; provides theoretical benefits over SAEs; empirical demonstrations of usefulness.

Conclusion: ConCA offers a theory-backed, unsupervised framework for concept extraction from LLMs; sparsity improves identifiability; potential for more principled interpretability than SAEs.

Abstract: Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs' activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical progress, SAEs suffer from a fundamental theoretical ambiguity: the well-defined correspondence between LLM representations and human-interpretable concepts remains unclear. This lack of theoretical grounding gives rise to several methodological challenges, including difficulties in principled method design and evaluation criteria. In this work, we show that, under mild assumptions, LLM representations can be approximated as a {linear mixture} of the log-posteriors over concepts given the input context, through the lens of a latent variable model where concepts are treated as latent variables. This motivates a principled framework for concept extraction, namely Concept Component Analysis (ConCA), which aims to recover the log-posterior of each concept from LLM representations through a {unsupervised} linear unmixing process. We explore a specific variant, termed sparse ConCA, which leverages a sparsity prior to address the inherent ill-posedness of the unmixing problem. We implement 12 sparse ConCA variants and demonstrate their ability to extract meaningful concepts across multiple LLMs, offering theory-backed advantages over SAEs.

</details>


### [158] [Nonlinear Dimensionality Reduction with Diffusion Maps in Practice](https://arxiv.org/abs/2601.20428)
*Sönke Beier,Paula Pirker-Díaz,Friedrich Pagenkopf,Karoline Wiesner*

Main category: cs.LG

TL;DR: Diffusion Map assessment: highlights practical pitfalls (preprocessing, parameter settings, component selection) and introduces a method to identify the most relevant components; notes that leading components may not be the most informative.


<details>
  <summary>Details</summary>
Motivation: to provide practical guidance on the diffusion map technique by examining how preprocessing, parameter choices, and component selection influence results, which is underrepresented in existing literature.

Method: practice-oriented review with illustrated pitfalls and discussion of a recently introduced technique for identifying relevant components; includes synthesis across applications.

Result: the first diffusion-map components are not necessarily the most relevant; a method is presented to identify the truly relevant components and guide practitioner decisions.

Conclusion: careful preprocessing, parameter tuning, and component relevance assessment are essential; adopting the relevance-identification technique can improve reliability and interpretability of diffusion-map analyses.

Abstract: Diffusion Map is a spectral dimensionality reduction technique which is able to uncover nonlinear submanifolds in high-dimensional data. And, it is increasingly applied across a wide range of scientific disciplines, such as biology, engineering, and social sciences. But data preprocessing, parameter settings and component selection have a significant influence on the resulting manifold, something which has not been comprehensively discussed in the literature so far. We provide a practice oriented review of the Diffusion Map technique, illustrate pitfalls and showcase a recently introduced technique for identifying the most relevant components. Our results show that the first components are not necessarily the most relevant ones.

</details>


### [159] [WFR-MFM: One-Step Inference for Dynamic Unbalanced Optimal Transport](https://arxiv.org/abs/2601.20606)
*Xinyu Wang,Ruoyu Wang,Qiangwei Peng,Peijie Zhou,Tiejun Li*

Main category: cs.LG

TL;DR: WFR-MFM: a mean-flow unbalanced OT method for fast, one-step dynamical predictions in single-cell data.


<details>
  <summary>Details</summary>
Motivation: Need scalable inference for dynamic, mass-varying transport in single-cell biology; current trajectory-based methods are slow.

Method: Propose mean-flow framework for unbalanced flow matching; summarize transport and mass-growth dynamics via mean velocity and mass-growth fields over time; eliminates trajectory simulation; extends to Wasserstein-Fisher-Rao geometry to develop WFR-MFM.

Result: Demonstrates orders-of-magnitude faster inference than baselines on synthetic and real scRNA-seq data while preserving predictive accuracy; enables efficient perturbation-response predictions on large synthetic datasets with thousands of conditions.

Conclusion: Mean-flow unbalanced OT with Wasserstein-Fisher-Rao geometry enables scalable, accurate dynamic modeling in single-cell data; one-step generation mitigates inference bottlenecks.

Abstract: Reconstructing dynamical evolution from limited observations is a fundamental challenge in single-cell biology, where dynamic unbalanced optimal transport provides a principled framework for modeling coupled transport and mass variation. However, existing approaches rely on trajectory simulation at inference time, making inference a key bottleneck for scalable applications. In this work, we propose a mean-flow framework for unbalanced flow matching that summarizes both transport and mass-growth dynamics over arbitrary time intervals using mean velocity and mass-growth fields, enabling fast one-step generation without trajectory simulation. To solve dynamic unbalanced optimal transport under the Wasserstein-Fisher-Rao geometry, we further build on this framework to develop Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM). Across synthetic and real single-cell RNA sequencing datasets, WFR-MFM achieves orders-of-magnitude faster inference than a range of existing baselines while maintaining high predictive accuracy, and enables efficient perturbation response prediction on large synthetic datasets with thousands of conditions.

</details>


### [160] [TimeCatcher: A Variational Framework for Volatility-Aware Forecasting of Non-Stationary Time Series](https://arxiv.org/abs/2601.20448)
*Zhiyu Chen,Minhao Liu,Yanru Zhang*

Main category: cs.LG

TL;DR: TimeCatcher is a volatility-aware variational forecasting framework that augments lightweight MLP-based time series models with a variational encoder and a volatility-aware module to handle non-stationarity and abrupt fluctuations, improving long-term forecasts across diverse domains; code available.


<details>
  <summary>Details</summary>
Motivation: Existing lightweight MLP models assume local stationarity and struggle with highly non-stationary series and abrupt fluctuations (e.g., web traffic), necessitating a method to capture latent dynamics and volatility.

Method: Extend linear architectures with a variational encoder to model latent dynamic patterns in historical data, and incorporate a volatility-aware enhancement to detect and amplify significant local variations, with evaluation on nine real-world datasets across traffic, financial, energy, and weather domains.

Result: TimeCatcher consistently outperforms state-of-the-art baselines, with particularly large improvements in long-term forecasting scenarios characterized by high volatility and sudden fluctuations.

Conclusion: The framework effectively addresses non-stationarity and volatility in time series forecasting, delivering strong long-horizon performance; code is publicly available.

Abstract: Recent lightweight MLP-based models have achieved strong performance in time series forecasting by capturing stable trends and seasonal patterns. However, their effectiveness hinges on an implicit assumption of local stationarity assumption, making them prone to errors in long-term forecasting of highly non-stationary series, especially when abrupt fluctuations occur, a common challenge in domains like web traffic monitoring. To overcome this limitation, we propose TimeCatcher, a novel Volatility-Aware Variational Forecasting framework. TimeCatcher extends linear architectures with a variational encoder to capture latent dynamic patterns hidden in historical data and a volatility-aware enhancement mechanism to detect and amplify significant local variations. Experiments on nine real-world datasets from traffic, financial, energy, and weather domains show that TimeCatcher consistently outperforms state-of-the-art baselines, with particularly large improvements in long-term forecasting scenarios characterized by high volatility and sudden fluctuations. Our code is available at https://github.com/ColaPrinceCHEN/TimeCatcher.

</details>


### [161] [Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability](https://arxiv.org/abs/2601.20642)
*Rohan Asthana,Vasileios Belagiannis*

Main category: cs.LG

TL;DR: A new memorization detection metric for diffusion models that combines isotropic norm and anisotropic angular alignment, computable from pure noise via two forward passes, enabling faster and more accurate detection than prior denoising-free methods; demonstrated on Stable Diffusion v1.4/v2 with a mitigation strategy using the metric.


<details>
  <summary>Details</summary>
Motivation: Memorization in diffusion models leads to exact copies of training data. Existing denoising-free metrics rely on score-norm differences under isotropic assumptions and perform poorly in low-noise, anisotropic regimes; there is a need for a fast, robust detector that works across noise regimes.

Method: Develop a hybrid metric that integrates isotropic score-norm and anisotropic guidance-score alignment. Compute on pure noise with two forward passes (conditional and unconditional) to estimate the relevant quantities, avoiding denoising. Evaluate memorization detection on Stable Diffusion v1.4 and v2, comparing to existing denoising-free methods; show a mitigation strategy by adapting memorized prompts based on the metric.

Result: The proposed metric outperforms existing denoising-free detection methods and is about 5x faster than the previous best approach. Demonstrated efficacy through detection experiments on Stable Diffusion v1.4/v2 and a practical mitigation using metric-informed prompt adaptation.

Conclusion: A unified detection metric that captures both isotropic and anisotropic features yields robust memorization detection across noise regimes, is computationally efficient, and supports practical mitigation of memorized prompts in diffusion-based image generation.

Abstract: Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.

</details>


### [162] [Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations](https://arxiv.org/abs/2601.20477)
*Kadircan Aksoy,Peter Jung,Protim Bhattacharjee*

Main category: cs.LG

TL;DR: Training dynamics of neural classifiers can be understood via Neyman–Pearson optimal binary decision rules; monotonic KL-divergence improvements along training correlate with better error-rate exponents and generalization, suggesting practical training/regularization strategies.


<details>
  <summary>Details</summary>
Motivation: Provide a statistical decision-theoretic lens to explain how supervised training leads to generalization by analyzing how representation-conditioned class-conditional distributions evolve toward NP-optimal tests.

Method: Treat classification as a set of binary hypothesis tests between class-conditional representations; empirically track KL divergence and NP-like likelihood ratios along training trajectories and relate changes to error-rate exponents across networks.

Result: Across training trajectories, well-generalizing networks show monotonic improvements in KL divergence toward Neyman–Pearson optimal rules, corresponding to better error-rate exponents; this behavior provides a unifying explanation across architectures.

Conclusion: The Neyman–Pearson framework offers an explanatory model for training dynamics and suggests training/regularization strategies to encourage NP-aligned decision rules in different neural network classes.

Abstract: We study the supervised training dynamics of neural classifiers through the lens of binary hypothesis testing. We model classification as a set of binary tests between class-conditional distributions of representations and empirically show that, along training trajectories, well-generalizing networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence that relate to error rate exponents. We finally discuss how this yields an explanation and possible training or regularization strategies for different classes of neural networks.

</details>


### [163] [Learning Contextual Runtime Monitors for Safe AI-Based Autonomy](https://arxiv.org/abs/2601.20666)
*Alejandro Luque-Cerpa,Mengyuan Wang,Emil Carlsson,Sanjit A. Seshia,Devdatt Dubhashi,Hazem Torfah*

Main category: cs.LG

TL;DR: Proposes a context-aware monitoring framework for AI-based control ensembles that uses contextual bandits to select the most suitable controller based on current context, yielding safety guarantees and improved utilization of controller diversity; validated in simulated autonomous driving tasks.


<details>
  <summary>Details</summary>
Motivation: ML controllers in cyber-physical systems can degrade in unfamiliar environments; traditional ensembles dilute specialized strengths; need to exploit contextual strengths to enhance safety and performance.

Method: Model the monitor as a contextual learning problem using contextual multi-armed bandits to select the controller best suited to the current context; provide theoretical safety guarantees during controller selection; validate in two autonomous driving simulations.

Result: Shows significant improvements in safety and performance over non-contextual baselines; better utilization of controller diversity; empirical support for theoretical guarantees.

Conclusion: Context-aware monitors enable safer and more effective AI-based control ensembles; framework shows promise for broader deployment, with future work on scaling, feature design, and real-world validation.

Abstract: We introduce a novel framework for learning context-aware runtime monitors for AI-based control ensembles. Machine-learning (ML) controllers are increasingly deployed in (autonomous) cyber-physical systems because of their ability to solve complex decision-making tasks. However, their accuracy can degrade sharply in unfamiliar environments, creating significant safety concerns. Traditional ensemble methods aim to improve robustness by averaging or voting across multiple controllers, yet this often dilutes the specialized strengths that individual controllers exhibit in different operating contexts. We argue that, rather than blending controller outputs, a monitoring framework should identify and exploit these contextual strengths. In this paper, we reformulate the design of safe AI-based control ensembles as a contextual monitoring problem. A monitor continuously observes the system's context and selects the controller best suited to the current conditions. To achieve this, we cast monitor learning as a contextual learning task and draw on techniques from contextual multi-armed bandits. Our approach comes with two key benefits: (1) theoretical safety guarantees during controller selection, and (2) improved utilization of controller diversity. We validate our framework in two simulated autonomous driving scenarios, demonstrating significant improvements in both safety and performance compared to non-contextual baselines.

</details>


### [164] [Continual GUI Agents](https://arxiv.org/abs/2601.20732)
*Ziwei Liu,Borui Kang,Hangjie Yuan,Zixiang Zhao,Wei Li,Yifan Zhu,Tao Feng*

Main category: cs.LG

TL;DR: GUI-AiF: a reinforcement fine-tuning framework for continual GUI agents using APR-iF and ARR-iF to stabilize grounding under shifting GUI distributions; outperforms baselines; first continual GUI agent framework.


<details>
  <summary>Details</summary>
Motivation: The flux of digital environments brings new GUI data, domains, and resolutions over time, causing performance degradation for agents trained on static environments. GUI grounding becomes unstable due to diverse, shifting interaction points and regions.

Method: Introduce GUI-Anchoring in Flux (GUI-AiF), a reinforcement fine-tuning framework with two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards incentivize alignment with shifting interaction points and regions to mitigate over-adaptation to static cues.

Result: Extensive experiments show that GUI-AiF surpasses state-of-the-art baselines in continual GUI agent scenarios under fluxing distributions.

Conclusion: First continual learning framework for GUI agents; demonstrates the untapped potential of reinforcement fine-tuning for continual GUI agents.

Abstract: As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.

</details>


### [165] [An explainable framework for the relationship between dementia and glucose metabolism patterns](https://arxiv.org/abs/2601.20480)
*C. Vázquez-García,F. J. Martínez-Murcia,F. Segovia Román,A. Forte,J. Ramírez,I. Illán,A. Hernández-Segura,C. Jiménez-Mesa,Juan M. Górriz*

Main category: cs.LG

TL;DR: A semi-supervised VAE with a flexible similarity regularization aligns a latent dimension with a cognitive score in ADNI PET data, revealing disease-related patterns (hippocampus and key networks) while other latent factors capture confounds; framework offers interpretable, adaptable analysis of neurodegenerative progression.


<details>
  <summary>Details</summary>
Motivation: High-dimensional neuroimaging data require powerful yet interpretable representations that relate to clinical measures of dementia progression; a semi-supervised VAE can embed disease-relevant structure into latent space while remaining flexible to available supervision.

Method: Train a semi-supervised variational autoencoder with a configurable similarity regularization term that forces selected latent variables to be similar to supervision targets (e.g., cognitive scores). Apply to ADNI PET data, guiding the first latent dimension to align with a cognitive score. Generate average reconstructions across cognitive impairment levels. Conduct voxel-wise GLM to identify metabolic changes and interpret latent axes (first axis disease-related; others as affine/intensity confounds).

Result: First latent dimension aligns with cognitive score; voxel-wise GLM shows reduced metabolism in hippocampus and within Default Mode and Central Executive Networks. Other latent variables encode affine transformations and inter-subject/site variability. Framework extracts disease-related patterns aligned with established Alzheimer's biomarkers.

Conclusion: An interpretable, adaptable semi-supervised VAE framework for studying neurodegenerative progression by aligning latent factors with clinical/biomarker measures, enabling targeted analysis and visualization of disease-related patterns.

Abstract: High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.

</details>


### [166] [Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions](https://arxiv.org/abs/2601.20714)
*Raul de la Rosa,Ivana Dusparic,Nicolas Cardozo*

Main category: cs.LG

TL;DR: MORPHIN is a self-adaptive Q-learning framework that detects non-stationarity and adaptively tunes learning and exploration parameters to changes in reward and action space while preserving past knowledge, yielding faster convergence and continual adaptation (up to 1.7x efficiency) in Gridworld and traffic signal control.


<details>
  <summary>Details</summary>
Motivation: RL agents often face non-stationary environments where reward functions shift and action spaces evolve; standard Q-learning retrains from scratch, risking forgetting and slow adaptation.

Method: MORPHIN combines concept drift detection with dynamic hyperparameter adjustment for learning rate, exploration, and other components; it also preserves prior policy knowledge to mitigate catastrophic forgetting; applied to Q-learning.

Result: Empirical validation shows faster convergence and continuous adaptation compared to standard Q-learning baseline; learning efficiency improved up to 1.7x in Gridworld and traffic signal control simulations.

Conclusion: MORPHIN offers robust online adaptation to non-stationarity for Q-learning, reducing need for full retraining and preserving prior knowledge; potential applicability to various domains with shifting rewards and expanding action spaces.

Abstract: Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.

</details>


### [167] [Reinforcement Unlearning via Group Relative Policy Optimization](https://arxiv.org/abs/2601.20568)
*Efstratios Zaradoukas,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: PURGE introduces Group Relative Policy Optimization for verifiable unlearning, achieving efficient forgetting with improved fluency and robustness while preserving utility.


<details>
  <summary>Details</summary>
Motivation: Current unlearning methods either leak data, degrade model performance, or rely on costly external reward models; there is a need for verifiable, scalable unlearning under data protection and AI governance requirements (e.g., GDPR, EU AI Act).

Method: Formulates unlearning as a verifiable optimization problem within the Group Relative Policy Optimization framework, using an intrinsic reward that penalizes mentions of forbidden concepts to drive safe forgetting and reduce target-token usage.

Result: Token usage per target is reduced by up to 46x compared with state-of-the-art methods; fluency improves by 5.48%, adversarial robustness by 12.02% over the base model; on RWKU benchmark, 11% unlearning effectiveness with 98% preservation of original utility.

Conclusion: Framing LLM unlearning as a verifiable task yields more reliable, efficient, and scalable forgetting, offering a promising direction that combines theoretical guarantees, safety, and deployment efficiency.

Abstract: During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach reduces token usage per target by up to a factor of 46 compared with SotA methods, while improving fluency by 5.48 percent and adversarial robustness by 12.02 percent over the base model. On the Real World Knowledge Unlearning (RWKU) benchmark, PURGE achieves 11 percent unlearning effectiveness while preserving 98 percent of original utility. PURGE shows that framing LLM unlearning as a verifiable task, enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency.

</details>


### [168] [HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs](https://arxiv.org/abs/2601.20745)
*Guoan Wang,Feiyu Wang,Zongwei Lv,Yikun Zong,Tong Yang*

Main category: cs.LG

TL;DR: Hestia: Hessian-guided differentiable QAT using temperature-controlled softmax and Hessian-based annealing to enable extremely low-bit LLMs.


<details>
  <summary>Details</summary>
Motivation: Memory bottleneck and demand for ultra-low-bit quantization; hard rounding plus STE induces gradient mismatch and hamstrings optimization.

Method: Replace hard step with temperature-controlled softmax relaxation; progressive hardening; use tensor-wise Hessian-trace as a curvature signal to drive fine-grained temperature annealing; sensitivity-aware discretization.

Result: On Llama-3.2 1B/3B, outperforms existing ternary QAT baselines with average zero-shot improvements of 5.39% (1B) and 4.34% (3B); demonstrates effective recovery of representational capacity and enables ~1.58-bit models; code available.

Conclusion: Hessian-guided relaxation yields robust optimization for extremely low-bit LLMs, addressing gradient mismatch and enabling better quantization performance.

Abstract: As large language models (LLMs) continue to scale, deployment is increasingly bottlenecked by the memory wall, motivating a shift toward extremely low-bit quantization. However, most quantization-aware training (QAT) methods apply hard rounding and the straight-through estimator (STE) from the beginning of the training, which prematurely discretizes the optimization landscape and induces persistent gradient mismatch between latent weights and quantized weights, hindering effective optimization of quantized models. To address this, we propose Hestia, a Hessian-guided differentiable QAT framework for extremely low-bit LLMs, which replaces the rigid step function with a temperature-controlled softmax relaxation to maintain gradient flow early in training while progressively hardening quantization. Furthermore, Hestia leverages a tensor-wise Hessian trace metric as a lightweight curvature signal to drive fine-grained temperature annealing, enabling sensitivity-aware discretization across the model. Evaluations on Llama-3.2 show that Hestia consistently outperforms existing ternary QAT baselines, yielding average zero-shot improvements of 5.39% and 4.34% for the 1B and 3B models. These results indicate that Hessian-guided relaxation effectively recovers representational capacity, establishing a more robust training path for 1.58-bit LLMs. The code is available at https://github.com/hestia2026/Hestia.

</details>


### [169] [Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical & Dynamic Search Spaces](https://arxiv.org/abs/2601.20800)
*Kaito Baba,Yoshihiko Ozaki,Shuhei Watanabe*

Main category: cs.LG

TL;DR: Introduces condPED-ANOVA, a conditional HPI estimator for top-performing regions in hyperparameter spaces with conditional dependencies; extends PED-ANOVA to handle activation/domain changes; shows naive methods fail and that the proposed estimator yields meaningful importances.


<details>
  <summary>Details</summary>
Motivation: Address the misalignment of hyperparameter importance estimation in conditional hyperparameter spaces by extending PED-ANOVA to conditional settings and providing a closed-form, informative estimator for HPI.

Method: Define conditional HPI for top-performing regions and derive a closed-form estimator that accounts for conditional activation and domain changes of hyperparameters; compare against naive adaptations of existing HPI estimators.

Result: Empirical results show naive adaptations give misleading/uninterpretable importances in conditional settings, while condPED-ANOVA produces importances that faithfully reflect the conditional structure in the search space.

Conclusion: CondPED-ANOVA offers a principled, consistent framework for estimating hyperparameter importance in conditional search spaces, improving interpretability and reliability of HPI in such contexts.

Abstract: We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and efficient way to estimate HPI within the top-performing regions of the search space, it assumes a fixed, unconditional search space and therefore cannot properly handle conditional hyperparameters. To address this, we introduce a conditional HPI for top-performing regions and derive a closed-form estimator that accurately reflects conditional activation and domain changes. Experiments show that naive adaptations of existing HPI estimators yield misleading or uninterpretable importance estimates in conditional settings, whereas condPED-ANOVA consistently provides meaningful importances that reflect the underlying conditional structure.

</details>


### [170] [CoBA: Integrated Deep Learning Model for Reliable Low-Altitude UAV Classification in mmWave Radio Networks](https://arxiv.org/abs/2601.20605)
*Junaid Sajid,Ivo Müürsepp,Luca Reggiani,Davide Scazzoli,Federico Francesco Luigi Mariani,Maurizio Magarini,Rizwan Ahmad,Muhammad Mahtab Alam*

Main category: cs.LG

TL;DR: A CNN-BiLSTM-Attention model (CoBA) uses 5G mmWave radio measurements to classify low-altitude UAVs as operating in authorized versus restricted airspaces, showing superior accuracy over baselines on a TalTech-collected dataset.


<details>
  <summary>Details</summary>
Motivation: Secure, low-altitude UAV operations in dense mmWave environments; current methods struggle with propagation variability and dynamic signal conditions; need reliable airspace authorization classification.

Method: Proposes CoBA: integrated CNN, BiLSTM, and Attention layers to capture spatial and temporal patterns in 5G mmWave measurements. Dataset collected via controlled UAV flights in authorized and restricted scenarios at low altitude (TalTech network). Evaluated against conventional ML models and a fingerprinting benchmark.

Result: CoBA achieves superior accuracy, significantly outperforming all baseline models, indicating strong potential for reliable regulatory UAV airspace monitoring.

Conclusion: CoBA demonstrates the feasibility of accurate low-altitude UAV airspace classification in 5G mmWave environments; further work should address generalization, deployment considerations, and robustness to real-world variability.

Abstract: Uncrewed Aerial Vehicles (UAVs) are increasingly used in civilian and industrial applications, making secure low-altitude operations crucial. In dense mmWave environments, accurately classifying low-altitude UAVs as either inside authorized or restricted airspaces remains challenging, requiring models that handle complex propagation and signal variability. This paper proposes a deep learning model, referred to as CoBA, which stands for integrated Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Attention which leverages Fifth Generation (5G) millimeter-wave (mmWave) radio measurements to classify UAV operations in authorized and restricted airspaces at low altitude. The proposed CoBA model integrates convolutional, bidirectional recurrent, and attention layers to capture both spatial and temporal patterns in UAV radio measurements. To validate the model, a dedicated dataset is collected using the 5G mmWave network at TalTech, with controlled low altitude UAV flights in authorized and restricted scenarios. The model is evaluated against conventional ML models and a fingerprinting-based benchmark. Experimental results show that CoBA achieves superior accuracy, significantly outperforming all baseline models and demonstrating its potential for reliable and regulated UAV airspace monitoring.

</details>


### [171] [Reinforcement Learning via Self-Distillation](https://arxiv.org/abs/2601.20802)
*Jonas Hübotter,Frederike Lübeck,Lejs Behric,Anton Baumann,Marco Bagatella,Daniel Marta,Ido Hakimi,Idan Shenfeld,Thomas Kleine Buening,Carlos Guestrin,Andreas Krause*

Main category: cs.LG

TL;DR: SDPO converts rich, text-based feedback into a dense learning signal for reinforcement learning with verifiable rewards, using self-distillation where the model acts as its own teacher conditioned on feedback. It improves sample efficiency and accuracy over RLVR baselines across reasoning, tool use, and coding tasks, and even enhances performance in scalar-feedback environments by using successful rollouts as implicit feedback. It also speeds discovery on hard binary-reward tasks at test time.


<details>
  <summary>Details</summary>
Motivation: Current RLVR approaches rely on sparse scalar rewards, ignoring rich textual feedback (e.g., error messages, judge feedback) that could guide learning. The paper aims to leverage this rich feedback without external teachers or reward models to address credit-assignment bottlenecks and improve sample efficiency and final performance.

Method: Introduce Self-Distillation Policy Optimization (SDPO). Treat the current model, conditioned on feedback, as a self-teacher and distill its feedback-informed next-token predictions back into the policy. Use in-context retrospective identification of mistakes to generate a denser learning signal from feedback tokens.

Result: SDPO yields better sample efficiency and higher final accuracy than strong RLVR baselines across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6. It also outperforms baselines in standard RLVR environments with only scalar feedback by using successful rollouts as implicit feedback. At test time, applying SDPO to individual questions accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.

Conclusion: SDPO effectively leverages rich feedback by self-distillation, improving RL in verifiable domains, reducing credit-assignment bottlenecks, and speeding downstream discovery and problem-solving, with broad applicability across tasks that provide textual feedback.

Abstract: Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.

</details>


### [172] [GNN Explanations that do not Explain and How to find Them](https://arxiv.org/abs/2601.20815)
*Steve Azzolin,Stefano Teso,Bruno Lepri,Andrea Passerini,Sagar Malhotra*

Main category: cs.LG

TL;DR: This work reveals a critical failure mode of SE-GNN explanations where they can be degenerate or unrelated to the model’s decision, introduces a faithful-explanation metric that detects such failure in both maliciously planted and naturally occurring cases, and provides code for auditing.


<details>
  <summary>Details</summary>
Motivation: SE-GNN explanations are assumed to faithfully reflect the model’s decision path, yet they can be degenerate and misleading. There is a lack of robust auditing tools to detect when explanations do not align with the inference process, potentially masking the use of sensitive attributes.

Method: The authors characterize a failure mode where explanations do not correspond to how SE-GNNs infer labels. They show that models can achieve optimal true risk while producing degenerate explanations and that existing faithfulness metrics often fail to identify these failures. They conduct empirical analyses to demonstrate both malicious planting and natural emergence of degenerate explanations and propose a new faithfulness metric that reliably flags such explanations as unfaithful across settings. Code is released in the supplemental material.

Result: A novel metric is proposed that consistently marks degenerate explanations as unfaithful, improving auditing of SE-GNN explanations. The analysis highlights potential misuse (hiding sensitive attributes) and natural emergence of degenerate explanations, with empirical validation.

Conclusion: Current faithfulness metrics are insufficient to detect degenerate explanations in SE-GNNs. The proposed metric enables reliable auditing in both adversarial and natural scenarios, underscoring the need for routine verification of explanation faithfulness in SE-GNN systems and providing code to facilitate such auditing.

Abstract: Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.

</details>


### [173] [ACFormer: Mitigating Non-linearity with Auto Convolutional Encoder for Time Series Forecasting](https://arxiv.org/abs/2601.20611)
*Gawon Lee,Hanbyeol Park,Minseop Kim,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: CNN-based TSF analysis introduces 'individual receptive field' and ACFormer to combine linear efficiency with nonlinear feature extraction; achieves state-of-the-art on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the gap where linear architectures capture global trends efficiently but struggle with non-linear signals and intra-/inter-channel dependencies in time series forecasting.

Method: Systematic receptive field analysis introducing the 'individual receptive field' (IRF) concept; design of ACFormer with a shared compression module to capture fine-grained information, gated attention to preserve temporal locality, and an independent patch expansion layer to reconstruct variable-specific temporal patterns.

Result: Extensive experiments on multiple TSF benchmarks show state-of-the-art performance and improved handling of high-frequency components, mitigating drawbacks of linear models.

Conclusion: ACFormer demonstrates that reconciling linear projection efficiency with nonlinear feature extraction is possible via IRF-informed architecture design, offering robust, scalable TSF performance.

Abstract: Time series forecasting (TSF) faces challenges in modeling complex intra-channel temporal dependencies and inter-channel correlations. Although recent research has highlighted the efficiency of linear architectures in capturing global trends, these models often struggle with non-linear signals. To address this gap, we conducted a systematic receptive field analysis of convolutional neural network (CNN) TSF models. We introduce the "individual receptive field" to uncover granular structural dependencies, revealing that convolutional layers act as feature extractors that mirror channel-wise attention while exhibiting superior robustness to non-linear fluctuations. Based on these insights, we propose ACFormer, an architecture designed to reconcile the efficiency of linear projections with the non-linear feature-extraction power of convolutions. ACFormer captures fine-grained information through a shared compression module, preserves temporal locality via gated attention, and reconstructs variable-specific temporal patterns using an independent patch expansion layer. Extensive experiments on multiple benchmark datasets demonstrate that ACFormer consistently achieves state-of-the-art performance, effectively mitigating the inherent drawbacks of linear models in capturing high-frequency components.

</details>


### [174] [Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning](https://arxiv.org/abs/2601.20829)
*Minwu Kim,Safal Shrestha,Keith Ross*

Main category: cs.LG

TL;DR: Failure-prefix conditioning is a simple, effective method to elicit informative failure signals in RLVR by training on prefixes from rare incorrect reasoning trajectories. It matches medium-difficulty training gains with token efficiency, improves robustness to misleading prefixes, and gains further when prefixes are refreshed iteratively.


<details>
  <summary>Details</summary>
Motivation: RLVR training saturates due to scarce informative failures; better exposure to failure-prone states can sustain learning.

Method: Condition training on prefixes derived from incorrect reasoning trajectories (failure prefixes) instead of starting from the original question; optionally refresh failure prefixes during training to maintain exploration.

Result: Performance gains comparable to medium-difficulty problem training; preserved token efficiency; improved robustness to misleading failure prefixes; mild trade-off in adhering to correct early reasoning.

Conclusion: Failure-prefix conditioning offers an effective approach to extend RLVR training on saturated problems; iterative refresh yields additional gains post-plateau.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.

</details>


### [175] [DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration](https://arxiv.org/abs/2601.20627)
*Gilles Eerlings,Brent Zoomers,Jori Liesenborgs,Gustavo Rovelo Ruiz,Kris Luyten*

Main category: cs.LG

TL;DR: DIVERSE uses FiLM-based modulation and CMA-ES to search a latent space, producing diverse, high-performing models within the Rashomon set without retraining, demonstrated on MNIST, PneumoniaMNIST, and CIFAR-10.


<details>
  <summary>Details</summary>
Motivation: To obtain multiple functionally distinct models that maintain accuracy (model multiplicity) for robustness, reliability, and fairness, while reducing computational costs associated with retraining.

Method: Augment a pretrained network with Feature-wise Linear Modulation (FiLM) layers and optimize a latent modulation vector via Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to generate FiLM parameters. The search is gradient-free and does not require retraining the base model, tested on MNIST, PneumoniaMNIST, and CIFAR-10.

Result: DIVERSE uncovers multiple high-performing models that differ in predictive behavior yet maintain accuracy, achieving competitive diversity with reduced computational cost compared to retraining to generate Rashomon sets.

Conclusion: DIVERSE offers an efficient, gradient-free framework for exploring the Rashomon set by modulating pretrained networks with FiLM layers and CMA-ES optimization, enabling multiplicity and robustness without retraining.

Abstract: We propose DIVERSE, a framework for systematically exploring the Rashomon set of deep neural networks, the collection of models that match a reference model's accuracy while differing in their predictive behavior. DIVERSE augments a pretrained model with Feature-wise Linear Modulation (FiLM) layers and uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models. Our experiments show that DIVERSE offers a competitive and efficient exploration of the Rashomon set, making it feasible to construct diverse sets that maintain robustness and performance while supporting well-balanced model multiplicity. While retraining remains the baseline to generate Rashomon sets, DIVERSE achieves comparable diversity at reduced computational cost.

</details>


### [176] [A Foundation Model for Virtual Sensors](https://arxiv.org/abs/2601.20634)
*Leon Götz,Lars Frederik Peiss,Erik Sauer,Andreas Udo Sass,Thorsten Bagdonat,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: A foundation model for virtual sensors that jointly predicts multiple sensors with automatic input selection, achieving massive efficiency gains and scalability while maintaining or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Current virtual sensors rely on bespoke models with hand-picked inputs, cannot exploit cross-sensor synergies, and lack benchmarks; time-series foundation models are costly and limited to predicting inputs. A unified, efficient model is needed to support many sensors and leverage synergies.

Method: A unified foundation model for virtual sensors that learns input signals for each sensor and predicts diverse outputs. Multi-task architecture with shared representations enables simultaneous sensing with automatic feature selection and explainability; scales with nearly constant parameter count across hundreds of sensors.

Result: Evaluation on standard benchmark and a large application dataset (>18B samples) shows 415x compute time reduction and 951x memory reduction vs baselines, with predictive quality maintained or improved.

Conclusion: The proposed foundation model overcomes limitations of prior virtual sensors and time-series baselines, enabling efficient, scalable, and explainable virtual sensing across large sensor networks.

Abstract: Virtual sensors use machine learning to predict target signals from available measurements, replacing expensive physical sensors in critical applications. Existing virtual sensor approaches require application-specific models with hand-selected inputs for each sensor, cannot leverage task synergies, and lack consistent benchmarks. At the same time, emerging time series foundation models are computationally expensive and limited to predicting their input signals, making them incompatible with virtual sensors. We introduce the first foundation model for virtual sensors addressing both limitations. Our unified model can simultaneously predict diverse virtual sensors exploiting synergies while maintaining computational efficiency. It learns relevant input signals for each virtual sensor, eliminating expert knowledge requirements while adding explainability. In our large-scale evaluation on a standard benchmark and an application-specific dataset with over 18 billion samples, our architecture achieves 415x reduction in computation time and 951x reduction in memory requirements, while maintaining or even improving predictive quality compared to baselines. Our model scales gracefully to hundreds of virtual sensors with nearly constant parameter count, enabling practical deployment in large-scale sensor networks.

</details>


### [177] [Reward Models Inherit Value Biases from Pretraining](https://arxiv.org/abs/2601.20838)
*Brian Christian,Jessica A. F. Thompson,Elle Michelle Yang,Vincent Adam,Hannah Rose Kirk,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.LG

TL;DR: Reward models inherit value biases from their base LLMs; across 10 open-weight RMs, agency preference correlates with Llama-based models and communion with Gemma-based models, driven by log-prob differences; implicit rewards mirror these biases and the effect persists under ablations, underscoring the importance of pretraining choices for alignment.


<details>
  <summary>Details</summary>
Motivation: To understand how the base model and pretraining influence reward models beyond the downstream fine-tuning and data, and to assess whether RMs reflect their source LLM biases on established human-value axes.

Method: Evaluate 10 open-weight RMs using validated psycholinguistic corpora and the Big Two axes of human values; compare instruction-tuned vs pre-trained configurations; analyze log-probability differences to derive implicit reward scores; conduct ablation studies varying preference data source/quantity.

Result: RMs show significant differences along agency and communion as a function of base model: Llama-based RMs prefer agency, Gemma-based RMs prefer communion. Logits of the base models underpin these differences; implicit reward scores derived from log-probabilities reproduce the same pattern. The effect remains under ablations of data source/quantity, indicating durability.

Conclusion: Open-source base models influence RM outputs and safety/alignment considerations; pretraining-stage choices are as important for value alignment as downstream training, impacting how RMs reflect human preferences.

Abstract: Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the "Big Two" psychological axes, we show a robust preference of Llama RMs for "agency" and a corresponding robust preference of Gemma RMs for "communion." This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.

</details>


### [178] [An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems](https://arxiv.org/abs/2601.20637)
*Panayiotis Ioannou,Pietro Liò,Pietro Cicuta*

Main category: cs.LG

TL;DR: NODEs extrapolate to new boundary conditions when the trajectories share dynamic similarity with training data; symbolic regression recovers equations from noisy data but depends on selecting the correct input variables; with data generated by a NODE trained on 10% of the full simulation, SR recovers two of the three governing equations and provides a good approximation for the third; overall, combining NODEs with SR is a promising approach for discovering physical laws.


<details>
  <summary>Details</summary>
Motivation: Accurately modeling complex dynamical systems and uncovering their governing differential equations, enabling extrapolation to unseen conditions and data-efficient discovery.

Method: Use noisy synthetic data from two damped oscillatory systems to (i) test NODE extrapolation to new boundary conditions and (ii) evaluate SR's ability to recover the underlying equations from noisy ground-truth data; additionally, generate data using a NODE trained on only 10% of the full simulation to assess SR's performance with enriched, limited data.

Result: NODEs can extrapolate effectively to boundary-condition changes when the resulting trajectories exhibit dynamic similarity to training data. SR successfully recovers the governing equations from noisy data, contingent on proper input-variable selection. With data generated by a NODE trained on 10% of the full simulation, SR recovers two of the three governing equations and yields a good approximation for the third.

Conclusion: Enriching limited data with NODEs to enable symbolic regression offers a promising avenue for scientific discovery, though improving SR robustness (especially for the third equation) and variable-selection strategies constitutes future work.

Abstract: Accurately modelling the dynamics of complex systems and discovering their governing differential equations are critical tasks for accelerating scientific discovery. Using noisy, synthetic data from two damped oscillatory systems, we explore the extrapolation capabilities of Neural Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR) to recover the underlying equations. Our study yields three key insights. First, we demonstrate that NODEs can extrapolate effectively to new boundary conditions, provided the resulting trajectories share dynamic similarity with the training data. Second, SR successfully recovers the equations from noisy ground-truth data, though its performance is contingent on the correct selection of input variables. Finally, we find that SR recovers two out of the three governing equations, along with a good approximation for the third, when using data generated by a NODE trained on just 10% of the full simulation. While this last finding highlights an area for future work, our results suggest that using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery.

</details>


### [179] [$\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval](https://arxiv.org/abs/2601.20844)
*Zihao Wang,Hang Yin,Lihui Liu,Hanghang Tong,Yangqiu Song,Ginny Wong,Simon See*

Main category: cs.LG

TL;DR: Defines Minimal Embeddable Dimension (MED) for embedding subset memberships (m elements and C(m,k) subsets of size at most k) in vector spaces. Derives tight theoretical bounds for MED across L2, inner product, and cosine similarity; validates empirically; in a centroid-based simulation, shows MED scales logarithmically with m. Concludes that embedding-based retrieval limits arise from learnability, not geometry, guiding design.


<details>
  <summary>Details</summary>
Motivation: To uncover the fundamental dimensional requirements for representing set memberships via embeddings and to distinguish geometric constraints from learnability limitations in embedding-based retrieval system design.

Method: Theoretical derivation of tight bounds on MED for multiple similarity notions (L2, inner product, cosine). Empirical validation of these bounds across distance metrics. Numerical simulation in a more practical setting where each subset embedding is the centroid of its contained elements, examining how MED grows with m and how it behaves across k (≤ k).

Result: Tight bounds on MED are established and supported numerically for several similarity notions. In the centroid-based simulation, a logarithmic relationship between MED and the number of elements (m) to embed is observed. This suggests that embedding-based retrieval bottlenecks are governed more by learnability than by geometric constraints.

Conclusion: Embedding-based retrieval limitations are likely driven by learnability challenges rather than geometric feasibility. The findings redirect focus toward learning objectives and training strategies for embedding models to improve retrieval performance.

Abstract: This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of "distances" or "similarities," including the $\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.

</details>


### [180] [Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation](https://arxiv.org/abs/2601.20848)
*Weixin Chen,Li Chen,Yuhan Zhao*

Main category: cs.LG

TL;DR: Cofair enables post-training fairness control in recommender systems within a single training run by learning a shared representation and fairness-conditioned adapters, plus a user-level regularization to ensure monotonic fairness improvements across levels; it provides theoretical guarantees and competitive empirical results without retraining for new fairness requirements.


<details>
  <summary>Details</summary>
Motivation: To address the rigidity of existing fairness-aware methods that fix fairness at training time and require retraining for different fairness criteria, which is costly in real-world deployments with shifting stakeholder demands.

Method: Introduce Cofair: a shared representation layer with fairness-conditioned adapter modules that produce user embeddings tailored to different fairness levels; adds a user-level regularization term to guarantee progressive, per-user fairness improvements across these levels; provide theoretical bounds showing the adversarial objective upper-bounds demographic parity and that regularization enforces monotonic fairness.

Result: Empirical validation on multiple datasets and backbone models shows dynamic fairness achievable at different levels, with fairness-accuracy curves comparable or superior to state-of-the-art baselines, and without retraining for each new fairness requirement; code is publicly available.

Conclusion: Cofair delivers post-training fairness control for recommender systems, achieving dynamic, per-user monotonic fairness improvements with solid theoretical guarantees and strong empirical performance, reducing the need for retraining when fairness requirements evolve.

Abstract: Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.

</details>


### [181] [MuRAL-CPD: Active Learning for Multiresolution Change Point Detection](https://arxiv.org/abs/2601.20686)
*Stefano Bertolasi,Diego Carrera,Diego Stucchi,Pasqualina Fragneto,Luigi Amedeo Bianchi*

Main category: cs.LG

TL;DR: Semi-supervised, interactive multiresolution CPD using wavelet decomposition (MuRAL-CPD) that aligns detected changes with user-defined notions of change; shows improved accuracy/interpretability under limited supervision.


<details>
  <summary>Details</summary>
Motivation: Unsupervised CPD lacks task-specific adaptation and cannot exploit user knowledge; need interactive mechanisms to tailor change definitions.

Method: Integrates wavelet-based multiresolution CPD with active learning; user feedback iteratively tunes hyperparameters; updates detection across multiple scales.

Result: Outperforms state-of-the-art CPD methods on real-world datasets, particularly under minimal supervision.

Conclusion: MuRAL-CPD offers improved accuracy and interpretability by incorporating user feedback into a multiresolution CPD framework; demonstrates potential for semi-supervised CPD with limited supervision.

Abstract: Change Point Detection (CPD) is a critical task in time series analysis, aiming to identify moments when the underlying data-generating process shifts. Traditional CPD methods often rely on unsupervised techniques, which lack adaptability to task-specific definitions of change and cannot benefit from user knowledge. To address these limitations, we propose MuRAL-CPD, a novel semi-supervised method that integrates active learning into a multiresolution CPD algorithm. MuRAL-CPD leverages a wavelet-based multiresolution decomposition to detect changes across multiple temporal scales and incorporates user feedback to iteratively optimize key hyperparameters. This interaction enables the model to align its notion of change with that of the user, improving both accuracy and interpretability. Our experimental results on several real-world datasets show the effectiveness of MuRAL-CPD against state-of-the-art methods, particularly in scenarios where minimal supervision is available.

</details>


### [182] [Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation](https://arxiv.org/abs/2601.20854)
*Aníbal Silva,Moisés Santos,André Restivo,Carlos Soares*

Main category: cs.LG

TL;DR: Transformers embedded in VAEs for tabular data reveal a fidelity–diversity trade-off and high Transformer-block redundancy, with near-linear input–output dynamics in the decoder.


<details>
  <summary>Details</summary>
Motivation: Tabular data present mixed data types and complex feature interactions that challenge vanilla VAEs (MLPs). Transformers’ attention offers a route to better capture relationships in such data.

Method: Empirical evaluation of Transformer placements within a VAE across 57 OpenML CC18 datasets, assessing fidelity and diversity, block-wise similarity of Transformer layers, and decoder input–output dynamics.

Result: 1) Placing Transformers to leverage latent and decoder representations yields a trade-off between fidelity and diversity. 2) Consecutive Transformer blocks show high similarity across all components; in the decoder, the input–output relation of a Transformer is approximately linear.

Conclusion: Transformers can enhance tabular-data VAEs but introduce trade-offs and architectural redundancy; findings motivate design choices such as where to place Transformers and the potential for block-sharing or pruning; decoder linearity suggests simpler decoding dynamics.

Abstract: Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.

</details>


### [183] [Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models](https://arxiv.org/abs/2601.20687)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Xiaobo Xia,Ming-Kun Xie,Dong-Dong Wu,Biao Liu,Yuheng Jia,Xin Geng,Masashi Sugiyama,Tat-Seng Chua*

Main category: cs.LG

TL;DR: A practical on-premise RL alignment method for small models using positive-unlabeled distillation that avoids labeled preferences and reward models by deriving preferences from teacher anchor responses and locally sampled candidates.


<details>
  <summary>Details</summary>
Motivation: On-premise deployment constraints (privacy, cost, latency) block traditional RLHF/RL alignment, necessitating a fully local, low-cost approach for small models.

Method: Query a teacher once per prompt to obtain an anchor response; locally sample multiple candidate responses from the student; perform anchor-conditioned self-ranking to derive pairwise or listwise preferences; train the student via direct preference optimization or group relative policy optimization using the induced local preferences (no human labels or reward models).
Theoretical analysis shows the induced preference signal is order-consistent and concentrates on near-optimal candidates, supporting stability.

Result: Experimental results show the approach achieves consistently strong performance under low-cost, on-premise constraints compared to baselines (likely SFT or other reporting methods), though specifics are not provided in the abstract.

Conclusion: PU RL distillation enables fully local RL alignment for small models on-premise by extracting a usable preference signal from a single teacher anchor and self-generated candidates, offering a cost-effective alternative to human-labeling or external reward models.

Abstract: Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.

</details>


### [184] [Evolutionary Strategies lead to Catastrophic Forgetting in LLMs](https://arxiv.org/abs/2601.20861)
*Immanuel Abdi,Akshat Gupta,Micah Mok,Alexander Lu,Nicholas Lee,Gopala Anumanchipalli*

Main category: cs.LG

TL;DR: ES can approach GRPO on math/reasoning with similar compute but exhibits significant forgetting in continual online learning; ES updates are less sparse and have much larger L2 norms than GRPO, which likely drives forgetting.


<details>
  <summary>Details</summary>
Motivation: Assess the viability of gradient-free Evolutionary Strategies (ES) for continual learning in LLMs, focusing on forgetting behavior and how update dynamics compare to gradient-based GRPO.

Method: Empirical analysis comparing ES and GRPO across tasks with increasing update steps; measure forgetting curves for prior abilities; analyze update sparsity and L2 norm magnitudes to explain observed forgetting.

Result: ES achieves performance close to GRPO on math/reasoning with comparable compute budget but shows significant forgetting of prior abilities as updates accumulate; ES updates are substantially less sparse and have orders of magnitude larger L2 norm than GRPO updates.

Conclusion: Forgetting is a major limitation of gradient-free ES in continual learning; future work should mitigate forgetting in ES and better understand its update dynamics to enable online training of models.

Abstract: One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.

</details>


### [185] [Optimal Transport Group Counterfactual Explanations](https://arxiv.org/abs/2601.20692)
*Enrique Valero-Leal,Bernd Bischl,Pedro Larrañaga,Concha Bielza,Giuseppe Casalicchio*

Main category: cs.LG

TL;DR: Explicit optimal-transport-based group counterfactuals enabling generalization to new group members with fewer parameters; reduces to convex optimization (QP/QCQP) for linear classifiers; preserves group geometry with negligible extra transport cost relative to baselines.


<details>
  <summary>Details</summary>
Motivation: Need for group counterfactual explanations that generalize to unseen group members, avoid re-optimizing per instance, and balance tractability with geometric fidelity.

Method: Learn an explicit optimal transport map that transports any group member to its counterfactual, minimizing the group’s total transport cost; enables generalization with a compact parameterization. For linear classifiers, prove that group counterfactuals arise from convex optimization (QP/QCQP).

Result: Demonstrates good generalization to new group members, preserves the group geometry, and incurs only negligible additional transport cost compared to baselines. When model linearity cannot be exploited, the approach still significantly outperforms baselines.

Conclusion: The approach enhances generalization, interpretability, and efficiency of group counterfactual explanations, with a theoretical link to convex optimization under linear models and strong empirical performance; future work could extend to non-linear settings and refined geometry control.

Abstract: Group counterfactual explanations find a set of counterfactual instances to explain a group of input instances contrastively. However, existing methods either (i) optimize counterfactuals only for a fixed group and do not generalize to new group members, (ii) strictly rely on strong model assumptions (e.g., linearity) for tractability or/and (iii) poorly control the counterfactual group geometry distortion. We instead learn an explicit optimal transport map that sends any group instance to its counterfactual without re-optimization, minimizing the group's total transport cost. This enables generalization with fewer parameters, making it easier to interpret the common actionable recourse. For linear classifiers, we prove that functions representing group counterfactuals are derived via mathematical optimization, identifying the underlying convex optimization type (QP, QCQP, ...). Experiments show that they accurately generalize, preserve group geometry and incur only negligible additional transport cost compared to baseline methods. If model linearity cannot be exploited, our approach also significantly outperforms the baselines.

</details>


### [186] [Is Pure Exploitation Sufficient in Exogenous MDPs with Linear Function Approximation?](https://arxiv.org/abs/2601.20694)
*Hao Liang,Jiayu Cheng,Sean R. Sinclair,Yali Du*

Main category: cs.LG

TL;DR: Exo-MDPs allow only exogenous uncertainty; this work shows exploitation-only learning with finite-sample regret bounds, introducing Pure Exploitation Learning (PEL) and LSVI-PE, with new analytical tools; results show no need for exploration.


<details>
  <summary>Details</summary>
Motivation: Address the theoretical gap: despite empirical success of greedy policies, regret analyses for Exo-MDPs have relied on explicit exploration or tabular assumptions. Provide formal justification for exploitation-only methods in Exo-MDPs and extend to continuous spaces.

Method: Introduce Pure Exploitation Learning (PEL) with finite-sample regret guarantees in tabular Exo-MDPs; develop LSVI-PE for continuous, high-dimensional endogenous state spaces; introduce two analytical tools—counterfactual trajectories and Bellman-closed feature transport—to enable accurate value estimation by greedy policies without optimism.

Result: In the tabular setting, PEL achieves a regret of ~O(H^2 |Ξ| sqrt(K)). For continuous endogenous spaces, LSVI-PE yields regret polynomial in feature dimension, exogenous state space, and horizon, independent of the sizes of endogenous state/action spaces. Empirical experiments show PEL consistently outperforms baselines.

Conclusion: Exploration is not necessary in Exo-MDPs; pure exploitation can achieve strong regret guarantees. The paper contributes new analytical tools and methods (PEL, LSVI-PE) that overturn the conventional wisdom and may inform both theory and application in operations research and related domains.

Abstract: Exogenous MDPs (Exo-MDPs) capture sequential decision-making where uncertainty comes solely from exogenous inputs that evolve independently of the learner's actions. This structure is especially common in operations research applications such as inventory control, energy storage, and resource allocation, where exogenous randomness (e.g., demand, arrivals, or prices) drives system behavior. Despite decades of empirical evidence that greedy, exploitation-only methods work remarkably well in these settings, theory has lagged behind: all existing regret guarantees for Exo-MDPs rely on explicit exploration or tabular assumptions. We show that exploration is unnecessary. We propose Pure Exploitation Learning (PEL) and prove the first general finite-sample regret bounds for exploitation-only algorithms in Exo-MDPs. In the tabular case, PEL achieves $\widetilde{O}(H^2|Ξ|\sqrt{K})$. For large, continuous endogenous state spaces, we introduce LSVI-PE, a simple linear-approximation method whose regret is polynomial in the feature dimension, exogenous state space, and horizon, independent of the endogenous state and action spaces. Our analysis introduces two new tools: counterfactual trajectories and Bellman-closed feature transport, which together allow greedy policies to have accurate value estimates without optimism. Experiments on synthetic and resource-management tasks show that PEL consistently outperforming baselines. Overall, our results overturn the conventional wisdom that exploration is required, demonstrating that in Exo-MDPs, pure exploitation is enough.

</details>


### [187] [Structurally Human, Semantically Biased: Detecting LLM-Generated References with Embeddings and GNNs](https://arxiv.org/abs/2601.20704)
*Melika Mobini,Vincent Holst,Floriano Tori,Andres Algaba,Vincent Ginis*

Main category: cs.LG

TL;DR: LLMs can imitate human citation networks in topology, but embedding-based content signals reveal detectable fingerprints; detection should focus on semantic content rather than graph structure.


<details>
  <summary>Details</summary>
Motivation: Assess whether GPT-generated bibliographies are distinguishable from human-generated ones using structure vs semantic content in large citation graphs; test robustness across models and baselines; understand implications for debiasing and evaluation.

Method: Construct paired citation graphs (ground truth vs GPT-4o-generated) for 10,000 focal papers (~275k refs) from SciSciNet; add a field-matched random baseline preserving out-degree and field distributions; compare structure-only features with 3072-dim title/abstract embeddings using random forest on graph aggregates and Graph Neural Networks with node features; replicate with Claude Sonnet 4.5 and other embeddings (OpenAI, SPECTER).

Result: Structure alone poorly separates GPT from ground truth (RF ~0.60) but rejects random baseline (~0.89–0.92); embeddings markedly improve separability (RF ~0.83; GNNs with embeddings ~93% test accuracy for GPT vs ground truth); Claude Sonnet replication yields RF separability ~0.77 with random baseline rejected; consistent finding: LLM bibliographies mimic human topology but carry detectable semantic fingerprints; detection/debiasing should target content signals rather than global graph structure.

Conclusion: Content-based signals, not structure, drive detectability; future work should focus on semantic analysis and debiasing of LLM-generated references.

Abstract: Large language models are increasingly used to curate bibliographies, raising the question: are their reference lists distinguishable from human ones? We build paired citation graphs, ground truth and GPT-4o-generated (from parametric knowledge), for 10,000 focal papers ($\approx$ 275k references) from SciSciNet, and added a field-matched random baseline that preserves out-degree and field distributions while breaking latent structure. We compare (i) structure-only node features (degree/closeness/eigenvector centrality, clustering, edge count) with (ii) 3072-D title/abstract embeddings, using an RF on graph-level aggregates and Graph Neural Networks with node features. Structure alone barely separates GPT from ground truth (RF accuracy $\approx$ 0.60) despite cleanly rejecting the random baseline ($\approx$ 0.89--0.92). By contrast, embeddings sharply increase separability: RF on aggregated embeddings reaches $\approx$ 0.83, and GNNs with embedding node features achieve 93\% test accuracy on GPT vs.\ ground truth. We show the robustness of our findings by replicating the pipeline with Claude Sonnet 4.5 and with multiple embedding models (OpenAI and SPECTER), with RF separability for ground truth vs.\ Claude $\approx 0.77$ and clean rejection of the random baseline. Thus, LLM bibliographies, generated purely from parametric knowledge, closely mimic human citation topology, but leave detectable semantic fingerprints; detection and debiasing should target content signals rather than global graph structure.

</details>


### [188] [Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis](https://arxiv.org/abs/2601.20729)
*Anchen Sun,Zhibin Chen,Xiaodong Cai*

Main category: cs.LG

TL;DR: Semi-supervised Mean-Teacher Cox models (Cox-MT) using labeled and unlabeled data to improve survival prognosis; multi-modal fusion yields strong improvements on TCGA cancer data.


<details>
  <summary>Details</summary>
Motivation: ANN-based Cox-PH models struggle with limited labeled time-to-event data, especially with high-dimensional features; unlabeled data is abundant; need methods to leverage unlabeled data.

Method: Adopt deep semi-supervised Mean Teacher framework to train single-modal (RNA-seq or WSIs) and multi-modal Cox models; use labeled and unlabeled data; Cox-MT; evaluate on TCGA cancers.

Result: Single-modal Cox-MT outperforms Cox-nnet across four cancers; accuracy improves with more unlabeled data; multi-modal Cox-MT substantially outperforms single-modal.

Conclusion: Cox-MT effectively leverages unlabeled data and cross-modal signals to enhance prognostic accuracy; demonstrates utility of semi-supervised, multi-modal learning in survival analysis.

Abstract: The Cox Proportional Hazards (PH) model is widely used in survival analysis. Recently, artificial neural network (ANN)-based Cox-PH models have been developed. However, training these Cox models with high-dimensional features typically requires a substantial number of labeled samples containing information about time-to-event. The limited availability of labeled data for training often constrains the performance of ANN-based Cox models. To address this issue, we employed a deep semi-supervised learning (DSSL) approach to develop single- and multi-modal ANN-based Cox models based on the Mean Teacher (MT) framework, which utilizes both labeled and unlabeled data for training. We applied our model, named Cox-MT, to predict the prognosis of several types of cancer using data from The Cancer Genome Atlas (TCGA). Our single-modal Cox-MT models, utilizing TCGA RNA-seq data or whole slide images, significantly outperformed the existing ANN-based Cox model, Cox-nnet, using the same data set across four types of cancer considered. As the number of unlabeled samples increased, the performance of Cox-MT significantly improved with a given set of labeled data. Furthermore, our multi-modal Cox-MT model demonstrated considerably better performance than the single-modal model. In summary, the Cox-MT model effectively leverages both labeled and unlabeled data to significantly enhance prediction accuracy compared to existing ANN-based Cox models trained solely on labeled data.

</details>


### [189] [SA-PEF: Step-Ahead Partial Error Feedback for Efficient Federated Learning](https://arxiv.org/abs/2601.20738)
*Dawit Kiros Redie,Reza Arablouei,Stefan Werner*

Main category: cs.LG

TL;DR: Step-ahead partial error feedback (SA-PEF) improves biased gradient compression in federated learning by integrating step-ahead correction with partial error feedback, yielding faster warm-up and stable convergence on non-IID data; theory matches standard Fed-SGD rates up to constants, with an optimal α balancing speed and stability; empirically outperforms pure EF.


<details>
  <summary>Details</summary>
Motivation: Address slow decay of residual error under EF in non-IID FL, which causes gradient mismatch and stalled progress in early rounds, while preserving long-term stability under heterogeneous data and partial participation.

Method: Introduce SA-PEF combining step-ahead correction with partial error feedback; parameter α controls the mix (α=0 recovers EF; α=1 yields SAEF). Prove second-moment bound and residual recursion for non-convex objectives with δ-contractive compressors, establishing convergence to stationarity with heterogeneous data and partial participation; derive a step-ahead-controlled residual contraction ρ_r indicating early training acceleration.

Result: Theoretical convergence rate matches standard non-convex Fed-SGD up to constants: O((η, η_0TR)^{-1}) to a variance/heterogeneity floor with fixed inner step size. Identify optimal α that balances warm-up and long-term stability. Empirical experiments across architectures/datasets show SA-PEF reaches target accuracy faster than EF.

Conclusion: SA-PEF effectively blends rapid early progress with long-term stability, with α tuned near its theoretical optimum. It provides provable convergence guarantees under non-convex objectives and heterogeneous data, and demonstrates practical speedups in training across diverse tasks.

Abstract: Biased gradient compression with error feedback (EF) reduces communication in federated learning (FL), but under non-IID data, the residual error can decay slowly, causing gradient mismatch and stalled progress in the early rounds. We propose step-ahead partial error feedback (SA-PEF), which integrates step-ahead (SA) correction with partial error feedback (PEF). SA-PEF recovers EF when the step-ahead coefficient $α=0$ and step-ahead EF (SAEF) when $α=1$. For non-convex objectives and $δ$-contractive compressors, we establish a second-moment bound and a residual recursion that guarantee convergence to stationarity under heterogeneous data and partial client participation. The resulting rates match standard non-convex Fed-SGD guarantees up to constant factors, achieving $O((η,η_0TR)^{-1})$ convergence to a variance/heterogeneity floor with a fixed inner step size. Our analysis reveals a step-ahead-controlled residual contraction $ρ_r$ that explains the observed acceleration in the early training phase. To balance SAEF's rapid warm-up with EF's long-term stability, we select $α$ near its theory-predicted optimum. Experiments across diverse architectures and datasets show that SA-PEF consistently reaches target accuracy faster than EF.

</details>


### [190] [GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning](https://arxiv.org/abs/2601.20753)
*Zhiheng Jiang,Yunzhe Wang,Ryan Marr,Ellen Novoseller,Benjamin T. Files,Volkan Ustun*

Main category: cs.LG

TL;DR: A graph-based benchmark, GraphAllocBench, for preference-conditioned policy learning in multi-objective RL, featuring the CityPlannerEnv sandbox, new metrics PNDS and OS, and evidence that graph-aware models better handle high-dimensional allocation tasks while exposing limits of existing MORL methods; code provided.


<details>
  <summary>Details</summary>
Motivation: Current PCPL benchmarks are toy-like and environment-limited, hindering realism, scalability, and transfer to complex allocation problems. A flexible, graph-enabled benchmark with diverse objectives and preferences is needed to advance PCPL research and promote graph-based methods.

Method: Introduce GraphAllocBench built on CityPlannerEnv, a graph-based resource allocation sandbox. Provides diverse objective functions, varying preference conditions, and high-dimensional scalability. Propose two evaluation metrics (PNDS and OS) to measure preference consistency alongside hypervolume. Evaluate with both MLPs and graph-aware models to reveal limitations of existing MORL approaches and highlight potential of graph neural networks.

Result: GraphAllocBench reveals limitations of current MORL approaches in realistic, high-dimensional tasks. Graph-based models (e.g., Graph Neural Networks) show promise for complex allocation problems. The new PNDS and OS metrics directly assess preference consistency and complement hypervolume, offering a more complete evaluation of PCPL performance.

Conclusion: GraphAllocBench is a versatile, extensible benchmark that enables flexible adjustment of objectives, preferences, and allocation rules, fostering progress in PCPL and encouraging graph-based methods for high-dimensional combinatorial allocation tasks. Code is available at the provided URL.

Abstract: Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL) aims to approximate diverse Pareto-optimal solutions by conditioning policies on user-specified preferences over objectives. This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front. However, existing benchmarks for PCPL are largely restricted to toy tasks and fixed environments, limiting their realism and scalability. To address this gap, we introduce GraphAllocBench, a flexible benchmark built on a novel graph-based resource allocation sandbox environment inspired by city management, which we call CityPlannerEnv. GraphAllocBench provides a rich suite of problems with diverse objective functions, varying preference conditions, and high-dimensional scalability. We also propose two new evaluation metrics -- Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS) -- that directly capture preference consistency while complementing the widely used hypervolume metric. Through experiments with Multi-Layer Perceptrons (MLPs) and graph-aware models, we show that GraphAllocBench exposes the limitations of existing MORL approaches and paves the way for using graph-based methods such as Graph Neural Networks in complex, high-dimensional combinatorial allocation tasks. Beyond its predefined problem set, GraphAllocBench enables users to flexibly vary objectives, preferences, and allocation rules, establishing it as a versatile and extensible benchmark for advancing PCPL. Code: https://anonymous.4open.science/r/GraphAllocBench

</details>


### [191] [Supervised Guidance Training for Infinite-Dimensional Diffusion Models](https://arxiv.org/abs/2601.20756)
*Elizabeth L. Baker,Alexander Denker,Jes Frellsen*

Main category: cs.LG

TL;DR: Proposes a function-space diffusion model framework for Bayesian posterior sampling in inverse problems, using an infinite-dimensional Doob's h-transform to condition on data and a novel Supervised Guidance Training to bypass intractable guidance terms.


<details>
  <summary>Details</summary>
Motivation: Enable sampling from posterior distributions over functions in inverse problems governed by PDEs, leveraging expressive priors from score-based diffusion models in function spaces.

Method: Extend diffusion conditioning to infinite dimensions via Doob's h-transform under two priors (Cameron–Martin or Gaussian-absolute continuity). Decompose conditional score into an unconditional score plus a tractable guidance term; introduce a simulation-free score-matching objective (Supervised Guidance Training) to estimate the guidance term.

Result: Theoretical framework establishing a functional-space conditioning mechanism; a practical training objective that enables posterior sampling without simulating the intractable guidance term; validation through numerical examples in Bayesian inverse problems over function spaces.

Conclusion: This work provides the first function-space method to fine-tune trained diffusion models for accurate posterior sampling, advancing posterior inference in infinite-dimensional settings.

Abstract: Score-based diffusion models have recently been extended to infinite-dimensional function spaces, with uses such as inverse problems arising from partial differential equations. In the Bayesian formulation of inverse problems, the aim is to sample from a posterior distribution over functions obtained by conditioning a prior on noisy observations. While diffusion models provide expressive priors in function space, the theory of conditioning them to sample from the posterior remains open. We address this, assuming that either the prior lies in the Cameron-Martin space, or is absolutely continuous with respect to a Gaussian measure. We prove that the models can be conditioned using an infinite-dimensional extension of Doob's $h$-transform, and that the conditional score decomposes into an unconditional score and a guidance term. As the guidance term is intractable, we propose a simulation-free score matching objective (called Supervised Guidance Training) enabling efficient and stable posterior sampling. We illustrate the theory with numerical examples on Bayesian inverse problems in function spaces. In summary, our work offers the first function-space method for fine-tuning trained diffusion models to accurately sample from a posterior.

</details>


### [192] [Less is More: Clustered Cross-Covariance Control for Offline RL](https://arxiv.org/abs/2601.20765)
*Nan Qiao,Sheng Yue,Shuning Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: Partitioned buffer sampling plus a gradient-based corrective penalty (C^4) aims to reduce TD cross-covariance bias in offline RL, improving stability and returns, especially on small data with out-of-distribution regions.


<details>
  <summary>Details</summary>
Motivation: Offline RL faces distributional shift where OOD data or scarce data cause harmful TD cross-covariance under standard squared-error objectives, biasing updates and degrading policy learning.

Method: Introduce two complementary strategies: (1) partitioned buffer sampling that restricts updates to localized replay partitions to attenuate cross-covariance and align update directions, and (2) an explicit gradient-based corrective penalty that cancels covariance-induced bias within each update. The approach is designed to be easy to integrate with existing implementations (Clustered Cross-Covariance Control for TD, C^4). Theoretical results show partitioning preserves the lower bound property of the maximization objective and mitigates excessive conservatism in extreme OOD areas without changing core PCRL behavior.

Result: The method yields higher stability and up to 30% improvement in returns over prior methods, particularly with small datasets and splits emphasizing OOD regions.

Conclusion: C^4 offers a practical and theoretically sound enhancement for offline RL under distributional shift, combining localized updates with a gradient-based correction to reduce covariance-induced bias while preserving key objective properties and improving empirical performance.

Abstract: A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.

</details>


### [193] [COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI](https://arxiv.org/abs/2601.20772)
*Shakhyar Gogoi*

Main category: cs.LG

TL;DR: A lightweight, stable autoregressive time-series predictor for edge devices, COMET-SG1, uses linear behavior-space encoding with memory-anchored transitions and deterministic state updates to achieve bounded long-horizon predictions. It delivers competitive short-horizon accuracy and substantially reduced drift versus MLP/LSTM/knn baselines, with a compact, fixed-point-friendly and interpretable design for edge/embedded settings.


<details>
  <summary>Details</summary>
Motivation: Edge and embedded AI require models with low computational footprint, predictable long-horizon behavior, and interpretability. RNNs/transformers can incur drift and instability in autoregressive long-horizon forecasting; hence a lightweight, stable alternative is desirable.

Method: A non-deep recurrent approach: linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates within a fully autoregressive framework. The model is designed to be compact and compatible with fixed-point arithmetic for edge deployment, avoiding traditional RNN/transformer architectures.

Result: On non-stationary synthetic time-series data, COMET-SG1 achieves competitive short-horizon accuracy and significantly reduced long-horizon drift compared to MLP, LSTM, and k-NN baselines.

Conclusion: COMET-SG1 offers a practical, interpretable, and stable autoregressive forecasting approach suitable for edge and embedded AI, with bounded long-horizon errors and a small parameter footprint compatible with fixed-point hardware.

Abstract: COMET-SG1 is a lightweight, stability-oriented autoregressive regression model designed for time-series prediction on edge and embedded AI systems. Unlike recurrent neural networks or transformer-based sequence models, COMET-SG1 operates through linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates. This structure prioritizes bounded long-horizon behavior under fully autoregressive inference, a critical requirement for edge deployment where prediction errors accumulate over time. Experiments on non-stationary synthetic time-series data demonstrate that COMET-SG1 achieves competitive short-horizon accuracy while exhibiting significantly reduced long-horizon drift compared to MLP, LSTM, and k-nearest neighbor baselines. With a compact parameter footprint and operations compatible with fixed-point arithmetic, COMET-SG1 provides a practical and interpretable approach for stable autoregressive prediction in edge and embedded AI applications.

</details>


### [194] [Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying](https://arxiv.org/abs/2601.20773)
*Rubén Jiménez,Oriol Pujol*

Main category: cs.LG

TL;DR: Distance-based copying (distillation) converts hard-label black-box copying into a smooth regression problem by using signed distances to the teacher's decision boundary, enabling better boundary geometry recovery and uncertainty signaling; it includes α-governed smoothing with Hölder/Lipschitz control and two label-only distance estimation algorithms, yielding improved fidelity and generalization on synthetic/UCI benchmarks.


<details>
  <summary>Details</summary>
Motivation: Deployed ML systems evolve with data, architectures, and regulations, often with only black-box access and no access to training data or internals. Hard-label copying yields discontinuous decision surfaces, hindering boundary geometry recovery. A distance-based approach uses signed distances to the decision boundary to enable smooth, geometry-aware replication and potential uncertainty estimates.

Method: A distance-based copying (distillation) framework that replaces hard-label supervision with signed distances to the teacher's decision boundary, casting copying as a smooth regression problem. It includes an α-governed smoothing and regularization scheme with Hölder/Lipschitz control over the target surface, and introduces two model-agnostic algorithms to estimate signed distances under label-only access.

Result: Experiments on synthetic problems and UCI benchmarks show consistent improvements in fidelity and generalization accuracy over hard-label baselines, and the method enables distance outputs as uncertainty-related signals for black-box replicas.

Conclusion: Distance-based copying improves boundary geometry recovery and replica quality through smooth regression and uncertainty signaling, offering a practical, model-agnostic pathway for black-box model replication when only label queries are available.

Abstract: Deployed machine learning systems must continuously evolve as data, architectures, and regulations change, often without access to original training data or model internals. In such settings, black-box copying provides a practical refactoring mechanism, i.e. upgrading legacy models by learning replicas from input-output queries alone. When restricted to hard-label outputs, copying turns into a discontinuous surface reconstruction problem from pointwise queries, severely limiting the ability to recover boundary geometry efficiently. We propose a distance-based copying (distillation) framework that replaces hard-label supervision with signed distances to the teacher's decision boundary, converting copying into a smooth regression problem that exploits local geometry. We develop an $α$-governed smoothing and regularization scheme with Hölder/Lipschitz control over the induced target surface, and introduce two model-agnostic algorithms to estimate signed distances under label-only access. Experiments on synthetic problems and UCI benchmarks show consistent improvements in fidelity and generalization accuracy over hard-label baselines, while enabling distance outputs as uncertainty-related signals for black-box replicas.

</details>


### [195] [When More Data Doesn't Help: Limits of Adaptation in Multitask Learning](https://arxiv.org/abs/2601.20774)
*Steve Hanneke,Mingyue Xu*

Main category: cs.LG

TL;DR: A stronger impossibility result for adaptation in multitask learning that persists even with arbitrarily large per-task sample sizes, extending previous no-free-lunch findings.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental statistical limits of multitask learning and explain why additional data per task may not surmount distributional uncertainty; to motivate the study of optimal adaptivity.

Method: Theoretical analysis establishing lower bounds/impossibility for adaptation in multitask learning; extends prior results (no-free-lunch) to the regime of unlimited per-task data; likely uses information-theoretic or worst-case distribution arguments.

Result: Proves a stronger adaptation impossibility that holds irrespective of per-task sample size; shows hardness remains unsolved by data abundance; clarifies the role of data in multitask learning.

Conclusion: Introduces the notion of optimal adaptivity as a potential future direction to characterize achievable performance under multitask learning constraints; highlights intrinsic limits of transfer across tasks.

Abstract: Multitask learning and related frameworks have achieved tremendous success in modern applications. In multitask learning problem, we are given a set of heterogeneous datasets collected from related source tasks and hope to enhance the performance above what we could hope to achieve by solving each of them individually. The recent work of arXiv:2006.15785 has showed that, without access to distributional information, no algorithm based on aggregating samples alone can guarantee optimal risk as long as the sample size per task is bounded.
  In this paper, we focus on understanding the statistical limits of multitask learning. We go beyond the no-free-lunch theorem in arXiv:2006.15785 by establishing a stronger impossibility result of adaptation that holds for arbitrarily large sample size per task. This improvement conveys an important message that the hardness of multitask learning cannot be overcame by having abundant data per task. We also discuss the notion of optimal adaptivity that may be of future interests.

</details>


### [196] [Active Learning for Decision Trees with Provable Guarantees](https://arxiv.org/abs/2601.20775)
*Arshia Soltani Moakhar,Tanapoom Laoaron,Faraz Ghahremani,Kiarash Banihashem,MohammadTaghi Hajiaghayi*

Main category: cs.LG

TL;DR: First analysis of the disagreement coefficient for decision trees in active learning. Establishes polylogarithmic label complexity under two natural assumptions (distinct dimensions along root-to-leaf paths and grid-like input). Presents the first general binary-classification active learning algorithm with a multiplicative (1+ε) guarantee. Combines these to yield a decision-tree active learner with polylog label queries; provides a near-optimal lower bound in ε.


<details>
  <summary>Details</summary>
Motivation: Bridge theory and practice for active learning with decision trees by tying label complexity to the disagreement coefficient, and identify conditions under which polylogarithmic label complexity is achievable.

Method: (i) Derive the disagreement coefficient for decision trees under two assumptions: (a) each root-to-leaf path queries distinct feature dimensions, (b) regular, grid-like input structure; (ii) Develop the first general active learning algorithm for binary classification with a multiplicative (1+ε) guarantee; (iii) Combine results to obtain a decision-tree active learner with polylogarithmic label queries relative to dataset size; (iv) Prove a lower bound showing ε-dependence is near-optimal.

Result: Under the stated assumptions, the disagreement coefficient analysis yields polylogarithmic label complexity in dataset size. The proposed general AL algorithm achieves a (1+ε) multiplicative approximation for binary classification. The decision-tree specific AL algorithm thus attains polylogarithmic label queries with a near-optimal ε-dependence. A lower bound corroborates the tightness of the ε-dependence.

Conclusion: The assumptions are essential: relaxing them leads to polynomial label complexity. The work provides the first polylogarithmic-label-complexity active learning framework for decision trees with a near-optimal dependence on ε, and suggests avenues for relaxing assumptions and extending to broader model classes.

Abstract: This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees-a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic label complexity, (i) each root-to-leaf path queries distinct feature dimensions, and (ii) the input data has a regular, grid-like structure. We show these assumptions are essential, as relaxing them leads to polynomial label complexity. Second, we present the first general active learning algorithm for binary classification that achieves a multiplicative error guarantee, producing a $(1+ε)$-approximate classifier. By combining these results, we design an active learning algorithm for decision trees that uses only a polylogarithmic number of label queries in the dataset size, under the stated assumptions. Finally, we establish a label complexity lower bound, showing our algorithm's dependence on the error tolerance $ε$ is close to optimal.

</details>


### [197] [PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting](https://arxiv.org/abs/2601.20845)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz*

Main category: cs.LG

TL;DR: PatchFormer is a patch-based time series foundation model that pretrains via hierarchical masked reconstruction and uses lightweight adapters for transfer. It delivers strong zero-shot, multi-horizon forecasting with minimal task-specific data and faster inference.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on domain-specific feature engineering and large labeled datasets for time series tasks; enable robust generalization across diverse domains with an efficient foundation model.

Method: Segment time series into learnable patches, build multiscale temporal representations with adaptive cross-scale aggregation, and pretrain using masked patch reconstruction with dynamic masking and objectives for local accuracy and global consistency. Employ cross-domain knowledge distillation and lightweight adapters for efficient transfer.

Result: Achieves state-of-the-art zero-shot multi-horizon forecasting across 24 datasets, reducing MSE by 27.3% relative to strong baselines and requiring 94% less task-specific data. Exhibits near log-linear scaling with up to 100B pretraining points and processes length-512 sequences 3.8× faster than full-sequence transformers.

Conclusion: PatchFormer demonstrates strong cross-domain generalization and data efficiency through a patch-based, multiscale, self-supervised pretraining approach, with efficient transfer and scalable performance.

Abstract: Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [198] [E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2601.19969)
*Haoyuan Deng,Yuanjiang Xue,Haoyang Du,Boyang Zhou,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: A sample-efficient HiL-RL framework, named \method, uses influence-function-based sample selection to stabilize policy entropy, reducing human interventions while improving success in real-world manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Address high sample inefficiency and substantial human labor costs in human-in-the-loop reinforcement learning by intelligently selecting informative samples to balance exploration and exploitation.

Method: Develop influence functions that quantify a sample's effect on policy entropy (estimated via covariance of action probabilities and soft advantages). Prune shortcut samples causing sharp entropy drops and noisy samples with little effect; select samples with moderate influence to guide learning.

Result: Empirical evaluation on four real-world manipulation tasks shows 42.1% higher success rate and 10.1% fewer human interventions compared to the state-of-the-art HiL-RL method; a project page with code and materials is provided.

Conclusion: The proposed sample-selection strategy improves sample efficiency and reduces human labor in HiL-RL, demonstrating practical effectiveness for real-world manipulation tasks.

Abstract: Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \method achieves a 42.1\% higher success rate while requiring 10.1\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.

</details>


### [199] [Just in time Informed Trees: Manipulability-Aware Asymptotically Optimized Motion Planning](https://arxiv.org/abs/2601.19972)
*Kuanqi Cai,Liding Zhang,Xinwen Su,Kejia Chen,Chaoqun Wang,Sami Haddadin,Alois Knoll,Arash Ajoudani,Luis Figueredo*

Main category: cs.RO

TL;DR: Introduces JIT*—an enhanced sampling-based motion planner for high-dimensional robotic systems, combining Just-in-Time connectivity and a Motion Performance module to improve path discovery and manipulability, outperforming traditional planners across R^4 to R^16, demonstrated in single- and dual-arm tasks.


<details>
  <summary>Details</summary>
Motivation: High-dimensional path planning with many obstacles, where achieving feasible and near-optimal paths is difficult due to obstacle clutter, kinematic singularities, and self-collisions in robotic manipulators.

Method: Two modules: (1) Just-in-Time module with Just-in-Time Edge (refines edge connectivity) and Just-in-Time Sample (adjusts sampling density in bottlenecks) to accelerate initial path discovery; (2) Motion Performance module with dynamic switching to balance manipulability and trajectory cost, reducing risk of singularities.

Result: Empirically outperforms traditional sampling-based planners across spaces from R^4 to R^16; validated on single-arm and dual-arm manipulation tasks; results supported by a demonstration video.

Conclusion: JIT* provides efficient and safer motion planning for high-dimensional robotic manipulation by adaptively refining connectivity and prioritizing manipulability, enabling faster initial solutions and improved trajectory quality.

Abstract: In high-dimensional robotic path planning, traditional sampling-based methods often struggle to efficiently identify both feasible and optimal paths in complex, multi-obstacle environments. This challenge is intensified in robotic manipulators, where the risk of kinematic singularities and self-collisions further complicates motion efficiency and safety. To address these issues, we introduce the Just-in-Time Informed Trees (JIT*) algorithm, an enhancement over Effort Informed Trees (EIT*), designed to improve path planning through two core modules: the Just-in-Time module and the Motion Performance module. The Just-in-Time module includes "Just-in-Time Edge," which dynamically refines edge connectivity, and "Just-in-Time Sample," which adjusts sampling density in bottleneck areas to enable faster initial path discovery. The Motion Performance module balances manipulability and trajectory cost through dynamic switching, optimizing motion control while reducing the risk of singularities. Comparative analysis shows that JIT* consistently outperforms traditional sampling-based planners across $\mathbb{R}^4$ to $\mathbb{R}^{16}$ dimensions. Its effectiveness is further demonstrated in single-arm and dual-arm manipulation tasks, with experimental results available in a video at https://youtu.be/nL1BMHpMR7c.

</details>


### [200] [Real-Time Robot Execution with Masked Action Chunking](https://arxiv.org/abs/2601.20130)
*Haoxuan Wang,Gengyu Zhang,Yan Yan,Yuzhang Shang,Ramana Rao Kompella,Gaowen Liu*

Main category: cs.RO

TL;DR: Proposes REMAC for real-time asynchronous robot control, addressing intra-chunk inconsistency and inter-chunk continuity via masked action chunking and prefix-preserved sampling to improve robustness and completion rates without added latency.


<details>
  <summary>Details</summary>
Motivation: Real-time cyber-physical systems require low-latency decision making. Asynchronous inference can预测 next action while current is executing, but misalignment between action and perception causes errors. Prior work focused on chunk-boundary smoothing; this work identifies intra-chunk mismatch as a critical factor needing mitigation.

Method: Introduce REMAC that learns corrective adjustments on a pretrained policy through masked action chunking. Use masked chunking to adapt the policy to execution-time mismatches. Add a prefix-preserved sampling procedure to strengthen inter-chunk continuity.

Result: In simulation and real-world experiments, REMAC enables faster task execution, maintains robustness to varying delays, and achieves higher task completion rates compared to baselines.

Conclusion: REMAC provides more reliable asynchronous inference for real-time robot manipulation without added latency, by addressing intra-chunk inconsistency and inter-chunk discontinuity; the approach yields improved robustness and efficiency across tasks.

Abstract: Real-time execution is essential for cyber-physical systems such as robots. These systems operate in dynamic real-world environments where even small delays can undermine responsiveness and compromise performance. Asynchronous inference has recently emerged as a system-level paradigm for real-time robot manipulation, enabling the next action chunk to be predicted while the current one is being executed. While this approach achieves real-time responsiveness, naive integration often results in execution failure. Previous methods attributed this failure to inter-chunk discontinuity and developed test-time algorithms to smooth chunk boundaries. In contrast, we identify another critical yet overlooked factor: intra-chunk inconsistency, where the robot's executed action chunk partially misaligns with its current perception. To address this, we propose REMAC, which learns corrective adjustments on the pretrained policy through masked action chunking, enabling the policy to remain resilient under mismatches between intended actions and actual execution during asynchronous inference. In addition, we introduce a prefix-preserved sampling procedure to reinforce inter-chunk continuity. Overall, our method delivers more reliable policies without incurring additional latency. Extensive experiments in both simulation and real-world settings demonstrate that our method enables faster task execution, maintains robustness across varying delays, and consistently achieves higher completion rates.

</details>


### [201] [A Taylor Series Approach to Correct Localization Errors in Robotic Field Mapping using Gaussian Processes](https://arxiv.org/abs/2601.20149)
*Muzaffar Qureshi,Tochukwu Elijah Ogri,Kyle Volle,Rushikesh Kamalapurkar*

Main category: cs.RO

TL;DR: GP regression with localization uncertainty updated incrementally via a second-order correction using kernel derivatives, improving accuracy and efficiency without full retraining.


<details>
  <summary>Details</summary>
Motivation: Real-world robot mapping faces state uncertainty from imperfect localization; misalignment between estimated and true measurement locations degrades GP estimates.

Method: Develop a second-order correction algorithm that uses precomputed Jacobians and Hessians of the GP mean and covariance functions and the differentiable kernel to refine GP predictions in real time as better location estimates become available; avoids full retraining by updating via local corrections based on location discrepancy data.

Result: Simulation results show improved prediction accuracy and computational efficiency compared to retraining the full GP model.

Conclusion: The approach enables real-time refinement of GP models under measurement-location discrepancy by exploiting kernel differentiability and precomputed derivatives, offering a practical solution for mobile robot field mapping.

Abstract: Gaussian Processes (GPs) are powerful non-parametric Bayesian models for regression of scalar fields, formulated under the assumption that measurement locations are perfectly known and the corresponding field measurements have Gaussian noise. However, many real-world scalar field mapping applications rely on sensor-equipped mobile robots to collect field measurements, where imperfect localization introduces state uncertainty. Such discrepancies between the estimated and true measurement locations degrade GP mean and covariance estimates. To address this challenge, we propose a method for updating the GP models when improved estimates become available. Leveraging the differentiability of the kernel function, a second-order correction algorithm is developed using the precomputed Jacobians and Hessians of the GP mean and covariance functions for real-time refinement based on measurement location discrepancy data. Simulation results demonstrate improved prediction accuracy and computational efficiency compared to full model retraining.

</details>


### [202] [TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement](https://arxiv.org/abs/2601.20208)
*Wanjun Jia,Kang Li,Fan Yang,Mengfei Duan,Wenrui Chen,Yiming Jiang,Hui Zhang,Kailun Yang,Zhiyong Li,Yaonan Wang*

Main category: cs.RO

TL;DR: TRACER introduces a texture-robust, cross-hierarchical framework for deformable-object affordance grounding that links high-level semantic reasoning to appearance-robust functional regions. It mitigates boundary overflow and improves long-horizon task success by refining spatial predictions via boundary refinement and convergence flow, demonstrated on Fine-AGDDO15 and a real robot, with open-source code planned.


<details>
  <summary>Details</summary>
Motivation: The manipulation of deformable objects suffers from near-infinite degrees of freedom, complex dynamics, and appearance variability, causing vision-based affordance methods to generate fragmented regions and boundary spillover. A need exists for a unified framework that preserves spatial integrity while aligning semantic guidance with physical interaction regions.

Method: TRACER combines: (1) TA-CoT to decompose tasks into hierarchical sub-goals; (2) SCBR to constrain predictions toward authentic interaction manifolds and suppress spillover; (3) ICRF to aggregate noisy pixels into coherent, physically plausible functional regions, enabling cross-hierarchical mapping from semantics to actionable affordances.

Result: Empirical evaluation on the Fine-AGDDO15 dataset and a real robotic platform shows improved affordance grounding precision across textures/patterns and higher success rates for long-horizon tasks, indicating better bridging between high-level reasoning and low-level execution. Code/dataset will be released publicly.

Conclusion: TRACER effectively addresses boundary spillover and fragmentation in deformable-object affordance grounding by enforcing spatial constraints and convergence-driven refinement, enabling robust transfer from semantic planning to physical manipulation.

Abstract: The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.

</details>


### [203] [TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance](https://arxiv.org/abs/2601.20239)
*Zhemeng Zhang,Jiahua Ma,Xincheng Yang,Xin Wen,Yuzhi Zhang,Boyan Li,Yiran Qin,Jin Liu,Can Zhao,Li Kang,Haoqin Hong,Zhenfei Yin,Philip Torr,Hao Su,Ruimao Zhang,Daolin Ma*

Main category: cs.RO

TL;DR: TouchGuide introduces a cross-policy visuo-tactile fusion framework that refines a coarse visual policy with a tactilely-guided Contact Physical Model (CPM) at inference, trained via contrastive learning; data collection via TacUMI enables cost-effective tactile data; across five contact-rich tasks, it outperforms state-of-the-art visuo-tactile methods.


<details>
  <summary>Details</summary>
Motivation: Tactile feedback is underutilized in fine-grained robotic manipulation. Fusing visual and tactile modalities in a low-dimensional action space can enforce physical contact constraints and improve feasibility and realism of actions.

Method: Two-stage inference: (1) a pre-trained visuomotor policy produces a coarse action using only visual inputs during early sampling; (2) a Contact Physical Model (CPM) provides tactile guidance and refines the action using a tactile-informed feasibility score learned via contrastive learning. TacUMI collects tactile data cheaply using rigid fingertips. Training is data-efficient through limited expert demonstrations.

Result: TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies on five challenging contact-rich tasks (e.g., shoe lacing, chip handover).

Conclusion: Cross-policy visuo-tactile fusion in a low-dimensional action space, with a tactile-refinement loop guided by a CPM and data-efficient TacUMI collection, yields substantial empirical gains and practical data-efficiency for contact-rich manipulation.

Abstract: Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

</details>


### [204] [Shallow-π: Knowledge Distillation for Flow-based VLAs](https://arxiv.org/abs/2601.20262)
*Boseong Jeon,Yunho Choi,Taehan Kim*

Main category: cs.RO

TL;DR: Shallow-pi reduces transformer depth in vision-language-action models via knowledge distillation, cutting layers from 18 to 6 to achieve ~2x faster on-device inference with <1% absolute drop in success rate, validated on Jetson devices and humanoid robotics.


<details>
  <summary>Details</summary>
Motivation: Real-time on-device VLA inference demands high efficiency. Prior work focused on token-level pruning; systematic transformer depth reduction for flow-based VLA models under distillation is underexplored, particularly for edge devices.

Method: A principled knowledge-distillation framework (Shallow-pi) that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers, while preserving performance.

Result: Achieves over twofold inference speedup with less than 1% absolute drop in success rate on standard manipulation benchmarks; sets a new state-of-the-art among reduced VLA models.

Conclusion: Demonstrates that substantial depth reduction via distillation enables reliable, real-time VLA deployment on edge hardware, with robust real-world validation across Jetson platforms and humanoid robots.

Abstract: The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.

</details>


### [205] [Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation](https://arxiv.org/abs/2601.20321)
*Yuzhe Huang,Pei Lin,Wanlin Li,Daohan Li,Jiajun Li,Jiaming Jiang,Chenxi Xiao,Ziyuan Jiao*

Main category: cs.RO

TL;DR: TaF-VLA introduces tactile-force alignment to ground tactile observations in interaction forces, addressing the gap of force-aware physical reasoning in vision-language-action (VLA) models for contact-rich robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Vision-only or vision-then-tactile approaches miss the core coupling between surface deformation and interaction dynamics; existing tactile-vision alignment fails to capture history-dependent and force-aware dynamics essential for precise manipulation.

Method: Develop TaF-VLA with an automated tactile-force data acquisition device and the TaF-Dataset (over 10M synchronized tactile observations, 6-axis force/torque, and matrix force maps). Introduce TaF-Adapter to encode tactile observations into discretized latent representations that reflect history-dependent dynamics, and integrate this into a VLA backbone.

Result: Empirical experiments show TaF-VLA outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, demonstrating robustness and force-aware manipulation through cross-modal physical reasoning.

Conclusion: Grounding tactile observations in interaction forces via the TaF-Adapter and TaF-Dataset enables more accurate physical reasoning in VLA models, improving performance on force-sensitive manipulation tasks.

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.

</details>


### [206] [Demonstration-Free Robotic Control via LLM Agents](https://arxiv.org/abs/2601.20334)
*Brian Y. Tsui,Alan Y. Fang,Tiffany J. Hwu*

Main category: cs.RO

TL;DR: Unmodified frontier-LLM agent (Claude Agent SDK) applied to embodied manipulation via FAEA achieves high success on LIBERO (84.9%), ManiSkill3 (85.7%), MetaWorld (96%) without demonstrations or fine-tuning; with optional minimal human feedback, performance improves; demonstrates that deliberative planning via LLMs can substitute for task-specific demonstrations in certain manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Address the reliance of vision-language-action (VLA) models on task-specific demonstrations and fine-tuning; explore whether general-purpose large language model (LLM) agent frameworks can serve as an effective control paradigm for embodied manipulation.

Method: Apply FAEA, an unmodified frontier-agent framework, to embodied manipulation with privileged environment-state access; use iterative reasoning akin to software agents debugging code; evaluate on LIBERO, ManiSkill3, and MetaWorld benchmarks; measure task success with optional one-round human feedback.

Result: Task success rates of 84.9% (LIBERO), 85.7% (ManiSkill3), and 96% (MetaWorld); demonstration-free performance approaches VLA models trained with under 100 demonstrations; with one round of human feedback, LIBERO rises to 88.2%; capable of autonomously exploring novel scenarios and generating trajectories for data augmentation in training.

Conclusion: General-purpose agents can suffice for manipulation tasks that are dominated by deliberative, task-level planning; suggests a practical path for robotics to leverage frontier-model infrastructure and ongoing LLM advances; code available at the provided GitHub link.

Abstract: Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim

</details>


### [207] [RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification](https://arxiv.org/abs/2601.20377)
*Xinyan Chen,Qinchun Li,Ruiqin Ma,Jiaqi Bai,Li Yi,Jianfei Yang*

Main category: cs.RO

TL;DR: RF-MatID is the first open-source, large-scale RF dataset for fine-grained material identification, spanning 4–43.5 GHz with 16 categories (5 superclasses), 142k samples, and geometry perturbations, plus a multi-protocol benchmark to assess in-distribution and cross-domain robustness for ML models.


<details>
  <summary>Details</summary>
Motivation: Address the lack of large public RF datasets and benchmark protocols for learning-based RF material identification, enabling reproducible research and robust real-world deployment.

Method: Data collection across wide frequency range (4–43.5 GHz) with both frequency-domain and time-domain representations, 16 fine-grained categories (5 superclasses), and controlled geometry perturbations (incidence angle, stand-off distance). Introduces five frequency-allocation protocols and a multi-setting, multi-protocol benchmark; evaluates state-of-the-art DL models on in-distribution and out-of-distribution scenarios (cross-angle, cross-distance).

Result: Creation of RF-MatID (142k samples) with diversity in geometry and frequency protocols; provides a comprehensive benchmark suite for RS-based material identification, including OOD assessment.

Conclusion: RF-MatID enables reproducible RF-material identification research, accelerates algorithmic advancement, and supports real-world deployment by offering a large-scale, diverse dataset and standardized benchmarks.

Abstract: Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.

</details>


### [208] [STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation](https://arxiv.org/abs/2601.20381)
*Alexandre Chapin,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: STORM adds a lightweight, multi-phase object-centric adaptor that augments frozen visual foundation models with semantic slots for robotic manipulation, boosting generalization and control without retraining large backbones.


<details>
  <summary>Details</summary>
Motivation: Visual foundation models offer rich perceptual features but lack explicit object-level structure. For robotics, robust manipulation requires task-aligned, object-centric representations that can generalize to distractors without expensive backbone retraining.

Method: Introduce a small set of semantic-aware slots atop frozen foundation features. Use a two-stage training: (1) stabilize object-centric slots via visual–semantic pretraining with language embeddings; (2) jointly adapt slots with a downstream manipulation policy, preserving semantic consistency and aligning perception with task goals.

Result: On object discovery benchmarks and simulated manipulation tasks, STORM improves generalization to visual distractors and manipulation performance compared to using frozen features or end-to-end object-centric training, indicating effective multi-phase adaptation.

Conclusion: Multi-phase adaptation is an efficient mechanism to convert generic foundation-model representations into task-aware, object-centric representations suitable for robotic control.

Abstract: Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.

</details>


### [209] [A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests](https://arxiv.org/abs/2601.20529)
*Julia Richter,David Oberacker,Gabriela Ligeza,Valentin T. Bickel,Philip Arm,William Talbot,Marvin Grosse Besselmann,Florian Kehl,Tristan Schnell,Hendrik Kolvenbach,Rüdiger Dillmann,Arne Roennau,Marco Hutter*

Main category: cs.RO

TL;DR: Authors propose a KPI framework for evaluating multi-robot lunar prospecting trials, linking performance to science objectives and enabling standardized, goal-driven comparisons across field tests.


<details>
  <summary>Details</summary>
Motivation: There is no standardized, science-driven evaluation metric for analog lunar exploration; existing metrics are scenario-specific and hinder cross-study comparisons across diverse robot platforms and setups.

Method: Derivation of a structured KPI framework from three realistic multi-robot lunar scenarios aligned with scientific objectives and operational constraints; validation through a multi-robot field test.

Result: The framework is practical and easy to apply for efficiency and robustness KPIs; precision-oriented KPIs require reliable ground-truth data not always feasible in outdoor analog environments; the framework provides a common evaluation standard for goal-oriented comparisons of field trials.

Conclusion: The proposed KPI framework enables consistent, goal-oriented comparison of multi-robot field trials and supports systematic development of robotic systems for future planetary exploration.

Abstract: Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.

</details>


### [210] [Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands](https://arxiv.org/abs/2601.20555)
*Wadhah Zai El Amri,Nicolás Navarro-Guerrero*

Main category: cs.RO

TL;DR: Scalable vibro-acoustic sensing on a robotic hand with seven piezo microphones and an Audio Spectrogram Transformer achieves sub-5 mm static touch localization; material properties influence cue types; robust to robot motion; datasets/models open-sourced.


<details>
  <summary>Details</summary>
Motivation: Provide affordable, scalable contact perception to replace expensive tactile skins while enabling high-precision localization across surfaces and tasks.

Method: Attach seven low-cost piezoelectric microphones to a robotic hand; record vibrational responses during contact events; process signals with an Audio Spectrogram Transformer to infer contact location; evaluate static and dynamic tasks; compare material effects; test robustness to self-motion; release open-source data and code.

Result: Localization error <5 mm in static conditions; stiff materials yield sharp impulse responses aiding localization; textured materials offer better friction-based features for trajectory tracking; system remains effective during active robot motion; open-source resources provided.

Conclusion: Complex contact dynamics can be decoded from simple vibrational signals, enabling affordable, scalable tactile perception for robotics and accelerating research via open datasets and models.

Abstract: Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.

</details>


### [211] [MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization](https://arxiv.org/abs/2601.20577)
*Baiqing Wang,Helei Cui,Bo Zhang,Xiaolong Zheng,Bin Guo,Zhiwen Yu*

Main category: cs.RO

TL;DR: MeCo introduces a similarity-aware memoization framework for multi-robot collaboration, enabling plan reuse across similar tasks and reducing planning cost, with a dedicated benchmark showing gains.


<details>
  <summary>Details</summary>
Motivation: LLM-based multi-robot planning often replans from scratch for similar tasks; there is a need to exploit task-level similarities to improve efficiency and adaptability.

Method: Proposes similarity testing to fetch relevant prior solutions and reuse plans without re-invoking LLMs; integrates memoization into a framework MeCo; introduces MeCoBench benchmark.

Result: Experiments show substantial reduction in planning costs and higher success rates relative to state-of-the-art methods.

Conclusion: Similarity-aware caching is effective for flexible, efficient LLM-empowered multi-robot collaboration; MeCo plus MeCoBench provide practical evaluation tools.

Abstract: Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.

</details>


### [212] [GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control](https://arxiv.org/abs/2601.20668)
*Shuhao Liao,Peizhuo Li,Xinrong Yang,Linnan Chang,Zhaoxin Fan,Qing Wang,Lei Shi,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: GPO progressively enlarges the action space during PPO-based training for torque-controlled legged robots, improving data collection, stability, and transfer to hardware.


<details>
  <summary>Details</summary>
Motivation: Torque-based legged locomotion has high-dimensional action spaces, hardware constraints, and insufficient exploration with standard RL approaches; existing heuristics (reward shaping, curricula, manual init) are environment-specific and suboptimal for pure torque control.

Method: Introduce Growing Policy Optimization: apply a time-varying action transformation that constrains the effective action space in early training and gradually expands it to enhance exploration. Prove that the transformation preserves the PPO update rule with bounded, vanishing gradient distortion. Validate on quadruped and hexapod agents, including zero-shot hardware deployment.

Result: GPO-trained policies consistently achieve better performance than baselines, with successful zero-shot transfer to hardware, indicating improved sample efficiency, stability, and generalization for legged locomotion.

Conclusion: GPO provides a general, environment-agnostic optimization framework for learning legged locomotion, especially for torque-based control, by shaping exploration through a progressive action-space transformation.

Abstract: Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.

</details>


### [213] [Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model](https://arxiv.org/abs/2601.20682)
*Péter Polcz,Katalin Schäffer,Miklós Koller*

Main category: cs.RO

TL;DR: Estimates joint angles in tendon-driven anthropomorphic hands from tendon displacements and tensions using a DH-based kinematic model and nonlinear optimization, then uses a Jacobian-based PI controller with feedforward for gesture tracking—validated in MuJoCo on a hand with 5-DOF per finger and 6-DOF thumb.


<details>
  <summary>Details</summary>
Motivation: Tendon-driven hands often lack joint encoders to maintain compactness and dexterity; direct sensing is challenging and increases complexity. The work aims to recover joint states from tendon measurements to enable closed-loop control without joint sensors.

Method: 1) Develop a Denavit-Hartenberg-based kinematic framework for an anthropomorphic hand. 2) Use a simplified tendon model to derive a system of nonlinear equations relating tendon states (displacements and tensions) to joint angles. 3) Solve the nonlinear system via nonlinear optimization to estimate joint positions. 4) Apply a Jacobian-based PI controller with an additional feedforward term for closed-loop gesture tracking using the estimated angles. 5) Validate in MuJoCo with the Anatomically Correct Biomechatronic Hand (5-DoF per long finger, 6-DoF on thumb).

Result: The approach yields estimated joint angles sufficient for gesture tracking in simulation, with demonstrated effectiveness and outlined limitations (e.g., model simplifications, tendon modeling assumptions, potential noise and real-time concerns).

Conclusion: Joint angle estimation from tendon states combined with a Jacobian-based PI control and feedforward enables gesture tracking without direct joint sensing in a tendon-driven hand; simulation demonstrates feasibility but highlights limitations related to model fidelity, measurement noise, and real-time execution.

Abstract: Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.

</details>


### [214] [One Step Is Enough: Dispersive MeanFlow Policy Optimization](https://arxiv.org/abs/2601.20701)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DMPO enables true one-step generation for real-time robotic control by combining MeanFlow, dispersive regularization, and RL fine-tuning, achieving 120 Hz+ inference with 5–20x speedups over multi-step baselines, validated in simulation and on a real Panda robot.


<details>
  <summary>Details</summary>
Motivation: Real-time control requires fast inference; existing diffusion/flow-based policies rely on multi-step sampling, causing latency; need a true one-step policy without distillation.

Method: Three components: MeanFlow for single-step inference without knowledge distillation; dispersive regularization to prevent representation collapse; RL fine-tuning to surpass expert demonstrations; lightweight architecture; evaluation on RoboMimic and Gym; real-robot deployment.

Result: DMPO is competitive or superior to multi-step baselines across RoboMimic and OpenAI Gym; exceeds real-time requirements (>120 Hz); 5-20x inference speedup; hundreds of Hz on GPUs; validated on Franka Panda robot.

Conclusion: DMPO demonstrates practical one-step diffusion-like policy generation for real-time robotic control, with strong performance and real-world applicability; scalable to manipulation and locomotion benchmarks with real hardware validation.

Abstract: Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.

</details>


### [215] [Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy](https://arxiv.org/abs/2601.20776)
*Huanyu Tian,Martin Huber,Lingyun Zeng,Zhe Han,Wayne Bennett,Giuseppe Silvestri,Gerardo Mendizabal-Ruiz,Tom Vercauteren,Alejandro Chavez-Badiola,Christos Bergeles*

Main category: cs.RO

TL;DR: A weakly supervised framework for steady-hand robotic micromanipulation combines calibration-aware perception with admittance control, using reusable warm-up trajectories to achieve depth-resolved perception without fiducials or manual depth annotation, yielding micrometer-level lateral accuracy and sub-millimeter depth accuracy, and substantially reducing operator workload.


<details>
  <summary>Details</summary>
Motivation: Reduce labeling burden and external depth cues in microscope-guided robotic manipulation while achieving depth-resolved perception and robust control.

Method: A weakly supervised perception framework that fuses calibration models with observation residuals, leveraging warm-up trajectories to extract implicit spatial information for calibration-aware, depth-resolved perception. Uses an explicit residual-based task-space error budget and an uncertainty budget to inform admittance-control, without external fiducials or manual depth annotation.

Result: Achieves lateral closed-loop accuracy ~49 μm (95% CI, worst-case subset) and depth accuracy ≤291 μm (95% CI) during large in-plane moves. In a within-subject study (N=8), the learned agent reduces NASA-TLX workload by 77.1% vs a simple steady-hand baseline.

Conclusion: Demonstrates that a weakly supervised agent can improve reliability of microscope-guided biomedical micromanipulation with minimal setup, offering a practical framework for microscope-guided intervention.

Abstract: This paper rethinks steady-hand robotic manipulation by using a weakly supervised framework that fuses calibration-aware perception with admittance control. Unlike conventional automation that relies on labor-intensive 2D labeling, our framework leverages reusable warm-up trajectories to extract implicit spatial information, thereby achieving calibration-aware, depth-resolved perception without the need for external fiducials or manual depth annotation. By explicitly characterizing residuals from observation and calibration models, the system establishes a task-space error budget from recorded warm-ups. The uncertainty budget yields a lateral closed-loop accuracy of approx. 49 micrometers at 95% confidence (worst-case testing subset) and a depth accuracy of <= 291 micrometers at 95% confidence bound during large in-plane moves. In a within-subject user study (N=8), the learned agent reduces overall NASA-TLX workload by 77.1% relative to the simple steady-hand assistance baseline. These results demonstrate that the weakly supervised agent improves the reliability of microscope-guided biomedical micromanipulation without introducing complex setup requirements, offering a practical framework for microscope-guided intervention.

</details>


### [216] [A Methodology for Designing Knowledge-Driven Missions for Robots](https://arxiv.org/abs/2601.20797)
*Guillermo GP-Lenza,Carmen DR. Pita-Romero,Miguel Fernandez-Cortizas,Pascual Campoy*

Main category: cs.RO

TL;DR: A methodology for integrating knowledge graphs into ROS 2 to improve autonomous drone missions, demonstrated in Aerostack2 Gazebo search-and-rescue.


<details>
  <summary>Details</summary>
Motivation: Enhance decision-making, data representation, and mission efficiency in ROS 2-based autonomous robotics by structuring and leveraging knowledge graphs.

Method: Define initial/target conditions; decompose tasks; plan sequence; encode task data into a knowledge graph; design the mission in a high-level language; implement in the Aerostack2 framework with Gazebo simulation (search and locate target).

Result: Improved decision-making and mission performance in a simulated search-and-rescue mission through knowledge-graph–driven planning and execution.

Conclusion: A knowledge-graph–based methodology for ROS 2 autonomous missions shows cohesive end-to-end integration from setup to execution and improves mission outcomes.

Abstract: This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.

</details>


### [217] [End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting](https://arxiv.org/abs/2601.20846)
*Jamie Hathaway,Alireza Rastegarpanah,Rustam Stolkin*

Main category: cs.RO

TL;DR: A VAEs-based style-transfer framework enabling sim-to-real RL with unpaired real data to synthesize realistic trajectories for contact-rich robotic manipulation, achieving better efficiency and stability with minimal real-world data.


<details>
  <summary>Details</summary>
Motivation: Address the sim-to-real gap and data scarcity in reinforcement learning for robotics, particularly in uncertain, contact-rich tasks where obtaining real rewards is difficult.

Method: Reinterpret neural style transfer for data synthesis. Use a variational autoencoder to jointly learn self-supervised features for style transfer and generate weakly paired source-target trajectories to improve realism of synthesized trajectories. Train policies on these augmented trajectories and compare to baselines (CycleGAN, CVAE-based time-series translation).

Result: Outperforms baselines in task completion time and behavioral stability with minimal real-world data; robust to geometric and material variation; demonstrates feasibility of policy adaptation without real-world reward information.

Conclusion: Demonstrates viability of sim-to-real policy transfer via self-supervised style transfer and weak pairing, enabling robust manipulation across varying materials and geometries even when real rewards are unavailable.

Abstract: Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.

</details>
