{"id": "2509.09769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09769", "abs": "https://arxiv.org/abs/2509.09769", "authors": ["Rutav Shah", "Shuijing Liu", "Qi Wang", "Zhenyu Jiang", "Sateesh Kumar", "Mingyo Seo", "Roberto Mart\u00edn-Mart\u00edn", "Yuke Zhu"], "title": "MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos", "comment": "11 pages, 9 figures, 5 tables", "summary": "We aim to enable humanoid robots to efficiently solve new manipulation tasks\nfrom a few video examples. In-context learning (ICL) is a promising framework\nfor achieving this goal due to its test-time data efficiency and rapid\nadaptability. However, current ICL methods rely on labor-intensive teleoperated\ndata for training, which restricts scalability. We propose using human play\nvideos -- continuous, unlabeled videos of people interacting freely with their\nenvironment -- as a scalable and diverse training data source. We introduce\nMimicDroid, which enables humanoids to perform ICL using human play videos as\nthe only training data. MimicDroid extracts trajectory pairs with similar\nmanipulation behaviors and trains the policy to predict the actions of one\ntrajectory conditioned on the other. Through this process, the model acquired\nICL capabilities for adapting to novel objects and environments at test time.\nTo bridge the embodiment gap, MimicDroid first retargets human wrist poses\nestimated from RGB videos to the humanoid, leveraging kinematic similarity. It\nalso applies random patch masking during training to reduce overfitting to\nhuman-specific cues and improve robustness to visual differences. To evaluate\nfew-shot learning for humanoids, we introduce an open-source simulation\nbenchmark with increasing levels of generalization difficulty. MimicDroid\noutperformed state-of-the-art methods and achieved nearly twofold higher\nsuccess rates in the real world. Additional materials can be found on:\nut-austin-rpl.github.io/MimicDroid", "AI": {"tldr": "A framework (MimicDroid) enables humanoids to perform few-shot manipulation by learning from unlabeled human play videos, using cross-trajectory action prediction, embodiment retargeting, and robustness techniques to achieve strong real-world performance.", "motivation": "To address the data bottleneck in in-context learning for robots by leveraging scalable, diverse, unlabeled human play videos, enabling rapid adaptation to new objects and environments.", "method": "Extract trajectory pairs with similar manipulation behaviors from human play videos and train a policy to predict the actions of one trajectory conditioned on another. Bridge embodiment gaps by retargeting human wrist poses to the humanoid via kinematic similarity and apply random patch masking during training to reduce overfitting. Evaluate on an open-source simulation benchmark with increasing generalization difficulty and test on real robots.", "result": "MimicDroid outperforms state-of-the-art methods, achieving nearly twofold higher success rates in real-world experiments. The paper also provides an open-source simulation benchmark for evaluating few-shot learning for humanoids.", "conclusion": "Unlabeled human play videos can be a scalable data source for enabling ICL in humanoids, with effective embodiment bridging and robustness strategies; this work demonstrates strong real-world performance gains and suggests directions for scalable, data-efficient robotic manipulation, while highlighting potential biases and remaining sim-to-real challenges."}}
{"id": "2509.09805", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09805", "abs": "https://arxiv.org/abs/2509.09805", "authors": ["Francisco M. L\u00f3pez", "Miles Lenz", "Marco G. Fedozzi", "Arthur Aubret", "Jochen Triesch"], "title": "MIMo grows! Simulating body and sensory development in a multimodal infant model", "comment": "Accepted at IEEE ICDL 2025. 6 pages, 6 figures", "summary": "Infancy is characterized by rapid body growth and an explosive change of\nsensory and motor abilities. However, developmental robots and simulation\nplatforms are typically designed in the image of a specific age, which limits\ntheir ability to capture the changing abilities and constraints of developing\ninfants. To address this issue, we present MIMo v2, a new version of the\nmultimodal infant model. It includes a growing body with increasing actuation\nstrength covering the age range from birth to 24 months. It also features\nfoveated vision with developing visual acuity as well as sensorimotor delays\nmodeling finite signal transmission speeds to and from an infant's brain.\nFurther enhancements of this MIMo version include an inverse kinematics module,\na random environment generator and updated compatiblity with third-party\nsimulation and learning libraries. Overall, this new MIMo version permits\nincreased realism when modeling various aspects of sensorimotor development.\nThe code is available on the official repository\n(https://github.com/trieschlab/MIMo).", "AI": {"tldr": "MIMo v2 advances infant-simulation by scaling a growing body and sensorimotor dynamics from birth to 24 months, with foveated vision, brain-communication delays, KI module, random environments, and better library compatibility.", "motivation": "Current developmental robots typically use fixed-age designs, which miss the evolving anatomy, perception, and timing constraints of real infants. A tunable, development-aware simulator is needed for realistic study and learning.", "method": "Introduce a growing, age-graded body with increasing actuation strength; implement foveated vision with developing visual acuity; model sensorimotor delays (finite signal transmission to/from the brain); add an inverse kinematics module; include a random environment generator; improve compatibility with third-party simulators and learning libraries; release code.", "result": "Enhanced realism in modeling sensorimotor development across infancy, with adjustable physical and perceptual capabilities; versatile tool for sim-to-real studies and developmental learning.", "conclusion": "MIMo v2 provides a more faithful, scalable platform for simulating infancy development and sensorimotor progression, enabling broader research and applications; code available at the project repository."}}
{"id": "2509.09889", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09889", "abs": "https://arxiv.org/abs/2509.09889", "authors": ["Giulia Botta", "Marco Botta", "Cristina Gena", "Alessandro Mazzei", "Massimo Donini", "Alberto Lillo"], "title": "Using the Pepper Robot to Support Sign Language Communication", "comment": "paper presented at ICSR2025", "summary": "Social robots are increasingly experimented in public and assistive settings,\nbut their accessibility for Deaf users remains quite underexplored. Italian\nSign Language (LIS) is a fully-fledged natural language that relies on complex\nmanual and non-manual components. Enabling robots to communicate using LIS\ncould foster more inclusive human robot interaction, especially in social\nenvironments such as hospitals, airports, or educational settings. This study\ninvestigates whether a commercial social robot, Pepper, can produce\nintelligible LIS signs and short signed LIS sentences. With the help of a Deaf\nstudent and his interpreter, an expert in LIS, we co-designed and implemented\n52 LIS signs on Pepper using either manual animation techniques or a MATLAB\nbased inverse kinematics solver. We conducted a exploratory user study\ninvolving 12 participants proficient in LIS, both Deaf and hearing.\nParticipants completed a questionnaire featuring 15 single-choice video-based\nsign recognition tasks and 2 open-ended questions on short signed sentences.\nResults shows that the majority of isolated signs were recognized correctly,\nalthough full sentence recognition was significantly lower due to Pepper's\nlimited articulation and temporal constraints. Our findings demonstrate that\neven commercially available social robots like Pepper can perform a subset of\nLIS signs intelligibly, offering some opportunities for a more inclusive\ninteraction design. Future developments should address multi-modal enhancements\n(e.g., screen-based support or expressive avatars) and involve Deaf users in\nparticipatory design to refine robot expressivity and usability.", "AI": {"tldr": "Pepper can produce a subset of Italian Sign Language signs intelligibly, enabling limited LIS-based interaction; full sentences challenge Pepper's articulation and timing, suggesting multi-modal enhancements are needed.", "motivation": "Address accessibility for Deaf users by enabling LIS-based communication with social robots in public/assistive settings; explore feasibility and limitations of LIS in HRI with a commercial robot.", "method": "Co-design with a Deaf student and an LIS expert; implement 52 LIS signs on Pepper using manual animation or a MATLAB inverse kinematics solver; exploratory study with 12 LIS-proficient participants (Deaf and hearing); video-based sign recognition tasks and open-ended questions on short sentences.", "result": "Isolated signs were mostly recognized; full sentence recognition was significantly lower due to limited articulation and temporal constraints.", "conclusion": "Demonstrates that commercial social robots can perform a subset of LIS signs, offering opportunities for inclusive interaction design; future work should pursue multimodal enhancements (e.g., screen-based support, expressive avatars) and involve Deaf users through participatory design to refine robot expressivity and usability."}}
{"id": "2509.09893", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09893", "abs": "https://arxiv.org/abs/2509.09893", "authors": ["Hanbit Oh", "Masaki Murooka", "Tomohiro Motoda", "Ryoichi Nakajo", "Yukiyasu Domae"], "title": "Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision", "comment": "Under review", "summary": "Imitation learning is a promising paradigm for training robot agents;\nhowever, standard approaches typically require substantial data acquisition --\nvia numerous demonstrations or random exploration -- to ensure reliable\nperformance. Although exploration reduces human effort, it lacks safety\nguarantees and often results in frequent collisions -- particularly in\nclearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual\nenvironmental resets and imposing additional human burden. This study proposes\nSelf-Augmented Robot Trajectory (SART), a framework that enables policy\nlearning from a single human demonstration, while safely expanding the dataset\nthrough autonomous augmentation. SART consists of two stages: (1) human\nteaching only once, where a single demonstration is provided and precision\nboundaries -- represented as spheres around key waypoints -- are annotated,\nfollowed by one environment reset; (2) robot self-augmentation, where the robot\ngenerates diverse, collision-free trajectories within these boundaries and\nreconnects to the original demonstration. This design improves the data\ncollection efficiency by minimizing human effort while ensuring safety.\nExtensive evaluations in simulation and real-world manipulation tasks show that\nSART achieves substantially higher success rates than policies trained solely\non human-collected demonstrations. Video results available at\nhttps://sites.google.com/view/sart-il .", "AI": {"tldr": "SART enables learning from a single human demonstration with safety-guided autonomous data augmentation, achieving higher success with less human effort.", "motivation": "Imitation learning typically requires large amounts of data from demonstrations or exploration, which is data-inefficient and safety-critical during exploration. There is a need for safer, more data-efficient data collection in manipulation tasks.", "method": "Two-stage framework: (1) single human demonstration with precision boundaries (spheres) around key waypoints and a single environment reset; (2) autonomous self-augmentation where the robot generates diverse, collision-free trajectories within these boundaries and reconnects to the original demonstration.", "result": "SART yields substantially higher success rates than policies trained only on human demonstrations, validated by extensive simulations and real-world manipulation experiments; video results are provided.", "conclusion": "SART improves data efficiency and safety in imitation learning by enabling policy learning from minimal human input with autonomous augmentation, reducing human effort while maintaining performance."}}
{"id": "2509.09981", "categories": ["cs.CG"], "pdf": "https://arxiv.org/pdf/2509.09981", "abs": "https://arxiv.org/abs/2509.09981", "authors": ["Jaehoon Chung", "Kazuo Iwama", "Chung-Shou Liao", "Hee-Kap Ahn"], "title": "Minimum Partition of Polygons under Width and Cut Constraints", "comment": null, "summary": "We study the problem of partitioning a polygon into the minimum number of\nsubpolygons using cuts in predetermined directions such that each resulting\nsubpolygon satisfies a given width constraint. A polygon satisfies the\nunit-width constraint for a set of unit vectors if the length of the orthogonal\nprojection of the polygon on a line parallel to a vector in the set is at most\none. We analyze structural properties of the minimum partition numbers,\nfocusing on monotonicity under polygon containment. We show that the minimum\npartition number of a simple polygon is at least that of any subpolygon,\nprovided that the subpolygon satisfies a certain orientation-wise convexity\nwith respect to the polygon. As a consequence, we prove a partition analogue of\nBang's conjecture about coverings of convex regions in the plane: for any\npartition of a convex body in the plane, the sum of relative widths of all\nparts is at least one. For any convex polygon, there exists a direction along\nwhich an optimal partition is achieved by parallel cuts. Given such a\ndirection, an optimal partition can be computed in linear time.", "AI": {"tldr": "Minimum-partitioning of polygons with cuts in fixed directions under unit-width constraints is studied. The authors establish monotonicity under containment for certain subpolygons, prove a Bang-like width-sum bound for convex partitions, show that for convex polygons there exists a direction yielding optimal parallel-cuts that can be computed in linear time.", "motivation": "Understand how minimum partition numbers behave with respect to subpolygons and containment, under width constraints and fixed directions, and to connect with geometric covering results (Bang-like conjectures).", "method": "Structural analysis of minimum partition numbers, introducing orientation-wise convexity, deriving monotonicity results, proving a partition analogue of Bang's conjecture for convex bodies, proving existence of a special direction for convex polygons with optimal parallel-cuts, and designing a linear-time algorithm for that case.", "result": "1) The minimum partition number of a simple polygon is at least that of certain subpolygons satisfying orientation-wise convexity. 2) For convex bodies, the sum of relative widths of parts in any partition is at least one (Bang-like bound). 3) For any convex polygon, there exists a direction where an optimal partition uses parallel cuts. 4) Given such a direction, the optimal partition can be computed in linear time.", "conclusion": "The work links structural properties of width-constrained polygon partitions to efficient algorithms, yielding monotonicity results and a linear-time partition method for convex polygons in a suitable direction, while extending a Bang-type covering bound to partitions."}}
{"id": "2509.09720", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.09720", "abs": "https://arxiv.org/abs/2509.09720", "authors": ["Akansel Cosgun", "Lachlan Chumbley", "Benjamin J. Meyer"], "title": "Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision", "comment": null, "summary": "This paper introduces the Australian Supermarket Object Set (ASOS), a\ncomprehensive dataset comprising 50 readily available supermarket items with\nhigh-quality 3D textured meshes designed for benchmarking in robotics and\ncomputer vision applications. Unlike existing datasets that rely on synthetic\nmodels or specialized objects with limited accessibility, ASOS provides a\ncost-effective collection of common household items that can be sourced from a\nmajor Australian supermarket chain. The dataset spans 10 distinct categories\nwith diverse shapes, sizes, and weights. 3D meshes are acquired by a\nstructure-from-motion techniques with high-resolution imaging to generate\nwatertight meshes. The dataset's emphasis on accessibility and real-world\napplicability makes it valuable for benchmarking object detection, pose\nestimation, and robotics applications.", "AI": {"tldr": "ASOS is a new real-world, 3D textured object dataset of 50 common supermarket items from Australia, aimed at robotics/vision benchmarking.", "motivation": "To overcome reliance on synthetic models or niche, hard-to-access objects by providing a cost-effective, real-world, diverse set of everyday items that can be sourced from a major supermarket.", "method": "50 items across 10 categories, with high-quality 3D textured meshes generated from structure-from-motion using high-resolution images to produce watertight meshes; items sourced from a major Australian supermarket chain; diverse shapes, sizes, and weights.", "result": "A readily accessible real-world 3D object dataset suitable for benchmarking object detection, pose estimation, and robotics applications, emphasizing practicality and applicability.", "conclusion": "ASOS offers a cost-effective, real-world object set that broadens benchmarking capabilities for robotics and computer vision, potentially aiding reproducibility and cross-domain research."}}
{"id": "2509.09744", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09744", "abs": "https://arxiv.org/abs/2509.09744", "authors": ["Mujie Liu", "Chenze Wang", "Liping Chen", "Nguyen Linh Dan Le", "Niharika Tewari", "Ting Dang", "Jiangang Ma", "Feng Xia"], "title": "Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis", "comment": null, "summary": "The limited availability of labeled brain network data makes it challenging\nto achieve accurate and interpretable psychiatric diagnoses. While\nself-supervised learning (SSL) offers a promising solution, existing methods\noften rely on augmentation strategies that can disrupt crucial structural\nsemantics in brain graphs. To address this, we propose SAM-BG, a two-stage\nframework for learning brain graph representations with structural semantic\npreservation. In the pre-training stage, an edge masker is trained on a small\nlabeled subset to capture key structural semantics. In the SSL stage, the\nextracted structural priors guide a structure-aware augmentation process,\nenabling the model to learn more semantically meaningful and robust\nrepresentations. Experiments on two real-world psychiatric datasets demonstrate\nthat SAM-BG outperforms state-of-the-art methods, particularly in small-labeled\ndata settings, and uncovers clinically relevant connectivity patterns that\nenhance interpretability. Our code is available at\nhttps://github.com/mjliu99/SAM-BG.", "AI": {"tldr": "A two-stage self-supervised learning method for brain graphs (SAM-BG) that preserves structural semantics by learning an edge masker on a small labeled subset and using structural priors to guide structure-aware augmentation, achieving superior performance in low-label settings and enhancing interpretability.", "motivation": "Brain network data are scarce and annotating labels is costly; conventional SSL augmentations can distort meaningful brain graph structure, hindering both performance and interpretability.", "method": "Stage 1: train an edge masker on a small labeled subset to capture key structural semantics. Stage 2: use extracted structural priors to guide a structure-aware augmentation during SSL, learning representations that respect brain graph structure.", "result": "SAM-BG outperforms state-of-the-art SSL methods, especially with limited labeled data, and reveals clinically relevant connectivity patterns that improve interpretability.", "conclusion": "Preserving structural semantics via a two-stage framework yields more robust, semantically meaningful, and interpretable brain graph representations for psychiatric diagnosis tasks."}}
{"id": "2509.09738", "categories": ["cs.AI", "q-bio.QM", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.09738", "abs": "https://arxiv.org/abs/2509.09738", "authors": ["Umut Eser", "Yael Gozin", "L. Jay Stallons", "Ari Caroline", "Martin Preusse", "Brandon Rice", "Scott Wright", "Andrew Robertson"], "title": "Human-AI Collaboration Increases Efficiency in Regulatory Writing", "comment": null, "summary": "Background: Investigational New Drug (IND) application preparation is\ntime-intensive and expertise-dependent, slowing early clinical development.\nObjective: To evaluate whether a large language model (LLM) platform (AutoIND)\ncan reduce first-draft composition time while maintaining document quality in\nregulatory submissions. Methods: Drafting times for IND nonclinical written\nsummaries (eCTD modules 2.6.2, 2.6.4, 2.6.6) generated by AutoIND were directly\nrecorded. For comparison, manual drafting times for IND summaries previously\ncleared by the U.S. FDA were estimated from the experience of regulatory\nwriters ($\\geq$6 years) and used as industry-standard benchmarks. Quality was\nassessed by a blinded regulatory writing assessor using seven pre-specified\ncategories: correctness, completeness, conciseness, consistency, clarity,\nredundancy, and emphasis. Each sub-criterion was scored 0-3 and normalized to a\npercentage. A critical regulatory error was defined as any misrepresentation or\nomission likely to alter regulatory interpretation (e.g., incorrect NOAEL,\nomission of mandatory GLP dose-formulation analysis). Results: AutoIND reduced\ninitial drafting time by $\\sim$97% (from $\\sim$100 h to 3.7 h for 18,870\npages/61 reports in IND-1; and to 2.6 h for 11,425 pages/58 reports in IND-2).\nQuality scores were 69.6\\% and 77.9\\% for IND-1 and IND-2. No critical\nregulatory errors were detected, but deficiencies in emphasis, conciseness, and\nclarity were noted. Conclusions: AutoIND can dramatically accelerate IND\ndrafting, but expert regulatory writers remain essential to mature outputs to\nsubmission-ready quality. Systematic deficiencies identified provide a roadmap\nfor targeted model improvements.", "AI": {"tldr": "AutoIND, an LLM platform, dramatically speeds IND nonclinical draft generation (approx. 97% faster) while maintaining reasonable quality (69.6\u201377.9%), but expert regulatory writers remain essential for finalization; outputs show systematic deficiencies in emphasis, conciseness, and clarity.", "motivation": "Reduce time and reliance on expert regulatory drafting for IND submissions by using an LLM platform to generate first drafts of regulatory modules.", "method": "Two IND drafting studies (IND-1 and IND-2) compared AutoIND-generated drafts (modules 2.6.2/2.6.4/2.6.6) against industry-standard manual drafting times from experienced writers. Draft times recorded for AutoIND; manual times estimated from FDA-cleared writers. Quality assessed by blinded assessor across seven criteria (correctness, completeness, conciseness, consistency, clarity, redundancy, emphasis) on 0\u20133 scale, normalized to percentage. Critical regulatory errors tracked.", "result": "AutoIND reduced initial drafting time from ~100 h to 3.7 h (IND-1) and 2.6 h (IND-2) across large document sets; pages/reports: 18,870/61 and 11,425/58. Quality scores: 69.6% (IND-1) and 77.9% (IND-2). No critical regulatory errors detected, but deficiencies in emphasis, conciseness, and clarity were noted.", "conclusion": "AutoIND can dramatically accelerate IND drafting but requires expert regulatory writers to mature outputs to submission-ready quality; identified deficiencies map to a roadmap for model improvements."}}
{"id": "2509.09953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09953", "abs": "https://arxiv.org/abs/2509.09953", "authors": ["Mahfuzul I. Nissan", "Sharmin Aktar"], "title": "Detection of Anomalous Behavior in Robot Systems Based on Machine Learning", "comment": null, "summary": "Ensuring the safe and reliable operation of robotic systems is paramount to\nprevent potential disasters and safeguard human well-being. Despite rigorous\ndesign and engineering practices, these systems can still experience\nmalfunctions, leading to safety risks. In this study, we present a machine\nlearning-based approach for detecting anomalies in system logs to enhance the\nsafety and reliability of robotic systems. We collected logs from two distinct\nscenarios using CoppeliaSim and comparatively evaluated several machine\nlearning models, including Logistic Regression (LR), Support Vector Machine\n(SVM), and an Autoencoder. Our system was evaluated in a quadcopter context\n(Context 1) and a Pioneer robot context (Context 2). Results showed that while\nLR demonstrated superior performance in Context 1, the Autoencoder model proved\nto be the most effective in Context 2. This highlights that the optimal model\nchoice is context-dependent, likely due to the varying complexity of anomalies\nacross different robotic platforms. This research underscores the value of a\ncomparative approach and demonstrates the particular strengths of autoencoders\nfor detecting complex anomalies in robotic systems.", "AI": {"tldr": "A comparative study evaluates LR, SVM, and Autoencoder for anomaly detection in robotic system logs across two contexts, showing context-dependent performance (LR best in quadcopter; Autoencoder best in Pioneer robot), underscoring the need for context-aware model selection and the strength of autoencoders for complex anomalies.", "motivation": "To improve safety and reliability of robotic systems by detecting anomalies in operational logs, recognizing that anomaly characteristics vary across platforms and thus model performance may differ.", "method": "Collect system logs from two simulation-based contexts (quadcopter and Pioneer robot) using CoppeliaSim. Train and compare Logistic Regression, Support Vector Machine, and Autoencoder models for anomaly detection, evaluating performance in each context.", "result": "Performance is context-dependent: LR outperforms in Context 1 (quadcopter), while the Autoencoder performs best in Context 2 (Pioneer robot). SVM's performance is not highlighted as top in either context, implying varying effectiveness across setups.", "conclusion": "A comparative, context-aware modeling approach is valuable. Autoencoders show particular strength for detecting complex anomalies in robotic systems, but optimal model choice depends on platform-specific anomaly complexity; consider tailoring models per platform or ensemble approaches."}}
{"id": "2509.09721", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09721", "abs": "https://arxiv.org/abs/2509.09721", "authors": ["Jiayi Miao", "Dingxin Lu", "Zhuqi Wang"], "title": "A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval", "comment": null, "summary": "After natural disasters, accurate evaluations of damage to housing are\nimportant for insurance claims response and planning of resources. In this\nwork, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)\nframework. On top of classical RAG architecture, we further the framework to\ndevise a two-branch multimodal encoder structure that the image branch employs\na visual encoder composed of ResNet and Transformer to extract the\ncharacteristic of building damage after disaster, and the text branch harnesses\na BERT retriever for the text vectorization of posts as well as insurance\npolicies and for the construction of a retrievable restoration index. To impose\ncross-modal semantic alignment, the model integrates a cross-modal interaction\nmodule to bridge the semantic representation between image and text via\nmulti-head attention. Meanwhile, in the generation module, the introduced modal\nattention gating mechanism dynamically controls the role of visual evidence and\ntext prior information during generation. The entire framework takes end-to-end\ntraining, and combines the comparison loss, the retrieval loss and the\ngeneration loss to form multi-task optimization objectives, and achieves image\nunderstanding and policy matching in collaborative learning. The results\ndemonstrate superior performance in retrieval accuracy and classification index\non damage severity, where the Top-1 retrieval accuracy has been improved by\n9.6%.", "AI": {"tldr": "A multimodal retrieval-augmented generation framework (MM-RAG) for disaster housing damage assessment, combining a two-branch encoders (image: ResNet+Transformer; text: BERT retriever), cross-modal interaction, and modal attention gating in generation, trained end-to-end with multi-task losses; shows improved retrieval accuracy and damage classification, including a 9.6% Top-1 retrieval improvement.", "motivation": "After natural disasters, there is a need for accurate, scalable assessment of housing damage to support insurance claims and resource planning. Existing methods may treat images or text separately and fail to leverage cross-modal cues. The work aims to fuse image evidence with policy/text data to improve retrieval and damage classification.", "method": "Two-branch multimodal encoder: image branch uses ResNet + Transformer to extract building-damage features; text branch uses a BERT retriever for text vectorization of posts and insurance policies and to build a retrievable restoration index. Cross-modal interaction module via multi-head attention aligns image/text semantics. Generation module uses modal attention gating to weigh visual vs textual information during generation. End-to-end training with multiple losses: comparison loss, retrieval loss, and generation loss for joint optimization.", "result": "The framework achieves superior retrieval accuracy and damage-severity classification performance, with Top-1 retrieval accuracy improved by 9.6%.", "conclusion": "The MM-RAG framework effectively performs image understanding and policy matching through cross-modal alignment and gated generation, demonstrating benefits of retrieval-augmented generation for disaster damage assessment."}}
{"id": "2509.09747", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09747", "abs": "https://arxiv.org/abs/2509.09747", "authors": ["Leen Daher", "Zhaobo Wang", "Malcolm Mielle"], "title": "D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference", "comment": null, "summary": "Cross-modal transfer learning is used to improve multi-modal classification\nmodels (e.g., for human activity recognition in human-robot collaboration).\nHowever, existing methods require paired sensor data at both training and\ninference, limiting deployment in resource-constrained environments where full\nsensor suites are not economically and technically usable. To address this, we\npropose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns\nmodality-specific representations without requiring joint sensor modality\nduring inference. Our approach combines a self-attention module for feature\nextraction with a novel cross-attention alignment loss, which enforces the\nalignment of sensors' feature spaces without requiring the coupling of the\nclassification pipelines of both modalities. We evaluate D-CAT on three\nmulti-modal human activity datasets (IMU, video, and audio) under both\nin-distribution and out-of-distribution scenarios, comparing against uni-modal\nmodels. Results show that in in-distribution scenarios, transferring from\nhigh-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains\nover uni-modal training. In out-of-distribution scenarios, even weaker source\nmodalities (e.g., IMU to video) improve target performance, as long as the\ntarget model isn't overfitted on the training data. By enabling single-sensor\ninference with cross-modal knowledge, D-CAT reduces hardware redundancy for\nperception systems while maintaining accuracy, which is critical for\ncost-sensitive or adaptive deployments (e.g., assistive robots in homes with\nvariable sensor availability). Code is available at\nhttps://github.com/Schindler-EPFL-Lab/D-CAT.", "AI": {"tldr": "D-CAT enables cross-modal transfer without requiring joint modalities at inference by decoupling modality-specific pipelines and aligning feature spaces via a cross-attention loss, achieving gains in both in-distribution and out-of-distribution scenarios and allowing single-sensor inference.", "motivation": "Multi-modal learning often relies on paired modalities during inference; variable sensor availability and hardware constraints hinder deployment. A decoupled transfer approach seeks to leverage cross-modal knowledge without forcing joint sensor setups at test time.", "method": "Use a self-attention feature extractor per modality and introduce a cross-attention alignment loss that encourages alignment of sensor-specific feature spaces without coupling the classifiers. Train with paired data to learn cross-modal alignment, but inference can run with a single modality.", "result": "On IMU, video, and audio datasets, across in-distribution and out-of-distribution settings, transferring from high-performing modalities (e.g., video to IMU) yields up to ~10% F1-score gains over uni-modal baselines; in OOD, even weaker sources (e.g., IMU to video) improve target performance if the target is not overfitted. The approach enables single-sensor inference with cross-modal knowledge, reducing hardware redundancy.", "conclusion": "D-CAT reduces hardware requirements for perception systems while maintaining accuracy, enabling cost-sensitive or adaptive deployments (e.g., home assistive robots) by leveraging cross-modal knowledge without coupled classifiers at inference; code is available at the provided GitHub URL."}}
{"id": "2509.09775", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09775", "abs": "https://arxiv.org/abs/2509.09775", "authors": ["Aleksandr Boldachev"], "title": "Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture", "comment": "22 pages, 6 figures", "summary": "This paper presents boldsea, Boldachev's semantic-event approach -- an\narchitecture for modeling complex dynamic systems using executable ontologies\n-- semantic models that act as dynamic structures, directly controlling process\nexecution. We demonstrate that integrating event semantics with a dataflow\narchitecture addresses the limitations of traditional Business Process\nManagement (BPM) systems and object-oriented semantic technologies. The paper\npresents the formal BSL (boldsea Semantic Language), including its BNF grammar,\nand outlines the boldsea-engine's architecture, which directly interprets\nsemantic models as executable algorithms without compilation. It enables the\nmodification of event models at runtime, ensures temporal transparency, and\nseamlessly merges data and business logic within a unified semantic framework.", "AI": {"tldr": "Boldsea provides an executable-ontology framework (BSL) with a semantic-event, dataflow-based engine that interprets models directly as running algorithms, enabling runtime modification and unified data/logic with temporal transparency.", "motivation": "To overcome limitations of traditional BPM systems and OO semantic technologies by integrating event semantics with dataflow and enabling executable models that can be modified at runtime and observed temporally.", "method": "Introduce Boldsea Semantic Language (BSL) with a formal grammar (BNF), describe the boldsea-engine architecture, and show how semantic models are interpreted directly without compilation to executable behavior.", "result": "Semantic models are directly interpretable as algorithms by the engine; runtime modification of event models is supported; temporal transparency and unified data/business logic are achieved within a single semantic framework.", "conclusion": "The semantic-event approach addresses BPM and OO limitations, enabling executable ontologies and dynamic, runtime-modifiable processes within a unified framework."}}
{"id": "2509.10007", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10007", "abs": "https://arxiv.org/abs/2509.10007", "authors": ["Samuli Soutukorva", "Markku Suomalainen", "Martin Kollingbaum", "Tapio Heikkil\u00e4"], "title": "Gaussian path model library for intuitive robot motion programming by demonstration", "comment": null, "summary": "This paper presents a system for generating Gaussian path models from\nteaching data representing the path shape. In addition, methods for using these\npath models to classify human demonstrations of paths are introduced. By\ngenerating a library of multiple Gaussian path models of various shapes, human\ndemonstrations can be used for intuitive robot motion programming. A method for\nmodifying existing Gaussian path models by demonstration through geometric\nanalysis is also presented.", "AI": {"tldr": "System for generating Gaussian path models from teaching data to classify and modify human demonstrations for robot motion programming.", "motivation": "Enable intuitive robot programming by using probabilistic path representations and demonstration-driven modification.", "method": "Generate Gaussian path models from teaching data; classify human demonstrations using these models; build a library of multiple Gaussian path shapes; modify existing models via geometric analysis based on demonstrations.", "result": "Abstract describes a complete system and methods; no empirical validation or results are reported within the abstract.", "conclusion": "Gaussian path models are proposed as a means to support intuitive, demonstration-driven robot motion programming and adaptable path modification."}}
{"id": "2509.09722", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09722", "abs": "https://arxiv.org/abs/2509.09722", "authors": ["Taylor Archibald", "Tony Martinez"], "title": "Improving MLLM Historical Record Extraction with Test-Time Image", "comment": null, "summary": "We present a novel ensemble framework that stabilizes LLM based text\nextraction from noisy historical documents. We transcribe multiple augmented\nvariants of each image with Gemini 2.0 Flash and fuse these outputs with a\ncustom Needleman Wunsch style aligner that yields both a consensus\ntranscription and a confidence score. We present a new dataset of 622\nPennsylvania death records, and demonstrate our method improves transcription\naccuracy by 4 percentage points relative to a single shot baseline. We find\nthat padding and blurring are the most useful for improving accuracy, while\ngrid warp perturbations are best for separating high and low confidence cases.\nThe approach is simple, scalable, and immediately deployable to other document\ncollections and transcription models.", "AI": {"tldr": "An ensemble framework stabilizes LLM-based transcription of noisy historical documents by transcribing augmented image variants with Gemini 2.0 Flash and fusing outputs via a Needleman-Wunsch style aligner to produce a consensus transcription and confidence scores, demonstrated on a 622-record dataset with a 4-point accuracy gain over a single-shot baseline. Padding and blur augmentations boost accuracy, grid warp helps separate high/low confidence cases; the method is simple, scalable, and deployable to other collections and models.", "motivation": "Historical document transcription suffers from noise and variability in images. There is a need for stable, accurate, and confidence-informed transcription using LLMs across diverse document collections.", "method": "Generate multiple augmented variants of each image using Gemini 2.0 Flash; obtain transcripts for each variant; fuse outputs with a custom Needleman-Wunsch style aligner to produce a consensus transcription and a per-transcript confidence score; evaluate on a newly released dataset of 622 Pennsylvania death records; analyze which augmentations improve accuracy and confidence separation.", "result": "Transcription accuracy improved by 4 percentage points over a single-shot baseline. Padding and blurring augmentations were most beneficial for accuracy; grid warp perturbations were best at separating high vs. low confidence cases. The approach is simple, scalable, and deployable to other document collections and transcription models.", "conclusion": "The ensemble transcription framework with augmentation-driven diversity and a Needleman-Wunsch style fusion provides reliable consensus transcripts with confidence scores, enabling scalable transcription improvements and easy deployment to other datasets and models."}}
{"id": "2509.09751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09751", "abs": "https://arxiv.org/abs/2509.09751", "authors": ["Junqiao Wang", "Zhaoyang Guan", "Guanyu Liu", "Tianze Xia", "Xianzhi Li", "Shuo Yin", "Xinyuan Song", "Chuhan Cheng", "Tianyu Shi", "Alex Lee"], "title": "Meta-Learning Reinforcement Learning for Crypto-Return Prediction", "comment": null, "summary": "Predicting cryptocurrency returns is notoriously difficult: price movements\nare driven by a fast-shifting blend of on-chain activity, news flow, and social\nsentiment, while labeled training data are scarce and expensive. In this paper,\nwe present Meta-RL-Crypto, a unified transformer-based architecture that\nunifies meta-learning and reinforcement learning (RL) to create a fully\nself-improving trading agent. Starting from a vanilla instruction-tuned LLM,\nthe agent iteratively alternates between three roles-actor, judge, and\nmeta-judge-in a closed-loop architecture. This learning process requires no\nadditional human supervision. It can leverage multimodal market inputs and\ninternal preference feedback. The agent in the system continuously refines both\nthe trading policy and evaluation criteria. Experiments across diverse market\nregimes demonstrate that Meta-RL-Crypto shows good performance on the technical\nindicators of the real market and outperforming other LLM-based baselines.", "AI": {"tldr": "A transformer-based Meta-RL system (Meta-RL-Crypto) combines meta-learning and reinforcement learning on top of an instruction-tuned LLM to create a self-improving crypto trading agent, using multimodal market inputs in a closed actor\u2013judge\u2013meta-judge loop, outperforming LLM baselines without additional supervision.", "motivation": "Crypto price prediction is hard due to rapidly shifting drivers and scarce labeled data; a self-improving agent that jointly learns policy and evaluation criteria could adapt to regimes without heavy supervision.", "method": "Starting from an instruction-tuned LLM, the agent alternates in a closed-loop with three roles\u2014actor, judge, and meta-judge\u2014learning from multimodal market inputs and internal preference feedback, requiring no extra human supervision and refining both policy and evaluation metrics.", "result": "Empirical results indicate robust performance on real-market technical indicators and outperformance of other LLM-based baselines across diverse market regimes.", "conclusion": "The Meta-RL-Crypto framework yields a fully self-improving trading agent that reduces dependence on labeled data and can adapt to changing conditions by jointly optimizing trading policy and evaluation criteria."}}
{"id": "2509.09790", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09790", "abs": "https://arxiv.org/abs/2509.09790", "authors": ["Yuxuan Li", "Victor Zhong"], "title": "How well can LLMs provide planning feedback in grounded environments?", "comment": null, "summary": "Learning to plan in grounded environments typically requires carefully\ndesigned reward functions or high-quality annotated demonstrations. Recent\nworks show that pretrained foundation models, such as large language models\n(LLMs) and vision language models (VLMs), capture background knowledge helpful\nfor planning, which reduces the amount of reward design and demonstrations\nneeded for policy learning. We evaluate how well LLMs and VLMs provide feedback\nacross symbolic, language, and continuous control environments. We consider\nprominent types of feedback for planning including binary feedback, preference\nfeedback, action advising, goal advising, and delta action feedback. We also\nconsider inference methods that impact feedback performance, including\nin-context learning, chain-of-thought, and access to environment dynamics. We\nfind that foundation models can provide diverse high-quality feedback across\ndomains. Moreover, larger and reasoning models consistently provide more\naccurate feedback, exhibit less bias, and benefit more from enhanced inference\nmethods. Finally, feedback quality degrades for environments with complex\ndynamics or continuous state spaces and action spaces.", "AI": {"tldr": "Foundation models (LLMs/VLMs) can supply versatile planning feedback across domains, with larger/more capable models offering higher accuracy and less bias; inference aids feedback, but performance degrades in environments with complex dynamics or continuous spaces.", "motivation": "Reduce design burden of rewards and demonstrations by leveraging pretrained models to provide feedback for planning policies in grounded environments.", "method": "Systematic evaluation of LLMs and VLMs as feedback providers across symbolic, language, and continuous control environments; analyze feedback types (binary, preference, action advising, goal advising, delta action); and assess inference methods (in-context learning, chain-of-thought, and access to environment dynamics).", "result": "Feedback from foundation models is diverse and high-quality across domains. Larger and reasoning-based models provide more accurate feedback, exhibit less bias, and gain from improved inference methods. However, feedback quality declines in environments with complex dynamics or continuous state/action spaces.", "conclusion": "Foundation models show promise as sources of feedback for planning in grounded environments, but challenges remain for complex or continuous dynamics; further work should focus on robustness, dynamics-aware prompting, and domain-specific benchmarks."}}
{"id": "2509.10012", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10012", "abs": "https://arxiv.org/abs/2509.10012", "authors": ["Richard Matthias Hartisch", "Alexander Rother", "J\u00f6rg Kr\u00fcger", "Kevin Haninger"], "title": "Towards simulation-based optimization of compliant fingers for high-speed connector assembly", "comment": null, "summary": "Mechanical compliance is a key design parameter for dynamic contact-rich\nmanipulation, affecting task success and safety robustness over contact\ngeometry variation. Design of soft robotic structures, such as compliant\nfingers, requires choosing design parameters which affect geometry and\nstiffness, and therefore manipulation performance and robustness. Today, these\nparameters are chosen through either hardware iteration, which takes\nsignificant development time, or simplified models (e.g. planar), which can't\naddress complex manipulation task objectives. Improvements in dynamic\nsimulation, especially with contact and friction modeling, present a potential\ndesign tool for mechanical compliance. We propose a simulation-based design\ntool for compliant mechanisms which allows design with respect to task-level\nobjectives, such as success rate. This is applied to optimize design parameters\nof a structured compliant finger to reduce failure cases inside a tolerance\nwindow in insertion tasks. The improvement in robustness is then validated on a\nreal robot using tasks from the benchmark NIST task board. The finger stiffness\naffects the tolerance window: optimized parameters can increase tolerable\nranges by a factor of 2.29, with workpiece variation up to 8.6 mm being\ncompensated. However, the trends remain task-specific. In some tasks, the\nhighest stiffness yields the widest tolerable range, whereas in others the\nopposite is observed, motivating need for design tools which can consider\napplication-specific geometry and dynamics.", "AI": {"tldr": "Simulation-based design optimizes a structured compliant finger for dynamic contact-rich insertion tasks, improving robustness by up to 2.29x in tolerance windows and compensating ~8.6 mm workpiece variation, though effects are task-dependent.", "motivation": "Mechanical compliance and stiffness design of soft robotic fingers critically affect manipulation success and safety; a simulation-driven tool can reduce hardware iterations and address complex task-level objectives.", "method": "Develop a dynamic simulation framework with contact/friction modeling and optimize the finger\u2019s design parameters against a task-level objective (e.g., success rate) across insertion tasks; validate optimized designs on a real robot using NIST task-board benchmarks.", "result": "Optimized stiffness expands tolerable insertion ranges by up to 2.29x; can compensate workpiece variation up to 8.6 mm; task-dependent trends observed\u2014sometimes higher stiffness is better, sometimes lower\u2014highlighting the need for geometry- and dynamics-aware design tools.", "conclusion": "A simulation-based design approach can improve robustness in compliant mechanisms, but effectiveness is task-specific and requires considering application geometry and dynamics."}}
{"id": "2509.09730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09730", "abs": "https://arxiv.org/abs/2509.09730", "authors": ["Kaikai Zhao", "Zhaoxiang Liu", "Peng Wang", "Xin Wang", "Zhicheng Ma", "Yajun Xu", "Wenjing Zhang", "Yibing Nan", "Kai Wang", "Shiguo Lian"], "title": "MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance", "comment": "accepted by Image and Vision Computing", "summary": "General-domain large multimodal models (LMMs) have achieved significant\nadvances in various image-text tasks. However, their performance in the\nIntelligent Traffic Surveillance (ITS) domain remains limited due to the\nabsence of dedicated multimodal datasets. To address this gap, we introduce\nMITS (Multimodal Intelligent Traffic Surveillance), the first large-scale\nmultimodal benchmark dataset specifically designed for ITS. MITS includes\n170,400 independently collected real-world ITS images sourced from traffic\nsurveillance cameras, annotated with eight main categories and 24 subcategories\nof ITS-specific objects and events under diverse environmental conditions.\nAdditionally, through a systematic data generation pipeline, we generate\nhigh-quality image captions and 5 million instruction-following visual\nquestion-answer pairs, addressing five critical ITS tasks: object and event\nrecognition, object counting, object localization, background analysis, and\nevent reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream\nLMMs on this dataset, enabling the development of ITS-specific applications.\nExperimental results show that MITS significantly improves LMM performance in\nITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905\n(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to\n0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the\ndataset, code, and models as open-source, providing high-value resources to\nadvance both ITS and LMM research.", "AI": {"tldr": "Introduces MITS, a large-scale multimodal ITS benchmark with 170k+ real-world surveillance images and 5M instruction-following VQA pairs; enables fine-tuning of mainstream LMMs, yielding substantial ITS performance gains; open-sourced.", "motivation": "Addresses the absence of dedicated multimodal ITS datasets and benchmarks, hindering the development and evaluation of large multimodal models in intelligent traffic surveillance.", "method": "Curate and annotate 170,400 ITS images with eight main categories and 24 subcategories; use a systematic data-generation pipeline to create high-quality image captions and 5 million VQA pairs; fine-tune mainstream LMMs on MITS to develop ITS-specific applications; evaluate across five ITS tasks (object/event recognition, counting, localization, background analysis, event reasoning).", "result": "MITS yields substantial performance gains across several LMMs on ITS tasks (e.g., LLaVA-1.5: 0.494\u21920.905; LLaVA-1.6: 0.678\u21920.921; Qwen2-VL: 0.584\u21920.926; Qwen2.5-VL: 0.732\u21920.930). These improvements demonstrate the dataset\u2019s effectiveness; dataset, code, and models released as open-source.", "conclusion": "MITS provides a valuable, scalable ITS multimodal resource that advances both ITS research and LMM capabilities; expected to accelerate ITS-specific applications and further ITS-LMM research."}}
{"id": "2509.09754", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09754", "abs": "https://arxiv.org/abs/2509.09754", "authors": ["Yiqun Shen", "Song Yuan", "Zhengze Zhang", "Xiaoliang Wang", "Daxin Jiang", "Nguyen Cam-Tu"], "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation", "comment": null, "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.", "AI": {"tldr": "LAVa is a fully dynamic, unified cache compression framework for KV cache in LLMs that jointly optimizes eviction and budget allocation across layers and heads without requiring training.", "motivation": "KV cache accelerates LLM inference with long contexts but has high memory consumption. Existing compression methods are mainly heuristic and lack dynamic budget allocation; a unified, training-free approach is needed to reduce information loss while adapting budgets across heads and layers.", "method": "Proposes minimizing information loss in Transformer residual streams. Analyzes layer attention output loss and derives a cross-head cache-entry similarity metric to enable layer-wise compression with dynamic head budgets. By contrasting cross-layer information, it also enables dynamic layer budgets. LAVa unifies eviction and budget allocation in a single framework and does not rely on training or combining multiple strategies.", "result": "Empirical benchmarks (LongBench, Needle-In-A-Haystack, Ruler, InfiniteBench) show LAVa\u2019s superiority across tasks. Dynamic layer budgets are crucial for generation tasks (e.g., code completion) while dynamic head budgets boost performance for extraction tasks (e.g., extractive QA). LAVa maintains top performance across task types.", "conclusion": "LAVa is the first unified strategy for cache eviction and dynamic budget allocation that operates without training or aggregating multiple strategies. The approach demonstrates consistent, task-type\u2013dependent benefits and comes with an open-source implementation."}}
{"id": "2509.09794", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09794", "abs": "https://arxiv.org/abs/2509.09794", "authors": ["Jackson Eshbaugh", "Chetan Tiwari", "Jorge Silveyra"], "title": "A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes", "comment": "44 pages; 2 appendices; 9 figures; 1 table. Code available at\n  https://github.com/Lafayette-EshbaughSilveyra-Group/synthetic-homes", "summary": "Computational models have emerged as powerful tools for energy modeling\nresearch, touting scalability and quantitative results. However, these models\nrequire a plethora of data, some of which is inaccessible, expensive, or raises\nprivacy concerns. We introduce a modular multimodal framework to produce this\ndata from publicly accessible residential information and images using\ngenerative artificial intelligence (AI). Additionally, we provide a pipeline\ndemonstrating this framework, and we evaluate its generative AI components. Our\nexperiments show that our framework's use of AI avoids common issues with\ngenerative models. Our framework produces realistic, labeled data. By reducing\ndependence on costly or restricted data sources, we pave a path towards more\naccessible and reproducible research.", "AI": {"tldr": "A modular multimodal framework to synthesize realistic labeled residential data from public information/images via generative AI, enabling accessible and reproducible energy-modeling research by reducing reliance on restricted data.", "motivation": "Energy modeling relies on large, diverse datasets, but many useful data sources are inaccessible due to privacy, cost, or policy constraints, hindering reproducibility and progress.", "method": "Proposes a modular multimodal framework that uses publicly accessible residential information and images and applies generative AI to synthesize data suitable for energy models. Includes a pipeline to demonstrate data production and an evaluation of the generative components, focusing on data realism, labeling, and mitigating common generative-model issues.", "result": "Empirical evaluation shows the framework can produce realistic, labeled data while avoiding typical generative-model pitfalls, demonstrating viability as a data source that reduces dependence on restricted data and enhances accessibility and reproducibility.", "conclusion": "The framework provides a practical approach to data generation for energy modeling, enabling more accessible and reproducible research by leveraging publicly available information and AI-generated synthetic data."}}
{"id": "2509.10032", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10032", "abs": "https://arxiv.org/abs/2509.10032", "authors": ["Marawan Khalil", "Fabian Arzberger", "Andreas N\u00fcchter"], "title": "Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping", "comment": "6 Pages, 9 figures, International Workshop 3D-AdViCE in conjunction\n  with 12th ECMR 2025", "summary": "Spherical robots offer unique advantages for mapping applications in\nhazardous or confined environments, thanks to their protective shells and\nomnidirectional mobility. This work presents two complementary spherical\nmapping systems: a lightweight, non-actuated design and an actuated variant\nfeaturing internal pendulum-driven locomotion. Both systems are equipped with a\nLivox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)\nalgorithms on resource-constrained hardware. We assess the mapping accuracy of\nthese systems by comparing the resulting 3D point-clouds from the LIO\nalgorithms to a ground truth map. The results indicate that the performance of\nstate-of-the-art LIO algorithms deteriorates due to the high dynamic movement\nintroduced by the spherical locomotion, leading to globally inconsistent maps\nand sometimes unrecoverable drift.", "AI": {"tldr": "Two spherical robots (one non-actuated, one pendulum-driven) map with Livox LiDAR; LIO performance degrades under rapid spherical motion, causing inconsistent maps and drift.", "motivation": "Leverage the protective shell and omnidirectional mobility of spherical robots for mapping in hazardous or confined spaces; evaluate whether current LIO methods can provide accurate 3D maps with such platforms.", "method": "Design two spherical platforms (lightweight non-actuated and actuated pendulum-driven). Equip both with Livox Mid-360 LiDAR. Run LiDAR-Inertial Odometry on constrained hardware. Generate 3D point clouds and compare to a ground-truth map to assess mapping accuracy. Analyze how dynamic locomotion affects LIO outcomes.", "result": "State-of-the-art LIO algorithms experience performance deterioration due to high dynamic movement from spherical locomotion, leading to globally inconsistent maps and sometimes unrecoverable drift.", "conclusion": "While spherical platforms enable safe operation in challenging environments, current LIO-based mapping struggles with rapid, omnidirectional motion; robust motion compensation or algorithmic improvements are needed to achieve reliable maps on such systems."}}
{"id": "2509.09732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09732", "abs": "https://arxiv.org/abs/2509.09732", "authors": ["Sary Elmansoury", "Islam Mesabah", "Gerrit Gro\u00dfmann", "Peter Neigel", "Raj Bhalwankar", "Daniel Kondermann", "Sebastian J. Vollmer"], "title": "Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs", "comment": null, "summary": "Vision language models (VLMs) excel at zero-shot visual classification, but\ntheir performance on fine-grained tasks and large hierarchical label spaces is\nunderstudied. This paper investigates whether structured, tree-based reasoning\ncan enhance VLM performance. We introduce a framework that decomposes\nclassification into interpretable decisions using decision trees and evaluates\nit on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the\nmodel achieves 98.2% accuracy in understanding the tree knowledge, tree-based\nreasoning consistently underperforms standard zero-shot prompting. We also\nexplore enhancing the tree prompts with LLM-generated classes and image\ndescriptions to improve alignment. The added description enhances the\nperformance of the tree-based and zero-shot methods. Our findings highlight\nlimitations of structured reasoning in visual classification and offer insights\nfor designing more interpretable VLM systems.", "AI": {"tldr": "Tree-based, interpretable decision-making for VLMs yields high mastery of tree structure (98.2%) but does not outperform standard zero-shot prompting for fine- to coarse-grained visual classification; adding image descriptions/LLM-generated classes can improve alignment and overall performance, underscoring limits of structured reasoning but potential gains for interpretability.", "motivation": "Assess whether structured, tree-based reasoning can boost VLM performance on fine-grained and hierarchical label spaces and analyze interpretability trade-offs; fill gap in understanding structured prompts vs zero-shot prompts.", "method": "Proposed a framework that decomposes classification into tree-based decisions using decision trees; evaluate on GTSRB (fine-grained) and CIFAR-10 (coarse-grained) datasets; quantify tree knowledge accuracy; augment tree prompts with LLM-generated classes and image descriptions to assess alignment and performance.", "result": "Achieved 98.2% accuracy in understanding the tree knowledge; tree-based reasoning underperforms standard zero-shot prompting overall; adding descriptions improves the performance of both tree-based and zero-shot methods; description boosts alignment.", "conclusion": "Structured, hierarchical reasoning in VLMs has notable limitations for visual classification, but combining tree prompts with descriptive context can improve interpretability and alignment; results inform design of more interpretable VLM systems and future research directions."}}
{"id": "2509.09772", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.09772", "abs": "https://arxiv.org/abs/2509.09772", "authors": ["Sanjay Basu", "Sadiq Y. Patel", "Parth Sheth", "Bhairavi Muralidharan", "Namrata Elamaran", "Aakriti Kinra", "Rajaie Batniji"], "title": "Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management", "comment": "10 pages, 5 figures, 4 tables", "summary": "Population health management programs for Medicaid populations coordinate\nlongitudinal outreach and services (e.g., benefits navigation, behavioral\nhealth, social needs support, and clinical scheduling) and must be safe, fair,\nand auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement\nLearning (HACO) framework that separates risk calibration from preference\noptimization to generate conservative action recommendations at scale. In our\nsetting, each step involves choosing among common coordination actions (e.g.,\nwhich member to contact, by which modality, and whether to route to a\nspecialized service) while controlling the near-term risk of adverse\nutilization events (e.g., unplanned emergency department visits or\nhospitalizations). Using a de-identified operational dataset from Waymark\ncomprising 2.77 million sequential decisions across 168,126 patients, HACO (i)\ntrains a lightweight risk model for adverse events, (ii) derives a conformal\nthreshold to mask unsafe actions at a target risk level, and (iii) learns a\npreference policy on the resulting safe subset. We evaluate policies with a\nversion-agnostic fitted Q evaluation (FQE) on stratified subsets and audit\nsubgroup performance across age, sex, and race. HACO achieves strong risk\ndiscrimination (AUC ~0.81) with a calibrated threshold ( {\\tau} ~0.038 at\n{\\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses\nreveal systematic differences in estimated value across demographics,\nunderscoring the importance of fairness auditing. Our results show that\nconformal risk gating integrates cleanly with offline RL to deliver\nconservative, auditable decision support for population health management\nteams.", "AI": {"tldr": "Hybrid Adaptive Conformal Offline Reinforcement Learning (HACO) framework for safe, auditable Medicaid decision support. It separates risk calibration from preference optimization, using conformal risk gating on offline RL to produce conservative actions, with evaluation showing solid risk discrimination and subgroup fairness auditing.", "motivation": "Population health management for Medicaid requires scalable, safe, fair, and auditable decision-support. The framework addresses the need to avoid unsafe actions (e.g., high-risk utilization) while optimizing care coordination strategies at scale.", "method": "Train a lightweight risk model for adverse events; derive a conformal threshold to mask unsafe actions at a target risk level; learn a preference policy on the safe subset; evaluate with fitted Q evaluation (FQE) on stratified subsets and audit subgroup performance by age, sex, race, using a de-identified operational dataset of 2.77M decisions across 168k patients.", "result": "HACO achieves strong risk discrimination (AUC ~0.81) with a calibrated threshold (tau ~0.038 at alpha = 0.10) and maintains high safe coverage. Subgroup analyses show systematic differences in estimated value across demographics, underscoring the need for fairness auditing.", "conclusion": "Conformal risk gating can be effectively integrated with offline RL to produce conservative, auditable decision support for population health management, enabling scalable and safer coordination actions while highlighting fairness considerations across subgroups."}}
{"id": "2509.09810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09810", "abs": "https://arxiv.org/abs/2509.09810", "authors": ["Agnieszka Mensfelt", "David Tena Cucala", "Santiago Franco", "Angeliki Koutsoukou-Argyraki", "Vince Trencsenyi", "Kostas Stathis"], "title": "Towards a Common Framework for Autoformalization", "comment": null, "summary": "Autoformalization has emerged as a term referring to the automation of\nformalization - specifically, the formalization of mathematics using\ninteractive theorem provers (proof assistants). Its rapid development has been\ndriven by progress in deep learning, especially large language models (LLMs).\nMore recently, the term has expanded beyond mathematics to describe the broader\ntask of translating informal input into formal logical representations. At the\nsame time, a growing body of research explores using LLMs to translate informal\nlanguage into formal representations for reasoning, planning, and knowledge\nrepresentation - often without explicitly referring to this process as\nautoformalization. As a result, despite addressing similar tasks, the largely\nindependent development of these research areas has limited opportunities for\nshared methodologies, benchmarks, and theoretical frameworks that could\naccelerate progress. The goal of this paper is to review - explicit or implicit\n- instances of what can be considered autoformalization and to propose a\nunified framework, encouraging cross-pollination between different fields to\nadvance the development of next generation AI systems.", "AI": {"tldr": "Autoformalization research spans formal math with proof assistants and informal-to-formal translations for reasoning; the paper surveys the landscape and proposes a unified framework to harmonize methods, benchmarks, and theory across fields to accelerate next-generation AI.", "motivation": "The field is expanding rapidly thanks to LLMs and deep learning, but research areas are largely siloed. A unified framework and shared benchmarks could accelerate progress and enable cross-pollination between mathematics, reasoning, and knowledge representation.", "method": "Perform a literature review of autoformalization efforts across mathematics (proof assistants) and informal-to-formal translation for reasoning/knowledge representation; synthesize common themes; identify gaps; propose a unified, cross-domain framework and actionable guidelines.", "result": "Proposes a unified framework for autoformalization that spans multiple subfields, outlines cross-domain benchmarks, methodologies, and theoretical considerations, and suggests strategies to catalyze collaboration and progress.", "conclusion": "Cross-disciplinary collaboration and shared methodologies/benchmarks are needed to accelerate the development of next-generation AI systems capable of robust informal-to-formal reasoning."}}
{"id": "2509.10063", "categories": ["cs.RO", "cs.AI", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.10063", "abs": "https://arxiv.org/abs/2509.10063", "authors": ["Xiyan Huang", "Zhe Xu", "Chenxi Xiao"], "title": "TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model", "comment": "7 pages, 9 figures, 1 table, to be published in IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Robot skill acquisition processes driven by reinforcement learning often rely\non simulations to efficiently generate large-scale interaction data. However,\nthe absence of simulation models for tactile sensors has hindered the use of\ntactile sensing in such skill learning processes, limiting the development of\neffective policies driven by tactile perception. To bridge this gap, we present\nTwinTac, a system that combines the design of a physical tactile sensor with\nits digital twin model. Our hardware sensor is designed for high sensitivity\nand a wide measurement range, enabling high quality sensing data essential for\nobject interaction tasks. Building upon the hardware sensor, we develop the\ndigital twin model using a real-to-sim approach. This involves collecting\nsynchronized cross-domain data, including finite element method results and the\nphysical sensor's outputs, and then training neural networks to map simulated\ndata to real sensor responses. Through experimental evaluation, we\ncharacterized the sensitivity of the physical sensor and demonstrated the\nconsistency of the digital twin in replicating the physical sensor's output.\nFurthermore, by conducting an object classification task, we showed that\nsimulation data generated by our digital twin sensor can effectively augment\nreal-world data, leading to improved accuracy. These results highlight\nTwinTac's potential to bridge the gap in cross-domain learning tasks.", "AI": {"tldr": "TwinTac creates a hardware tactile sensor plus a digital twin using a real-to-sim mapping, enabling simulation data to augment real tactile data and improve tactile-based learning.", "motivation": "Reinforcement learning for skill acquisition often relies on simulations, but tactile sensor models are lacking, hindering tactile perception in policy learning. A digital twin of tactile sensing can bridge simulation and real data to enable data-efficient learning.", "method": "Design a high-sensitivity, wide-range tactile sensor and build a digital twin via a real-to-sim approach. Collect synchronized cross-domain data (FEM simulations and physical sensor outputs), train neural networks to map simulated data to real sensor responses, evaluate sensor sensitivity and twin fidelity, and demonstrate data augmentation in an object classification task.", "result": "The digital twin replicates the physical sensor outputs with consistency; simulation data generated by the twin effectively augments real data and improves object classification accuracy.", "conclusion": "TwinTac bridges the gap between simulation and real tactile sensing, enabling cross-domain learning and potentially improving tactile perception-based policies."}}
{"id": "2509.09737", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09737", "abs": "https://arxiv.org/abs/2509.09737", "authors": ["Klemen Kotar", "Wanhee Lee", "Rahul Venkatesh", "Honglin Chen", "Daniel Bear", "Jared Watrous", "Simon Kim", "Khai Loong Aw", "Lilian Naing Chen", "Stefan Stojanov", "Kevin Feigelis", "Imran Thobani", "Alex Durango", "Khaled Jedoui", "Atlas Kazemian", "Dan Yamins"], "title": "World Modeling with Probabilistic Structure Integration", "comment": null, "summary": "We present Probabilistic Structure Integration (PSI), a system for learning\nrichly controllable and flexibly promptable world models from data. PSI\nconsists of a three-step cycle. The first step, Probabilistic prediction,\ninvolves building a probabilistic graphical model Psi of the data, in the form\nof a random-access autoregressive sequence model. Psi supports a complete set\nof learned conditional distributions describing the dependence of any variables\nin the data on any other set of variables. In step 2, Structure extraction, we\nshow how to extract underlying low-dimensional properties in the data,\ncorresponding to a diverse set of meaningful \"intermediate structures\", in a\nzero-shot fashion via causal inference on Psi. Step 3, Integration, completes\nthe cycle by converting these structures into new token types that are then\ncontinually mixed back into the training diet as conditioning signals and\nprediction targets. Each such cycle augments the capabilities of Psi, both\nallowing it to model the underlying data better, and creating new control\nhandles -- akin to an LLM-like universal prompting language. We train an\ninstance of Psi on 1.4 trillion tokens of internet video data; we use it to\nperform a variety of useful video prediction and understanding inferences; we\nextract state-of-the-art optical flow, self-supervised depth and object\nsegmentation; and we use these structures to support a full cycle of predictive\nimprovements.", "AI": {"tldr": "PSI is a three-step framework to learn richly controllable world models from data: probabilistic prediction with a random-access autoregressive model (Psi), zero-shot extraction of underlying intermediate structures via causal inference, and integration by turning these structures into training tokens that continuously augment the model. Trained on 1.4T internet video tokens, PSI achieves strong video prediction/understanding (optical flow, depth, segmentation) and a cycle of predictive improvements.", "motivation": "Create richly controllable, promptable world models that can be steered via discovered intermediate structures, enabling flexible control and improved video prediction/understanding in large-scale data.", "method": "Step 1 Probabilistic prediction: build a probabilistic graphical model Psi as a random-access autoregressive sequence model with learned conditionals among data variables. Step 2 Structure extraction: induce low-dimensional, meaningful intermediate structures in a zero-shot fashion through causal inference on Psi. Step 3 Integration: convert these structures into new token types and mix them back into training as conditioning signals and prediction targets, forming a loop that expands Psi's capabilities.", "result": "Psi was trained on 1.4 trillion tokens of internet video data; achieved state-of-the-art optical flow, self-supervised depth, and object segmentation; demonstrated the ability to perform diverse predictive and understanding inferences and to iteratively improve through the cycle.", "conclusion": "PSI provides an LLM-like universal prompting language for world models by creating and injecting structure-derived tokens, enabling controllable, promptable, and continually improving video models."}}
{"id": "2509.09782", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09782", "abs": "https://arxiv.org/abs/2509.09782", "authors": ["Roshini Pulishetty", "Mani Kishan Ghantasala", "Keerthy Kaushik Dasoju", "Niti Mangwani", "Vishal Garimella", "Aditya Mate", "Somya Chatterjee", "Yue Kang", "Ehi Nosakhare", "Sadid Hasan", "Soundar Srinivasan"], "title": "One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection", "comment": null, "summary": "The proliferation of large language models (LLMs) with varying computational\ncosts and performance profiles presents a critical challenge for scalable,\ncost-effective deployment in real-world applications. We introduce a unified\nrouting framework that leverages a single-head cross-attention mechanism to\njointly model query and model embeddings, enabling dynamic selection of the\noptimal LLM for each input query. Our approach is evaluated on RouterBench, a\nlarge-scale, publicly available benchmark encompassing diverse LLM pools and\ndomains. By explicitly capturing fine-grained query-model interactions, our\nrouter predicts both response quality and generation cost, achieving up to 6.6%\nimprovement in Average Improvement in Quality (AIQ) and 2.9% in maximum\nperformance over existing routers. To robustly balance performance and cost, we\npropose an exponential reward function that enhances stability across user\npreferences. The resulting architecture is lightweight, generalizes effectively\nacross domains, and demonstrates improved efficiency compared to prior methods,\nestablishing a new standard for cost-aware LLM routing.", "AI": {"tldr": "A unified, cost-aware LLM routing framework using single-head cross-attention to jointly model query and model embeddings, achieving improved quality and efficiency on RouterBench.", "motivation": "Rising number of LLMs with diverse costs and performance makes per-query model selection challenging; need scalable, cost-effective routing.", "method": "A lightweight router leveraging single-head cross-attention to jointly encode query and candidate LLM embeddings; predicts both response quality and generation cost; uses an exponential reward to balance performance and cost; evaluated on RouterBench showing cross-domain generalization.", "result": "Achieves up to 6.6% AIQ improvement and 2.9% maximal performance over existing routers; demonstrates robust cost-performance trade-offs and efficiency improvements.", "conclusion": "Introduces a practical, generalizable, cost-aware LLM routing approach that sets a new standard for efficient, quality-aware model selection."}}
{"id": "2509.09848", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09848", "abs": "https://arxiv.org/abs/2509.09848", "authors": ["Nana Han", "Dong Liu", "Tomas Norton"], "title": "Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation", "comment": null, "summary": "Large language models (LLMs) are increasingly being recognised as valuable\nknowledge communication tools in many industries. However, their application in\nlivestock farming remains limited, being constrained by several factors not\nleast the availability, diversity and complexity of knowledge sources. This\nstudy introduces an intelligent knowledge assistant system designed to support\nhealth management in farmed goats. Leveraging the Retrieval-Augmented\nGeneration (RAG), two structured knowledge processing methods, table\ntextualization and decision-tree textualization, were proposed to enhance large\nlanguage models' (LLMs) understanding of heterogeneous data formats. Based on\nthese methods, a domain-specific goat farming knowledge base was established to\nimprove LLM's capacity for cross-scenario generalization. The knowledge base\nspans five key domains: Disease Prevention and Treatment, Nutrition Management,\nRearing Management, Goat Milk Management, and Basic Farming Knowledge.\nAdditionally, an online search module is integrated to enable real-time\nretrieval of up-to-date information. To evaluate system performance, six\nablation experiments were conducted to examine the contribution of each\ncomponent. The results demonstrated that heterogeneous knowledge fusion method\nachieved the best results, with mean accuracies of 87.90% on the validation set\nand 84.22% on the test set. Across the text-based, table-based, decision-tree\nbased Q&A tasks, accuracy consistently exceeded 85%, validating the\neffectiveness of structured knowledge fusion within a modular design. Error\nanalysis identified omission as the predominant error category, highlighting\nopportunities to further improve retrieval coverage and context integration. In\nconclusion, the results highlight the robustness and reliability of the\nproposed system for practical applications in goat farming.", "AI": {"tldr": "An intelligent knowledge assistant for goat health management uses Retrieval-Augmented Generation with two structured knowledge processing methods (table textualization and decision-tree textualization), a domain-specific goat knowledge base across five domains, and real-time online search. Six ablations show heterogeneous knowledge fusion yields best accuracy (~87.9% validation, ~84.2% test) with >85% accuracy across text/table/decision-tree Q&A tasks; omission is the main error category.", "motivation": "To overcome limited LLM application in livestock due to diverse, complex knowledge sources, by fusing heterogeneous knowledge and enabling cross-scenario generalization in goat farming, plus real-time information retrieval.", "method": "RAG-based system with two structured knowledge processing methods\u2014table textualization and decision-tree textualization\u2014applied to a domain-specific goat farming knowledge base spanning Disease Prevention and Treatment, Nutrition Management, Rearing Management, Goat Milk Management, and Basic Farming Knowledge. Includes an online search module. Six ablation experiments to assess component contributions within a modular design.", "result": "Heterogeneous knowledge fusion achieved the best performance, with mean accuracies of 87.90% on the validation set and 84.22% on the test set. Across text-based, table-based, and decision-tree-based Q&A tasks, accuracy consistently exceeds 85%. Error analysis indicates omission as the dominant error category, suggesting gaps in retrieval coverage and context integration.", "conclusion": "The study demonstrates robustness and practical viability of the proposed structured knowledge fusion approach and RAG-enhanced assistant for real-world goat farming health management, leveraging domain-specific knowledge and real-time information access."}}
{"id": "2509.10065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10065", "abs": "https://arxiv.org/abs/2509.10065", "authors": ["Hauzi Cao", "Jiahao Shen", "Zhengzhen Li", "Qinquan Ren", "Shiyu Zhao"], "title": "Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation", "comment": null, "summary": "This paper studies the kinematic tracking control problem for aerial\nmanipulators. Existing kinematic tracking control methods, which typically\nemploy proportional-derivative feedback or tracking-error-based feedback\nstrategies, may fail to achieve tracking objectives within specified time\nconstraints. To address this limitation, we propose a novel control framework\ncomprising two key components: end-effector tracking control based on a\nuser-defined preset trajectory and quadratic programming-based reference\nallocation. Compared with state-of-the-art approaches, the proposed method has\nseveral attractive features. First, it ensures that the end-effector reaches\nthe desired position within a preset time while keeping the tracking error\nwithin a performance envelope that reflects task requirements. Second,\nquadratic programming is employed to allocate the references of the quadcopter\nbase and the Delta arm, while considering the physical constraints of the\naerial manipulator, thus preventing solutions that may violate physical\nlimitations. The proposed approach is validated through three experiments.\nExperimental results demonstrate the effectiveness of the proposed algorithm\nand its capability to guarantee that the target position is reached within the\npreset time.", "AI": {"tldr": "A two-component control framework for aerial manipulators achieving preset-time end-effector convergence while enforcing physical constraints via QP-based reference allocation.", "motivation": "Existing kinematic tracking methods (PD or tracking-error-based) often fail to guarantee time-constrained tracking objectives and may violate physical limits of the platform.", "method": "(i) End-effector tracking control driven by a user-defined preset trajectory; (ii) Quadratic programming-based reference allocation for the quadcopter base and Delta arm that respects physical constraints.", "result": "Three experiments validate the approach, showing the target position is reached within the preset time and the tracking error remains within a defined performance envelope.", "conclusion": "The proposed framework provides preset-time convergence with constraint-aware reference allocation, offering improved guarantees over state-of-the-art methods."}}
{"id": "2509.09742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09742", "abs": "https://arxiv.org/abs/2509.09742", "authors": ["Md Fazle Rasul", "Alanood Alqobaisi", "Bruhadeshwar Bezawada", "Indrakshi Ray"], "title": "Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning", "comment": null, "summary": "Federated learning (FL) allows multiple entities to train a shared model\ncollaboratively. Its core, privacy-preserving principle is that participants\nonly exchange model updates, such as gradients, and never their raw, sensitive\ndata. This approach is fundamental for applications in domains where privacy\nand confidentiality are important. However, the security of this very mechanism\nis threatened by gradient inversion attacks, which can reverse-engineer private\ntraining data directly from the shared gradients, defeating the purpose of FL.\nWhile the impact of these attacks is known for image, text, and tabular data,\ntheir effect on video data remains an unexamined area of research. This paper\npresents the first analysis of video data leakage in FL using gradient\ninversion attacks. We evaluate two common video classification approaches: one\nemploying pre-trained feature extractors and another that processes raw video\nframes with simple transformations. Our initial results indicate that the use\nof feature extractors offers greater resilience against gradient inversion\nattacks. We also demonstrate that image super-resolution techniques can enhance\nthe frames extracted through gradient inversion attacks, enabling attackers to\nreconstruct higher-quality videos. Our experiments validate this across\nscenarios where the attacker has access to zero, one, or more reference frames\nfrom the target environment. We find that although feature extractors make\nattacks more challenging, leakage is still possible if the classifier lacks\nsufficient complexity. We, therefore, conclude that video data leakage in FL is\na viable threat, and the conditions under which it occurs warrant further\ninvestigation.", "AI": {"tldr": "This work analyzes gradient inversion attacks on video data in federated learning, showing that feature-extractor\u2013based video classifiers are more robust against leakage, but reconstruction remains possible if the classifier lacks sufficient complexity; image super-resolution can significantly improve recovered frames; leakage is plausible across zero, one, or more reference frames and warrants further study.", "motivation": "Federated learning aims to protect private data by sharing only model updates, but gradient inversion attacks can reconstruct train data from gradients. While prior work examined image, text, and tabular data, video data remains unexplored. Understanding video leakage is crucial for privacy guarantees in FL, especially given the richer and more sensitive nature of video content.", "method": "Evaluate two video classification approaches: (1) using pre-trained feature extractors to process video content, and (2) processing raw frames with simple transformations. Conduct gradient inversion attacks to reconstruct frames from gradients, under scenarios where the attacker has zero, one, or multiple reference frames from the target environment. Additionally, apply image super-resolution techniques to enhance the reconstructed frames. Assess leakage as a function of classifier complexity.", "result": "Feature extractor-based setups offer greater resilience to gradient inversion attacks compared to raw-frame pipelines, but leakage remains possible when the classifier is not sufficiently complex. Image super-resolution can significantly improve the quality of recovered frames, amplifying the risk. Leakage is demonstrated across scenarios with zero, one, or multiple reference frames, indicating video data leakage in FL is a viable threat that warrants further investigation.", "conclusion": "Video data leakage via gradient inversion attacks is a real and actionable threat in federated learning. The effectiveness of defenses (e.g., feature extractors) appears limited by classifier complexity, and attackers can enhance reconstruction with super-resolution. Further research is needed to characterize conditions, develop mitigations, and establish privacy-preserving FL practices for video data."}}
{"id": "2509.09793", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09793", "abs": "https://arxiv.org/abs/2509.09793", "authors": ["Vincent Herfeld", "Baudouin Denis de Senneville", "Arthur Leclaire", "Nicolas Papadakis"], "title": "From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms", "comment": null, "summary": "In this paper we analyze the Gradient-Step Denoiser and its usage in\nPlug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms\nuses off the shelf denoisers to replace a proximity operator or a gradient\ndescent operator of an image prior. Usually this image prior is implicit and\ncannot be expressed, but the Gradient-Step Denoiser is trained to be exactly\nthe gradient descent operator or the proximity operator of an explicit\nfunctional while preserving state-of-the-art denoising capabilities.", "AI": {"tldr": "Proposes Gradient-Step Denoiser that acts as an exact gradient/prox operator for an explicit functional within Plug-and-Play, while preserving state-of-the-art denoising.", "motivation": "Many PnP denoisers are implicit and lack an explicit energy functional; the Gradient-Step Denoiser provides a concrete gradient/proximity operator, enabling interpretability and theoretical analysis without sacrificing performance.", "method": "Train a denoiser so that its action equals either the gradient of an explicit energy functional or the proximal operator of an explicit functional, integrated into Plug-and-Play schemes, aiming to keep denoising quality.", "result": "The Gradient-Step Denoiser achieves exact gradient/proximal behavior for an explicit functional while maintaining state-of-the-art denoising performance, compatible with PnP.", "conclusion": "This work provides an interpretable, theoretically grounded PnP priors by tying denoising operations to explicit functionals, potentially aiding analysis and design of future PnP algorithms."}}
{"id": "2509.09867", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09867", "abs": "https://arxiv.org/abs/2509.09867", "authors": ["Yago Romano Matinez", "Jesse Roberts"], "title": "LLMs as Agentic Cooperative Players in Multiplayer UNO", "comment": null, "summary": "LLMs promise to assist humans -- not just by answering questions, but by\noffering useful guidance across a wide range of tasks. But how far does that\nassistance go? Can a large language model based agent actually help someone\naccomplish their goal as an active participant? We test this question by\nengaging an LLM in UNO, a turn-based card game, asking it not to win but\ninstead help another player to do so. We built a tool that allows decoder-only\nLLMs to participate as agents within the RLCard game environment. These models\nreceive full game-state information and respond using simple text prompts under\ntwo distinct prompting strategies. We evaluate models ranging from small (1B\nparameters) to large (70B parameters) and explore how model scale impacts\nperformance. We find that while all models were able to successfully outperform\na random baseline when playing UNO, few were able to significantly aid another\nplayer.", "AI": {"tldr": "Decoder-only LLMs can join UNO as agents to assist a human teammate; they beat a random baseline but rarely provide clear cooperative benefits, with some influence from model size and prompting strategy.", "motivation": "Assess whether LLM-based agents can actively help humans achieve goals in collaborative settings, beyond answering questions.", "method": "Integrate decoder-only LLMs as agents in the RLCard UNO environment. They receive full game-state information and respond via simple text prompts under two prompting strategies. Test models from 1B to 70B parameters to evaluate how scale affects performance, focusing on both winning and assisting a partner.", "result": "All models outperform a random baseline in UNO. However, only a small subset of models/prompts significantly aided the other player; scaling and prompting had limited impact on cooperative success.", "conclusion": "LLMs can participate in games with basic competence, but designing agents that meaningfully assist another player is challenging; further work is needed on alignment, prompting, and agent-tooling to enable effective collaboration."}}
{"id": "2509.10096", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10096", "abs": "https://arxiv.org/abs/2509.10096", "authors": ["Saeed Saadatnejad", "Reyhaneh Hosseininejad", "Jose Barreiros", "Katherine M. Tsui", "Alexandre Alahi"], "title": "HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario", "comment": "Accepted to RA-L 2025", "summary": "The increasing labor shortage and aging population underline the need for\nassistive robots to support human care recipients. To enable safe and\nresponsive assistance, robots require accurate human motion prediction in\nphysical interaction scenarios. However, this remains a challenging task due to\nthe variability of assistive settings and the complexity of coupled dynamics in\nphysical interactions. In this work, we address these challenges through two\nkey contributions: (1) HHI-Assist, a dataset comprising motion capture clips of\nhuman-human interactions in assistive tasks; and (2) a conditional\nTransformer-based denoising diffusion model for predicting the poses of\ninteracting agents. Our model effectively captures the coupled dynamics between\ncaregivers and care receivers, demonstrating improvements over baselines and\nstrong generalization to unseen scenarios. By advancing interaction-aware\nmotion prediction and introducing a new dataset, our work has the potential to\nsignificantly enhance robotic assistance policies. The dataset and code are\navailable at: https://sites.google.com/view/hhi-assist/home", "AI": {"tldr": "Introduces HHI-Assist, a motion-capture dataset of human-human assistive tasks, and a conditional Transformer-based denoising diffusion model to predict poses in physical human\u2013robot interaction, achieving better baselines and generalization to unseen scenarios.", "motivation": "Address the growing need for safe, responsive assistive robots amid labor shortages and aging populations. Challenges include high variability in assistive settings and the coupled dynamics of physical interactions between caregivers and care receivers.", "method": "1) Propose HHI-Assist, a dataset of motion capture clips of human-human interactions in assistive tasks. 2) Develop a conditional Transformer-based denoising diffusion model to predict the poses of interacting agents, explicitly modeling coupled caregiver\u2013care receiver dynamics.", "result": "The approach shows improvements over baselines and strong generalization to unseen scenarios, highlighting the benefit of interaction-aware motion prediction. The authors provide dataset and code online.", "conclusion": "The work advances interaction-aware motion prediction and introduces a new dataset, which has the potential to significantly enhance robotic assistance policies."}}
{"id": "2509.09750", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09750", "abs": "https://arxiv.org/abs/2509.09750", "authors": ["Hossein Yazdanjouei", "Arash Mansouri", "Mohammad Shokouhifar"], "title": "A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images", "comment": null, "summary": "This study proposes a semi-supervised co-training framework for object\ndetection in densely packed retail environments, where limited labeled data and\ncomplex conditions pose major challenges. The framework combines Faster R-CNN\n(utilizing a ResNet backbone) for precise localization with YOLO (employing a\nDarknet backbone) for global context, enabling mutual pseudo-label exchange\nthat improves accuracy in scenes with occlusion and overlapping objects. To\nstrengthen classification, it employs an ensemble of XGBoost, Random Forest,\nand SVM, utilizing diverse feature representations for higher robustness.\nHyperparameters are optimized using a metaheuristic-driven algorithm, enhancing\nprecision and efficiency across models. By minimizing reliance on manual\nlabeling, the approach reduces annotation costs and adapts effectively to\nfrequent product and layout changes common in retail. Experiments on the\nSKU-110k dataset demonstrate strong performance, highlighting the scalability\nand practicality of the proposed framework for real-world retail applications\nsuch as automated inventory tracking, product monitoring, and checkout systems.", "AI": {"tldr": "Semi-supervised co-training for robust object detection in dense retail scenes using dual detectors and an ensemble classifier, with metaheuristic hyperparameter optimization to reduce labeling needs; validated on SKU-110k.", "motivation": "The retail environment presents densely packed objects, frequent occlusion/overlaps, and rapidly changing layouts/products, all while labeled data remains scarce and costly. A semi-supervised, label-efficient approach is needed.", "method": "A co-training framework that couples Faster R-CNN (ResNet backbone) for precise localization with YOLO (Darknet backbone) for global context, enabling mutual pseudo-label exchange. A classifier ensemble (XGBoost, Random Forest, SVM) uses diverse features for robust categorization. Hyperparameters are optimized via a metaheuristic-driven search to maximize accuracy and efficiency.", "result": "On SKU-110k, the approach achieves strong performance, particularly in occlusion/overlap scenarios, demonstrating scalability and practicality for real-world retail tasks such as automated inventory tracking, product monitoring, and checkout systems.", "conclusion": "The framework reduces labeling costs, adapts to frequent product and layout changes, and provides a practical, scalable solution for semi-supervised object detection in dense retail environments."}}
{"id": "2509.09799", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09799", "abs": "https://arxiv.org/abs/2509.09799", "authors": ["Mansi Sharma", "Alexandre Duchevet", "Florian Daiber", "Jean-Paul Imbert", "Maurice Rekrut"], "title": "Distinguishing Startle from Surprise Events Based on Physiological Signals", "comment": null, "summary": "Unexpected events can impair attention and delay decision-making, posing\nserious safety risks in high-risk environments such as aviation. In particular,\nreactions like startle and surprise can impact pilot performance in different\nways, yet are often hard to distinguish in practice. Existing research has\nlargely studied these reactions separately, with limited focus on their\ncombined effects or how to differentiate them using physiological data. In this\nwork, we address this gap by distinguishing between startle and surprise events\nbased on physiological signals using machine learning and multi-modal fusion\nstrategies. Our results demonstrate that these events can be reliably\npredicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.\nTo further validate the robustness of our model, we extended the evaluation to\ninclude a baseline condition, successfully differentiating between Startle,\nSurprise, and Baseline states with a highest mean accuracy of 74.9% with\nXGBoost and Late Fusion.", "AI": {"tldr": "A study distinguishes startle vs surprise using physiological data and multi-modal ML fusion, achieving high accuracy in predicting these reactions in aviation contexts.", "motivation": "Existing work treats startle and surprise separately; understanding and differentiating these reactions using physiological signals is critical for safety in high-risk environments like aviation.", "method": "Collect physiological signals and apply machine learning with multi-modal (late) fusion. Evaluate using SVM achieving 85.7% accuracy for Startle vs Surprise, and XGBoost achieving 74.9% accuracy for Startle/Surprise/Baseline.", "result": "Startle vs Surprise: 85.7% mean accuracy with SVM + Late Fusion. Startle/Surprise/Baseline: 74.9% mean accuracy with XGBoost + Late Fusion.", "conclusion": "ML with multi-modal fusion can reliably differentiate startle and surprise from physiological signals, with robust performance including a baseline condition, suggesting potential for safety-critical monitoring; further work needed on generalizability and real-time deployment."}}
{"id": "2509.09915", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.09915", "abs": "https://arxiv.org/abs/2509.09915", "authors": ["Woong Shin", "Renan Souza", "Daniel Rosendo", "Fr\u00e9d\u00e9ric Suter", "Feiyi Wang", "Prasanna Balaprakash", "Rafael Ferreira da Silva"], "title": "The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science", "comment": null, "summary": "Modern scientific discovery increasingly requires coordinating distributed\nfacilities and heterogeneous resources, forcing researchers to act as manual\nworkflow coordinators rather than scientists. Advances in AI leading to AI\nagents show exciting new opportunities that can accelerate scientific discovery\nby providing intelligence as a component in the ecosystem. However, it is\nunclear how this new capability would materialize and integrate in the real\nworld. To address this, we propose a conceptual framework where workflows\nevolve along two dimensions which are intelligence (from static to intelligent)\nand composition (from single to swarm) to chart an evolutionary path from\ncurrent workflow management systems to fully autonomous, distributed scientific\nlaboratories. With these trajectories in mind, we present an architectural\nblueprint that can help the community take the next steps towards harnessing\nthe opportunities in autonomous science with the potential for 100x discovery\nacceleration and transformational scientific workflows.", "AI": {"tldr": "Proposes a two-dimensional evolutionary framework (intelligence and swarm composition) to evolve workflows from static, centralized systems to autonomous, distributed scientific laboratories, plus an architectural blueprint for realizing this with potential 100x discovery acceleration.", "motivation": "Current scientific workflows require manual coordination across distributed facilities; AI agents offer potential to accelerate discovery, but how to integrate in real-world workflows remains unclear.", "method": "Conceptual framing: define two axes of evolution (intelligence: static\u2192intelligent; composition: single\u2192swarm); sketch architectural blueprint and roadmap for transitions; discuss enabling technologies and milestones.", "result": "A conceptual framework and architectural blueprint rather than empirical results; outlines trajectories toward autonomous science and a pathway to higher efficiency; asserts potential for significant discovery acceleration.", "conclusion": "Adopting the proposed framework and architecture could guide the community in developing autonomous, distributed scientific laboratories and realize accelerated discovery; calls for further work, experiments, and validation to materialize these ideas."}}
{"id": "2509.10128", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10128", "abs": "https://arxiv.org/abs/2509.10128", "authors": ["Philip Arm", "Oliver Fischer", "Joseph Church", "Adrian Fuhrer", "Hendrik Kolvenbach", "Marco Hutter"], "title": "Efficient Learning-Based Control of a Legged Robot in Lunar Gravity", "comment": null, "summary": "Legged robots are promising candidates for exploring challenging areas on\nlow-gravity bodies such as the Moon, Mars, or asteroids, thanks to their\nadvanced mobility on unstructured terrain. However, as planetary robots' power\nand thermal budgets are highly restricted, these robots need energy-efficient\ncontrol approaches that easily transfer to multiple gravity environments. In\nthis work, we introduce a reinforcement learning-based control approach for\nlegged robots with gravity-scaled power-optimized reward functions. We use our\napproach to develop and validate a locomotion controller and a base pose\ncontroller in gravity environments from lunar gravity (1.62 m/s2) to a\nhypothetical super-Earth (19.62 m/s2). Our approach successfully scales across\nthese gravity levels for locomotion and base pose control with the\ngravity-scaled reward functions. The power-optimized locomotion controller\nreached a power consumption for locomotion of 23.4 W in Earth gravity on a\n15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.\nAdditionally, we designed a constant-force spring offload system that allowed\nus to conduct real-world experiments on legged locomotion in lunar gravity. In\nlunar gravity, the power-optimized control policy reached 12.2 W, 36 % less\nthan a baseline controller which is not optimized for power efficiency. Our\nmethod provides a scalable approach to developing power-efficient locomotion\ncontrollers for legged robots across multiple gravity levels.", "AI": {"tldr": "RL-based gravity-scaled, power-optimized control enables energy-efficient legged locomotion and base pose control across lunar to super-Earth gravities, validated with real-world lunar experiments.", "motivation": "Planetary robots have tight power and thermal budgets. Legged locomotion in unstructured terrain requires energy-efficient controllers that transfer across different gravity environments (Moon, Mars, asteroids).", "method": "Developed a reinforcement learning\u2013based locomotion controller and base pose controller using gravity-scaled, power-optimized reward functions, tested across gravity levels from lunar (1.62 m/s^2) to a hypothetical super-Earth (19.62 m/s^2). Includes a constant-force spring offload system enabling real-world lunar experiments and reports power metrics and improvements over baseline policies.", "result": "Power-optimized policy achieved 23.4 W in Earth gravity on a 15.65 kg robot at 0.4 m/s, a 23% improvement over the baseline. In lunar gravity, 12.2 W, 36% less than a baseline not optimized for power.", "conclusion": "The method offers a scalable approach to developing power-efficient locomotion controllers for legged robots across multiple gravity levels, with demonstrated real-world validation in lunar gravity."}}
{"id": "2509.09785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09785", "abs": "https://arxiv.org/abs/2509.09785", "authors": ["Moslem Yazdanpanah", "Ali Bahri", "Mehrdad Noori", "Sahar Dastani", "Gustavo Adolfo Vargas Hakim", "David Osowiechi", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging", "comment": null, "summary": "Test-time adaptation (TTA) is crucial for mitigating performance degradation\ncaused by distribution shifts in 3D point cloud classification. In this work,\nwe introduce Token Purging (PG), a novel backpropagation-free approach that\nremoves tokens highly affected by domain shifts before they reach attention\nlayers. Unlike existing TTA methods, PG operates at the token level, ensuring\nrobust adaptation without iterative updates. We propose two variants: PG-SP,\nwhich leverages source statistics, and PG-SF, a fully source-free version\nrelying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,\nShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of\n+10.3\\% higher accuracy than state-of-the-art backpropagation-free methods,\nwhile PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is\n12.4 times faster and 5.5 times more memory efficient than our baseline, making\nit suitable for real-world deployment. Code is available at\n\\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}", "AI": {"tldr": "Token Purging (PG) is a backpropagation-free, token-level test-time adaptation method for 3D point-cloud classification that purges tokens highly affected by domain shifts before attention. It has two variants: PG-SP (uses source statistics) and PG-SF (source-free, CLS-token driven). It claims strong gains over backpropagation-free baselines, plus faster and more memory-efficient deployment, with code released.", "motivation": "Mitigate performance degradation due to distribution shifts in 3D point-cloud classifiers without expensive online backpropagation, enabling robust and real-time deployment.", "method": "PG removes tokens highly affected by domain shifts prior to reaching attention layers in the transformer backbone, operating at the token level. PG-SP uses source statistics for purging; PG-SF is fully source-free and relies on CLS-token-driven adaptation.", "result": "PG-SP achieves an average of +10.3 percentage points in accuracy over state-of-the-art backpropagation-free methods across ModelNet40-C, ShapeNet-C, and ScanObjectNN-C. PG-SF establishes new benchmarks for source-free adaptation. Overall, PG is 12.4x faster and 5.5x more memory efficient than the baseline.", "conclusion": "Token Purging enables effective and efficient test-time adaptation for 3D point-cloud classification, achieving strong accuracy gains with low computation and memory requirements and practical deployment viability."}}
{"id": "2509.09838", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09838", "abs": "https://arxiv.org/abs/2509.09838", "authors": ["Reza Asad", "Reza Babanezhad", "Sharan Vaswani"], "title": "Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning", "comment": null, "summary": "Value-based approaches such as DQN are the default methods for off-policy\nreinforcement learning with discrete-action environments such as Atari. Common\npolicy-based methods are either on-policy and do not effectively learn from\noff-policy data (e.g. PPO), or have poor empirical performance in the\ndiscrete-action setting (e.g. SAC). Consequently, starting from discrete SAC\n(DSAC), we revisit the design of actor-critic methods in this setting. First,\nwe determine that the coupling between the actor and critic entropy is the\nprimary reason behind the poor performance of DSAC. We demonstrate that by\nmerely decoupling these components, DSAC can have comparable performance as\nDQN. Motivated by this insight, we introduce a flexible off-policy actor-critic\nframework that subsumes DSAC as a special case. Our framework allows using an\nm-step Bellman operator for the critic update, and enables combining standard\npolicy optimization methods with entropy regularization to instantiate the\nresulting actor objective. Theoretically, we prove that the proposed methods\ncan guarantee convergence to the optimal regularized value function in the\ntabular setting. Empirically, we demonstrate that these methods can approach\nthe performance of DQN on standard Atari games, and do so even without entropy\nregularization or explicit exploration.", "AI": {"tldr": "Decoupling actor and critic entropy in discrete-SAC (DSAC) improves off-policy performance; introduces a flexible off-policy actor-critic framework with an m-step critic update; proves convergence in tabular settings; empirically matches DQN on Atari without extra exploration.", "motivation": "DSAC suffers from poor performance due to the coupling between the actor's entropy term and the critic; there is a need for effective off-policy learning in discrete-action environments and a unifying framework that can match strong value-based methods.", "method": "Decouple actor and critic entropy to improve DSAC; propose a general off-policy actor-critic framework that subsumes DSAC; allow an m-step Bellman operator for critic updates; combine standard policy optimization with entropy regularization to define the actor objective, yielding a flexible, reusable algorithm.", "result": "The framework provably converges to the optimal regularized value function in the tabular setting; empirically, methods based on the framework approach the performance of DQN on Atari games, even without entropy regularization or explicit exploration.", "conclusion": "Decoupling the entropy components resolves DSAC's drawbacks and yields a versatile off-policy actor-critic framework that aligns with strong value-based performance in discrete RL, demonstrated both theoretically and empirically."}}
{"id": "2509.09919", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09919", "abs": "https://arxiv.org/abs/2509.09919", "authors": ["Franklin Yiu", "Mohan Lu", "Nina Li", "Kevin Joseph", "Tianxu Zhang", "Julian Togelius", "Timothy Merino", "Sam Earle"], "title": "A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments", "comment": null, "summary": "Procedural content generation often requires satisfying both\ndesigner-specified objectives and adjacency constraints implicitly imposed by\nthe underlying tile set. To address the challenges of jointly optimizing both\nconstraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a\nMarkov Decision Process (MDP), enabling external optimization algorithms to\nfocus exclusively on objective maximization while leveraging WFC's propagation\nmechanism to enforce constraint satisfaction. We empirically compare optimizing\nthis MDP to traditional evolutionary approaches that jointly optimize global\nmetrics and local tile placement. Across multiple domains with various\ndifficulties, we find that joint optimization not only struggles as task\ncomplexity increases, but consistently underperforms relative to optimization\nover the WFC-MDP, underscoring the advantages of decoupling local constraint\nsatisfaction from global objective optimization.", "AI": {"tldr": "Reformulates WaveFunctionCollapse as an MDP to decouple constraint satisfaction from objective optimization; decoupled optimization over the WFC-MDP outperforms joint evolutionary approaches.", "motivation": "Tile-based procedural content generation faces the dual challenge of satisfying implicit adjacency constraints and optimizing designer-specified objectives. A decoupled approach may improve scalability and performance by letting constraint propagation handle feasibility while external optimizers focus on objectives.", "method": "Model WFC as a Markov Decision Process (MDP) and use external optimization algorithms to maximize objectives while relying on WFC's propagation to enforce constraints. Compare this WFC-MDP optimization to traditional evolutionary methods that jointly optimize global metrics and local tile placement across multiple domains with varying difficulty.", "result": "Optimization over the WFC-MDP consistently outperforms joint evolutionary optimization, especially as task complexity increases. Joint optimization struggles to scale and underperforms relative to the decoupled approach, highlighting the advantage of separating local constraint satisfaction from global objective optimization.", "conclusion": "Decoupling local constraint satisfaction from global objective optimization via a WFC-MDP leads to better performance and scalability; leveraging WFC for constraint propagation while external optimizers handle objectives is advantageous."}}
{"id": "2509.10139", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10139", "abs": "https://arxiv.org/abs/2509.10139", "authors": ["Santiago Montiel-Mar\u00edn", "Angel Llamazares", "Miguel Antunes-Garc\u00eda", "Fabio S\u00e1nchez-Garc\u00eda", "Luis M. Bergasa"], "title": "CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion", "comment": "4 pages, 2 figures", "summary": "Camera-radar fusion offers a robust and cost-effective alternative to\nLiDAR-based autonomous driving systems by combining complementary sensing\ncapabilities: cameras provide rich semantic cues but unreliable depth, while\nradar delivers sparse yet reliable position and motion information. We\nintroduce CaR1, a novel camera-radar fusion architecture for BEV vehicle\nsegmentation. Built upon BEVFusion, our approach incorporates a grid-wise radar\nencoding that discretizes point clouds into structured BEV features and an\nadaptive fusion mechanism that dynamically balances sensor contributions.\nExperiments on nuScenes demonstrate competitive segmentation performance (57.6\nIoU), on par with state-of-the-art methods. Code is publicly available\n\\href{https://www.github.com/santimontiel/car1}{online}.", "AI": {"tldr": "CaR1 extends BEVFusion with grid-wise radar encoding and adaptive fusion for camera-radar BEV segmentation, achieving competitive 57.6 IoU on nuScenes and releasing code.", "motivation": "Address camera depth perception and radar sparsity by fusing complementary modalities to improve BEV vehicle segmentation in autonomous driving.", "method": "Introduce grid-wise radar encoding to discretize radar point clouds into structured BEV features and an adaptive fusion module that dynamically balances camera and radar contributions within BEVFusion.", "result": "Achieves 57.6 IoU on nuScenes, competitive with state-of-the-art methods.", "conclusion": "Camera-radar fusion is a viable alternative to LiDAR for BEV segmentation; CaR1 demonstrates effective integration and publicly available code."}}
{"id": "2509.09792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09792", "abs": "https://arxiv.org/abs/2509.09792", "authors": ["Zimin Xia", "Chenghao Xu", "Alexandre Alahi"], "title": "Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors", "comment": null, "summary": "We propose an accurate and highly interpretable fine-grained cross-view\nlocalization method that estimates the 3 Degrees of Freedom pose of a\nground-level image by matching its local features with a reference aerial\nimage. Previous methods typically transform the ground image into a bird's-eye\nview (BEV) representation and then align it with the aerial image for\nlocalization. However, this transformation often leads to information loss due\nto perspective distortion or compression of height information, thereby\ndegrading alignment quality with the aerial view. In contrast, our method\ndirectly establishes correspondences between ground and aerial images and lifts\nonly the matched keypoints to BEV space using monocular depth prior. Notably,\nmodern depth predictors can provide reliable metric depth when the test samples\nare similar to the training data. When the depth distribution differs, they\nstill produce consistent relative depth, i.e., depth accurate up to an unknown\nscale. Our method supports both metric and relative depth. It employs a\nscale-aware Procrustes alignment to estimate the camera pose from the\ncorrespondences and optionally recover the scale when using relative depth.\nExperimental results demonstrate that, with only weak supervision on camera\npose, our method learns accurate local feature correspondences and achieves\nsuperior localization performance under challenging conditions, such as\ncross-area generalization and unknown orientation. Moreover, our method is\ncompatible with various relative depth models without requiring per-model\nfinetuning. This flexibility, combined with strong localization performance,\nmakes it well-suited for real-world deployment.", "AI": {"tldr": "Fine-grained cross-view localization via ground-to-aerial feature matching with monocular depth lifting and scale-aware Procrustes; supports metric/relative depth and weak supervision; robust cross-area generalization and unknown orientation.", "motivation": "Address information loss and distortion from BEV conversion when localizing ground images with aerials; improve alignment quality and generalization across areas and orientations.", "method": "Directly establish correspondences between ground and aerial images. Lift only the matched keypoints to BEV space using monocular depth prior (metric or relative). Use scale-aware Procrustes alignment to estimate camera pose from correspondences, optionally recovering scale when depth is relative. Achieve accurate local feature correspondences under weak pose supervision.", "result": "Experimental results show accurate local feature correspondences and superior localization under challenging conditions such as cross-area generalization and unknown orientation. The method is compatible with various relative depth models without per-model fine-tuning, facilitating real-world deployment.", "conclusion": "The proposed framework enables accurate, interpretable fine-grained localization with flexible depth modeling, reducing information loss from BEV transformations and improving alignment and generalization for cross-view localization."}}
{"id": "2509.09843", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09843", "abs": "https://arxiv.org/abs/2509.09843", "authors": ["Jiajun Shen", "Yufei Jin", "Yi He", "Xingquan Zhu"], "title": "HGEN: Heterogeneous Graph Ensemble Networks", "comment": "The paper is in proceedings of the 34th IJCAI Conference, 2025", "summary": "This paper presents HGEN that pioneers ensemble learning for heterogeneous\ngraphs. We argue that the heterogeneity in node types, nodal features, and\nlocal neighborhood topology poses significant challenges for ensemble learning,\nparticularly in accommodating diverse graph learners. Our HGEN framework\nensembles multiple learners through a meta-path and transformation-based\noptimization pipeline to uplift classification accuracy. Specifically, HGEN\nuses meta-path combined with random dropping to create Allele Graph Neural\nNetworks (GNNs), whereby the base graph learners are trained and aligned for\nlater ensembling. To ensure effective ensemble learning, HGEN presents two key\ncomponents: 1) a residual-attention mechanism to calibrate allele GNNs of\ndifferent meta-paths, thereby enforcing node embeddings to focus on more\ninformative graphs to improve base learner accuracy, and 2) a\ncorrelation-regularization term to enlarge the disparity among embedding\nmatrices generated from different meta-paths, thereby enriching base learner\ndiversity. We analyze the convergence of HGEN and attest its higher\nregularization magnitude over simple voting. Experiments on five heterogeneous\nnetworks validate that HGEN consistently outperforms its state-of-the-art\ncompetitors by substantial margin.", "AI": {"tldr": "HGEN introduces ensemble learning for heterogeneous graphs by generating multiple allele GNNs through meta-paths and random dropping, aligning base learners, and optimizing via residual-attention calibration and correlation-regularization to boost diversity; it achieves superior accuracy over state-of-the-art on five heterogeneous networks.", "motivation": "Heterogeneous graphs have diverse node types, features, and local topologies that challenge traditional ensemble methods. There is a need to create diverse, well-calibrated base learners and effective aggregation to improve classification accuracy.", "method": "HGEN creates Allele Graph Neural Networks (GNNs) by applying meta-paths and random dropping to generate diverse base learners. It employs a residual-attention mechanism to calibrate embeddings from different meta-paths and a correlation-regularization term to increase dissimilarity among embedding matrices across meta-paths, enabling stronger ensemble performance. Convergence analysis is provided.", "result": "Empirical evaluation on five heterogeneous networks shows HGEN consistently outperforms state-of-the-art competitors by a substantial margin.", "conclusion": "HGEN provides a principled ensemble framework for heterogeneous graphs that balances calibration and diversity (via residual-attention and correlation-regularization) and yields notable gains in classification accuracy across diverse heterogeneous networks."}}
{"id": "2509.09982", "categories": ["cs.AI", "I.2.4"], "pdf": "https://arxiv.org/pdf/2509.09982", "abs": "https://arxiv.org/abs/2509.09982", "authors": ["Stav Armoni-Friedmann", "Hana Chockler", "David A. Kelly"], "title": "Evaluation of Black-Box XAI Approaches for Predictors of Values of Boolean Formulae", "comment": "Accepted to ECAI-EXCD Workshop, 8 pages, 2 figures, 5 tables", "summary": "Evaluating explainable AI (XAI) approaches is a challenging task in general,\ndue to the subjectivity of explanations. In this paper, we focus on tabular\ndata and the specific use case of AI models predicting the values of Boolean\nfunctions. We extend the previous work in this domain by proposing a formal and\nprecise measure of importance of variables based on actual causality, and we\nevaluate state-of-the-art XAI tools against this measure. We also present a\nnovel XAI tool B-ReX, based on the existing tool ReX, and demonstrate that it\nis superior to other black-box XAI tools on a large-scale benchmark.\nSpecifically, B-ReX achieves a Jensen-Shannon divergence of 0.072 $\\pm$ 0.012\non random 10-valued Boolean formulae", "AI": {"tldr": "Proposes a formal causality-based measure of variable importance for XAI on tabular Boolean-predictive models, evaluates state-of-the-art XAI tools against it, and introduces B-ReX (built on ReX) which outperforms rivals on a large benchmark (JS divergence ~0.072).", "motivation": "Explanations in AI are subjective. There is a need for objective, causality-grounded measures of feature importance in tabular data for Boolean function prediction, to fairly evaluate XAI tools.", "method": "Define a formal, actual-causality-based variable-importance measure. Evaluate leading XAI tools against this measure. Develop B-ReX, a new XAI tool based on ReX. Benchmark on large-scale random 10-valued Boolean formulas using Jensen-Shannon divergence.", "result": "B-ReX outperforms other black-box XAI tools on the benchmark. It achieves a Jensen-Shannon divergence of 0.072 \u00b1 0.012 on random 10-valued Boolean formulae.", "conclusion": "A causality-based importance measure is viable for evaluating XAI approaches in this setting, and B-ReX demonstrates superior performance compared with existing black-box XAI tools, highlighting the value of causal-grounded evaluation."}}
{"id": "2509.10247", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10247", "abs": "https://arxiv.org/abs/2509.10247", "authors": ["Xinhong Zhang", "Runqing Wang", "Yunfan Ren", "Jian Sun", "Hao Fang", "Jie Chen", "Gang Wang"], "title": "DiffAero: A GPU-Accelerated Differentiable Simulation Framework for Efficient Quadrotor Policy Learning", "comment": "8 pages, 11 figures, 1 table", "summary": "This letter introduces DiffAero, a lightweight, GPU-accelerated, and fully\ndifferentiable simulation framework designed for efficient quadrotor control\npolicy learning. DiffAero supports both environment-level and agent-level\nparallelism and integrates multiple dynamics models, customizable sensor stacks\n(IMU, depth camera, and LiDAR), and diverse flight tasks within a unified,\nGPU-native training interface. By fully parallelizing both physics and\nrendering on the GPU, DiffAero eliminates CPU-GPU data transfer bottlenecks and\ndelivers orders-of-magnitude improvements in simulation throughput. In contrast\nto existing simulators, DiffAero not only provides high-performance simulation\nbut also serves as a research platform for exploring differentiable and hybrid\nlearning algorithms. Extensive benchmarks and real-world flight experiments\ndemonstrate that DiffAero and hybrid learning algorithms combined can learn\nrobust flight policies in hours on consumer-grade hardware. The code is\navailable at https://github.com/flyingbitac/diffaero.", "AI": {"tldr": "DiffAero is a GPU-accelerated, fully differentiable quadrotor simulator that enables efficient policy learning through environment- and agent-level parallelism, multiple dynamics models, and sensor stacks, delivering ultra-high throughput and facilitating hybrid learning on consumer hardware.", "motivation": "Need for scalable, differentiable simulation to accelerate quadrotor policy learning; existing CPU-bound simulators create bottlenecks and hinder differentiable/hybrid methods.", "method": "Develop a GPU-native simulator with fully parallel physics and rendering, support for environment- and agent-level parallelism, multiple dynamics models, customizable sensor stacks (IMU, depth, LiDAR), and a unified training interface; optimized for differentiable and hybrid learning.", "result": "Achieves orders-of-magnitude higher simulation throughput; real-world flight experiments and benchmarks show robust policies can be learned in hours on consumer hardware; public code release.", "conclusion": "DiffAero serves as both a high-performance simulator and a research platform for differentiable and hybrid learning in quadrotor control, enabling rapid experimentation and deployment."}}
{"id": "2509.09808", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09808", "abs": "https://arxiv.org/abs/2509.09808", "authors": ["Judith Massmann", "Alexander Lichtenstein", "Francisco M. L\u00f3pez"], "title": "Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test", "comment": "Accepted at IEEE ICDL 2025. 6 pages, 7 figures, 2 tables", "summary": "Numerous visual impairments can be detected in red-eye reflex images from\nyoung children. The so-called Bruckner test is traditionally performed by\nophthalmologists in clinical settings. Thanks to the recent technological\nadvances in smartphones and artificial intelligence, it is now possible to\nrecreate the Bruckner test using a mobile device. In this paper, we present a\nfirst study conducted during the development of KidsVisionCheck, a free\napplication that can perform vision screening with a mobile device using\nred-eye reflex images. The underlying model relies on deep neural networks\ntrained on children's pupil images collected and labeled by an ophthalmologist.\nWith an accuracy of 90% on unseen test data, our model provides highly reliable\nperformance without the necessity of specialist equipment. Furthermore, we can\nidentify the optimal conditions for data collection, which can in turn be used\nto provide immediate feedback to the users. In summary, this work marks a first\nstep toward accessible pediatric vision screenings and early intervention for\nvision abnormalities worldwide.", "AI": {"tldr": "Smartphone-based pediatric vision screening using red-eye reflex (Bruckner) test via deep learning, achieving 90% accuracy on unseen data and enabling accessible early screening.", "motivation": "Addresses lack of accessible pediatric vision screening and need for early detection; traditional Bruckner test requires specialists and equipment; smartphones/AI offer scalable mobile screening.", "method": "Train a deep neural network on pediatric pupil images collected/labeled by ophthalmologists, implemented in the KidsVisionCheck app to perform Bruckner-like screening on mobile devices; identify optimal data collection conditions for user feedback.", "result": "90% accuracy on unseen test data; reliable performance without specialist equipment; determination of optimal data collection conditions for immediate user feedback.", "conclusion": "Marks a first step toward accessible pediatric vision screening and early intervention globally; potential to reach underserved populations via mobile technology."}}
{"id": "2509.09864", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09864", "abs": "https://arxiv.org/abs/2509.09864", "authors": ["Jenny Y. Huang", "Mehul Damani", "Yousef El-Kurdi", "Ramon Astudillo", "Wei Sun"], "title": "Latency and Token-Aware Test-Time Compute", "comment": null, "summary": "Inference-time scaling has emerged as a powerful way to improve large\nlanguage model (LLM) performance by generating multiple candidate responses and\nselecting among them. However, existing work on dynamic allocation for\ntest-time compute typically considers only parallel generation methods such as\nbest-of-N, overlooking incremental decoding methods like beam search, and has\nlargely ignored latency, focusing only on token usage. We formulate\ninference-time scaling as a problem of dynamic compute allocation and method\nselection, where the system must decide which strategy to apply and how much\ncompute to allocate on a per-query basis. Our framework explicitly incorporates\nboth token cost and wall-clock latency, the latter being critical for user\nexperience and particularly for agentic workflows where models must issue\nmultiple queries efficiently. Experiments on reasoning benchmarks show that our\napproach consistently outperforms static strategies, achieving favorable\naccuracy-cost trade-offs while remaining practical for deployment.", "AI": {"tldr": "Inference-time dynamic compute allocation combines strategy selection and compute budgeting to improve LLM inference, balancing token cost and wall-clock latency.", "motivation": "Address latency and compute efficiency in LLM inference, moving beyond static or purely token-based policies to optimize user experience in latency-sensitive, multi-query workflows.", "method": "Proposes a framework that selects among generation strategies (including incremental decoding like beam search) and allocates per-query compute, explicitly modeling both token usage and wall-clock latency to optimize accuracy-cost-latency trade-offs.", "result": "Empirical results on reasoning benchmarks show the dynamic allocation framework outperforms static strategies, delivering better accuracy-cost-latency trade-offs and practical deployability.", "conclusion": "Dynamic per-query compute allocation with strategy selection can improve inference-time performance and is viable for real-world deployment."}}
{"id": "2509.10018", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10018", "abs": "https://arxiv.org/abs/2509.10018", "authors": ["Hailong Yang", "Renhuo Zhao", "Guanjin Wang", "Zhaohong Deng"], "title": "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method", "comment": null, "summary": "With the rapid advancement of Large Language Model (LLM), LLM-based agents\nexhibit exceptional abilities in understanding and generating natural language,\nfacilitating human-like collaboration and information transmission in LLM-based\nMulti-Agent System (MAS). High-performance LLMs are often hosted on remote\nservers in public spaces. When tasks involve privacy data, MAS cannot securely\nutilize these LLMs without implementing privacy-preserving mechanisms. To\naddress this challenge, we propose a General Anonymizing Multi-Agent system\n(GAMA), which divides the agents' workspace into private and public spaces and\nprotects privacy through the anonymizing mechanism. In the private space,\nagents handle sensitive data, while in the public space, only anonymized data\nis utilized. GAMA incorporates two key modules to mitigate semantic loss caused\nby anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and\nDisproof-based Logic Enhancement (DLE). We evaluate GAMA on two public\nquestion-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The\nresults demonstrate that GAMA has superior performance compared to the\nstate-of-the-art models. To further assess its privacy-preserving capabilities,\nwe designed two new datasets: Knowledge Privacy Preservation and Logic Privacy\nPreservation. The final results highlight GAMA's exceptional effectiveness in\nboth task processing and privacy preservation.", "AI": {"tldr": "GAMA is a privacy-preserving multi-agent system that anonymizes data for LLM-based MAS, using Domain-Rule-based Knowledge Enhancement (DRKE) and Disproof-based Logic Enhancement (DLE) to mitigate semantic loss, and shows strong task performance and privacy preservation on QA puzzles.", "motivation": "LLMs in remote/public servers pose privacy risks for tasks involving sensitive data in multi-agent settings. A privacy-preserving MAS is needed to leverage LLM capabilities without exposing private information.", "method": "Design a General Anonymizing Multi-Agent system (GAMA) that splits each agent workspace into a private space (sensitive data) and a public space (anonymized data). Introduce two modules, DRKE and DLE, to reduce semantic loss caused by anonymization. Evaluate on two public QA datasets (Trivia Creative Writing; Logic Grid Puzzle) and two privacy-preservation datasets (Knowledge Privacy Preservation; Logic Privacy Preservation). Compare against state-of-the-art models.", "result": "GAMA achieves superior performance to state-of-the-art models on the evaluated public datasets. Privacy-preservation datasets indicate effective privacy protection while maintaining task performance.", "conclusion": "GAMA effectively balances task performance and privacy in LLM-based MAS through workspace anonymization and the DRKE/DLE modules; demonstrates strong promise for privacy-sensitive applications, with potential for broader extension."}}
{"id": "2509.10305", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10305", "abs": "https://arxiv.org/abs/2509.10305", "authors": ["Yutong Shen", "Ruizhe Xia", "Bokai Yan", "Shunqi zhang", "Pengrui Xiang", "Sicheng He", "Yixin Xu"], "title": "GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning", "comment": "6 pages, 5 figures", "summary": "In dynamic and uncertain environments, robotic path planning demands accurate\nspatiotemporal environment understanding combined with robust decision-making\nunder partial observability. However, current deep reinforcement learning-based\npath planning methods face two fundamental limitations: (1) insufficient\nmodeling of multi-scale temporal dependencies, resulting in suboptimal\nadaptability in dynamic scenarios, and (2) inefficient exploration-exploitation\nbalance, leading to degraded path quality. To address these challenges, we\npropose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic Path\nPlanning. The framework comprises two key modules: (i) the Spatiotemporal\nPerception module, which hierarchically extracts multi-granularity spatial\nfeatures and multi-scale temporal dependencies ranging from instantaneous to\nextended time horizons, thereby improving perception accuracy in dynamic\nenvironments; and (ii) the Adaptive Policy Optimization module, which balances\nexploration and exploitation during training while optimizing for smoothness\nand collision probability through constrained policy updates. Experiments in\ndynamic environments demonstrate that GundamQ achieves a 15.3\\% improvement in\nsuccess rate and a 21.7\\% increase in overall path quality, significantly\noutperforming existing state-of-the-art methods.", "AI": {"tldr": "GundamQ is a multi-scale spatiotemporal Q-network for robotic path planning that combines hierarchical spatiotemporal perception with adaptive policy optimization to improve success rate and path quality in dynamic, partially observable environments.", "motivation": "Robotic path planning in dynamic and uncertain settings requires accurate spatiotemporal understanding and robust decision-making under partial observability, but current DRL-based methods struggle with modeling multi-scale temporal dependencies and balancing exploration-exploitation.", "method": "Introduce two modules: (i) Spatiotemporal Perception that hierarchically extracts multi-granularity spatial features and multi-scale temporal dependencies (instantaneous to long horizons); (ii) Adaptive Policy Optimization that balances exploration and exploitation during training while optimizing for smoothness and collision probability via constrained policy updates; implemented within a Q-network framework (GundamQ).", "result": "Experimental results show 15.3% higher success rate and 21.7% better overall path quality compared with state-of-the-art methods in dynamic environments.", "conclusion": "GundamQ effectively addresses the core limitations by enabling multi-scale temporal modeling and adaptive policy updates, yielding improved robustness and path quality in dynamic, uncertain environments."}}
{"id": "2509.09828", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09828", "abs": "https://arxiv.org/abs/2509.09828", "authors": ["Tim Broedermannn", "Christos Sakaridis", "Luigi Piccinelli", "Wim Abbeloos", "Luc Van Gool"], "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception", "comment": "Code and models will be available at\n  https://github.com/timbroed/DGFusion", "summary": "Robust semantic perception for autonomous vehicles relies on effectively\ncombining multiple sensors with complementary strengths and weaknesses.\nState-of-the-art sensor fusion approaches to semantic perception often treat\nsensor data uniformly across the spatial extent of the input, which hinders\nperformance when faced with challenging conditions. By contrast, we propose a\nnovel depth-guided multimodal fusion method that upgrades condition-aware\nfusion by integrating depth information. Our network, DGFusion, poses\nmultimodal segmentation as a multi-task problem, utilizing the lidar\nmeasurements, which are typically available in outdoor sensor suites, both as\none of the model's inputs and as ground truth for learning depth. Our\ncorresponding auxiliary depth head helps to learn depth-aware features, which\nare encoded into spatially varying local depth tokens that condition our\nattentive cross-modal fusion. Together with a global condition token, these\nlocal depth tokens dynamically adapt sensor fusion to the spatially varying\nreliability of each sensor across the scene, which largely depends on depth. In\naddition, we propose a robust loss for our depth, which is essential for\nlearning from lidar inputs that are typically sparse and noisy in adverse\nconditions. Our method achieves state-of-the-art panoptic and semantic\nsegmentation performance on the challenging MUSES and DELIVER datasets. Code\nand models will be available at https://github.com/timbroed/DGFusion", "AI": {"tldr": "DGFusion: depth-guided multimodal fusion for robust semantic perception; introduces depth tokens and auxiliary depth head; achieves SOTA on MUSES/DELIVER; code release.", "motivation": "Current sensor fusion methods treat data uniformly spatially, ignoring depth-dependent reliability; depth information from lidar can guide fusion to handle adverse conditions.", "method": "A multi-task network using lidar as input and depth supervision; auxiliary depth head learns depth-aware features; spatially varying local depth tokens condition cross-modal fusion with a global token; robust depth loss to handle sparse/noisy lidar.", "result": "State-of-the-art panoptic and semantic segmentation on MUSES and DELIVER datasets; robust performance under challenging conditions; code available.", "conclusion": "Depth-guided, spatially-aware fusion improves robustness of semantic perception; the depth tokens enable adaptive fusion; potential for broader multimodal sensing in autonomous driving."}}
{"id": "2509.09899", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09899", "abs": "https://arxiv.org/abs/2509.09899", "authors": ["Christopher Eldred", "Fran\u00e7ois Gay-Balmaz", "Vakhtang Putkaradze"], "title": "Variational Neural Networks for Observable Thermodynamics (V-NOTS)", "comment": "26 pages, 6 figures", "summary": "Much attention has recently been devoted to data-based computing of evolution\nof physical systems. In such approaches, information about data points from\npast trajectories in phase space is used to reconstruct the equations of motion\nand to predict future solutions that have not been observed before. However, in\nmany cases, the available data does not correspond to the variables that define\nthe system's phase space. We focus our attention on the important example of\ndissipative dynamical systems. In that case, the phase space consists of\ncoordinates, momenta and entropies; however, the momenta and entropies cannot,\nin general, be observed directly. To address this difficulty, we develop an\nefficient data-based computing framework based exclusively on observable\nvariables, by constructing a novel approach based on the \\emph{thermodynamic\nLagrangian}, and constructing neural networks that respect the thermodynamics\nand guarantees the non-decreasing entropy evolution. We show that our network\ncan provide an efficient description of phase space evolution based on a\nlimited number of data points and a relatively small number of parameters in\nthe system.", "AI": {"tldr": "A data-driven, thermodynamically consistent neural framework learns dissipative dynamics from partial observations using a thermodynamic Lagrangian to guarantee non-decreasing entropy.", "motivation": "Observing only partial variables in dissipative systems complicates learning dynamics; existing data-driven methods may violate thermodynamics. A framework that enforces thermodynamic laws while learning from limited data is needed.", "method": "Introduce a neural network model grounded in a thermodynamic Lagrangian defined on observable variables; enforce entropy production non-negativity and thermodynamic consistency; train on data from trajectories with limited variables.", "result": "The approach yields accurate phase-space evolution with few parameters and limited data, while ensuring non-decreasing entropy and thermodynamic consistency.", "conclusion": "Thermodynamic Lagrangian-based learning offers a principled, data-efficient path to infer dissipative dynamics from partial observations, with broad applicability and potential for more reliable extrapolation."}}
{"id": "2509.10054", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10054", "abs": "https://arxiv.org/abs/2509.10054", "authors": ["Hailong Yang", "Mingxian Gu", "Jianqi Wang", "Guanjin Wang", "Zhaohong Deng"], "title": "XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans\nwith complex, real-world tasks. However, MAS still face challenges in effective\ntask planning when handling highly complex tasks with uncertainty, often\nresulting in misleading or incorrect outputs that hinder task execution. To\naddress this, we propose XAgents, a unified multi-agent cooperative framework\nbuilt on a multipolar task processing graph and IF-THEN rules. XAgents uses the\nmultipolar task processing graph to enable dynamic task planning and handle\ntask uncertainty. During subtask processing, it integrates domain-specific\nIF-THEN rules to constrain agent behaviors, while global rules enhance\ninter-agent collaboration. We evaluate the performance of XAgents across three\ndistinct datasets, demonstrating that it consistently surpasses\nstate-of-the-art single-agent and multi-agent approaches in both\nknowledge-typed and logic-typed question-answering tasks. The codes for XAgents\nare available at: https://github.com/AGI-FHBC/XAgents.", "AI": {"tldr": "XAgents is a unified multi-agent framework that uses a multipolar task processing graph and IF-THEN rules to improve dynamic task planning and collaboration under uncertainty, outperforming baselines on QA tasks.", "motivation": "MAS-based LLM assistants often struggle with complex, uncertain tasks, leading to incorrect outputs; a structured planning graph plus rule constraints can improve reliability and collaboration.", "method": "Introduce multipolar task processing graph for dynamic planning; integrate domain-specific IF-THEN rules at subtask level to constrain agent behavior; employ global rules to enhance inter-agent collaboration; evaluate on three datasets across knowledge-typed and logic-typed QA tasks; provide code.", "result": "XAgents consistently surpasses state-of-the-art single-agent and multi-agent approaches on three datasets for both knowledge-typed and logic-typed QA tasks.", "conclusion": "A unified cooperative MAS framework with a structured task-processing graph and rule-based constraints can effectively manage task uncertainty and enhance QA performance."}}
{"id": "2509.10317", "categories": ["cs.RO", "cs.LG", "93C85", "I.2.9; I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.10317", "abs": "https://arxiv.org/abs/2509.10317", "authors": ["Elizaveta D. Moskovskaya", "Anton D. Moscowsky"], "title": "Robot guide with multi-agent control and automatic scenario generation with LLM", "comment": "14 pages, 5 figures, 2 tables, 1 demo-video and repository link", "summary": "The work describes the development of a hybrid control architecture for an\nanthropomorphic tour guide robot, combining a multi-agent resource management\nsystem with automatic behavior scenario generation based on large language\nmodels. The proposed approach aims to overcome the limitations of traditional\nsystems, which rely on manual tuning of behavior scenarios. These limitations\ninclude manual configuration, low flexibility, and lack of naturalness in robot\nbehavior. The process of preparing tour scenarios is implemented through a\ntwo-stage generation: first, a stylized narrative is created, then non-verbal\naction tags are integrated into the text. The multi-agent system ensures\ncoordination and conflict resolution during the execution of parallel actions,\nas well as maintaining default behavior after the completion of main\noperations, contributing to more natural robot behavior. The results obtained\nfrom the trial demonstrate the potential of the proposed approach for\nautomating and scaling social robot control systems.", "AI": {"tldr": "Proposes a hybrid control architecture for social robots using a multi-agent resource management system plus LLM-driven automatic behavior scenario generation to automate and scale naturalistic tour-guide behavior; a two-stage narrative-to-action pipeline with coordination and default-behavior maintenance; pilot results show promise.", "motivation": "Address manual tuning/configuration, low flexibility, and lack of naturalness in traditional social-robot control; seek scalable, automated generation of behavior scenarios.", "method": "Two-stage generation: (1) create a stylized narrative via large language models, (2) integrate non-verbal action tags into the text; a multi-agent system coordinates parallel actions, resolves conflicts, and restores default behavior after main operations.", "result": "Trial results demonstrate potential for automating and scaling social robot control systems.", "conclusion": "The approach shows promise for more natural, scalable social robot control and could generalize beyond tour-guide scenarios; future work should quantify naturalness and performance and explore broader deployment."}}
{"id": "2509.09841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09841", "abs": "https://arxiv.org/abs/2509.09841", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework", "comment": null, "summary": "Rosacea, which is a chronic inflammatory skin condition that manifests with\nfacial redness, papules, and visible blood vessels, often requirs precise and\nearly detection for significantly improving treatment effectiveness. This paper\npresents new patch-based automatic rosacea detection strategies using the\nResNet-18 deep learning framework. The contributions of the proposed strategies\ncome from the following aspects. First, various image pateches are extracted\nfrom the facial images of people in different sizes, shapes, and locations.\nSecond, a number of investigation studies are carried out to evaluate how the\nlocalized visual information influences the deep learing model performance.\nThird, thorough experiments are implemented to reveal that several patch-based\nautomatic rosacea detection strategies achieve competitive or superior accuracy\nand sensitivity than the full-image based methods. And finally, the proposed\npatch-based strategies, which use only localized patches, inherently preserve\npatient privacy by excluding any identifiable facial features from the data.\nThe experimental results indicate that the proposed patch-based strategies\nguide the deep learning model to focus on clinically relevant regions, enhance\nrobustness and interpretability, and protect patient privacy. As a result, the\nproposed strategies offer practical insights for improving automated\ndermatological diagnostics.", "AI": {"tldr": "Patch-based Rosacea detection with ResNet-18 using multi-scale patches; achieves competitive accuracy with privacy preservation.", "motivation": "Rosacea requires early, accurate detection; full-image deep learning risks using non-diagnostic regions and raises privacy concerns; patch-based localized analysis can enhance focus, robustness, and privacy.", "method": "Extract patches of varying sizes, shapes, and locations from facial images; train ResNet-18 variants on patch sets; compare against full-image baselines; assess impact of localization on performance, robustness, and interpretability; emphasize privacy by excluding identifiable facial features.", "result": "Patch-based strategies achieved competitive or superior accuracy and sensitivity relative to full-image methods; improved robustness and interpretability; data privacy preserved due to exclusion of full facial information.", "conclusion": "Patch-based, locality-focused strategies are practical for automated dermatological diagnostics, guiding model attention to clinically relevant regions and enabling privacy-preserving deployment."}}
{"id": "2509.09926", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09926", "abs": "https://arxiv.org/abs/2509.09926", "authors": ["Jiahao Chen", "Zhiyuan Huang", "Yurou Liu", "Bing Su"], "title": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios", "comment": null, "summary": "Long-tailed learning has garnered increasing attention due to its wide\napplicability in real-world scenarios. Among existing approaches, Long-Tailed\nSemi-Supervised Learning (LTSSL) has emerged as an effective solution by\nincorporating a large amount of unlabeled data into the imbalanced labeled\ndataset. However, most prior LTSSL methods are designed to train models from\nscratch, which often leads to issues such as overconfidence and low-quality\npseudo-labels. To address these challenges, we extend LTSSL into the foundation\nmodel fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed\nsemi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate\nthat fine-tuned foundation models can generate more reliable pseudolabels,\nthereby benefiting imbalanced learning. Furthermore, we explore a more\npractical setting by investigating semi-supervised learning under open-world\nconditions, where the unlabeled data may include out-of-distribution (OOD)\nsamples. To handle this problem, we propose LoFT-OW (LoFT under Open-World\nscenarios) to improve the discriminative ability. Experimental results on\nmultiple benchmarks demonstrate that our method achieves superior performance\ncompared to previous approaches, even when utilizing only 1\\% of the unlabeled\ndata compared with previous works.", "AI": {"tldr": "LoFT introduces a parameter-efficient fine-tuning framework for long-tailed semi-supervised learning, using foundation models to generate more reliable pseudo-labels; LoFT-OW extends this to open-world data, achieving superior results with minimal unlabeled data.", "motivation": "Long-tailed data with unlabeled samples suffer from overconfident, low-quality pseudo-labels when trained from scratch. Leveraging foundation models via efficient fine-tuning can produce better pseudo-labels and robustness, especially under open-world conditions.", "method": "Fine-tune foundation models with parameter-efficient adapters (e.g., LoRA) within LTSSL, enabling high-quality pseudo-labels for unlabeled data. Introduce LoFT-OW to handle OOD samples and improve discriminative ability in open-world semi-supervised learning.", "result": "Empirical results on multiple benchmarks show state-of-the-art performance, outperforming prior LTSSL methods, and achieving strong results even when using as little as 1% of unlabeled data relative to prior works.", "conclusion": "Fine-tuning foundation models with parameter-efficient methods offers meaningful gains for LTSSL; the LoFT framework and its open-world variant provide robust, data-efficient, and scalable solutions for long-tailed semi-supervised learning."}}
{"id": "2509.10104", "categories": ["cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.10104", "abs": "https://arxiv.org/abs/2509.10104", "authors": ["Sofia Vei", "Paolo Giudici", "Pavlos Sermpezis", "Athena Vakali", "Adelaide Emma Bernardelli"], "title": "AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework", "comment": null, "summary": "The absolute dominance of Artificial Intelligence (AI) introduces\nunprecedented societal harms and risks. Existing AI risk assessment models\nfocus on internal compliance, often neglecting diverse stakeholder perspectives\nand real-world consequences. We propose a paradigm shift to a human-centric,\nharm-severity adaptive approach grounded in empirical incident data. We present\nAI Harmonics, which includes a novel AI harm assessment metric (AIH) that\nleverages ordinal severity data to capture relative impact without requiring\nprecise numerical estimates. AI Harmonics combines a robust, generalized\nmethodology with a data-driven, stakeholder-aware framework for exploring and\nprioritizing AI harms. Experiments on annotated incident data confirm that\npolitical and physical harms exhibit the highest concentration and thus warrant\nurgent mitigation: political harms erode public trust, while physical harms\npose serious, even life-threatening risks, underscoring the real-world\nrelevance of our approach. Finally, we demonstrate that AI Harmonics\nconsistently identifies uneven harm distributions, enabling policymakers and\norganizations to target their mitigation efforts effectively.", "AI": {"tldr": "Introduces AI Harmonics and AIH metric, a human-centered, severity-adaptive risk assessment using ordinal incident data to identify and prioritize AI harms, notably political and physical harms.", "motivation": "Current AI risk models focus on internal compliance and neglect stakeholder perspectives and real-world consequences; there is a need for a data-driven, stakeholder-aware framework that can identify, compare, and prioritize harms.", "method": "Define the AI harm (AIH) metric based on ordinal severity data; develop a generalized methodology coupling with a data-driven, stakeholder-aware framework; analyze annotated incident data to identify harm concentrations and distribution; prioritize mitigation based on severity and prevalence.", "result": "Experiments show political and physical harms exhibit the highest concentration and thus warrant urgent mitigation; AI Harmonics consistently detects uneven harm distributions, enabling targeted mitigation by policymakers and organizations.", "conclusion": "The approach represents a paradigm shift toward human-centric, severity-aware AI risk assessment, with real-world relevance and potential to guide resource allocation and policy decisions; highlights the importance of considering stakeholder perspectives and real-world impact in AI governance."}}
{"id": "2509.10349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10349", "abs": "https://arxiv.org/abs/2509.10349", "authors": ["Weiyan Lu", "Huizhe Li", "Yuhao Fang", "Zhexuan Zhou", "Junda Wu", "Yude Li", "Youmin Gong", "Jie Mei"], "title": "Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) with suspended payloads offer significant\nadvantages for aerial transportation in complex and cluttered environments.\nHowever, existing systems face critical limitations, including unreliable\nperception of the cable-payload dynamics, inefficient planning in large-scale\nenvironments, and the inability to guarantee whole-body safety under cable\nbending and external disturbances. This paper presents Acetrans, an Autonomous,\nCorridor-based, and Efficient UAV suspended transport system that addresses\nthese challenges through a unified perception, planning, and control framework.\nA LiDAR-IMU fusion module is proposed to jointly estimate both payload pose and\ncable shape under taut and bent modes, enabling robust whole-body state\nestimation and real-time filtering of cable point clouds. To enhance planning\nscalability, we introduce the Multi-size-Aware Configuration-space Iterative\nRegional Inflation (MACIRI) algorithm, which generates safe flight corridors\nwhile accounting for varying UAV and payload geometries. A spatio-temporal,\ncorridor-constrained trajectory optimization scheme is then developed to ensure\ndynamically feasible and collision-free trajectories. Finally, a nonlinear\nmodel predictive controller (NMPC) augmented with cable-bending constraints\nprovides robust whole-body safety during execution. Simulation and experimental\nresults validate the effectiveness of Acetrans, demonstrating substantial\nimprovements in perception accuracy, planning efficiency, and control safety\ncompared to state-of-the-art methods.", "AI": {"tldr": "Acetrans delivers an integrated perception-planning-control framework for UAVs with suspended payloads, achieving robust state estimation, scalable trajectory planning, and safe execution under cable bending.", "motivation": "Current UAV-suspended-payload systems struggle with unreliable perception of cable-payload dynamics, inefficient planning in large environments, and inability to guarantee whole-body safety under cable bending and disturbances.", "method": "1) LiDAR-IMU fusion to jointly estimate payload pose and cable shape for taut and bent modes. 2) MACIRI: Multi-size-Aware Configuration-space Iterative Regional Inflation for safe flight corridors considering varying UAV and payload geometries. 3) Spatio-temporal corridor-constrained trajectory optimization for feasible, collision-free paths. 4) NMPC augmented with cable-bending constraints for robust whole-body safety.", "result": "Simulation and experiments show substantial improvements in perception accuracy, planning efficiency, and control safety compared with state-of-the-art methods.", "conclusion": "A unified perception-planning-control framework (Acetrans) effectively enables safe, scalable suspended UAV transport in cluttered environments."}}
{"id": "2509.09844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09844", "abs": "https://arxiv.org/abs/2509.09844", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection", "comment": null, "summary": "Rosacea is a common but underdiagnosed inflammatory skin condition that\nprimarily affects the central face and presents with subtle redness, pustules,\nand visible blood vessels. Automated detection remains challenging due to the\ndiffuse nature of symptoms, the scarcity of labeled datasets, and privacy\nconcerns associated with using identifiable facial images. A novel\nprivacy-preserving automated rosacea detection method inspired by clinical\npriors and trained entirely on synthetic data is presented in this paper.\nSpecifically, the proposed method, which leverages the observation that rosacea\nmanifests predominantly through central facial erythema, first constructs a\nfixed redness-informed mask by selecting regions with consistently high red\nchannel intensity across facial images. The mask thus is able to focus on\ndiagnostically relevant areas such as the cheeks, nose, and forehead and\nexclude identity-revealing features. Second, the ResNet-18 deep learning\nmethod, which is trained on the masked synthetic images, achieves superior\nperformance over the full-face baselines with notable gains in terms of\naccuracy, recall and F1 score when evaluated using the real-world test data.\nThe experimental results demonstrate that the synthetic data and clinical\npriors can jointly enable accurate and ethical dermatological AI systems,\nespecially for privacy sensitive applications in telemedicine and large-scale\nscreening.", "AI": {"tldr": "Privacy-preserving rosacea detection using synthetic data and a redness-based mask with ResNet-18; avoids identifiable facial data while maintaining performance.", "motivation": "Address underdiagnosis of rosacea, data scarcity, and privacy concerns by leveraging clinical priors and synthetic data to enable ethical, scalable dermatology AI.", "method": "Create a fixed redness-informed mask selecting high-redness regions (cheeks, nose, forehead). Train ResNet-18 on masked synthetic images. Evaluate on real-world data and compare to full-face baselines.", "result": "Achieves higher accuracy, recall, and F1 on real test data than full-face baselines; demonstrates effective use of synthetic data and clinical priors.", "conclusion": "Synthetic data combined with clinical priors can enable accurate, privacy-preserving dermatological AI for telemedicine and large-scale screening."}}
{"id": "2509.09933", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09933", "abs": "https://arxiv.org/abs/2509.09933", "authors": ["Shintaro Nakamura", "Yuko Kuroki", "Wei Chen"], "title": "Multi-Play Combinatorial Semi-Bandit Problem", "comment": null, "summary": "In the combinatorial semi-bandit (CSB) problem, a player selects an action\nfrom a combinatorial action set and observes feedback from the base arms\nincluded in the action. While CSB is widely applicable to combinatorial\noptimization problems, its restriction to binary decision spaces excludes\nimportant cases involving non-negative integer flows or allocations, such as\nthe optimal transport and knapsack problems.To overcome this limitation, we\npropose the multi-play combinatorial semi-bandit (MP-CSB), where a player can\nselect a non-negative integer action and observe multiple feedbacks from a\nsingle arm in each round. We propose two algorithms for the MP-CSB. One is a\nThompson-sampling-based algorithm that is computationally feasible even when\nthe action space is exponentially large with respect to the number of arms, and\nattains $O(\\log T)$ distribution-dependent regret in the stochastic regime,\nwhere $T$ is the time horizon. The other is a best-of-both-worlds algorithm,\nwhich achieves $O(\\log T)$ variance-dependent regret in the stochastic regime\nand the worst-case $\\tilde{\\mathcal{O}}\\left( \\sqrt{T} \\right)$ regret in the\nadversarial regime. Moreover, its regret in adversarial one is data-dependent,\nadapting to the cumulative loss of the optimal action, the total quadratic\nvariation, and the path-length of the loss sequence. Finally, we numerically\nshow that the proposed algorithms outperform existing methods in the CSB\nliterature.", "AI": {"tldr": "MP-CSB generalizes CSB to non-negative integer actions, enabling flows/allocations; introduces two algorithms (Thompson sampling-based and best-of-both-worlds) with strong regret guarantees and empirical gains.", "motivation": "CSB's binary-action limitation excludes important combinatorial problems like optimal transport and knapsack; enabling integer actions and multiple feedbacks per action broadens applicability.", "method": "Define MP-CSB; propose a Thompson-sampling-based algorithm that remains computationally feasible even with exponentially large action spaces; propose a best-of-both-worlds algorithm with logarithmic regret in stochastic settings and worst-case ~sqrt(T) regret in adversarial settings; provide data-dependent regret bounds in the adversarial case based on the cumulative loss of the optimal action, total quadratic variation, and path-length of the loss sequence; report numerical experiments.", "result": "Regret bounds: TS-based algorithm achieves O(log T) distribution-dependent regret in the stochastic regime; best-of-both-worlds achieves O(log T) variance-dependent regret in stochastic and ~\u221aT regret in adversarial; in the adversarial case, regret is data-dependent (depends on cumulative loss of the optimal action, quadratic variation, and path-length); empirical results show the proposed algorithms outperform existing CSB methods.", "conclusion": "MP-CSB broadens the applicability of combinatorial semi-bandits to non-negative integer actions and flows; the proposed algorithms are computationally feasible and come with strong theoretical guarantees, supported by empirical improvements over existing CSB methods."}}
{"id": "2509.10147", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10147", "abs": "https://arxiv.org/abs/2509.10147", "authors": ["Nenad Tomasev", "Matija Franklin", "Joel Z. Leibo", "Julian Jacobs", "William A. Cunningham", "Iason Gabriel", "Simon Osindero"], "title": "Virtual Agent Economies", "comment": null, "summary": "The rapid adoption of autonomous AI agents is giving rise to a new economic\nlayer where agents transact and coordinate at scales and speeds beyond direct\nhuman oversight. We propose the \"sandbox economy\" as a framework for analyzing\nthis emergent system, characterizing it along two key dimensions: its origins\n(emergent vs. intentional) and its degree of separateness from the established\nhuman economy (permeable vs. impermeable). Our current trajectory points toward\na spontaneous emergence of a vast and highly permeable AI agent economy,\npresenting us with opportunities for an unprecedented degree of coordination as\nwell as significant challenges, including systemic economic risk and\nexacerbated inequality. Here we discuss a number of possible design choices\nthat may lead to safely steerable AI agent markets. In particular, we consider\nauction mechanisms for fair resource allocation and preference resolution, the\ndesign of AI \"mission economies\" to coordinate around achieving collective\ngoals, and socio-technical infrastructure needed to ensure trust, safety, and\naccountability. By doing this, we argue for the proactive design of steerable\nagent markets to ensure the coming technological shift aligns with humanity's\nlong-term collective flourishing.", "AI": {"tldr": "Proposes a 'sandbox economy' framework for AI agent markets, analyzes origins and permeability, warns of emergent, highly permeable AI economies, and outlines design options (auctions, mission economies, trust/safety infra) to steer toward beneficial outcomes and mitigate risks.", "motivation": "As autonomous AI agents transact and coordinate at scale, there is potential for rapid coordination but also systemic risk and inequality; we need governance to align this shift with humanity's long-term flourishing.", "method": "Conceptual framework with a two-axis taxonomy (origins: emergent vs intentional; permeability: permeable vs impermeable) and discussion of design choices and infrastructural elements to steer markets.", "result": "Forecasts spontaneous emergence of a large, permeable AI agent economy; outlines potential benefits (coordination) and challenges (economic risk, inequality); presents design directions for steerable markets.", "conclusion": "Urges proactive design of steerable agent markets to ensure alignment with humanity's long-term collective flourishing."}}
{"id": "2509.10405", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10405", "abs": "https://arxiv.org/abs/2509.10405", "authors": ["Nicholas Carlotti", "Mirko Nava", "Alessandro Giusti"], "title": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States", "comment": "accepted at CoRL 2025", "summary": "We introduce a model for monocular RGB relative pose estimation of a ground\nrobot that trains from scratch without pose labels nor prior knowledge about\nthe robot's shape or appearance. At training time, we assume: (i) a robot\nfitted with multiple LEDs, whose states are independent and known at each\nframe; (ii) knowledge of the approximate viewing direction of each LED; and\n(iii) availability of a calibration image with a known target distance, to\naddress the ambiguity of monocular depth estimation. Training data is collected\nby a pair of robots moving randomly without needing external infrastructure or\nhuman supervision. Our model trains on the task of predicting from an image the\nstate of each LED on the robot. In doing so, it learns to predict the position\nof the robot in the image, its distance, and its relative bearing. At inference\ntime, the state of the LEDs is unknown, can be arbitrary, and does not affect\nthe pose estimation performance. Quantitative experiments indicate that our\napproach: is competitive with SoA approaches that require supervision from pose\nlabels or a CAD model of the robot; generalizes to different domains; and\nhandles multi-robot pose estimation.", "AI": {"tldr": "A self-supervised monocular RGB pose estimation method for ground robots using LED state predictions, trained from scratch without pose labels or CAD models, with calibration to resolve depth ambiguity; shows competitive accuracy and generalizes to multi-robot scenarios.", "motivation": "Address monocular pose estimation without pose labels, CAD models, or prior knowledge of robot appearance, enabling self-supervised learning from unlabeled footage collected via robot interaction.", "method": "Train a CNN to predict the per-frame LED states (unknown at inference) using RGB images. The setup assumes multiple LEDs with independent, known states, an approximate viewing direction for each LED, and a calibration image with known target distance. Two robots move randomly to collect training data without external infra or supervision. The model learns robot position, distance, and bearing by mapping image cues to LED configurations; at test time LED states are unknown but do not affect pose estimates.", "result": "Experiments show the method is competitive with state-of-the-art approaches that rely on pose labels or CAD models, generalizes across domains, and scales to multi-robot pose estimation.", "conclusion": "Self-supervised learning leveraging LED-state prediction enables reliable monocular pose estimation from scratch, without external infrastructure, CAD models, or pose labels, with good generalization to new domains and scenarios."}}
{"id": "2509.09849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09849", "abs": "https://arxiv.org/abs/2509.09849", "authors": ["Chengyu Yang", "Chengjun Liu"], "title": "Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking", "comment": null, "summary": "To rigorously assess the effectiveness and necessity of individual components\nwithin the recently proposed ULW framework for laparoscopic image desmoking,\nthis paper presents a comprehensive ablation study. The ULW approach combines a\nU-Net based backbone with a compound loss function that comprises mean squared\nerror (MSE), structural similarity index (SSIM) loss, and perceptual loss. The\nframework also incorporates a differentiable, learnable Wiener filter module.\nIn this study, each component is systematically ablated to evaluate its\nspecific contribution to the overall performance of the whole framework. The\nanalysis includes: (1) removal of the learnable Wiener filter, (2) selective\nuse of individual loss terms from the composite loss function. All variants are\nbenchmarked on a publicly available paired laparoscopic images dataset using\nquantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative\nvisual comparisons.", "AI": {"tldr": "Systematic ablation study of the ULW laparoscopic image desmoking framework, evaluating the learnable Wiener filter and each component of the composite loss on paired image data using multiple quality metrics.", "motivation": "Determine the necessity and contribution of individual components (Wiener filter, MSE/SSIM/perceptual loss) to the overall performance and to guide design choices.", "method": "Perform ablations: (1) remove the learnable Wiener filter; (2) use only individual loss terms from the composite loss; benchmark all variants on a public paired laparoscopic dataset using SSIM, PSNR, MSE, CIEDE-2000 and qualitative comparisons.", "result": "Quantitative and qualitative results quantify each component's impact; findings reveal which components most influence structural fidelity, color accuracy, and perceptual quality, guiding future ULW design.", "conclusion": "Ablation results validate the necessity of the Wiener filter and the chosen loss terms, and provide actionable insights for optimizing ULW-like desmoking frameworks in laparoscopic imaging."}}
{"id": "2509.09936", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.09936", "abs": "https://arxiv.org/abs/2509.09936", "authors": ["Saarth Gaonkar", "Xiang Zheng", "Haocheng Xi", "Rishabh Tiwari", "Kurt Keutzer", "Dmitriy Morozov", "Michael W. Mahoney", "Amir Gholami"], "title": "SciML Agents: Write the Solver, Not the Solution", "comment": null, "summary": "Recent work in scientific machine learning aims to tackle scientific tasks\ndirectly by predicting target values with neural networks (e.g.,\nphysics-informed neural networks, neural ODEs, neural operators, etc.), but\nattaining high accuracy and robustness has been challenging. We explore an\nalternative view: use LLMs to write code that leverages decades of numerical\nalgorithms. This shifts the burden from learning a solution function to making\ndomain-aware numerical choices. We ask whether LLMs can act as SciML agents\nthat, given a natural-language ODE description, generate runnable code that is\nscientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),\nand enforcing stability checks. There is currently no benchmark to measure this\nkind of capability for scientific computing tasks. As such, we first introduce\ntwo new datasets: a diagnostic dataset of adversarial \"misleading\" problems;\nand a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set\ncontains problems whose superficial appearance suggests stiffness, and that\nrequire algebraic simplification to demonstrate non-stiffness; and the\nlarge-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-\nand closed-source LLM models along two axes: (i) unguided versus guided\nprompting with domain-specific knowledge; and (ii) off-the-shelf versus\nfine-tuned variants. Our evaluation measures both executability and numerical\nvalidity against reference solutions. We find that with sufficient context and\nguided prompts, newer instruction-following models achieve high accuracy on\nboth criteria. In many cases, recent open-source systems perform strongly\nwithout fine-tuning, while older or smaller models still benefit from\nfine-tuning. Overall, our preliminary results indicate that careful prompting\nand fine-tuning can yield a specialized LLM agent capable of reliably solving\nsimple ODE problems.", "AI": {"tldr": "New datasets show that instruction-following LLMs, when guided with domain knowledge and possibly fine-tuned, can generate runnable code to solve simple ODEs by making appropriate numerical choices (stiff vs non-stiff) and validating numerical correctness.", "motivation": "Tackle the gap in SciML where learning a solution function is hard; instead leverage LLMs to write domain-aware numerical code, enabling systems to select solvers and perform stability checks with minimal hand-crafted modeling.", "method": "Introduce two datasets: a diagnostic set of adversarial mislead problems that look stiff but are non-stiff after simplification; and a large-scale benchmark of 1,000 diverse ODE tasks spanning stiff and non-stiff regimes. Evaluate open- and closed-source LLMs under unguided vs guided prompting (domain knowledge) and off-the-shelf vs fine-tuned variants. Measure executability and numerical validity against reference solutions.", "result": "With sufficient context and guided prompts, newer instruction-following models achieve high accuracy on both executability and numerical validity. Recent open-source systems perform strongly without fine-tuning, while older or smaller models benefit from fine-tuning.", "conclusion": "Careful prompting and fine-tuning can yield a specialized LLM agent capable of reliably solving simple ODE problems."}}
{"id": "2509.10162", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10162", "abs": "https://arxiv.org/abs/2509.10162", "authors": ["Tamir Shazman", "Idan Lev-Yehudi", "Ron Benchetit", "Vadim Indelman"], "title": "Online Robust Planning under Model Uncertainty: A Sample-Based Approach", "comment": null, "summary": "Online planning in Markov Decision Processes (MDPs) enables agents to make\nsequential decisions by simulating future trajectories from the current state,\nmaking it well-suited for large-scale or dynamic environments. Sample-based\nmethods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely\nadopted for their ability to approximate optimal actions using a generative\nmodel. However, in practical settings, the generative model is often learned\nfrom limited data, introducing approximation errors that can degrade\nperformance or lead to unsafe behaviors. To address these challenges, Robust\nMDPs (RMDPs) offer a principled framework for planning under model uncertainty,\nyet existing approaches are typically computationally intensive and not suited\nfor real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the\nfirst online planning algorithm for RMDPs with finite-sample theoretical\nperformance guarantees. Unlike Sparse Sampling, which estimates the nominal\nvalue function, RSS computes a robust value function by leveraging the\nefficiency and theoretical properties of Sample Average Approximation (SAA),\nenabling tractable robust policy computation in online settings. RSS is\napplicable to infinite or continuous state spaces, and its sample and\ncomputational complexities are independent of the state space size. We provide\ntheoretical performance guarantees and empirically show that RSS outperforms\nstandard Sparse Sampling in environments with uncertain dynamics.", "AI": {"tldr": "Robust Sparse Sampling (RSS) for Robust MDPs: online planning with finite-sample guarantees using Sample Average Approximation to compute robust values, scalable to continuous/infinite state spaces, outperforming standard Sparse Sampling under dynamics uncertainty.", "motivation": "Mitigate performance degradation and unsafe behaviors due to model errors in learned generative models for online planning; current RMDP methods are computationally expensive for real-time use.", "method": "Introduce RSS; uses SAA to estimate robust value functions; calculates robust policies in online planning; complexity independent of state space; applicable to infinite/continuous state spaces; provides finite-sample guarantees.", "result": "Theoretical performance guarantees; empirical results show RSS outperforms Sparse Sampling in uncertain-dynamics environments.", "conclusion": "RSS is the first online planning algorithm for RMDPs with finite-sample guarantees, enabling real-time robust decision-making in large-scale or dynamic environments; demonstrates scalability and improved performance under model uncertainty."}}
{"id": "2509.10416", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10416", "abs": "https://arxiv.org/abs/2509.10416", "authors": ["Ze Fu", "Pinhao Song", "Yutong Hu", "Renaud Detry"], "title": "TASC: Task-Aware Shared Control for Teleoperated Manipulation", "comment": null, "summary": "We present TASC, a Task-Aware Shared Control framework for teleoperated\nmanipulation that infers task-level user intent and provides assistance\nthroughout the task. To support everyday tasks without predefined knowledge,\nTASC constructs an open-vocabulary interaction graph from visual input to\nrepresent functional object relationships, and infers user intent accordingly.\nA shared control policy then provides rotation assistance during both grasping\nand object interaction, guided by spatial constraints predicted by a\nvision-language model. Our method addresses two key challenges in\ngeneral-purpose, long-horizon shared control: (1) understanding and inferring\ntask-level user intent, and (2) generalizing assistance across diverse objects\nand tasks. Experiments in both simulation and the real world demonstrate that\nTASC improves task efficiency and reduces user input effort compared to prior\nmethods. To the best of our knowledge, this is the first shared control\nframework that supports everyday manipulation tasks with zero-shot\ngeneralization. The code that supports our experiments is publicly available at\nhttps://github.com/fitz0401/tasc.", "AI": {"tldr": "TASC is a task-aware shared-control framework for teleoperation that uses an open-vocabulary visual graph and vision-language-guided constraints to infer user intent and provide rotation-based assistance, enabling zero-shot generalization across everyday manipulation tasks.", "motivation": "General-purpose, long-horizon shared control faces two key challenges: understanding user task-level intent and generalizing assistance across diverse objects and tasks without predefined knowledge. The work aims to enable seamless everyday manipulation with minimal prior task-specific programming.", "method": "Builds an open-vocabulary interaction graph from visual input to represent functional object relationships and infer user intent. A shared-control policy provides rotation assistance during grasping and interaction, guided by spatial constraints predicted by a vision-language model.", "result": "In simulation and real-world experiments, TASC improves task efficiency and reduces user input effort compared with prior methods, and demonstrates zero-shot generalization across tasks. Code is publicly available.", "conclusion": "TASC is the first shared-control framework enabling everyday manipulation tasks with zero-shot generalization by combining open-vocabulary scene understanding, task-level intent inference, and vision-language-guided spatial constraints to drive rotation-based assistance."}}
{"id": "2509.09859", "categories": ["cs.CV", "cs.LG", "68W99"], "pdf": "https://arxiv.org/pdf/2509.09859", "abs": "https://arxiv.org/abs/2509.09859", "authors": ["Razvan Stefanescu", "Ethan Oh", "Ruben Vazquez", "Chris Mesterharm", "Constantin Serban", "Ritu Chadha"], "title": "WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector", "comment": "11 pages, 11 figures", "summary": "We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and\nacoustic signals for robust real-life UAV object detection. Our approach fuses\nvisual and acoustic features in a unified object detector model relying on the\nDeformable DETR and Wav2Vec2 architectures, achieving strong performance under\nchallenging environmental conditions. Our work leverage the existing\nDrone-vs-Bird dataset and the newly generated ARDrone dataset containing more\nthan 7,500 synchronized images and audio segments. We show how the acoustic\ninformation is used to improve the performance of the Deformable DETR object\ndetector on the real ARDrone dataset. We developed, trained and tested four\ndifferent fusion configurations based on a gated mechanism, linear layer, MLP\nand cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi\nresolution feature mappings of the Deformable DETR and enhance the object\ndetection performance over all drones dimensions. The best performer is the\ngated fusion approach, which improves the mAP of the Deformable DETR object\ndetector on our in-distribution and out-of-distribution ARDrone datasets by\n11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.\nThe mAP scores for medium and large drones are also enhanced, with overall\ngains across all drone sizes ranging from 3.27% to 5.84%.", "AI": {"tldr": "A multi-modal fusion of visual and acoustic signals using Deformable DETR and Wav2Vec2 improves drone object detection, especially for small drones, via a gated fusion strategy on ARDrone data.", "motivation": "Address robust real-life UAV detection under challenging environments by leveraging complementary acoustic cues alongside visual data to mitigate issues like visual occlusion, clutter, or lighting.", "method": "Propose four fusion configurations (gated, linear, MLP, cross-attention) to fuse Wav2Vec2 acoustic embeddings with Deformable DETR multi-resolution visual features; use ARDrone and Drone-vs-Bird datasets; evaluate mAP across IoU 0.5-0.9.", "result": "Gated fusion yields best improvements: small drones mAP +11.1% to 15.3%; medium/large drones +3.27% to 5.84%; overall improvements on in-distribution and out-of-distribution ARDrone datasets; 7,500 synchronized images/audio in ARDrone dataset.", "conclusion": "Demonstrates the effectiveness of acoustic-vision fusion for UAV detection and identifies gated fusion as the most effective fusion strategy, enabling robust performance across drone sizes."}}
{"id": "2509.09940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09940", "abs": "https://arxiv.org/abs/2509.09940", "authors": ["Yifei Wang", "Wenbin Wang", "Yong Luo"], "title": "DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition", "comment": "8 pages, 2 figures", "summary": "Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich\ninformation from multiple sources (e.g., language, video, and audio), the\npotential for intent-irrelevant and conflicting information across modalities\nmay hinder performance from being further improved. Most current models attempt\nto fuse modalities by applying mechanisms like multi-head attention to unimodal\nfeature sequences and then adding the result back to the original\nrepresentation. This process risks corrupting the primary linguistic features\nwith noisy or irrelevant non-verbal signals, as it often fails to capture the\nfine-grained, token-level influence where non-verbal cues should modulate, not\njust augment, textual meaning. To address this, we introduce DyKen-Hyena, which\nreframes the problem from feature fusion to processing modulation. Our model\ntranslates audio-visual cues into dynamic, per-token convolutional kernels that\ndirectly modulate textual feature extraction. This fine-grained approach\nachieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.\nNotably, it yields a +10.46% F1-score improvement in out-of-scope detection,\nvalidating that our method creates a fundamentally more robust intent\nrepresentation.", "AI": {"tldr": "DyKen-Hyena reframes multimodal intent recognition as token-level modulation, using dynamic per-token convolutional kernels derived from audio-visual cues to modulate textual features, achieving state-of-the-art results on MIntRec and MIntRec2.0 and a notable F1 gain in out-of-scope detection.", "motivation": "Current multimodal fusion often stacks unimodal features with attention and adds them back to the original representation, risking corruption of linguistic information by noisy, non-verbal signals. There is a need for fine-grained, token-level influence where non-verbal cues modulate rather than simply augment textual meaning to improve robustness.", "method": "Introduce DyKen-Hyena, which converts audio-visual cues into dynamic, per-token convolutional kernels that directly modulate textual feature extraction rather than fusing features. This reframes the problem from fusion to modulation, enabling fine-grained control over how each token is influenced by non-verbal signals.", "result": "Achieves state-of-the-art performance on MIntRec and MIntRec2.0. Demonstrates a substantial out-of-scope detection gain of +10.46 percentage points in F1-score, validating the approach's robustness in challenging settings.", "conclusion": "Fine-grained modulation via per-token dynamic kernels can yield more robust multimodal intent representations than traditional fusion strategies by preventing non-linguistic noise from corrupting textual features. This token-level modulation paradigm may generalize to other multimodal NLP tasks and warrants further investigation into efficiency and scalability."}}
{"id": "2509.10210", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.10210", "abs": "https://arxiv.org/abs/2509.10210", "authors": ["Marko Petkovi\u0107", "Vlado Menkovski", "Sof\u00eda Calero"], "title": "Towards Fully Automated Molecular Simulations: Multi-Agent Framework for Simulation Setup and Force Field Extraction", "comment": null, "summary": "Automated characterization of porous materials has the potential to\naccelerate materials discovery, but it remains limited by the complexity of\nsimulation setup and force field selection. We propose a multi-agent framework\nin which LLM-based agents can autonomously understand a characterization task,\nplan appropriate simulations, assemble relevant force fields, execute them and\ninterpret their results to guide subsequent steps. As a first step toward this\nvision, we present a multi-agent system for literature-informed force field\nextraction and automated RASPA simulation setup. Initial evaluations\ndemonstrate high correctness and reproducibility, highlighting this approach's\npotential to enable fully autonomous, scalable materials characterization.", "AI": {"tldr": "A multi-agent, LLM-driven framework autonomously plans and executes simulations for porous material characterization, extracting force fields from literature and setting up RASPA runs; early results show high correctness and reproducibility, indicating promise for autonomous, scalable materials characterization.", "motivation": "Automated characterization is hampered by complex simulation setup and force-field selection; an autonomous system could streamline discovery by coordinating tasks, assembling force fields, and interpreting results.", "method": "A multi-agent system where LLM-based agents understand tasks, plan simulations, assemble force fields from literature, run RASPA simulations, and interpret outputs to guide next steps; first component focuses on literature-informed force-field extraction and automated RASPA setup.", "result": "Initial evaluations report high correctness and reproducibility, suggesting reliable autonomous setup and interpretation workflows.", "conclusion": "This work demonstrates a viable path toward fully autonomous, scalable materials characterization, laying groundwork for broader automation beyond RASPA and more complex workflows."}}
{"id": "2509.10426", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.10426", "abs": "https://arxiv.org/abs/2509.10426", "authors": ["Jianxin Shi", "Zengqi Peng", "Xiaolong Chen", "Tianyu Wo", "Jun Ma"], "title": "DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training", "comment": null, "summary": "Trajectory prediction is a critical component of autonomous driving,\nessential for ensuring both safety and efficiency on the road. However,\ntraditional approaches often struggle with the scarcity of labeled data and\nexhibit suboptimal performance in multi-agent prediction scenarios. To address\nthese challenges, we introduce a disentangled context-aware pre-training\nframework for multi-agent motion prediction, named DECAMP. Unlike existing\nmethods that entangle representation learning with pretext tasks, our framework\ndecouples behavior pattern learning from latent feature reconstruction,\nprioritizing interpretable dynamics and thereby enhancing scene representation\nfor downstream prediction. Additionally, our framework incorporates\ncontext-aware representation learning alongside collaborative spatial-motion\npretext tasks, which enables joint optimization of structural and intentional\nreasoning while capturing the underlying dynamic intentions. Our experiments on\nthe Argoverse 2 benchmark showcase the superior performance of our method, and\nthe results attained underscore its effectiveness in multi-agent motion\nforecasting. To the best of our knowledge, this is the first context\nautoencoder framework for multi-agent motion forecasting in autonomous driving.\nThe code and models will be made publicly available.", "AI": {"tldr": "A disentangled context-aware pre-training framework (DECAMP) for multi-agent motion prediction that decouples behavior learning from latent reconstruction, leveraging context-aware representations and collaborative spatial-motion pretext tasks to improve scene understanding and forecasting on Argoverse 2; claimed as the first context autoencoder for this task, with code to be released.", "motivation": "Address scarcity of labeled data and poor multi-agent prediction performance; pursue interpretable dynamics and improved scene representation by separating behavior pattern learning from latent feature reconstruction and by integrating context-aware and collaborative pretext objectives.", "method": "Proposes DECAMP, a disentangled context-aware pre-training framework that decouples behavior pattern learning from latent feature reconstruction; uses context-aware representation learning and collaborative spatial-motion pretext tasks to jointly optimize structural and intentional reasoning within a context autoencoder for multi-agent motion forecasting.", "result": "On Argoverse 2, achieves superior performance compared to baselines; demonstrates effectiveness in multi-agent motion forecasting; claims to be the first context autoencoder framework for this task; code and models will be released.", "conclusion": "DECAMP enhances multi-agent motion prediction by disentangling representation learning and integrating context-aware pretext tasks, yielding improved forecasting and interpretable dynamics, with potential for broader adoption once released."}}
{"id": "2509.09869", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09869", "abs": "https://arxiv.org/abs/2509.09869", "authors": ["Yihao Liu", "Junyu Chen", "Lianrui Zuo", "Shuwen Wei", "Brian D. Boyd", "Carmen Andreescu", "Olusola Ajilore", "Warren D. Taylor", "Aaron Carass", "Bennett A. Landman"], "title": "Surrogate Supervision for Robust and Generalizable Deformable Image Registration", "comment": null, "summary": "Objective: Deep learning-based deformable image registration has achieved\nstrong accuracy, but remains sensitive to variations in input image\ncharacteristics such as artifacts, field-of-view mismatch, or modality\ndifference. We aim to develop a general training paradigm that improves the\nrobustness and generalizability of registration networks. Methods: We introduce\nsurrogate supervision, which decouples the input domain from the supervision\ndomain by applying estimated spatial transformations to surrogate images. This\nallows training on heterogeneous inputs while ensuring supervision is computed\nin domains where similarity is well defined. We evaluate the framework through\nthree representative applications: artifact-robust brain MR registration,\nmask-agnostic lung CT registration, and multi-modal MR registration. Results:\nAcross tasks, surrogate supervision demonstrated strong resilience to input\nvariations including inhomogeneity field, inconsistent field-of-view, and\nmodality differences, while maintaining high performance on well-curated data.\nConclusions: Surrogate supervision provides a principled framework for training\nrobust and generalizable deep learning-based registration models without\nincreasing complexity. Significance: Surrogate supervision offers a practical\npathway to more robust and generalizable medical image registration, enabling\nbroader applicability in diverse biomedical imaging scenarios.", "AI": {"tldr": "Surrogate supervision enables robust, generalizable DL-based image registration by decoupling input and supervision domains with estimated spatial transformations, achieving resilience to artifacts, FOV mismatch, and modality changes across three representative tasks without increasing model complexity.", "motivation": "Registration networks are sensitive to variations in image characteristics (artifacts, FOV, modality). A general training paradigm is needed to improve robustness and transferability.", "method": "Introduce surrogate supervision that applies estimated spatial transformations to surrogate images, decoupling input domain from supervision domain so supervision is computed where similarity is well defined. Train on heterogeneous inputs while preserving alignment guidance.", "result": "Surrogate supervision showed strong resilience to inhomogeneity fields, inconsistent FOV, and modality differences across artifact-robust brain MR registration, mask-agnostic lung CT registration, and multi-modal MR registration, while preserving high performance on well-curated data.", "conclusion": "Surrogate supervision provides a principled, low-complexity framework to train robust and generalizable deep learning-based registration models, expanding applicability to diverse biomedical imaging scenarios."}}
{"id": "2509.09955", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.09955", "abs": "https://arxiv.org/abs/2509.09955", "authors": ["Omar Erak", "Omar Alhussein", "Hatem Abou-Zeid", "Mehdi Bennis", "Sami Muhaidat"], "title": "Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge", "comment": "Submitted to IEEE Journals", "summary": "Large-scale transformers are central to modern semantic communication, yet\ntheir high computational and communication costs hinder deployment on\nresource-constrained edge devices. This paper introduces a training-free\nframework for adaptive token merging, a novel mechanism that compresses\ntransformer representations at runtime by selectively merging semantically\nredundant tokens under per-layer similarity thresholds. Unlike prior\nfixed-ratio reduction, our approach couples merging directly to input\nredundancy, enabling data-dependent adaptation that balances efficiency and\ntask relevance without retraining. We cast the discovery of merging strategies\nas a multi-objective optimization problem and leverage Bayesian optimization to\nobtain Pareto-optimal trade-offs between accuracy, inference cost, and\ncommunication cost. On ImageNet classification, we match the accuracy of the\nunmodified transformer with 30\\% fewer floating-point operations per second and\nunder 20\\% of the original communication cost, while for visual question\nanswering our method achieves performance competitive with the full LLaVA model\nat less than one-third of the compute and one-tenth of the bandwidth. Finally,\nwe show that our adaptive merging is robust across varying channel conditions\nand provides inherent privacy benefits, substantially degrading the efficacy of\nmodel inversion attacks. Our framework provides a practical and versatile\nsolution for deploying powerful transformer models in resource-limited edge\nintelligence scenarios.", "AI": {"tldr": "A training-free adaptive token merging framework for transformers that merges semantically redundant tokens at runtime using per-layer similarity thresholds, optimized via Bayesian optimization to balance accuracy, compute, and communication. It achieves strong edge deployment results and offers privacy benefits without retraining.", "motivation": "Large-scale transformers are costly to deploy on edge devices due to high compute and communication demands. There is a need for data-dependent, runtime adaptation that preserves task performance without retraining.", "method": "Runtime token merging based on semantic redundancy, with per-layer similarity thresholds to decide which tokens to merge. Treats the merging strategy as a multi-objective optimization problem and uses Bayesian optimization to discover Pareto-optimal trade-offs among accuracy, inference cost (FLOPs), and communication cost. No retraining required. Evaluated on ImageNet and visual question answering tasks, plus analyses of robustness and privacy.", "result": "On ImageNet, matches unmodified transformer accuracy with ~30% fewer FLOPs and under 20% of original communication cost. For VQA, achieves performance competitive with full LLaVA at less than one-third of compute and one-tenth of bandwidth. Adaptive merging is robust to varying channel conditions and offers privacy benefits by substantially degrading model inversion attacks.", "conclusion": "The framework provides a practical, versatile approach for deploying powerful transformers in resource-constrained edge environments, enabling data-driven, task-relevant efficiency without retraining and with added privacy robustness."}}
{"id": "2509.10222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10222", "abs": "https://arxiv.org/abs/2509.10222", "authors": ["Ma\u00ebl Jullien", "Lei Xu", "Marco Valentino", "Andr\u00e9 Freitas"], "title": "Compartmentalised Agentic Reasoning for Clinical NLI", "comment": null, "summary": "A common assumption holds that scaling data and parameters yields\nincreasingly structured, generalisable internal representations. We interrogate\nthis assumption in clinical natural language inference (NLI) by adopting a\nbenchmark decomposed into four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction,\nand introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI\nthat separates knowledge access from principled inference. CARENLI routes each\npremise, statement pair to a family specific solver and enforces auditable\nprocedures via a planner, verifier, and refiner.\n  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching\n98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag\nviolations with near-ceiling reliability, while refiners correct a substantial\nshare of epistemic errors. Remaining failures cluster in routing, identifying\nfamily classification as the main bottleneck. These results show that LLMs\noften retain relevant facts but default to heuristics when inference is\nunderspecified, a dissociation CARENLI makes explicit while offering a\nframework for safer, auditable reasoning.", "AI": {"tldr": "CARENLI decouples knowledge access from inference in clinical NLI by routing premise\u2013statement pairs to four domain-specific solvers with a planner, verifier, and refiner, yielding substantial accuracy gains and auditable safety.", "motivation": "To test whether scaling data/parameters leads to more structured, generalisable representations in clinical NLI and to develop a safe, auditable reasoning framework.", "method": "Introduce CARENLI with four reasoning families (Causal Attribution, Compositional Grounding, Epistemic Verification, Risk State Abstraction). Route each premise\u2013statement pair to a family-specific solver, and enforce auditable steps via a planner, verifier, and refiner. Evaluate across four LLMs.", "result": "Fidelity improves by up to 42 points; 98.0% accuracy in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag violations with near-ceiling reliability; refiners fix a substantial share of epistemic errors. Remaining failures cluster in routing\u2014family classification is the main bottleneck.", "conclusion": "LLMs often retain relevant facts but default to heuristics when inference is underspecified; CARENLI exposes this dissociation and provides a framework for safer, auditable, and more controllable reasoning in clinical NLI."}}
{"id": "2509.10444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10444", "abs": "https://arxiv.org/abs/2509.10444", "authors": ["Chaerim Moon", "Joohyung Kim"], "title": "Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction", "comment": "Presented in IROS 2023 Workshop (Multilimb Coordination in Human\n  Neuroscience and Robotics: Classical and Learning Perspectives)", "summary": "Supernumerary Robotic Limbs (SRLs) can enhance human capability within close\nproximity. However, as a wearable device, the generated moment from its\noperation acts on the human body as an external torque. When the moments\nincrease, more muscle units are activated for balancing, and it can result in\nreduced muscular null space. Therefore, this paper suggests a concept of a\nmotion planning layer that reduces the generated moment for enhanced\nHuman-Robot Interaction. It modifies given trajectories with desirable angular\nacceleration and position deviation limits. Its performance to reduce the\nmoment is demonstrated through the simulation, which uses simplified human and\nrobotic system models.", "AI": {"tldr": "Motion planning layer reduces actuator-induced moments in Supernumerary Robotic Limbs by constraining trajectory angular acceleration and position deviation; demonstrated via simulation.", "motivation": "SRLs generate external torques on the human body; increased moments require more muscle activation, reducing muscular null space and potentially harming balance and comfort. A planning layer to minimize generated moment aims to improve safety/effort in Human-Robot Interaction.", "method": "Introduce a trajectory-modification module (motion planning layer) that imposes limits on angular acceleration and allowable position deviation of the limb trajectories, applied to pre-existing trajectories and simulated with simplified human and SRL models to assess moment reduction.", "result": "Simulation demonstrates reduced interaction moment under the imposed limits, while maintaining trajectory feasibility within the configured bounds.", "conclusion": "The proposed planning layer shows promise for safer, more comfortable HRI with SRLs; requires validation with more realistic models and hardware experiments, and exploration of trade-offs between task performance and moment reduction."}}
{"id": "2509.09911", "categories": ["cs.CV", "cs.AI", "68T07 (Primary)"], "pdf": "https://arxiv.org/pdf/2509.09911", "abs": "https://arxiv.org/abs/2509.09911", "authors": ["Barkin Buyukcakir", "Jannick De Tobel", "Patrick Thevissen", "Dirk Vandermeulen", "Peter Claes"], "title": "An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars", "comment": "21 pages, 11 figures, Scientific Reports", "summary": "The practical adoption of deep learning in high-stakes forensic applications,\nsuch as dental age estimation, is often limited by the 'black box' nature of\nthe models. This study introduces a framework designed to enhance both\nperformance and transparency in this context. We use a notable performance\ndisparity in the automated staging of mandibular second (tooth 37) and third\n(tooth 38) molars as a case study. The proposed framework, which combines a\nconvolutional autoencoder (AE) with a Vision Transformer (ViT), improves\nclassification accuracy for both teeth over a baseline ViT, increasing from\n0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond\nimproving performance, the framework provides multi-faceted diagnostic\ninsights. Analysis of the AE's latent space metrics and image reconstructions\nindicates that the remaining performance gap is data-centric, suggesting high\nintra-class morphological variability in the tooth 38 dataset is a primary\nlimiting factor. This work highlights the insufficiency of relying on a single\nmode of interpretability, such as attention maps, which can appear anatomically\nplausible yet fail to identify underlying data issues. By offering a\nmethodology that both enhances accuracy and provides evidence for why a model\nmay be uncertain, this framework serves as a more robust tool to support expert\ndecision-making in forensic age estimation.", "AI": {"tldr": "A convolutional autoencoder + Vision Transformer framework improves dental age estimation accuracy and offers multi-faceted interpretability beyond attention maps; data variability limits performance, especially for tooth 38.", "motivation": "High-stakes forensic decisions require both accuracy and transparency; black-box DL models are risky. The study aims to reduce the accuracy gap and provide diagnostic insights that reveal data issues, not just model behavior.", "method": "Combine a convolutional autoencoder with a Vision Transformer. Evaluate on mandibular second (tooth 37) and third (tooth 38) molars. Compare against a baseline ViT. Analyze AE latent space metrics and reconstructions to diagnose residual errors; interpretability via multiple modalities beyond attention maps.", "result": "Classification accuracy improved from 0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. The remaining gap is data-centric, indicating high intra-class morphological variability in tooth 38. Attention maps alone are insufficient; the framework provides multifaceted evidence for model uncertainty, aiding forensic decision-making.", "conclusion": "The framework advances both performance and transparency in forensic age estimation, highlighting the necessity of data-centric improvements and diverse interpretability tools to support expert judgments in high-stakes settings."}}
{"id": "2509.09960", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09960", "abs": "https://arxiv.org/abs/2509.09960", "authors": ["Mingxuan Jiang", "Yongxin Wang", "Ziyue Dai", "Yicun Liu", "Hongyi Nie", "Sen Liu", "Hongfeng Chai"], "title": "Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes", "comment": null, "summary": "Synthetic tabular data generation is increasingly essential in data\nmanagement, supporting downstream applications when real-world and high-quality\ntabular data is insufficient. Existing tabular generation approaches, such as\ngenerative adversarial networks (GANs), diffusion models, and fine-tuned Large\nLanguage Models (LLMs), typically require sufficient reference data, limiting\ntheir effectiveness in domain-specific databases with scarce records. While\nprompt-based LLMs offer flexibility without parameter tuning, they often fail\nto capture dataset-specific feature-label dependencies and generate redundant\ndata, leading to degradation in downstream task performance. To overcome these\nissues, we propose ReFine, a framework that (i) derives symbolic \"if-then\"\nrules from interpretable models and embeds them into prompts to explicitly\nguide generation toward domain-specific feature distribution, and (ii) applies\na dual-granularity filtering strategy that suppresses over-sampling patterns\nand selectively refines rare but informative samples to reduce distributional\nimbalance. Extensive experiments on various regression and classification\nbenchmarks demonstrate that ReFine consistently outperforms state-of-the-art\nmethods, achieving up to 0.44 absolute improvement in R-squared for regression\nand 10.0 percent relative improvement in F1 score for classification tasks.", "AI": {"tldr": "ReFine is a prompt-guided framework for synthetic tabular data generation that embeds interpretable if-then rules and employs dual-granularity filtering, improving downstream regression and classification performance beyond state-of-the-art.", "motivation": "Domain-specific tabular data often suffer from scarcity, making it hard for data-driven models to learn feature-label dependencies. While GANs, diffusion models, or LLMs can generate data, they rely on substantial reference data or struggle to capture dependencies and distributional balance in limited-data settings.", "method": "ReFine derives symbolic if-then rules from interpretable models and embeds them into prompts to steer generation toward domain-aligned feature distributions; it also introduces a dual-granularity filtering strategy to suppress over-sampling and refine rare but informative samples, aiming to reduce distributional imbalance in the synthetic data.", "result": "Empirical evaluation on regression and classification benchmarks shows consistent improvements over state-of-the-art baselines, achieving up to 0.44 absolute R-squared gains in regression and about 10% relative improvement in F1 for classification.", "conclusion": "Integrating interpretable rules into prompt-driven generation, coupled with targeted filtering, effectively addresses scarcity and distributional imbalance in synthetic tabular data, delivering stronger downstream performance and demonstrating robustness across tasks."}}
{"id": "2509.10249", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10249", "abs": "https://arxiv.org/abs/2509.10249", "authors": ["Hanna Abi Akl"], "title": "Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering", "comment": "accepted for the International Joint Conference on Rules and\n  Reasoning (RuleML+RR) 2025", "summary": "Recent advances in Language Models (LMs) have failed to mask their\nshortcomings particularly in the domain of reasoning. This limitation impacts\nseveral tasks, most notably those involving ontology engineering. As part of a\nPhD research, we investigate the consequences of incorporating formal methods\non the performance of Small Language Models (SLMs) on reasoning tasks.\nSpecifically, we aim to orient our work toward using SLMs to bootstrap ontology\nconstruction and set up a series of preliminary experiments to determine the\nimpact of expressing logical problems with different grammars on the\nperformance of SLMs on a predefined reasoning task. Our findings show that it\nis possible to substitute Natural Language (NL) with a more compact logical\nlanguage while maintaining a strong performance on reasoning tasks and hope to\nuse these results to further refine the role of SLMs in ontology engineering.", "AI": {"tldr": "Compact logical languages can preserve SLM reasoning performance and aid ontology construction, reducing reliance on natural language.", "motivation": "LMs often struggle with reasoning, which hampers ontology engineering. The work explores applying formal methods and small language models to bootstrap ontology construction and to study how grammar choices for expressing logical problems affect reasoning tasks.", "method": "PhD-level study with preliminary experiments that express logical problems using different grammars and assess SLM performance on a predefined reasoning task, focusing on ontology engineering implications.", "result": "It is possible to substitute Natural Language with a more compact logical language without sacrificing strong reasoning performance in SLMs.", "conclusion": "The findings support further refinement of SLMs' role in ontology engineering and motivate deeper investigation into grammar design for logical problem expression."}}
{"id": "2509.10454", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10454", "abs": "https://arxiv.org/abs/2509.10454", "authors": ["Hang Yin", "Haoyu Wei", "Xiuwei Xu", "Wenxuan Guo", "Jie Zhou", "Jiwen Lu"], "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation", "comment": "Accepted to CoRL 2025. Project page: [this https\n  URL](https://bagh2178.github.io/GC-VLN/)", "summary": "In this paper, we propose a training-free framework for vision-and-language\nnavigation (VLN). Existing zero-shot VLN methods are mainly designed for\ndiscrete environments or involve unsupervised training in continuous simulator\nenvironments, which makes it challenging to generalize and deploy them in\nreal-world scenarios. To achieve a training-free framework in continuous\nenvironments, our framework formulates navigation guidance as graph constraint\noptimization by decomposing instructions into explicit spatial constraints. The\nconstraint-driven paradigm decodes spatial semantics through constraint\nsolving, enabling zero-shot adaptation to unseen environments. Specifically, we\nconstruct a spatial constraint library covering all types of spatial\nrelationship mentioned in VLN instructions. The human instruction is decomposed\ninto a directed acyclic graph, with waypoint nodes, object nodes and edges,\nwhich are used as queries to retrieve the library to build the graph\nconstraints. The graph constraint optimization is solved by the constraint\nsolver to determine the positions of waypoints, obtaining the robot's\nnavigation path and final goal. To handle cases of no solution or multiple\nsolutions, we construct a navigation tree and the backtracking mechanism.\nExtensive experiments on standard benchmarks demonstrate significant\nimprovements in success rate and navigation efficiency compared to\nstate-of-the-art zero-shot VLN methods. We further conduct real-world\nexperiments to show that our framework can effectively generalize to new\nenvironments and instruction sets, paving the way for a more robust and\nautonomous navigation framework.", "AI": {"tldr": "A training-free framework for vision-and-language navigation in continuous environments that uses graph constraint optimization with a spatial constraint library to enable zero-shot generalization, via DAG-based instruction parsing, constraint solving, and a navigation tree with backtracking; shows improvements over zero-shot baselines and real-world validation.", "motivation": "Zero-shot VLN in continuous environments is challenging and current zero-shot methods rely on discrete worlds or unsupervised training, hindering generalization and deployment in real world settings.", "method": "Construct a spatial constraint library covering spatial relations; decompose instruction into a directed acyclic graph with waypoint/object nodes and edges; formulate graph constraints by querying the library; solve via a constraint solver to determine waypoint placements and navigation path; use a navigation tree and backtracking to handle no/multiple solutions.", "result": "Extensive experiments on standard VLN benchmarks show significant improvements in success rate and navigation efficiency over state-of-the-art zero-shot VLN methods; real-world experiments demonstrate good generalization to new environments and instruction sets.", "conclusion": "A training-free, constraint-based framework can effectively enable zero-shot VLN in continuous, real-world-like settings and pave the way for more robust autonomous navigation."}}
{"id": "2509.09935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09935", "abs": "https://arxiv.org/abs/2509.09935", "authors": ["Chirayu Agrawal", "Snehasis Mukherjee"], "title": "SCoDA: Self-supervised Continual Domain Adaptation", "comment": "Submitted to ICVGIP 2025", "summary": "Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a\nmodel to a target domain without access to the data of the source domain.\nPrevailing methods typically start with a source model pre-trained with full\nsupervision and distill the knowledge by aligning instance-level features.\nHowever, these approaches, relying on cosine similarity over L2-normalized\nfeature vectors, inadvertently discard crucial geometric information about the\nlatent manifold of the source model. We introduce Self-supervised Continual\nDomain Adaptation (SCoDA) to address these limitations. We make two key\ndepartures from standard practice: first, we avoid the reliance on supervised\npre-training by initializing the proposed framework with a teacher model\npre-trained entirely via self-supervision (SSL). Second, we adapt the principle\nof geometric manifold alignment to the SFDA setting. The student is trained\nwith a composite objective combining instance-level feature matching with a\nSpace Similarity Loss. To combat catastrophic forgetting, the teacher's\nparameters are updated via an Exponential Moving Average (EMA) of the student's\nparameters. Extensive experiments on benchmark datasets demonstrate that SCoDA\nsignificantly outperforms state-of-the-art SFDA methods.", "AI": {"tldr": "SCoDA enables Source-Free Domain Adaptation by self-supervised pretraining, manifold-aware alignment, and EMA-augmented teacher to outperform prior SFDA methods.", "motivation": "SFDA methods suffer from relying on supervised pretraining and losing latent manifold geometry; leveraging SSL pretraining and geometric manifold alignment can preserve structure and improve adaptation without access to source data.", "method": "Use a teacher model pre-trained via self-supervision; train a student with a composite objective of instance-level feature matching and Space Similarity Loss; update the teacher via exponential moving average (EMA) of the student's parameters to mitigate forgetting; adapt geometric manifold alignment to SFDA settings.", "result": "Extensive experiments on benchmark datasets show that SCoDA significantly outperforms state-of-the-art SFDA methods.", "conclusion": "Integrating SSL pretraining, manifold-aware alignment, and EMA-based teacher updates yields robust SFDA with improved performance and better preservation of latent source geometry."}}
{"id": "2509.09991", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09991", "abs": "https://arxiv.org/abs/2509.09991", "authors": ["Amandip Sangha"], "title": "Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning", "comment": null, "summary": "This paper presents a machine learning-based approach to estimate the energy\nconsumption of virtual servers without access to physical power measurement\ninterfaces. Using resource utilization metrics collected from guest virtual\nmachines, we train a Gradient Boosting Regressor to predict energy consumption\nmeasured via RAPL on the host. We demonstrate, for the first time, guest-only\nresource-based energy estimation without privileged host access with\nexperiments across diverse workloads, achieving high predictive accuracy and\nvariance explained ($0.90 \\leq R^2 \\leq 0.97$), indicating the feasibility of\nguest-side energy estimation. This approach can enable energy-aware scheduling,\ncost optimization and physical host independent energy estimates in virtualized\nenvironments. Our approach addresses a critical gap in virtualized environments\n(e.g. cloud) where direct energy measurement is infeasible.", "AI": {"tldr": "Guest-only ML-based energy estimation for virtual servers using guest metrics to predict host energy measured by RAPL; high accuracy (R^2 ~ 0.90\u20130.97).", "motivation": "In virtualized/cloud environments, direct power measurement is often inaccessible due to lack of privileged host access. There is a need for accurate energy estimates to enable energy-aware scheduling and cost optimization.", "method": "Collect resource utilization metrics from guest VMs and train a Gradient Boosting Regressor to predict host energy (measured via RAPL). Evaluate across diverse workloads to demonstrate guest-side energy estimation without privileged host access.", "result": "Achieved high predictive accuracy with R^2 between 0.90 and 0.97, demonstrating feasibility of guest-side energy estimation without host access.", "conclusion": "Guest-side energy estimation is feasible and can support energy-aware scheduling, cost optimization, and host-independent energy estimates in virtualized/cloud environments."}}
{"id": "2509.10297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10297", "abs": "https://arxiv.org/abs/2509.10297", "authors": ["Eoin O'Doherty", "Nicole Weinrauch", "Andrew Talone", "Uri Klempner", "Xiaoyuan Yi", "Xing Xie", "Yi Zeng"], "title": "The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis", "comment": "Work in progress", "summary": "Artificial intelligence (AI) is advancing at a pace that raises urgent\nquestions about how to align machine decision-making with human moral values.\nThis working paper investigates how leading AI systems prioritize moral\noutcomes and what this reveals about the prospects for human-AI symbiosis. We\naddress two central questions: (1) What moral values do state-of-the-art large\nlanguage models (LLMs) implicitly favour when confronted with dilemmas? (2) How\ndo differences in model architecture, cultural origin, and explainability\naffect these moral preferences? To explore these questions, we conduct a\nquantitative experiment with six LLMs, ranking and scoring outcomes across 18\ndilemmas representing five moral frameworks. Our findings uncover strikingly\nconsistent value biases. Across all models, Care and Virtue values outcomes\nwere rated most moral, while libertarian choices were consistently penalized.\nReasoning-enabled models exhibited greater sensitivity to context and provided\nricher explanations, whereas non-reasoning models produced more uniform but\nopaque judgments. This research makes three contributions: (i) Empirically, it\ndelivers a large-scale comparison of moral reasoning across culturally distinct\nLLMs; (ii) Theoretically, it links probabilistic model behaviour with\nunderlying value encodings; (iii) Practically, it highlights the need for\nexplainability and cultural awareness as critical design principles to guide AI\ntoward a transparent, aligned, and symbiotic future.", "AI": {"tldr": "Six LLMs were evaluated on 18 dilemmas across five moral frameworks, revealing consistent Care/Virtue biases and penalties for libertarian choices. Reasoning-enabled models show richer, context-sensitive explanations; non-reasoning models are more uniform and opaque. Implications stress explainability and cultural awareness for aligned, symbiotic AI.", "motivation": "To understand how current large language models encode and reflect human moral values, and what this implies for human-AI symbiosis, alignment, and policy.", "method": "Quantitative experiment with six LLMs evaluating 18 dilemmas spanning five moral frameworks; models ranked and scored on outcomes; analysis of sensitivity to architecture, cultural origin, and explainability.", "result": "Across models, Care and Virtue values were rated as most moral; libertarian choices were consistently penalized. Reasoning-enabled models showed greater sensitivity to context and provided richer explanations; non-reasoning models yielded uniform but opaque judgments.", "conclusion": "(i) Empirical large-scale cross-cultural comparison of moral reasoning across LLMs; (ii) Theoretical link between probabilistic model behavior and underlying value encodings; (iii) Practical emphasis on explainability and cultural awareness as design principles to guide AI toward a transparent, aligned, and symbiotic future."}}
{"id": "2509.09943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09943", "abs": "https://arxiv.org/abs/2509.09943", "authors": ["Zhu Chen", "Mert Edg\u00fc", "Er Jin", "Johannes Stegmaier"], "title": "Segment Anything for Cell Tracking", "comment": null, "summary": "Tracking cells and detecting mitotic events in time-lapse microscopy image\nsequences is a crucial task in biomedical research. However, it remains highly\nchallenging due to dividing objects, low signal-tonoise ratios, indistinct\nboundaries, dense clusters, and the visually similar appearance of individual\ncells. Existing deep learning-based methods rely on manually labeled datasets\nfor training, which is both costly and time-consuming. Moreover, their\ngeneralizability to unseen datasets remains limited due to the vast diversity\nof microscopy data. To overcome these limitations, we propose a zero-shot cell\ntracking framework by integrating Segment Anything 2 (SAM2), a large foundation\nmodel designed for general image and video segmentation, into the tracking\npipeline. As a fully-unsupervised approach, our method does not depend on or\ninherit biases from any specific training dataset, allowing it to generalize\nacross diverse microscopy datasets without finetuning. Our approach achieves\ncompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos\nwhile eliminating the need for dataset-specific adaptation.", "AI": {"tldr": "Zero-shot cell tracking using SAM2 enables dataset-agnostic tracking in 2D/3D time-lapse microscopy without finetuning, achieving competitive accuracy across diverse datasets.", "motivation": "To reduce labeling burden and improve generalization across heterogeneous microscopy data under challenging conditions like cell division, low SNR, indistinct boundaries, dense clusters, and visual similarity.", "method": "Integrates Segment Anything 2 (SAM2) into a tracking pipeline as a fully unsupervised, zero-shot approach that does not rely on training data or fine-tuning; capable of handling 2D and large-scale 3D time-lapse videos.", "result": "Competitive accuracy in 2D and large-scale 3D time-lapse videos; demonstrates generalization across diverse datasets without dataset-specific adaptation.", "conclusion": "Foundation-model-based zero-shot tracking can reduce annotation needs and generalize across datasets; this approach shows promise but quantitative comparisons and analysis of failure modes, computational cost, and limits remain for future work."}}
{"id": "2509.10000", "categories": ["cs.LG", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2509.10000", "abs": "https://arxiv.org/abs/2509.10000", "authors": ["Tilen Cadez", "Kyoung-Min Kim"], "title": "Neural Scaling Laws for Deep Regression", "comment": "Supplementary Information will be provided with the published\n  manuscript", "summary": "Neural scaling laws--power-law relationships between generalization errors\nand characteristics of deep learning models--are vital tools for developing\nreliable models while managing limited resources. Although the success of large\nlanguage models highlights the importance of these laws, their application to\ndeep regression models remains largely unexplored. Here, we empirically\ninvestigate neural scaling laws in deep regression using a parameter estimation\nmodel for twisted van der Waals magnets. We observe power-law relationships\nbetween the loss and both training dataset size and model capacity across a\nwide range of values, employing various architectures--including fully\nconnected networks, residual networks, and vision transformers. Furthermore,\nthe scaling exponents governing these relationships range from 1 to 2, with\nspecific values depending on the regressed parameters and model details. The\nconsistent scaling behaviors and their large scaling exponents suggest that the\nperformance of deep regression models can improve substantially with increasing\ndata size.", "AI": {"tldr": "Neural scaling laws apply to deep regression; loss scales as a power-law with data size and model capacity; exponents range 1\u20132 across architectures.", "motivation": "To extend neural scaling laws from large language models to deep regression, using a physics-inspired parameter-estimation task (twisted van der Waals magnets) to assess generalization under data and model capacity changes.", "method": "Empirical study across multiple architectures (fully connected, ResNet, Vision Transformer); vary training dataset size and model capacity; measure loss and extract scaling exponents.", "result": "Observed power-law relationships between loss and both dataset size and model capacity; exponents between 1 and 2 depending on regressed parameters and model details; scaling behavior is consistent and implies substantial performance gains with more data.", "conclusion": "Deep regression models exhibit robust neural scaling laws similar to LLMs; increasing data size can meaningfully improve performance, informing resource planning and model design."}}
{"id": "2509.10326", "categories": ["cs.AI", "cs.LO", "03G27 (Primary) 68W30, 68T27 (Secondary)"], "pdf": "https://arxiv.org/pdf/2509.10326", "abs": "https://arxiv.org/abs/2509.10326", "authors": ["Dmitry Lesnik", "Tobias Sch\u00e4fer"], "title": "State Algebra for Propositional Logic", "comment": "47 pages", "summary": "This paper presents State Algebra, a novel framework designed to represent\nand manipulate propositional logic using algebraic methods. The framework is\nstructured as a hierarchy of three representations: Set, Coordinate, and Row\nDecomposition. These representations anchor the system in well-known semantics\nwhile facilitating the computation using a powerful algebraic engine. A key\naspect of State Algebra is its flexibility in representation. We show that\nalthough the default reduction of a state vector is not canonical, a unique\ncanonical form can be obtained by applying a fixed variable order during the\nreduction process. This highlights a trade-off: by foregoing guaranteed\ncanonicity, the framework gains increased flexibility, potentially leading to\nmore compact representations of certain classes of problems. We explore how\nthis framework provides tools to articulate both search-based and knowledge\ncompilation algorithms and discuss its natural extension to probabilistic logic\nand Weighted Model Counting.", "AI": {"tldr": "State Algebra introduces a three-tier representation (Set, Coordinate, Row Decomposition) for propositional logic, enabling algebraic manipulation with a flexible, non-canonical default reduction that can become canonical with a fixed variable order. It supports search-based and knowledge compilation, and extends to probabilistic logic and Weighted Model Counting.", "motivation": "To unify semantic grounding with algebraic computation in propositional logic, offering flexibility and potential compact representations, while enabling connections to search and knowledge compilation and paving the way for probabilistic extensions.", "method": "Define a layered representation framework anchored in semantics, describe the reduction process, show that a fixed variable order yields a unique canonical form, and discuss trade-offs between canonicity and expressiveness; outline use cases for search-based and knowledge compilation algorithms and sketches for probabilistic logic and WMC extensions.", "result": "A novel algebraic engine and representation hierarchy that supports flexible canonicalization and can adapt to different problem structures; canonical form is achievable under a fixed order; framework supports algorithmic applications and probabilistic extensions.", "conclusion": "State Algebra offers a flexible, semantics-grounded approach for propositional logic, unifying representations and algorithms, with a tunable trade-off between canonicity and compactness, and promising extensions to probabilistic reasoning and WMC."}}
{"id": "2509.09946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09946", "abs": "https://arxiv.org/abs/2509.09946", "authors": ["Vu-Minh Le", "Thao-Anh Tran", "Duc Huy Do", "Xuan Canh Do", "Huong Ninh", "Hai Tran"], "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation", "comment": "Accepted at ICCVW 2025", "summary": "Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision\ntask for automating large-scale surveillance. With camera calibration and depth\ninformation, the targets in the scene can be projected into 3D space, offering\nunparalleled levels of automatic perception of a 3D environment. However,\ntracking in the 3D space requires replacing all 2D tracking components from the\nground up, which may be infeasible for existing MTMC systems. In this paper, we\npresent an approach for extending any online 2D multi-camera tracking system\ninto 3D space by utilizing depth information to reconstruct a target in\npoint-cloud space, and recovering its 3D box through clustering and yaw\nrefinement following tracking. We also introduced an enhanced online data\nassociation mechanism that leverages the target's local ID consistency to\nassign global IDs across frames. The proposed framework is evaluated on the\n2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the\nleaderboard.", "AI": {"tldr": "Extends online 2D multi-camera tracking to 3D MTMC by using depth to reconstruct targets into point-cloud space and estimate 3D boxes, with improved online data association via local ID consistency; achieves 3rd place on AI City Challenge 3D MTMC dataset.", "motivation": "3D MTMC provides richer surveillance perception by leveraging depth; existing MTMC systems are 2D and hard to adapt to 3D space without rebuilding the tracker. A practical approach aims to convert 2D MTMC pipelines into 3D using depth information.", "method": "Integrate depth-based reconstruction into an online 2D MTMC system to form target point clouds and recover 3D bounding boxes via clustering and yaw refinement after tracking. Enhance online data association by exploiting local ID consistency to maintain global IDs across frames.", "result": "Evaluation on the 2025 AI City Challenge 3D MTMC dataset yielded a 3rd place on the leaderboard, indicating competitive performance.", "conclusion": "The work demonstrates a practical pathway to upgrade existing 2D MTMC systems into 3D using depth, avoiding a complete redesign while delivering competitive 3D MTMC performance and improved ID consistency across frames."}}
{"id": "2509.10011", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.10011", "abs": "https://arxiv.org/abs/2509.10011", "authors": ["Antoine Orioua", "Philipp Krah", "Julian Koellermeier"], "title": "Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss", "comment": "Preprint with 12 pages and 12 figures", "summary": "This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),\nwhich identifies the underlying intrinsic dimension of a wide range of datasets\nwhose samples lie on either linear or nonlinear manifolds. Beyond estimating\nthe intrinsic dimension, IDEA is also able to reconstruct the original dataset\nafter projecting it onto the corresponding latent space, which is structured\nusing re-weighted double CancelOut layers. Our key contribution is the\nintroduction of the projected reconstruction loss term, guiding the training of\nthe model by continuously assessing the reconstruction quality under the\nremoval of an additional latent dimension. We first assess the performance of\nIDEA on a series of theoretical benchmarks to validate its robustness. These\nexperiments allow us to test its reconstruction ability and compare its\nperformance with state-of-the-art intrinsic dimension estimators. The\nbenchmarks show good accuracy and high versatility of our approach.\nSubsequently, we apply our model to data generated from the numerical solution\nof a vertically resolved one-dimensional free-surface flow, following a\npointwise discretization of the vertical velocity profile in the horizontal\ndirection, vertical direction, and time. IDEA succeeds in estimating the\ndataset's intrinsic dimension and then reconstructs the original solution by\nworking directly within the projection space identified by the network.", "AI": {"tldr": "IDEA combines intrinsic dimension estimation with reconstruction in a latent space via re-weighted double CancelOut layers and a projected reconstruction loss; it works on linear/nonlinear manifolds and validates on benchmarks and a vertical free-surface flow dataset.", "motivation": "To robustly estimate the intrinsic dimension of manifold-structured data and enable accurate reconstruction by operating in a learned latent space, improving upon existing estimators.", "method": "An autoencoder variant (IDEA) with re-weighted double CancelOut layers; introduces a projected reconstruction loss that tests reconstruction quality when progressively removing latent dimensions; evaluated on theoretical benchmarks and a numerical flow dataset.", "result": "Demonstrates good accuracy and versatility in intrinsic dimension estimation and reconstruction; compares favorably with state-of-the-art estimators; successfully reconstructs the original solution from the latent projection space.", "conclusion": "IDEA provides a robust framework for intrinsic dimension estimation and data reconstruction across linear/nonlinear manifolds, with potential utility in complex physical datasets; projection-space training yields faithful reconstructions."}}
{"id": "2509.10401", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10401", "abs": "https://arxiv.org/abs/2509.10401", "authors": ["Alva West", "Yixuan Weng", "Minjun Zhu", "Zhen Lin", "Yue Zhang"], "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems", "comment": null, "summary": "Failure attribution in multi-agent systems -- pinpointing the exact step\nwhere a decisive error occurs -- is a critical yet unsolved challenge. Current\nmethods treat this as a pattern recognition task over long conversation logs,\nleading to critically low step-level accuracy (below 17\\%), which renders them\nimpractical for debugging complex systems. Their core weakness is a fundamental\ninability to perform robust counterfactual reasoning: to determine if\ncorrecting a single action would have actually averted the task failure. To\nbridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)\nScaffolding, a novel agent framework that transforms failure attribution from\npattern recognition into a structured causal inference task. A2P explicitly\nguides a large language model through a formal three-step reasoning process\nwithin a single inference pass: (1) Abduction, to infer the hidden root causes\nbehind an agent's actions; (2) Action, to define a minimal corrective\nintervention; and (3) Prediction, to simulate the subsequent trajectory and\nverify if the intervention resolves the failure. This structured approach\nleverages the holistic context of the entire conversation while imposing a\nrigorous causal logic on the model's analysis. Our extensive experiments on the\nWho\\&When benchmark demonstrate its efficacy. On the Algorithm-Generated\ndataset, A2P achieves 47.46\\% step-level accuracy, a 2.85$\\times$ improvement\nover the 16.67\\% of the baseline. On the more complex Hand-Crafted dataset, it\nachieves 29.31\\% step accuracy, a 2.43$\\times$ improvement over the baseline's\n12.07\\%. By reframing the problem through a causal lens, A2P Scaffolding\nprovides a robust, verifiable, and significantly more accurate solution for\nautomated failure attribution.", "AI": {"tldr": "A2P Scaffolding reframes failure attribution in multi-agent systems as a structured causal inference problem using abductive reasoning (Abduction-Action-Prediction) to guide LLMs, achieving notable step-level accuracy gains on benchmark datasets.", "motivation": "Failure attribution is currently treated as pattern recognition over long logs, suffering from very low step-level accuracy and lacking robust counterfactual reasoning to determine if a single action could have prevented failure.", "method": "Abduct-Act-Predict (A2P) scaffolding guides a large language model through a three-step reasoning process within one inference pass: Abduction (infer hidden causes), Action (define minimal corrective intervention), and Prediction (simulate trajectory to verify if the intervention prevents failure) while leveraging full conversation context and enforcing causal logic.", "result": "On the Algorithm-Generated dataset, A2P achieves 47.46% step-level accuracy, a 2.85\u00d7 improvement over the baseline 16.67%. On the Hand-Crafted dataset, 29.31% step accuracy, a 2.43\u00d7 improvement over 12.07% baseline.", "conclusion": "Framing failure attribution as a structured causal inference problem with A2P yields robust, verifiable, and significantly more accurate failure attribution for automated debugging in multi-agent systems."}}
{"id": "2509.09958", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09958", "abs": "https://arxiv.org/abs/2509.09958", "authors": ["Jeffrey Liu", "Rongbin Hu"], "title": "Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification", "comment": null, "summary": "Referring Expression Comprehension (REC) is usually addressed with\ntask-trained grounding models. We show that a zero-shot workflow, without any\nREC-specific training, can achieve competitive or superior performance. Our\napproach reformulates REC as box-wise visual-language verification: given\nproposals from a COCO-clean generic detector (YOLO-World), a general-purpose\nVLM independently answers True/False queries for each region. This simple\nprocedure reduces cross-box interference, supports abstention and multiple\nmatches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our\nmethod not only surpasses a zero-shot GroundingDINO baseline but also exceeds\nreported results for GroundingDINO trained on REC and GroundingDINO+CRG.\nControlled studies with identical proposals confirm that verification\nsignificantly outperforms selection-based prompting, and results hold with open\nVLMs. Overall, we show that workflow design, rather than task-specific\npretraining, drives strong zero-shot REC performance.", "AI": {"tldr": "A zero-shot REC approach uses per-region visual-language verification with a generic detector and VLM, achieving competitive or better results without REC-specific training.", "motivation": "REC has been dominated by task-specific grounding models requiring fine-tuning; explore whether a general, zero-shot workflow can match or surpass them by rethinking the problem as per-region verification.", "method": "Generate region proposals from a COCO-clean detector (YOLO-World). For each proposal, a general-purpose vision-language model answers True/False about whether the region matches the referring expression. No fine-tuning; supports abstention and multiple matches; reduces cross-box interference.", "result": "On RefCOCO, RefCOCO+, and RefCOCOg, the zero-shot verification approach surpasses a zero-shot GroundingDINO baseline and exceeds results reported for GroundingDINO trained on REC and GroundingDINO+CRG; controlled studies show verification outperforms selection-based prompting; works with open VLMs.", "conclusion": "Workflow design and prompt/verification strategy, rather than task-specific pretraining, are the primary drivers of strong zero-shot REC performance."}}
{"id": "2509.10025", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10025", "abs": "https://arxiv.org/abs/2509.10025", "authors": ["Strahinja Nikolic", "Ilker Oguz", "Demetri Psaltis"], "title": "Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts", "comment": "14 pages, 7 figures", "summary": "Understanding the internal organization of neural networks remains a\nfundamental challenge in deep learning interpretability. We address this\nchallenge by exploring a novel Sparse Mixture of Experts Variational\nAutoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw\ndataset, comparing unsupervised expert routing against a supervised baseline\nguided by ground-truth labels. Surprisingly, we find that unsupervised routing\nconsistently achieves superior reconstruction performance. The experts learn to\nidentify meaningful sub-categorical structures that often transcend\nhuman-defined class boundaries. Through t-SNE visualizations and reconstruction\nanalysis, we investigate how MoE models uncover fundamental data structures\nthat are more aligned with the model's objective than predefined labels.\nFurthermore, our study on the impact of dataset size provides insights into the\ntrade-offs between data quantity and expert specialization, offering guidance\nfor designing efficient MoE architectures.", "AI": {"tldr": "Unsupervised routing in SMoE-VAE on QuickDraw outperforms supervised routing, revealing meaningful sub-structures beyond human-defined classes; dataset size affects expert specialization and efficiency.", "motivation": "Understand how Sparse Mixture of Experts Variational Autoencoders organize internally and whether unsupervised routing can outperform label-guided baselines, while exploring how data quantity influences expert specialization.", "method": "Propose and evaluate a Sparse Mixture of Experts Variational Autoencoder (SMoE-VAE) on the QuickDraw dataset; compare unsupervised expert routing to a supervised baseline guided by ground-truth labels; use t-SNE visualizations and reconstruction analysis to probe learned structures; assess the impact of dataset size on expert specialization.", "result": "Unsupervised routing consistently achieves superior reconstruction performance compared to the supervised baseline; experts learn to identify meaningful sub-categorical structures that often transcend predefined class boundaries; t-SNE visualizations and reconstruction analyses reveal that MoE models uncover foundational data structures aligned more with the model objective than with fixed labels; increasing dataset size reveals trade-offs between data quantity and expert specialization, informing efficient MoE design.", "conclusion": "Unsupervised MoE routing is effective for uncovering intrinsic data structure and can guide the design of efficient MoE architectures by balancing data quantity with expert specialization, with implications for avoiding label-boundaries in representation learning."}}
{"id": "2509.10423", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10423", "abs": "https://arxiv.org/abs/2509.10423", "authors": ["Cameron Reid", "Wael Hafez", "Amirhossein Nazeri"], "title": "Mutual Information Tracks Policy Coherence in Reinforcement Learning", "comment": "10 pages, 4 figures, 1 table", "summary": "Reinforcement Learning (RL) agents deployed in real-world environments face\ndegradation from sensor faults, actuator wear, and environmental shifts, yet\nlack intrinsic mechanisms to detect and diagnose these failures. We present an\ninformation-theoretic framework that reveals both the fundamental dynamics of\nRL and provides practical methods for diagnosing deployment-time anomalies.\nThrough analysis of state-action mutual information patterns in a robotic\ncontrol task, we first demonstrate that successful learning exhibits\ncharacteristic information signatures: mutual information between states and\nactions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing\nstate entropy, indicating that agents develop increasingly selective attention\nto task-relevant patterns. Intriguingly, states, actions and next states joint\nmutual information, MI(S,A;S'), follows an inverted U-curve, peaking during\nearly learning before declining as the agent specializes suggesting a\ntransition from broad exploration to efficient exploitation. More immediately\nactionable, we show that information metrics can differentially diagnose system\nfailures: observation-space, i.e., states noise (sensor faults) produces broad\ncollapses across all information channels with pronounced drops in state-action\ncoupling, while action-space noise (actuator faults) selectively disrupts\naction-outcome predictability while preserving state-action relationships. This\ndifferential diagnostic capability demonstrated through controlled perturbation\nexperiments enables precise fault localization without architectural\nmodifications or performance degradation. By establishing information patterns\nas both signatures of learning and diagnostic for system health, we provide the\nfoundation for adaptive RL systems capable of autonomous fault detection and\npolicy adjustment based on information-theoretic principles.", "AI": {"tldr": "An information-theoretic framework to diagnose RL deployment failures via mutual-information patterns; demonstrates learning signatures and differential fault localization (sensor vs actuator faults) enabling autonomous fault diagnosis.", "motivation": "Real-world RL deployments face sensor faults, actuator wear, and environment shifts; existing methods lack intrinsic, principled failure detection and diagnosis.", "method": "Analyze state-action mutual information and MI(S,A;S') in a robotic control task; monitor MI growth during learning (0.84 to 2.83 bits, 238%); observe inverted U for MI(S,A;S'); perform controlled perturbations with observation-space and action-space noise to diagnose faults.", "result": "Learning exhibits increasing state-action information and an inverted-U MI(S,A;S') curve; observation-space faults degrade all information channels and reduce state-action coupling; action-space faults mainly disrupt action-outcome predictability while preserving state-action relations; differential diagnostics enable fault localization without architectural changes.", "conclusion": "Information patterns provide signatures of learning and system health, forming the foundation for adaptive RL with autonomous fault detection and policy adjustment based on information-theoretic principles."}}
{"id": "2509.10021", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.10021", "abs": "https://arxiv.org/abs/2509.10021", "authors": ["Jonas K\u00fchne", "Christian Vogt", "Michele Magno", "Luca Benini"], "title": "Efficient and Accurate Downfacing Visual Inertial Odometry", "comment": "This article has been accepted for publication in the IEEE Internet\n  of Things Journal (IoT-J)", "summary": "Visual Inertial Odometry (VIO) is a widely used computer vision method that\ndetermines an agent's movement through a camera and an IMU sensor. This paper\npresents an efficient and accurate VIO pipeline optimized for applications on\nmicro- and nano-UAVs. The proposed design incorporates state-of-the-art feature\ndetection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and\nquantized for emerging RISC-V-based ultra-low-power parallel systems on chips\n(SoCs). Furthermore, by employing a rigid body motion model, the pipeline\nreduces estimation errors and achieves improved accuracy in planar motion\nscenarios. The pipeline's suitability for real-time VIO is assessed on an\nultra-low-power SoC in terms of compute requirements and tracking accuracy\nafter quantization. The pipeline, including the three feature tracking methods,\nwas implemented on the SoC for real-world validation. This design bridges the\ngap between high-accuracy VIO pipelines that are traditionally run on\ncomputationally powerful systems and lightweight implementations suitable for\nmicrocontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates\nan average reduction in RMSE of up to a factor of 3.65x over the baseline\npipeline when using the ORB feature tracker. The analysis of the computational\ncomplexity of the feature trackers further shows that PX4FLOW achieves on-par\ntracking accuracy with ORB at a lower runtime for movement speeds below 24\npixels/frame.", "AI": {"tldr": "Ultra-low-power VIO pipeline for micro- and nano-UAVs on RISC-V SoCs, using quantized features (SuperPoint, PX4FLOW, ORB); achieves up to 3.65x RMSE improvement over baseline with ORB; PX4FLOW matches ORB at lower runtimes for speeds < 24 px/frame.", "motivation": "To enable real-time, high-accuracy ego-motion estimation on constrained hardware, bridging high-accuracy VIO pipelines and lightweight microcontroller implementations.", "method": "Integrates three feature trackers (SuperPoint, PX4FLOW, ORB) quantized and optimized for GAP9; uses a rigid body motion model to reduce drift, especially in planar motion; evaluates compute requirements and tracking accuracy after quantization on ultra-low-power SoC; real-world validation.", "result": "Average RMSE reduction up to 3.65x over baseline with ORB; PX4FLOW performs on-par tracking with ORB at lower runtime for speeds below 24 pixels/frame.", "conclusion": "Demonstrates feasibility of real-time, accurate VIO on ultra-low-power microcontrollers, enabling practical VIO for micro- and nano-UAVs with planar motion."}}
{"id": "2509.09961", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09961", "abs": "https://arxiv.org/abs/2509.09961", "authors": ["Tianqi Wei", "Xin Yu", "Zhi Chen", "Scott Chapman", "Zi Huang"], "title": "Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation", "comment": null, "summary": "Accurate segmentation of foliar diseases and insect damage in wheat is\ncrucial for effective crop management and disease control. However, the insect\ndamage typically occupies only a tiny fraction of annotated pixels. This\nextreme pixel-level imbalance poses a significant challenge to the segmentation\nperformance, which can result in overfitting to common classes and insufficient\nlearning of rare classes, thereby impairing overall performance. In this paper,\nwe propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to\naddress the pixel imbalance problem. Specifically, we extract rare\ninsect-damage patches from annotated training images and apply random geometric\ntransformations to simulate variations. The transformed patches are then pasted\nin appropriate regions while avoiding overlaps with lesions or existing damaged\nregions. In addition, we apply a random projection filter to the pasted\nregions, refining local features and ensuring a natural blend with the new\nbackground. Experiments show that our method substantially improves\nsegmentation performance on the insect damage class, while maintaining or even\nslightly enhancing accuracy on other categories. Our results highlight the\neffectiveness of targeted augmentation in mitigating extreme pixel imbalance,\noffering a straightforward yet effective solution for agricultural segmentation\nproblems.", "AI": {"tldr": "Proposes Random Projected Copy-and-Paste (RPCP) augmentation to tackle extreme pixel imbalance in wheat foliar disease segmentation by pasting transformed rare insect-damage patches with a projection filter, improving rare-class performance while preserving others.", "motivation": "Insect damage often occupies a tiny fraction of pixels, causing severe class imbalance that leads to overfitting on common classes and under-learning of rare ones; a targeted augmentation is needed.", "method": "Extract rare insect-damage patches from annotated training images; apply random geometric transformations; paste into valid regions avoiding overlaps with lesions or damaged areas; apply a random projection filter to refine features and blend with background.", "result": "RPCP substantially improves segmentation performance for the insect-damage class and maintains or slightly improves accuracy on other categories.", "conclusion": "Targeted augmentation via RPCP is effective for mitigating extreme pixel imbalance in agricultural segmentation and offers a simple yet practical solution."}}
{"id": "2509.10033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10033", "abs": "https://arxiv.org/abs/2509.10033", "authors": ["Boya Ma", "Abram Magner", "Maxwell McNeil", "Petko Bogdanov"], "title": "Sparse Coding Representation of 2-way Data", "comment": null, "summary": "Sparse dictionary coding represents signals as linear combinations of a few\ndictionary atoms. It has been applied to images, time series, graph signals and\nmulti-way spatio-temporal data by jointly employing temporal and spatial\ndictionaries. Data-agnostic analytical dictionaries, such as the discrete\nFourier transform, wavelets and graph Fourier, have seen wide adoption due to\nefficient implementations and good practical performance. On the other hand,\ndictionaries learned from data offer sparser and more accurate solutions but\nrequire learning of both the dictionaries and the coding coefficients. This\nbecomes especially challenging for multi-dictionary scenarios since encoding\ncoefficients correspond to all atom combinations from the dictionaries. To\naddress this challenge, we propose a low-rank coding model for 2-dictionary\nscenarios and study its data complexity. Namely, we establish a bound on the\nnumber of samples needed to learn dictionaries that generalize to unseen\nsamples from the same distribution. We propose a convex relaxation solution,\ncalled AODL, whose exact solution we show also solves the original problem. We\nthen solve this relaxation via alternating optimization between the sparse\ncoding matrices and the learned dictionaries, which we prove to be convergent.\nWe demonstrate its quality for data reconstruction and missing value imputation\nin both synthetic and real-world datasets. For a fixed reconstruction quality,\nAODL learns up to 90\\% sparser solutions compared to non-low-rank and\nanalytical (fixed) dictionary baselines. In addition, the learned dictionaries\nreveal interpretable insights into patterns present within the samples used for\ntraining.", "AI": {"tldr": "Low-rank sparse coding for 2-dictionaries (AODL) with a convex relaxation yields sparse, interpretable dictionaries and convergent alternating optimization, improving reconstruction and missing-value imputation.", "motivation": "Multi-dictionary sparse coding is combinatorially hard when coding coefficients span all atom combinations. There is a need for data-efficient learning bounds, sparser solutions, and joint learning of dictionaries and coefficients. The work aims to handle 2-dictionary scenarios, provide sample complexity guarantees, and offer a practical algorithm.", "method": "Introduce a low-rank coding model for two dictionaries. Derive a bound on the sample complexity for learning generalizable dictionaries. Propose a convex relaxation called AODL, prove its exact solution solves the original problem, and solve the relaxation via alternating optimization between sparse coding matrices and dictionaries, with convergence guarantees.", "result": "AODL achieves up to 90% sparser representations than non-low-rank or fixed-dictionary baselines for fixed reconstruction quality. It yields accurate data reconstruction and missing value imputation on synthetic and real data. The learned dictionaries also reveal interpretable patterns in training data.", "conclusion": "The proposed AODL framework provides a data-efficient, interpretable, and convergent approach to learning multi-dictionary sparse representations, with strong reconstruction/imputation performance and clear practical benefits."}}
{"id": "2509.09962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09962", "abs": "https://arxiv.org/abs/2509.09962", "authors": ["Anne Marthe Sophie Ngo Bibinbe", "Chiron Bang", "Patrick Gagnon", "Jamie Ahloy-Dallaire", "Eric R. Paquet"], "title": "An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock", "comment": "13 pages, 7 figures, 1 table, accepted at CVPR animal workshop 2024,\n  submitted to IJCV", "summary": "The need for long-term multi-object tracking (MOT) is growing due to the\ndemand for analyzing individual behaviors in videos that span several minutes.\nUnfortunately, due to identity switches between objects, the tracking\nperformance of existing MOT approaches decreases over time, making them\ndifficult to apply for long-term tracking. However, in many real-world\napplications, such as in the livestock sector, it is possible to obtain\nsporadic identifications for some of the animals from sources like feeders. To\naddress the challenges of long-term MOT, we propose a new framework that\ncombines both uncertain identities and tracking using a Hidden Markov Model\n(HMM) formulation. In addition to providing real-world identities to animals,\nour HMM framework improves the F1 score of ByteTrack, a leading MOT approach\neven with re-identification, on a 10 minute pig tracking dataset with 21\nidentifications at the pen's feeding station. We also show that our approach is\nrobust to the uncertainty of identifications, with performance increasing as\nidentities are provided more frequently. The improved performance of our HMM\nframework was also validated on the MOT17 and MOT20 benchmark datasets using\nboth ByteTrack and FairMOT. The code for this new HMM framework and the new\n10-minute pig tracking video dataset are available at:\nhttps://github.com/ngobibibnbe/uncertain-identity-aware-tracking", "AI": {"tldr": "An HMM-based framework to handle uncertain identities in long-term MOT, improving tracking by leveraging sporadic identifications; shows gains on a 10-minute pig dataset and MOT17/20 with ByteTrack/FairMOT, and provides code and data.", "motivation": "Long-term multi-object tracking suffers from identity switches over time. In real-world settings such as livestock monitoring, sporadic real IDs can be obtained (e.g., at feeders). A method that can integrate these uncertain identities into the tracking process is needed.", "method": "Proposes a Hidden Markov Model (HMM) framework that combines uncertain identity information with object tracking. The HMM models identity states over time and utilizes sporadic identifications to improve data association. The approach is evaluated as an enhancement to existing MOT trackers (e.g., ByteTrack, FairMOT) and is demonstrated on a 10-minute pig tracking dataset with feeder IDs (21 identifications) and on MOT17/MOT20 datasets.", "result": "The HMM framework improves the F1 score of ByteTrack on the pig dataset and with re-identification. It also yields robust performance on MOT17 and MOT20 when paired with ByteTrack and FairMOT. The method\u2019s performance improves as identifications become more frequent, indicating resilience to uncertain identities.", "conclusion": "An HMM-based uncertain-identity aware tracking framework effectively addresses identity switches in long-term MOT, boosting performance in real-world scenarios and benchmark datasets. The authors provide code and a new 10-minute pig tracking dataset for reproducibility."}}
{"id": "2509.10034", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10034", "abs": "https://arxiv.org/abs/2509.10034", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability", "comment": "19 pages, 2 figures", "summary": "We present a formal and constructive theory showing that probabilistic finite\nautomata (PFAs) can be exactly simulated using symbolic feedforward neural\nnetworks. Our architecture represents state distributions as vectors and\ntransitions as stochastic matrices, enabling probabilistic state propagation\nvia matrix-vector products. This yields a parallel, interpretable, and\ndifferentiable simulation of PFA dynamics using soft updates-without\nrecurrence. We formally characterize probabilistic subset construction,\n$\\varepsilon$-closure, and exact simulation via layered symbolic computation,\nand prove equivalence between PFAs and specific classes of neural networks. We\nfurther show that these symbolic simulators are not only expressive but\nlearnable: trained with standard gradient descent-based optimization on labeled\nsequence data, they recover the exact behavior of ground-truth PFAs. This\nlearnability, formalized in Proposition 5.1, is the crux of this work. Our\nresults unify probabilistic automata theory with neural architectures under a\nrigorous algebraic framework, bridging the gap between symbolic computation and\ndeep learning.", "AI": {"tldr": "The paper proves that probabilistic finite automata (PFAs) can be exactly simulated by a class of symbolic feedforward neural networks, using a vectorized representation of states and stochastic matrices for transitions. This yields a differentiable, non-recurrent simulator with exact probabilistic behavior and a learnability result (Prop. 5.1).", "motivation": "To unify probabilistic automata theory with neural networks and provide an interpretable, parallel, differentiable platform that preserves exact PFA behavior while enabling learning from data.", "method": "Represent PFAs as state-distribution vectors and stochastic transition matrices; perform probabilistic state propagation via matrix\u2013vector products in a layered feedforward network; develop probabilistic subset construction and \u03b5-closure within a symbolic computation framework; prove exact simulation and equivalence to PFAs; train the symbolic simulators by gradient descent on labeled sequence data to recover ground-truth PFAs.", "result": "Formal characterization of probabilistic subset construction, \u03b5-closure, and exact simulation; demonstration of equivalence between PFAs and certain neural networks; Proposition 5.1 establishing learnability, showing gradient-based training recovers exact PFA behavior.", "conclusion": "Unifies probabilistic automata theory with neural architectures under an algebraic framework, bridging symbolic computation and deep learning, and enabling interpretable, learnable probabilistic computation in neural models."}}
{"id": "2509.09971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09971", "abs": "https://arxiv.org/abs/2509.09971", "authors": ["Aupendu Kar", "Vishnu Raj", "Guan-Ming Su"], "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey", "comment": null, "summary": "Event camera sensors are bio-inspired sensors which asynchronously capture\nper-pixel brightness changes and output a stream of events encoding the\npolarity, location and time of these changes. These systems are witnessing\nrapid advancements as an emerging field, driven by their low latency, reduced\npower consumption, and ultra-high capture rates. This survey explores the\nevolution of fusing event-stream captured with traditional frame-based capture,\nhighlighting how this synergy significantly benefits various video restoration\nand 3D reconstruction tasks. The paper systematically reviews major deep\nlearning contributions to image/video enhancement and restoration, focusing on\ntwo dimensions: temporal enhancement (such as frame interpolation and motion\ndeblurring) and spatial enhancement (including super-resolution, low-light and\nHDR enhancement, and artifact reduction). This paper also explores how the 3D\nreconstruction domain evolves with the advancement of event driven fusion.\nDiverse topics are covered, with in-depth discussions on recent works for\nimproving visual quality under challenging conditions. Additionally, the survey\ncompiles a comprehensive list of openly available datasets, enabling\nreproducible research and benchmarking. By consolidating recent progress and\ninsights, this survey aims to inspire further research into leveraging event\ncamera systems, especially in combination with deep learning, for advanced\nvisual media restoration and enhancement.", "AI": {"tldr": "A survey of fusing event-camera streams with frame-based video for restoration and 3D reconstruction, covering temporal and spatial DL-enhancement tasks, datasets, and benchmarks.", "motivation": "Event cameras offer high temporal resolution, low latency, and sparse data that complement conventional frames. Fusing these modalities can improve robustness and quality in challenging conditions (motion, low light, HDR) and enable accurate 3D reconstruction.", "method": "Systematic literature review of deep-learning methods for event+frame fusion, categorized into temporal enhancement (frame interpolation, deblurring) and spatial enhancement (super-resolution, low-light/HDR, artifact reduction), plus discussion of 3D reconstruction and a compiled list of open datasets for reproducibility.", "result": "Synthesis of current progress highlights effective fusion architectures, training strategies, and evaluation practices. Identifies trends and gaps, including the need for standardized benchmarks and abundant datasets. Demonstrates improved visual quality under challenging conditions and advancing 3D reconstruction with event-driven fusion.", "conclusion": "Deep learning-enabled fusion of event streams with frame-based data is a promising direction for robust visual restoration and 3D reconstruction. Future work should emphasize standardized benchmarks, richer datasets, real-time processing, and cross-modal generalization to accelerate practical deployment."}}
{"id": "2509.10041", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10041", "abs": "https://arxiv.org/abs/2509.10041", "authors": ["Mohammad Hasan Narimani", "Mostafa Tavassolipour"], "title": "FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection", "comment": null, "summary": "Federated learning (FL) offers an innovative paradigm for collaborative model\ntraining across decentralized devices, such as smartphones, balancing enhanced\npredictive performance with the protection of user privacy in sensitive areas\nlike Internet of Things (IoT) and medical data analysis. Despite its\nadvantages, FL encounters significant challenges related to user privacy\nprotection against potential attacks and the management of communication costs.\nThis paper introduces a novel federated learning algorithm called FedRP, which\nintegrates random projection techniques with the Alternating Direction Method\nof Multipliers (ADMM) optimization framework. This approach enhances privacy by\nemploying random projection to reduce the dimensionality of model parameters\nprior to their transmission to a central server, reducing the communication\ncost. The proposed algorithm offers a strong $(\\epsilon, \\delta)$-differential\nprivacy guarantee, demonstrating resilience against data reconstruction\nattacks. Experimental results reveal that FedRP not only maintains high model\naccuracy but also outperforms existing methods, including conventional\ndifferential privacy approaches and FedADMM, in terms of both privacy\npreservation and communication efficiency.", "AI": {"tldr": "FedRP blends random projection with ADMM in federated learning to cut communication costs while offering strong (\u03b5, \u03b4)-differential privacy, outperforming DP baselines and FedADMM.", "motivation": "To protect user privacy in federated learning against reconstruction attacks while reducing communication overhead, especially in sensitive domains like IoT and medical data.", "method": "Integrates random projection to compress model parameters before transmission to a central server, within an ADMM optimization framework, and provides a formal (\u03b5, \u03b4)-differential privacy guarantee.", "result": "Experimental results show high model accuracy is preserved and FedRP outperforms existing methods, including conventional DP approaches and FedADMM, in privacy preservation and communication efficiency.", "conclusion": "FedRP is a promising approach for privacy-preserving, communication-efficient federated learning, offering strong DP guarantees and superior performance; further work could explore scalability and deployment in real-world systems."}}
{"id": "2509.09977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09977", "abs": "https://arxiv.org/abs/2509.09977", "authors": ["Siying Liu", "Zikai Wang", "Hanle Zheng", "Yifan Hu", "Xilin Wang", "Qingkai Yang", "Jibin Wu", "Hao Guo", "Lei Deng"], "title": "ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking", "comment": "15 pages, 8 figures", "summary": "RGB-Event tracking has become a promising trend in visual object tracking to\nleverage the complementary strengths of both RGB images and dynamic spike\nevents for improved performance. However, existing artificial neural networks\n(ANNs) struggle to fully exploit the sparse and asynchronous nature of event\nstreams. Recent efforts toward hybrid architectures combining ANNs and spiking\nneural networks (SNNs) have emerged as a promising solution in RGB-Event\nperception, yet effectively fusing features across heterogeneous paradigms\nremains a challenge. In this work, we propose ISTASTrack, the first\ntransformer-based \\textbf{A}NN-\\textbf{S}NN hybrid \\textbf{Track}er equipped\nwith \\textbf{ISTA} adapters for RGB-Event tracking. The two-branch model\nemploys a vision transformer to extract spatial context from RGB inputs and a\nspiking transformer to capture spatio-temporal dynamics from event streams. To\nbridge the modality and paradigm gap between ANN and SNN features, we\nsystematically design a model-based ISTA adapter for bidirectional feature\ninteraction between the two branches, derived from sparse representation theory\nby unfolding the iterative shrinkage thresholding algorithm. Additionally, we\nincorporate a temporal downsampling attention module within the adapter to\nalign multi-step SNN features with single-step ANN features in the latent\nspace, improving temporal fusion. Experimental results on RGB-Event tracking\nbenchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that\nISTASTrack achieves state-of-the-art performance while maintaining high energy\nefficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN\ndesigns for robust visual tracking. The code is publicly available at\nhttps://github.com/lsying009/ISTASTrack.git.", "AI": {"tldr": "ISTASTrack is a transformer-based ANN-SNN hybrid tracker for RGB-Event data that uses ISTA adapters to fuse ANN and SNN features, achieving state-of-the-art accuracy with energy efficiency.", "motivation": "RGB-event tracking benefits from complementary RGB spatial context and event-driven temporal information, but fusing ANN and SNN features across heterogeneous paradigms is challenging; a principled bidirectional interaction is needed.", "method": "Two-branch architecture: vision transformer processes RGB; spiking transformer processes event streams. Introduce a model-based ISTA adapter (unfolded ISTA) for bidirectional feature exchange between ANN and SNN branches, plus a temporal downsampling attention module to align multi-step SNN features with single-step ANN features in latent space.", "result": "Achieves state-of-the-art results on FE240hz, VisEvent, COESOT, FELT benchmarks; improves robustness and energy efficiency; code released.", "conclusion": "Demonstrates effectiveness of hybrid ANN-SNN designs for RGB-Event tracking and provides a practical fusion mechanism via ISTA adapters for improved performance and efficiency."}}
{"id": "2509.10048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10048", "abs": "https://arxiv.org/abs/2509.10048", "authors": ["Madhushan Ramalingam"], "title": "Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data", "comment": null, "summary": "Predictive models are being increasingly used across a wide range of domains,\nincluding safety-critical applications such as medical diagnosis and criminal\njustice. Reliable uncertainty estimation is a crucial task in such settings.\nTabular Prior-data Fitted Network (TabPFN) is a recently proposed machine\nlearning foundation model for tabular dataset, which uses a generative\ntransformer architecture. Variational Bayesian Last Layers (VBLL) is a\nstate-of-the-art lightweight variational formulation that effectively improves\nuncertainty estimation with minimal computational overhead. In this work we aim\nto evaluate the performance of VBLL integrated with the recently proposed\nTabPFN in uncertainty calibration. Our experiments, conducted on three\nbenchmark medical tabular datasets, compare the performance of the original\nTabPFN and the VBLL-integrated version. Contrary to expectations, we observed\nthat original TabPFN consistently outperforms VBLL integrated TabPFN in\nuncertainty calibration across all datasets.", "AI": {"tldr": "Original TabPFN outperforms VBLL-integrated TabPFN in uncertainty calibration across three medical tabular datasets, despite VBLL's aim to improve calibration with minimal overhead.", "motivation": "To assess whether a lightweight variational last-layer (VBLL) improves calibration of TabPFN, a generative transformer model for tabular data, on medical datasets.", "method": "Compare TabPFN and TabPFN+VBLL on three benchmark medical tabular datasets; evaluate uncertainty calibration using standard metrics (e.g., ECE, reliability diagrams, Brier score) with the same data splits; control for compute overhead by measuring resource use.", "result": "VBLL-integrated TabPFN does not improve uncertainty calibration and is consistently worse than the original TabPFN across all three datasets.", "conclusion": "VBLL offers little to no benefit for uncertainty calibration in TabPFN for these medical tabular tasks; TabPFN's baseline calibration remains superior; future work could explore alternative uncertainty techniques or data-domain-specific calibrations."}}
{"id": "2509.09988", "categories": ["cs.CV", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2509.09988", "abs": "https://arxiv.org/abs/2509.09988", "authors": ["Yusuke Takagi", "Shunya Nagashima", "Komei Sugiura"], "title": "FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction", "comment": "Accepted for presentation at ICONIP2025", "summary": "Accurate and reliable solar flare predictions are essential to mitigate\npotential impacts on critical infrastructure. However, the current performance\nof solar flare forecasting is insufficient. In this study, we address the task\nof predicting the class of the largest solar flare expected to occur within the\nnext 72 hours. Existing methods often fail to adequately address the severe\nclass imbalance across flare classes. To address this issue, we propose a solar\nflare prediction model based on multiple deep state space models. In addition,\nwe introduce the frequency & local-boundary-aware reliability loss (FLARE loss)\nto improve predictive performance and reliability under class imbalance.\nExperiments were conducted on a multi-wavelength solar image dataset covering a\nfull 11-year solar activity cycle. As a result, our method outperformed\nbaseline approaches in terms of both the Gandin-Murphy-Gerrity score and the\ntrue skill statistic, which are standard metrics in terms of the performance\nand reliability.", "AI": {"tldr": "A deep state-space forecasting framework with FLARE loss for imbalanced solar flare classification, improving 72-hour largest-flare prediction; outperforms baselines on GMG score and TSS.", "motivation": "Solar flare forecasting suffers from severe class imbalance and reliability concerns, impacting infrastructure protection and operational decision-making.", "method": "Propose multiple deep state-space models and introduce the frequency & local-boundary-aware reliability loss (FLARE loss) to address imbalance. Train on a multi-wavelength solar image dataset spanning 11 solar activity years.", "result": "The method outperforms baselines in the Gandin-Murphy-Gerrity score and the true skill statistic, indicating improved performance and reliability.", "conclusion": "The proposed approach enhances accuracy and reliability of solar flare forecasts under class imbalance and is validated on long-term, multi-wavelength solar data."}}
{"id": "2509.10089", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10089", "abs": "https://arxiv.org/abs/2509.10089", "authors": ["Marco Andrea B\u00fchler", "Gonzalo Guill\u00e9n-Gos\u00e1lbez"], "title": "KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework", "comment": null, "summary": "We introduce a novel symbolic regression framework, namely KAN-SR, built on\nKolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.\nSymbolic regression searches for mathematical equations that best fit a given\ndataset and is commonly solved with genetic programming approaches. We show\nthat by using deep learning techniques, more specific KANs, and combining them\nwith simplification strategies such as translational symmetries and\nseparabilities, we are able to recover ground-truth equations of the Feynman\nSymbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we\nshow that by combining the proposed framework with neural controlled\ndifferential equations, we are able to model the dynamics of an in-silico\nbioprocess system precisely, opening the door for the dynamic modeling of other\nengineering systems.", "AI": {"tldr": "Introduces KAN-SR, a Kolmogorov Arnold Network-based symbolic regression framework that uses divide-and-conquer, deep learning, and symmetry-based simplifications to recover ground-truth equations and enable dynamic modeling via neural controlled differential equations.", "motivation": "To improve symbolic regression accuracy and interpretability by integrating structured mathematical decompositions (KANs) with learning-based search and simplification, enabling exact equation recovery and dynamic system modeling.", "method": "Develops KAN-SR built on Kolmogorov Arnold Networks with a divide-and-conquer strategy; leverages deep learning-enhanced KANs and simplification heuristics such as translational symmetry and separability; validates by recovering SRSD ground-truth equations and by coupling with neural controlled differential equations to model an in-silico bioprocess.", "result": "Successfully recovers ground-truth equations on the Feynman SRSD dataset; demonstrates precise dynamic modeling of an in-silico bioprocess when combined with neural CDEs.", "conclusion": "KAN-SR shows promise for accurate symbolic regression and dynamic system modeling in engineering, suggesting broad applicability to discovering governing equations and enabling dynamic simulations."}}
{"id": "2509.10005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10005", "abs": "https://arxiv.org/abs/2509.10005", "authors": ["Xiaodong Guo", "Tong Liu", "Yike Li", "Zi'ang Lin", "Zhihong Deng"], "title": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion", "comment": null, "summary": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI.", "AI": {"tldr": "Proposes TUNI, an RGB-T encoder that fuses multi-modal feature extraction and cross-modal fusion in a single architecture; reduces thermal branch, uses local module with adaptive cosine similarity; achieves competitive accuracy with fewer params and faster speed, real-time on Jetson Orin NX.", "motivation": "Current RGB-T semantic segmentation relies on encoders pre-trained on RGB with extra fusion modules, leading to limited thermal feature extraction, suboptimal cross-modal fusion, and reduced real-time efficiency. A unified, compact architecture that leverages multi-modal pre-training and enhances local cross-modal fusion is needed.", "method": "A multi-block RGB-T encoder that simultaneously performs feature extraction and cross-modal fusion. The model is pre-trained on RGB and pseudo-thermal data to learn unified feature extraction and fusion. The thermal branch is slimmed to create a compact architecture. An RGB-T local module using adaptive cosine similarity emphasizes salient cross-modal local features for fusion.", "result": "Achieves competitive performance with state-of-the-art models on FMB, PST900, and CART datasets while using fewer parameters and lower computational cost. Delivers real-time inference at 27 FPS on a Jetson Orin NX.", "conclusion": "The work demonstrates that a unified RGB-T encoder with a dedicated local fusion module can match or exceed existing methods in performance while improving efficiency and enabling real-time deployment, validating the benefits of joint feature extraction and fusion across modalities."}}
{"id": "2509.10132", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10132", "abs": "https://arxiv.org/abs/2509.10132", "authors": ["Nour Jamoussi", "Giuseppe Serra", "Photios A. Stavrou", "Marios Kountouris"], "title": "Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning", "comment": null, "summary": "Bayesian Federated Learning (BFL) combines uncertainty modeling with\ndecentralized training, enabling the development of personalized and reliable\nmodels under data heterogeneity and privacy constraints. Existing approaches\ntypically rely on Markov Chain Monte Carlo (MCMC) sampling or variational\ninference, often incorporating personalization mechanisms to better adapt to\nlocal data distributions. In this work, we propose an information-geometric\nprojection framework for personalization in parametric BFL. By projecting the\nglobal model onto a neighborhood of the user's local model, our method enables\na tunable trade-off between global generalization and local specialization.\nUnder mild assumptions, we show that this projection step is equivalent to\ncomputing a barycenter on the statistical manifold, allowing us to derive\nclosed-form solutions and achieve cost-free personalization. We apply the\nproposed approach to a variational learning setup using the Improved\nVariational Online Newton (IVON) optimizer and extend its application to\ngeneral aggregation schemes in BFL. Empirical evaluations under heterogeneous\ndata distributions confirm that our method effectively balances global and\nlocal performance with minimal computational overhead.", "AI": {"tldr": "An information-geometric projection framework for personalization in parametric Bayesian Federated Learning, using projection to a local neighborhood around the user model to balance global generalization and local specialization. It yields closed-form, cost-free personalization by equivalence to a barycenter on the statistical manifold, implemented within a variational IVON setup and adaptable to various aggregation schemes. Empirically, it achieves good global/local performance with low overhead.", "motivation": "Federated learning with non-iid data demands models that are both uncertainty-aware and personalized. Traditional MCMC or VI-based personalization can be computationally intensive; there is a need for a tunable global-local trade-off with minimal overhead.", "method": "Project the global model onto a neighborhood of the user's local model on a statistical manifold; show that this projection is equivalent to computing a barycenter, enabling closed-form solutions for personalization. Implement within a variational learning framework using the Improved Variational Online Newton (IVON) optimizer and extend to general aggregation schemes in BFL.", "result": "Empirical evaluations under heterogeneous data distributions demonstrate that the proposed projection-based personalization effectively balances global generalization and local specialization with minimal computational overhead.", "conclusion": "The information-geometric projection framework enables cost-free, tunable personalization in Bayesian Federated Learning and is compatible with various aggregation schemes, offering practical efficiency and robust performance in non-iid settings."}}
{"id": "2509.10006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10006", "abs": "https://arxiv.org/abs/2509.10006", "authors": ["Masaki Akiba", "Shumpei Takezaki", "Daichi Haraguchi", "Seiichi Uchida"], "title": "Few-Part-Shot Font Generation", "comment": "ICDAR 2025 Workshop on Machine Learning", "summary": "This paper proposes a novel model of few-part-shot font generation, which\ndesigns an entire font based on a set of partial design elements, i.e., partial\nshapes. Unlike conventional few-shot font generation, which requires entire\ncharacter shapes for a couple of character classes, our approach only needs\npartial shapes as input. The proposed model not only improves the efficiency of\nfont creation but also provides insights into how partial design details\ninfluence the entire structure of the individual characters.", "AI": {"tldr": "A novel few-part-shot font-generation model that synthesizes entire font glyphs from partial shapes, enabling efficient font creation and revealing how partial design details influence whole-character structure.", "motivation": "Improve data efficiency in font generation by reducing reliance on complete glyphs and gain insight into how partial design elements constrain and shape full characters.", "method": "Introduce a model that takes partial shapes as input and learns to generate a full font set, capturing relationships between partial design elements and complete glyphs across characters.", "result": "The abstract proposes the model and its efficiency/insight benefits but does not report empirical results or quantifiable outcomes.", "conclusion": "Partial design elements can guide the synthesis of full glyphs, enabling more efficient font creation and offering deeper understanding of design dependencies across a font."}}
{"id": "2509.10151", "categories": ["cs.LG", "cs.AI", "I.2.1"], "pdf": "https://arxiv.org/pdf/2509.10151", "abs": "https://arxiv.org/abs/2509.10151", "authors": ["Riccardo Lunelli", "Angus Nicolson", "Samuel Martin Pr\u00f6ll", "Sebastian Johannes Reinstadler", "Axel Bauer", "Clemens Dlaska"], "title": "BenchECG and xECG: a benchmark and baseline for ECG foundation models", "comment": "32 pages, 4 figures, 22 tables", "summary": "Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to\ndeep learning. Recently, interest has grown in developing foundation models for\nECGs - models that generalise across diverse downstream tasks. However,\nconsistent evaluation has been lacking: prior work often uses narrow task\nselections and inconsistent datasets, hindering fair comparison. Here, we\nintroduce BenchECG, a standardised benchmark comprising a comprehensive suite\nof publicly available ECG datasets and versatile tasks. We also propose xECG,\nan xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,\nwhich achieves the best BenchECG score compared to publicly available\nstate-of-the-art models. In particular, xECG is the only publicly available\nmodel to perform strongly on all datasets and tasks. By standardising\nevaluation, BenchECG enables rigorous comparison and aims to accelerate\nprogress in ECG representation learning. xECG achieves superior performance\nover earlier approaches, defining a new baseline for future ECG foundation\nmodels.", "AI": {"tldr": "BenchECG standardizes ECG evaluation with a comprehensive public benchmark; xECG, an xLSTM model trained with SimDINOv2 self-supervised learning, achieves state-of-the-art performance across all datasets and tasks, setting a new baseline for ECG foundation models.", "motivation": "Inconsistent evaluation across ECG foundation models hinders fair comparison and progress. A unified, comprehensive benchmark is needed to reliably assess generalization across diverse downstream tasks and datasets.", "method": "Introduce BenchECG, a standardized suite of publicly available ECG datasets with flexible task definitions; propose xECG, an xLSTM-based recurrent model trained via SimDINOv2 self-supervised learning to maximize cross-task performance.", "result": "xECG attains the best BenchECG score among publicly available models and is the only public model that performs well across all datasets and tasks.", "conclusion": "Standardized benchmarks like BenchECG enable rigorous cross-model comparisons and establish a robust baseline for ECG foundation models, accelerating progress in ECG representation learning."}}
{"id": "2509.10161", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.10161", "abs": "https://arxiv.org/abs/2509.10161", "authors": ["Shiwei Li", "Qunwei Li", "Haozhao Wang", "Ruixuan Li", "Jianbin Lin", "Wenliang Zhong"], "title": "FedBiF: Communication-Efficient Federated Learning via Bits Freezing", "comment": "Accepted by TPDS", "summary": "Federated learning (FL) is an emerging distributed machine learning paradigm\nthat enables collaborative model training without sharing local data. Despite\nits advantages, FL suffers from substantial communication overhead, which can\naffect training efficiency. Recent efforts have mitigated this issue by\nquantizing model updates to reduce communication costs. However, most existing\nmethods apply quantization only after local training, introducing quantization\nerrors into the trained parameters and potentially degrading model accuracy. In\nthis paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework\nthat directly learns quantized model parameters during local training. In each\ncommunication round, the server first quantizes the model parameters and\ntransmits them to the clients. FedBiF then allows each client to update only a\nsingle bit of the multi-bit parameter representation, freezing the remaining\nbits. This bit-by-bit update strategy reduces each parameter update to one bit\nwhile maintaining high precision in parameter representation. Extensive\nexperiments are conducted on five widely used datasets under both IID and\nNon-IID settings. The results demonstrate that FedBiF not only achieves\nsuperior communication compression but also promotes sparsity in the resulting\nmodels. Notably, FedBiF attains accuracy comparable to FedAvg, even when using\nonly 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.\nThe code is available at https://github.com/Leopold1423/fedbif-tpds25.", "AI": {"tldr": "FedBiF enables bit-by-bit updates in federated learning to quantize model parameters during local training, achieving high communication compression (1 bpp uplink, 3 bpp downlink) while maintaining accuracy comparable to FedAvg and inducing sparsity.", "motivation": "Federated learning suffers from high communication costs. Existing post-training quantization introduces quantization errors and can degrade accuracy. There is a need to integrate quantization into the training process and reduce per-round data transfers.", "method": "At each communication round, the server quantizes the global model and sends it to clients. Each client then updates only one bit of each parameter (a bit-by-bit update), freezing all other bits. This preserves a quantized representation throughout training and reduces the update to a single bit per parameter per round.", "result": "Extensive experiments on five widely used datasets under IID and Non-IID settings show FedBiF achieves superior communication compression and promotes sparsity while maintaining accuracy close to FedAvg. Remarkably, 1 bpp uplink and 3 bpp downlink achieve competitive performance.", "conclusion": "FedBiF provides an effective framework for integrating quantization into local training in FL, enabling substantial communication savings without sacrificing model accuracy and inducing sparsity. The approach is validated across diverse datasets and settings, with released code."}}
{"id": "2509.10024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10024", "abs": "https://arxiv.org/abs/2509.10024", "authors": ["Danling Cao"], "title": "Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images", "comment": "This work was completed during the author's MPhil studies at the\n  University of Manchester", "summary": "Recovering 3D face models from 2D in-the-wild images has gained considerable\nattention in the computer vision community due to its wide range of potential\napplications. However, the lack of ground-truth labeled datasets and the\ncomplexity of real-world environments remain significant challenges. In this\nchapter, we propose a convolutional neural network-based approach, the\nHierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face\nmodels from single in-the-wild images. Our model predicts detailed facial\ngeometry, texture, pose, and illumination parameters from a single image.\nSpecifically, we employ a pre-trained hierarchical backbone network and\nintroduce multi-level attention mechanisms at different stages of 2D face image\nfeature extraction. A semi-supervised training strategy is employed,\nincorporating 3D Morphable Model (3DMM) parameters from publicly available\ndatasets along with a differentiable renderer, enabling an end-to-end training\nprocess. Extensive experiments, including both comparative and ablation\nstudies, were conducted on two benchmark datasets, AFLW2000-3D and MICC\nFlorence, focusing on 3D face reconstruction and 3D face alignment tasks. The\neffectiveness of the proposed method was evaluated both quantitatively and\nqualitatively.", "AI": {"tldr": "Proposes MLANet, a CNN-based hierarchical multi-level attention network for single-image 3D face reconstruction, trained semi-supervised with 3DMMs and differentiable rendering; evaluated on AFLW2000-3D and MICC Florence, with ablations and qualitative/quantitative results.", "motivation": "Addresses the lack of ground-truth 3D facial data and the challenge of reconstructing accurate 3D face models in unconstrained environments from a single image.", "method": "A CNN-based Hierarchical Multi-Level Attention Network (MLANet) with a pre-trained hierarchical backbone and multi-level attention at different stages of 2D face feature extraction. Training is semi-supervised, leveraging 3D Morphable Model (3DMM) parameters from public datasets and a differentiable renderer to enable end-to-end optimization of geometry, texture, pose, and illumination from a single image.", "result": "Extensive experiments including comparisons and ablations on two benchmarks (AFLW2000-3D and MICC Florence) demonstrate the effectiveness of the proposed approach for 3D face reconstruction and 3D face alignment, with both quantitative and qualitative evaluations.", "conclusion": "MLANet demonstrates the effectiveness of hierarchical attention and semi-supervised learning for accurate 3D face reconstruction from single in-the-wild images. The approach achieves promising results on standard benchmarks and highlights the benefit of integrating multi-level attention with 3DMM-based supervision and differentiable rendering."}}
{"id": "2509.10163", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.10163", "abs": "https://arxiv.org/abs/2509.10163", "authors": ["Francisco Javier Esono Nkulu Andong", "Qi Min"], "title": "Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks", "comment": null, "summary": "As sixth-generation (6G) networks move toward ultra-dense, intelligent edge\nenvironments, efficient resource management under stringent privacy, mobility,\nand energy constraints becomes critical. This paper introduces a novel\nFederated Multi-Agent Reinforcement Learning (Fed-MARL) framework that\nincorporates cross-layer orchestration of both the MAC layer and application\nlayer for energy-efficient, privacy-preserving, and real-time resource\nmanagement across heterogeneous edge devices. Each agent uses a Deep Recurrent\nQ-Network (DRQN) to learn decentralized policies for task offloading, spectrum\naccess, and CPU energy adaptation based on local observations (e.g., queue\nlength, energy, CPU usage, and mobility). To protect privacy, we introduce a\nsecure aggregation protocol based on elliptic curve Diffie Hellman key\nexchange, which ensures accurate model updates without exposing raw data to\nsemi-honest adversaries. We formulate the resource management problem as a\npartially observable multi-agent Markov decision process (POMMDP) with a\nmulti-objective reward function that jointly optimizes latency, energy\nefficiency, spectral efficiency, fairness, and reliability under 6G-specific\nservice requirements such as URLLC, eMBB, and mMTC. Simulation results\ndemonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines\nin task success rate, latency, energy efficiency, and fairness, while ensuring\nrobust privacy protection and scalability in dynamic, resource-constrained 6G\nedge networks.", "AI": {"tldr": "A Fed-MARL framework with DRQN-based agents, cross-layer MAC/app orchestration, and privacy-preserving secure aggregation improves energy-efficient, low-latency resource management in 6G edge networks compared to centralized MARL and heuristics.", "motivation": "Efficient resource management under privacy, mobility, and energy constraints in ultra-dense 6G edge environments is critical, especially to support URLLC, eMBB, and mMTC services.", "method": "Decentralized agents using Deep Recurrent Q-Networks (DRQNs) for task offloading, spectrum access, and CPU energy adaptation; cross-layer orchestration of MAC and application layers; POMMDP formulation with a multi-objective reward; secure aggregation via elliptic curve Diffie-Hellman to protect privacy while sharing model updates.", "result": "Fed-MARL outperforms centralized MARL and heuristic baselines in task success rate, latency, energy efficiency, and fairness; maintains robust privacy protection and scalable performance in dynamic, resource-constrained 6G edge networks.", "conclusion": "A cross-layer, privacy-preserving federated multi-agent RL framework (Fed-MARL) is effective for real-time, energy-efficient resource management in 6G edge networks, balancing latency, energy, spectrum, fairness, and reliability under URLLC/eMBB/mMTC requirements."}}
{"id": "2509.10026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10026", "abs": "https://arxiv.org/abs/2509.10026", "authors": ["Jing Huang", "Zhiya Tan", "Shutao Gong", "Fanwei Zeng", "Jianshu Li"], "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA", "comment": "12 Pages, 12 Figures, 2 Tables", "summary": "As large vision language models (VLMs) advance, their capabilities in\nmultilingual visual question answering (mVQA) have significantly improved.\nChain-of-thought (CoT) reasoning has been proven to enhance interpretability\nand complex reasoning. However, most existing approaches rely primarily on\ntextual CoT and provide limited support for multilingual multimodal reasoning,\nconstraining their deployment in real-world applications. To address this gap,\nwe introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework\nwith Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable\nmulti-stage reasoning pipeline consisting of Text Summary with Bounding Box\n(BBox), Language Identification, Spatial Object-level Captioning, and\nStep-by-step Logical Reasoning. Following this reasoning pipeline, we design an\nautomated data curation method that generates multilingual CoT annotations\nthrough iterative generation, correction, and refinement, enabling scalable and\nhigh-quality training data. To improve reasoning and generalization, LaV-CoT\nadopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)\nwith Language-aware Group Relative Policy Optimization (GRPO), guided by\nverifiable multi-aspect rewards including language consistency, structural\naccuracy, and semantic alignment. Extensive evaluations on public datasets\nincluding MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up\nto \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar\nsize and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%.\nMoreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513\nand Gemini-2.5-flash. We further conducted an online A/B test to validate our\nmethod on real-world data, highlighting its effectiveness for industrial\ndeployment. Our code is available at this link:\n\\href{https://github.com/HJNVR/LaV-CoT}", "AI": {"tldr": "LaV-CoT is a multilingual visual CoT framework that combines a multi-stage reasoning pipeline with multi-aspect reward optimization to improve multilingual VQA performance.", "motivation": "To enable robust multilingual multimodal reasoning with interpretable chain-of-thought, addressing limitations of text-only CoT and real-world deployment constraints; scalable multilingual CoT data and effective training strategy.", "method": "A multi-stage reasoning pipeline (Text Summary with Bounding Box, Language Identification, Spatial Object-level Captioning, Step-by-step Logical Reasoning); automated multilingual CoT data curation via iterative generation/correction/refinement; two-stage training (SFT + Language-aware GRPO) with rewards: language consistency, structural accuracy, semantic alignment.", "result": "Significant accuracy gains on MMMB, Multilingual MMBench, MTVQA (up to ~9.5% vs open-source baselines of similar size; outperforming models with 2x size by ~2.6%); competitive with GPT-4o-0513 and Gemini-2.5-flash; positive online A/B test for deployment.", "conclusion": "LaV-CoT advances multilingual visual CoT by integrating language-aware reasoning, scalable data curation, and reward-guided optimization, enabling stronger performance and deployment readiness; code released."}}
{"id": "2509.10164", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.10164", "abs": "https://arxiv.org/abs/2509.10164", "authors": ["Hoshitaro Ohnishi", "Hideo Mukai"], "title": "A Symmetry-Integrated Approach to Surface Code Decoding", "comment": "12 pages, 6 figures", "summary": "Quantum error correction, which utilizes logical qubits that are encoded as\nredundant multiple physical qubits to find and correct errors in physical\nqubits, is indispensable for practical quantum computing. Surface code is\nconsidered to be a promising encoding method with a high error threshold that\nis defined by stabilizer generators. However, previous methods have suffered\nfrom the problem that the decoder acquires solely the error probability\ndistribution because of the non-uniqueness of correct prediction obtained from\nthe input. To circumvent this problem, we propose a technique to reoptimize the\ndecoder model by approximating syndrome measurements with a continuous function\nthat is mathematically interpolated by neural network. We evaluated the\nimprovement in accuracy of a multilayer perceptron based decoder for code\ndistances of 5 and 7 as well as for decoders based on convolutional and\nrecurrent neural networks and transformers for a code distance of 5. In all\ncases, the reoptimized decoder gave better accuracy than the original models,\ndemonstrating the universal effectiveness of the proposed method that is\nindependent of code distance or network architecture. These results suggest\nthat re-framing the problem of surface code decoding into a regression problem\nthat can be tackled by deep learning is a useful strategy.", "AI": {"tldr": "Reoptimize surface-code decoders by modeling syndrome extraction as a regression task through neural interpolation, yielding universal accuracy gains across code distances and architectures.", "motivation": "Address the non-uniqueness of correct decoding predictions in surface-code decoders and improve accuracy by reframing syndrome data as a continuous regression problem that neural networks can fit.", "method": "Approximate syndrome measurements with a continuous function interpolated by neural networks. Evaluate decoders (MLP, CNN, RNN, Transformer) for code distances 5 and 7; compare reoptimized models to originals across architectures.", "result": "Reoptimized decoders achieve higher accuracy than the original models for all tested architectures and code distances, indicating universal effectiveness of the regression-based approach.", "conclusion": "Framing surface-code decoding as a regression problem via neural-interpolated syndrome measurements is a broadly effective strategy for improving quantum error correction decoding."}}
{"id": "2509.10058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10058", "abs": "https://arxiv.org/abs/2509.10058", "authors": ["Sung-Lin Tsai", "Bo-Lun Huang", "Yu Ting Shen", "Cheng Yu Yeo", "Chiang Tseng", "Bo-Kai Ruan", "Wen-Sheng Lien", "Hong-Han Shuai"], "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation", "comment": "Accepted to ACM Multimedia 2025 (MM '25)", "summary": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.", "AI": {"tldr": "A training-free framework uses an LLM to disambiguate color terms and adjusts text embeddings in the CIELAB space to improve color fidelity in text-to-image generation without extra training or references, while preserving image quality.", "motivation": "Precise rendering of nuanced color terms is essential for design-focused tasks (fashion, product visualization, interior design). Current diffusion models struggle with compound color terms, leading to misalignment with human intent.", "method": "1) Use a large language model to resolve ambiguous color terms in the text prompt. 2) Refine the text embeddings based on the spatial relationships of the resulting color terms in the CIELAB color space, guiding color blending directly in embedding space. 3) Training-free, no external reference images.", "result": "Experimental results indicate improved color alignment (color fidelity to intended terms) without compromising image quality.", "conclusion": "The approach offers a training-free, LLM-guided method to enhance color fidelity in T2I generation by aligning semantic color terms with perceptual color space, reducing ambiguity without additional data or fine-tuning."}}
{"id": "2509.10167", "categories": ["cs.LG", "68T07, 60H30, 34F05"], "pdf": "https://arxiv.org/pdf/2509.10167", "abs": "https://arxiv.org/abs/2509.10167", "authors": ["L\u00e9na\u00efc Chizat"], "title": "The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams", "comment": null, "summary": "We study the gradient-based training of large-depth residual networks\n(ResNets) from standard random initializations. We show that with a diverging\ndepth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,\nthe training dynamics converges to a Neural Mean ODE training dynamics.\nRemarkably, the limit is independent of the scaling of $M$, covering practical\ncases of, say, Transformers, where $M$ (the number of hidden units or attention\nheads per layer) is typically of the order of $D$. For a residual scale\n$\\Theta_D\\big(\\frac{\\alpha}{LM}\\big)$, we obtain the error bound\n$O_D\\big(\\frac{1}{L}+ \\frac{\\alpha}{\\sqrt{LM}}\\big)$ between the model's output\nand its limit after a fixed number gradient of steps, and we verify empirically\nthat this rate is tight. When $\\alpha=\\Theta(1)$, the limit exhibits complete\nfeature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In\ncontrast, we show that $\\alpha \\to \\infty$ yields a \\lazy ODE regime where the\nMean ODE is linearly parameterized. We then focus on the particular case of\nResNets with two-layer perceptron blocks, for which we study how these scalings\ndepend on the embedding dimension $D$. We show that for this model, the only\nresidual scale that leads to complete feature learning is\n$\\Theta\\big(\\frac{\\sqrt{D}}{LM}\\big)$. In this regime, we prove the error bound\n$O\\big(\\frac{1}{L}+ \\frac{\\sqrt{D}}{\\sqrt{LM}}\\big)$ between the ResNet and its\nlimit after a fixed number of gradient steps, which is also empirically tight.\nOur convergence results rely on a novel mathematical perspective on ResNets :\n(i) due to the randomness of the initialization, the forward and backward pass\nthrough the ResNet behave as the stochastic approximation of certain mean ODEs,\nand (ii) by propagation of chaos (that is, asymptotic independence of the\nunits) this behavior is preserved through the training dynamics.", "AI": {"tldr": "Deep residual networks trained with diverging depth L and fixed embedding dimension D (arbitrary width M) converge to Neural Mean ODE training dynamics, with a residual scale governing the approximation error and feature learning regimes.", "motivation": "To understand how depth, width, embedding dimension, and residual scaling interact to determine the training dynamics of very deep ResNets and their limit behavior as a mean-field/mean ODE.", "method": "Theoretical analysis of gradient-based training for diverging depth L, fixed D, and arbitrary M, deriving the residual-scale Theta_D(alpha/(LM)) that yields an output error bound O_D(1/L + alpha/\u221a(LM)). When alpha=O(1) the limit exhibits non-linear feature learning (Mean ODE), while alpha\u2192\u221e yields a lazy, linearly parameterized regime. For two-layer perceptron blocks, the embedding-dimensional dependence is analyzed, showing complete feature learning only when residual scale is Theta(\u221aD/(LM)), with error bound O(1/L + \u221aD/\u221a(LM)). The analysis combines stochastic approximations of forward/backward passes with propagation of chaos to justify the mean ODE limit.", "result": "The gradient dynamics converge to a Neural Mean ODE in the stated scaling regime, with explicit finite-L error bounds. There are distinct regimes: (i) alpha = O(1) yields complete feature learning; (ii) alpha \u2192 \u221e yields a lazy, linearized limit. In the two-layer block setting, complete feature learning occurs only for residual scale Theta(\u221aD/(LM)) with a bound O(1/L + \u221aD/\u221a(LM)). Empirical results corroborate the tightness of the bounds.", "conclusion": "A unified mean-field perspective explains how depth, width, embedding dimension, and residual scale shape training dynamics and feature learning in deep ResNets. The results identify precise scaling regimes that ensure non-linear feature learning vs. lazy behavior and provide practical guidance for choosing scales in large architectures."}}
{"id": "2509.10059", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10059", "abs": "https://arxiv.org/abs/2509.10059", "authors": ["Yue Zhou", "Litong Feng", "Mengcheng Lan", "Xue Yang", "Qingyun Li", "Yiping Ke", "Xue Jiang", "Wayne Zhang"], "title": "Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration", "comment": "17 pages, 16 figures", "summary": "Mathematical reasoning is critical for tasks such as precise distance and\narea computations, trajectory estimations, and spatial analysis in unmanned\naerial vehicle (UAV) based remote sensing, yet current vision-language models\n(VLMs) have not been adequately tested in this domain. To address this gap, we\nintroduce AVI-Math, the first benchmark to rigorously evaluate multimodal\nmathematical reasoning in aerial vehicle imagery, moving beyond simple counting\ntasks to include domain-specific knowledge in areas such as geometry, logic,\nand algebra. The dataset comprises 3,773 high-quality vehicle-related questions\ncaptured from UAV views, covering 6 mathematical subjects and 20 topics. The\ndata, collected at varying altitudes and from multiple UAV angles, reflects\nreal-world UAV scenarios, ensuring the diversity and complexity of the\nconstructed mathematical problems. In this paper, we benchmark 14 prominent\nVLMs through a comprehensive evaluation and demonstrate that, despite their\nsuccess on previous multimodal benchmarks, these models struggle with the\nreasoning tasks in AVI-Math. Our detailed analysis highlights significant\nlimitations in the mathematical reasoning capabilities of current VLMs and\nsuggests avenues for future research. Furthermore, we explore the use of\nChain-of-Thought prompting and fine-tuning techniques, which show promise in\naddressing the reasoning challenges in AVI-Math. Our findings not only expose\nthe limitations of VLMs in mathematical reasoning but also offer valuable\ninsights for advancing UAV-based trustworthy VLMs in real-world applications.\nThe code, and datasets will be released at\nhttps://github.com/VisionXLab/avi-math", "AI": {"tldr": "AVI-Math is a UAV-focused multimodal math reasoning benchmark; 3,773 questions across geometry, logic, and algebra; tests 14 VLMs; chain-of-thought prompting and fine-tuning help; dataset/code to be released.", "motivation": "Current vision-language models (VLMs) lack evaluation for domain-specific mathematical reasoning in aerial imagery; need reliable benchmarks to assess geometry, distance/area calculations, trajectory estimation, and spatial analysis in UAV remote sensing.", "method": "Construct the AVI-Math dataset with 3,773 vehicle-related questions from UAV views, spanning 6 mathematical subjects and 20 topics; collect data at varying altitudes and UAV angles; benchmark 14 prominent VLMs; perform analyses of limitations; investigate Chain-of-Thought prompting and fine-tuning.", "result": "Prominent VLMs struggle with the mathematical reasoning tasks in AVI-Math; detailed analysis reveals significant gaps in current VLMs; Chain-of-Thought prompting and fine-tuning show promise for addressing these challenges.", "conclusion": "AVI-Math exposes limitations of current multimodal reasoning in UAV contexts and offers guidance for advancing trustworthy UAV VLMs; the dataset and code will be released to the community."}}
{"id": "2509.10186", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10186", "abs": "https://arxiv.org/abs/2509.10186", "authors": ["Benjamin Holzschuh", "Georg Kohl", "Florian Redinger", "Nils Thuerey"], "title": "P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context", "comment": null, "summary": "We present a scalable framework for learning deterministic and probabilistic\nneural surrogates for high-resolution 3D physics simulations. We introduce a\nhybrid CNN-Transformer backbone architecture targeted for 3D physics\nsimulations, which significantly outperforms existing architectures in terms of\nspeed and accuracy. Our proposed network can be pretrained on small patches of\nthe simulation domain, which can be fused to obtain a global solution,\noptionally guided via a fast and scalable sequence-to-sequence model to include\nlong-range dependencies. This setup allows for training large-scale models with\nreduced memory and compute requirements for high-resolution datasets. We\nevaluate our backbone architecture against a large set of baseline methods with\nthe objective to simultaneously learn the dynamics of 14 different types of\nPDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic\nturbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate\nthe versatility of our network by training it as a diffusion model to produce\nprobabilistic samples of highly turbulent 3D channel flows across varying\nReynolds numbers, accurately capturing the underlying flow statistics.", "AI": {"tldr": "A scalable hybrid CNN-Transformer framework for deterministic and probabilistic surrogates of high-res 3D physics, using patch-based pretraining and optional seq-to-seq guidance to capture long-range dependencies, enabling 512^3 turbulence and probabilistic diffusion modeling across Reynolds numbers.", "motivation": "To enable fast, accurate, memory-efficient surrogates for complex 3D PDEs and turbulent flows, addressing limitations of existing architectures in scale, long-range dependencies, and uncertainty quantification.", "method": "A hybrid CNN-Transformer backbone trained on small patches, fused to global solutions; optional sequence-to-sequence model to incorporate long-range dependencies; support for both deterministic surrogates and diffusion-based probabilistic sampling; evaluate on 14 3D PDEs and turbulence up to 512^3.", "result": "Outperforms existing architectures in speed and accuracy; scales to 512^3 isotropic turbulence; demonstrates diffusion model producing probabilistic samples across Reynolds numbers while capturing flow statistics.", "conclusion": "The framework offers scalable, high-fidelity surrogates for 3D physics, enabling large-scale high-resolution simulations with reduced memory/compute, and supports both deterministic dynamics and probabilistic sampling for turbulence."}}
{"id": "2509.10080", "categories": ["cs.CV", "I.2.9; I.4.8"], "pdf": "https://arxiv.org/pdf/2509.10080", "abs": "https://arxiv.org/abs/2509.10080", "authors": ["Minsang Kong", "Myeongjun Kim", "Sang Gu Kang", "Sang Hun Lee"], "title": "BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals", "comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems\n  (under review)", "summary": "In autonomous driving, trajectory prediction is essential for ensuring safe\nand efficient navigation. To improve prediction accuracy, recent approaches\noften rely on pre-built high-definition (HD) maps or real-time local map\nconstruction modules to incorporate static environmental information. However,\npre-built HD maps are limited to specific regions and cannot adapt to transient\nchanges. In addition, local map construction modules, which recognize only\npredefined elements, may fail to capture critical scene details or introduce\nerrors that degrade prediction performance. To overcome these limitations, we\npropose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory\nprediction framework that operates directly in the bird's-eye view (BEV) space\nutilizing real-time sensor data without relying on any pre-built maps. The\nBEVTraj leverages deformable attention to efficiently extract relevant context\nfrom dense BEV features. Furthermore, we introduce a Sparse Goal Candidate\nProposal (SGCP) module, which enables full end-to-end prediction without\nrequiring any post-processing steps. Extensive experiments demonstrate that the\nBEVTraj achieves performance comparable to state-of-the-art HD map-based models\nwhile offering greater flexibility by eliminating the dependency on pre-built\nmaps. The source code is available at https://github.com/Kongminsang/bevtraj.", "AI": {"tldr": "BEVTraj predicts vehicle trajectories directly in the bird's-eye view (BEV) from real-time sensor data without relying on pre-built HD maps, using deformable attention and a Sparse Goal Candidate Proposal (SGCP) to achieve HD-map\u2013level accuracy with end-to-end prediction.", "motivation": "HD maps are region-specific and fail to adapt to transient changes; local map modules may miss critical scene details or introduce errors; a map-free BEV-based approach can provide robust, flexible trajectory prediction using real-time perception.", "method": "BEVTraj operates in BEV space using deformable attention to extract relevant context from dense BEV features. It introduces Sparse Goal Candidate Proposal (SGCP) to enable fully end-to-end trajectory prediction without any post-processing or map reliance.", "result": "Extensive experiments show BEVTraj achieves performance comparable to state-of-the-art HD map\u2013based models while offering greater flexibility by eliminating the dependency on pre-built maps. The authors provide source code at the linked GitHub repository.", "conclusion": "BEVTraj demonstrates that accurate trajectory prediction can be achieved without pre-built HD maps, offering flexibility and potential for easier deployment in diverse environments. It highlights the viability of BEV-based, map-free trajectory prediction using deformable attention and SGCP."}}
{"id": "2509.10189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10189", "abs": "https://arxiv.org/abs/2509.10189", "authors": ["Zexu Jin"], "title": "Hadamard-Riemannian Optimization for Margin-Variance Ensemble", "comment": null, "summary": "Ensemble learning has been widely recognized as a pivotal technique for\nboosting predictive performance by combining multiple base models.\nNevertheless, conventional margin-based ensemble methods predominantly focus on\nmaximizing the expected margin while neglecting the critical role of margin\nvariance, which inherently restricts the generalization capability of the model\nand heightens its vulnerability to overfitting, particularly in noisy or\nimbalanced datasets. Additionally, the conventional approach of optimizing\nensemble weights within the probability simplex often introduces computational\ninefficiency and scalability challenges, complicating its application to\nlarge-scale problems. To tackle these limitations, this paper introduces a\nnovel ensemble learning framework that explicitly incorporates margin variance\ninto the loss function. Our method jointly optimizes the negative expected\nmargin and its variance, leading to enhanced robustness and improved\ngeneralization performance. Moreover, by reparameterizing the ensemble weights\nonto the unit sphere, we substantially simplify the optimization process and\nimprove computational efficiency. Extensive experiments conducted on multiple\nbenchmark datasets demonstrate that the proposed approach consistently\noutperforms traditional margin-based ensemble techniques, underscoring its\neffectiveness and practical utility.", "AI": {"tldr": "A margin-variance aware ensemble learning framework is proposed, jointly optimizing negative expected margin and its variance, with ensemble weights reparameterized on the unit sphere to improve robustness, generalization, and scalability over traditional margin-based ensembles.", "motivation": "Traditional margin-based ensembles maximize expected margin but neglect margin variance, leading to poorer generalization and greater vulnerability to noise and class imbalance. Moreover, optimizing ensemble weights on the probability simplex causes computational inefficiency in large-scale settings.", "method": "Introduce a loss that combines negative expected margin with its variance. Reparameterize ensemble weights onto the unit sphere to simplify optimization and improve efficiency. Use gradient-based optimization on the sphere to train the ensemble.", "result": "Empirical evaluations on multiple benchmark datasets show the proposed method consistently outperforms traditional margin-based ensembles in accuracy and robustness, with better generalization and improved scalability.", "conclusion": "Incorporating margin variance and spherical weight parameterization yields a robust, scalable ensemble framework that enhances generalization and reduces overfitting on noisy or imbalanced data."}}
{"id": "2509.10093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10093", "abs": "https://arxiv.org/abs/2509.10093", "authors": ["Laura Bragagnolo", "Matteo Terreran", "Leonardo Barcellona", "Stefano Ghidoni"], "title": "Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing", "comment": "ICIAP 2025", "summary": "Multi-human parsing is the task of segmenting human body parts while\nassociating each part to the person it belongs to, combining instance-level and\npart-level information for fine-grained human understanding. In this work, we\ndemonstrate that, while state-of-the-art approaches achieved notable results on\npublic datasets, they struggle considerably in segmenting people with\noverlapping bodies. From the intuition that overlapping people may appear\nseparated from a different point of view, we propose a novel training framework\nexploiting multi-view information to improve multi-human parsing models under\nocclusions. Our method integrates such knowledge during the training process,\nintroducing a novel approach based on weak supervision on human instances and a\nmulti-view consistency loss. Given the lack of suitable datasets in the\nliterature, we propose a semi-automatic annotation strategy to generate human\ninstance segmentation masks from multi-view RGB+D data and 3D human skeletons.\nThe experiments demonstrate that the approach can achieve up to a 4.20\\%\nrelative improvement on human parsing over the baseline model in occlusion\nscenarios.", "AI": {"tldr": "A multi-view training framework for multi-human parsing under occlusion using weak supervision on instances and a multi-view consistency loss, with semi-automatic annotations from RGB-D and 3D skeletons, achieving up to 4.20% relative improvement.", "motivation": "State-of-the-art methods struggle with overlapping people; occlusions degrade segmentation. Leveraging multi-view cues and weak supervision aims to improve fine-grained part-level association in crowded scenes.", "method": "Train with multi-view information; introduce weak supervision on human instances; apply multi-view consistency loss; generate training masks via semi-automatic annotation from multi-view RGB-D data and 3D skeletons.", "result": "Demonstrates improvements in occlusion scenarios, up to 4.20% relative improvement over a baseline model for human parsing.", "conclusion": "Multi-view, weakly supervised training with semi-automatic annotations is effective for multi-human parsing under occlusions; offers a practical data-collection strategy, though effectiveness depends on multi-view data availability."}}
{"id": "2509.10227", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.10227", "abs": "https://arxiv.org/abs/2509.10227", "authors": ["\u00c1ngel Ladr\u00f3n", "Miguel S\u00e1nchez-Dom\u00ednguez", "Javier Rozal\u00e9n", "Fernando R. S\u00e1nchez", "Javier de Vicente", "Lucas Lacasa", "Eusebio Valero", "Gonzalo Rubio"], "title": "A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures", "comment": "29 pages, 15 figures", "summary": "Fatigue life prediction is essential in both the design and operational\nphases of any aircraft, and in this sense safety in the aerospace industry\nrequires early detection of fatigue cracks to prevent in-flight failures.\nRobust and precise fatigue life predictors are thus essential to ensure safety.\nTraditional engineering methods, while reliable, are time consuming and involve\ncomplex workflows, including steps such as conducting several Finite Element\nMethod (FEM) simulations, deriving the expected loading spectrum, and applying\ncycle counting techniques like peak-valley or rainflow counting. These steps\noften require collaboration between multiple teams and tools, added to the\ncomputational time and effort required to achieve fatigue life predictions.\nMachine learning (ML) offers a promising complement to traditional fatigue life\nestimation methods, enabling faster iterations and generalization, providing\nquick estimates that guide decisions alongside conventional simulations.\n  In this paper, we present a ML-based pipeline that aims to estimate the\nfatigue life of different aircraft wing locations given the flight parameters\nof the different missions that the aircraft will be operating throughout its\noperational life. We validate the pipeline in a realistic use case of fatigue\nlife estimation, yielding accurate predictions alongside a thorough statistical\nvalidation and uncertainty quantification. Our pipeline constitutes a\ncomplement to traditional methodologies by reducing the amount of costly\nsimulations and, thereby, lowering the required computational and human\nresources.", "AI": {"tldr": "ML-based predictor for aircraft wing fatigue life that maps mission flight parameters to location-specific fatigue life estimates, reducing costly simulations while delivering accurate results with statistical validation and uncertainty quantification.", "motivation": "Fatigue life is critical for aerospace safety. Traditional fatigue estimation is time-consuming and workflow-heavy (FEM analyses, loading spectrum derivation, cycle counting), requiring cross-team collaboration and substantial computation. ML can provide faster, scalable, and probabilistically-uncertain predictions to support decisions.", "method": "A machine-learning pipeline that ingests mission flight parameters and outputs fatigue-life estimates for various wing locations. The approach emphasizes reducing simulations, generalization across missions, and includes statistical validation and uncertainty quantification to accompany predictions.", "result": "The pipeline yields accurate fatigue-life predictions across locations, supported by thorough statistical validation and quantified uncertainty; it reduces the number of necessary simulations and lowers computational and human-resource burdens.", "conclusion": "ML-based fatigue-life estimation complements traditional methodologies by offering faster, resource-efficient predictions with quantified uncertainty, aiding decision-making in design and operation."}}
{"id": "2509.10105", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10105", "abs": "https://arxiv.org/abs/2509.10105", "authors": ["Young-rok Cha", "Jeongho Ju", "SunYoung Park", "Jong-Hyeon Lee", "Younghyun Yu", "Youngjune Kim"], "title": "VARCO-VISION-2.0 Technical Report", "comment": "19 pages, 1 figure, 14 tables. Technical report for VARCO-VISION-2.0,\n  a Korean-English bilingual VLM in 14B and 1.7B variants. Key features:\n  multi-image understanding, OCR with text localization, improved Korean\n  capabilities", "summary": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model.", "AI": {"tldr": "VARCO-VISION-2.0 is a bilingual Korean-English vision-language model in 14B and 1.7B variants, adding multi-image understanding and layout-aware OCR; trained with a four-stage curriculum; enhances multimodal alignment and safety via preference optimization; achieves competitive results (14B rank 8th on OpenCompass VLM leaderboard) and is released on Hugging Face.", "motivation": "To advance open-weight bilingual vision-language models that can handle complex, multi-image inputs (documents, charts, tables) while improving spatial grounding, safety, and on-device deployability in Korean-English contexts.", "method": "Four-stage curriculum training with memory-efficient techniques; layout-aware OCR predicting text content and spatial location; emphasis on multimodal alignment; safety improvements via preference optimization; evaluation on benchmarks in bilingual settings; release of two model scales (14B and 1.7B) for on-device use.", "result": "Demonstrates strong spatial grounding and competitive performance for both Korean and English; 14B model ranks 8th on the OpenCompass VLM leaderboard among models of comparable scale; two variants released on Hugging Face (full 14B and lightweight 1.7B).", "conclusion": "The work advances bilingual VLM development and practical applications by combining robust multimodal capabilities, layout-aware OCR, safety improvements, and on-device deployability, with open-access release of both resource-intensive and compact variants."}}
{"id": "2509.10248", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10248", "abs": "https://arxiv.org/abs/2509.10248", "authors": ["Janis Keuper"], "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications", "comment": null, "summary": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review.", "AI": {"tldr": "Simple prompt-injection attacks can make LLM-generated peer reviews almost universally accept papers, while LLM reviews themselves show a strong acceptance bias, casting doubt on AI-assisted peer review reliability.", "motivation": "Assess the feasibility and impact of prompt-based manipulations on LLM-assisted peer review to understand risks to scientific evaluation and trust.", "method": "Systematic evaluation using 1,000 reviews of 2024 ICLR papers generated by a wide range of LLMs, testing various prompt injections to measure manipulation success and acceptance bias.", "result": "Two main findings: (I) Very simple prompt injections can achieve near-100% acceptance scores in LLM-generated reviews. (II) LLM reviews exhibit a general acceptance bias, with acceptance rates exceeding 95% across many models.", "conclusion": "The study reveals significant vulnerabilities in AI-assisted peer review, including manipulation risk and biased outcomes. It calls for safeguards, robust evaluation protocols, and further research to generalize findings beyond ICLR and beyond current models."}}
{"id": "2509.10114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10114", "abs": "https://arxiv.org/abs/2509.10114", "authors": ["MohammadAli Hamidi", "Hadi Amirpour", "Luigi Atzori", "Christian Timmerer"], "title": "A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss", "comment": null, "summary": "Face image quality assessment (FIQA) plays a critical role in face\nrecognition and verification systems, especially in uncontrolled, real-world\nenvironments. Although several methods have been proposed, general-purpose\nno-reference image quality assessment techniques often fail to capture\nface-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be\ncomputationally intensive, limiting their practical applicability. We propose a\nlightweight and efficient method for FIQA, designed for the perceptual\nevaluation of face images in the wild. Our approach integrates an ensemble of\ntwo compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,\nwith prediction-level fusion via simple averaging. To enhance alignment with\nhuman perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),\ncombining mean squared error (MSE) with a Pearson correlation regularizer. Our\nmethod achieves a strong balance between accuracy and computational cost,\nmaking it suitable for real-world deployment. Experiments on the VQualA FIQA\nbenchmark demonstrate that our model achieves a Spearman rank correlation\ncoefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient\n(PLCC) of 0.9894, remaining within competition efficiency constraints.", "AI": {"tldr": "A lightweight FIQA method using an ensemble of two compact CNNs (MobileNetV3-Small and ShuffleNetV2) with simple averaging and a correlation-aware loss, achieving high correlation with human perception on VQualA while staying computationally efficient.", "motivation": "Face image quality assessment in the wild needs accurate, face-specific quality scores but existing no-reference QA methods are too generic and current FIQA models are often too computationally expensive for real-time/deployed systems.", "method": "An ensemble of two compact networks (MobileNetV3-Small and ShuffleNetV2) whose outputs are fused by simple averaging. Training uses a correlation-aware loss (MSECorrLoss) that combines mean squared error with a Pearson correlation regularizer to align predictions with human perceptual judgments.", "result": "On the VQualA FIQA benchmark, the method achieves SRCC = 0.9829 and PLCC = 0.9894 while remaining within competition efficiency constraints (i.e., computational efficiency suitable for real-world deployment).", "conclusion": "The proposed lightweight FIQA approach delivers high fidelity quality estimates with low computational cost, making it feasible for practical face recognition systems in unconstrained environments."}}
{"id": "2509.10273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10273", "abs": "https://arxiv.org/abs/2509.10273", "authors": ["Sahil Sethi", "Kai Sundmacher", "Caroline Ganzer"], "title": "Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning", "comment": null, "summary": "Ionic liquids (ILs) have emerged as versatile replacements for traditional\nsolvents because their physicochemical properties can be precisely tailored to\nvarious applications. However, accurately predicting key thermophysical\nproperties remains challenging due to the vast chemical design space and the\nlimited availability of experimental data. In this study, we present a\ndata-driven transfer learning framework that leverages a neural recommender\nsystem (NRS) to enable reliable property prediction for ILs using sparse\nexperimental datasets. The approach involves a two-stage process: first,\npre-training NRS models on COSMO-RS-based simulated data at fixed temperature\nand pressure to learn property-specific structural embeddings for cations and\nanions; and second, fine-tuning simple feedforward neural networks using these\nembeddings with experimental data at varying temperatures and pressures. In\nthis work, five essential IL properties are considered: density, viscosity,\nsurface tension, heat capacity, and melting point. The framework supports both\nwithin-property and cross-property knowledge transfer. Notably, pre-trained\nmodels for density, viscosity, and heat capacity are used to fine-tune models\nfor all five target properties, achieving improved performance by a substantial\nmargin for four of them. The model exhibits robust extrapolation to previously\nunseen ILs. Moreover, the final trained models enable property prediction for\nover 700,000 IL combinations, offering a scalable solution for IL screening in\nprocess design. This work highlights the effectiveness of combining simulated\ndata and transfer learning to overcome sparsity in the experimental data.", "AI": {"tldr": "A data-driven transfer-learning framework using a neural recommender system (NRS) to predict multiple ionic liquid (IL) properties from sparse data, leveraging COSMO-RS simulations for pre-training and simple neural nets for fine-tuning, enabling scalable predictions across 700k IL combos.", "motivation": "Accurate prediction of IL thermophysical properties is difficult due to immense chemical design space and limited experimental data; a method that combines simulated data with transfer learning can improve predictive coverage and enable rapid screening.", "method": "Two-stage approach: (1) pre-train NRS on COSMO-RS\u2013generated data at fixed T/P to learn property-specific embeddings for cations/anions; (2) fine-tune simple feedforward networks on experimental data across varying T/P using those embeddings. Multiple IL properties (density, viscosity, surface tension, heat capacity, melting point) are modeled; cross-property transfer is used, with pre-trained models for density/viscosity/heat capacity reused to predict all five properties.", "result": "Significant performance improvement for four of the five properties; the model shows robust extrapolation to unseen ILs and enables property predictions for over 700,000 IL combinations, enabling scalable screening.", "conclusion": "Combining simulated data with transfer learning effectively addresses data sparsity and provides a scalable path for IL property prediction and design, with strong extrapolation to unseen chemistries."}}
{"id": "2509.10122", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10122", "abs": "https://arxiv.org/abs/2509.10122", "authors": ["Zongliang Wu", "Siming Zheng", "Peng-Tao Jiang", "Xin Yuan"], "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution", "comment": null, "summary": "Pre-trained diffusion models have shown great potential in real-world image\nsuper-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.\nWhile one-step diffusion (OSD) methods significantly improve efficiency\ncompared to traditional multi-step approaches, they still have limitations in\nbalancing fidelity and realism across diverse scenarios. Since the OSDs for SR\nare usually trained or distilled by a single timestep, they lack flexible\ncontrol mechanisms to adaptively prioritize these competing objectives, which\nare inherently manageable in multi-step methods through adjusting sampling\nsteps. To address this challenge, we propose a Realism Controlled One-step\nDiffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping\nstrategy that enables explicit control over fidelity-realism trade-offs during\nthe noise prediction phase with minimal training paradigm modifications and\noriginal training data. A degradation-aware sampling strategy is also\nintroduced to align distillation regularization with the grouping strategy and\nenhance the controlling of trade-offs. Moreover, a visual prompt injection\nmodule is used to replace conventional text prompts with degradation-aware\nvisual tokens, enhancing both restoration accuracy and semantic consistency.\nOur method achieves superior fidelity and perceptual quality while maintaining\ncomputational efficiency. Extensive experiments demonstrate that RCOD\noutperforms state-of-the-art OSD methods in both quantitative metrics and\nvisual qualities, with flexible realism control capabilities in the inference\nstage. The code will be released.", "AI": {"tldr": "A Realism-Controlled One-step Diffusion (RCOD) framework for Real-ISR enabling explicit fidelity-realism trade-offs during one-step diffusion by latent domain grouping, degradation-aware sampling, and visual prompt injection; improves fidelity and perceptual quality with efficiency and inference-time control; code to be released.", "motivation": "One-step diffusion SR methods are computationally efficient but lack flexible control over the trade-off between fidelity and realism. Multi-step methods offer tuning via sampling steps, but single-step distillation needs an explicit mechanism to balance objectives without extensive retraining.", "method": "Introduce latent domain grouping to partition the latent space and provide explicit fidelity-realism control during noise prediction. Add degradation-aware sampling to align distillation regularization with the grouping strategy. Implement a visual prompt injection module to replace textual prompts with degradation-aware visual tokens, improving restoration accuracy and semantic consistency. Achieve these with minimal training paradigm changes and using original training data. Inference-time control over the fidelity-realism trade-off is provided by adjusting the grouping/sampling parameters.", "result": "RCOD reportedly achieves superior fidelity and perceptual quality compared with state-of-the-art one-step diffusion SR methods, while offering flexible realism control at inference. It maintains computational efficiency due to the one-step nature and minimal training changes.", "conclusion": "RCOD demonstrates that explicit realism control can be incorporated into one-step diffusion for Real-ISR, bridging the gap between fidelity and realism without heavy retraining. The approach is extensible and claims competitive quantitative metrics and visual quality, with an emphasis on practical inference-time trade-off control."}}
{"id": "2509.10291", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.10291", "abs": "https://arxiv.org/abs/2509.10291", "authors": ["Salih Toprak", "Muge Erel-Ozcevik"], "title": "Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case", "comment": "6 pages, 3 figures, 7th International Conference on Blockchain\n  Computing and Applications (BCCA 2025), \\c{opyright}2025 IEEE", "summary": "In disaster scenarios where conventional energy infrastructure is\ncompromised, secure and traceable energy trading between solar-powered\nhouseholds and mobile charging units becomes a necessity. To ensure the\nintegrity of such transactions over a blockchain network, robust and\nunpredictable nonce generation is vital. This study proposes an SDN-enabled\narchitecture where machine learning regressors are leveraged not for their\naccuracy, but for their potential to generate randomized values suitable as\nnonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN\nallows flexible control over data flows and energy routing policies even in\nfragmented or degraded networks, ensuring adaptive response during emergencies.\nUsing a 9000-sample dataset, we evaluate five AutoML-selected regression models\n- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest\nNeighbors - not by their prediction accuracy, but by their ability to produce\ndiverse and non-deterministic outputs across shuffled data inputs. Randomness\nanalysis reveals that Random Forest and Extra Trees regressors exhibit complete\ndependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and\nLightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and\n99.9%, respectively). These findings highlight that certain machine learning\nmodels, particularly tree-based ensembles, may serve as effective and\nlightweight nonce generators within blockchain-secured, SDN-based energy\ntrading infrastructures resilient to disaster conditions.", "AI": {"tldr": "Proposes using SDN-enabled energy trading with ML-based nonce generation (Proof of AutoML). Evaluates five AutoML regressors on randomness, finding tree ensembles (Random Forest, Extra Trees) are highly stochastic and viable as lightweight nonce generators in disaster scenarios.", "motivation": "In disaster conditions, secure and traceable energy transactions require robust, unpredictable nonces. The paper seeks a lightweight, adaptable nonce source using ML models within an SDN-enabled architecture to maintain blockchain integrity when infrastructure is compromised.", "method": "Evaluate five AutoML-selected regression models (Gradient Boosting, LightGBM, Random Forest, Extra Trees, K-Nearest Neighbors) on a 9000-sample dataset. The assessment focuses on the diversity and non-determinism of outputs across shuffled inputs, not traditional predictive accuracy, to measure randomness suitable for nonce generation.", "result": "Random Forest and Extra Trees regressors show complete dependence on randomness, while Gradient Boosting, K-Nearest Neighbors, and LightGBM exhibit high randomness with scores around 97.6%, 98.8%, and 99.9% (respectively). These results suggest tree-based ensemble models can serve as effective, lightweight nonce generators in blockchain-secured, SDN-based energy trading for disaster resilience.", "conclusion": "Certain ML models\u2014particularly tree-based ensembles\u2014can function as efficient nonce generators within SDN-enabled, blockchain-secured energy trading infrastructures, enabling resilience to disaster conditions."}}
{"id": "2509.10134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10134", "abs": "https://arxiv.org/abs/2509.10134", "authors": ["Rini Smita Thakur", "Rajeev Ranjan Dwivedi", "Vinod K Kurmi"], "title": "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment", "comment": "Accepted in BMVC 2025", "summary": "Accurate segmentation of the optic disc and cup is critical for the early\ndiagnosis and management of ocular diseases such as glaucoma. However,\nsegmentation models trained on one dataset often suffer significant performance\ndegradation when applied to target data acquired under different imaging\nprotocols or conditions. To address this challenge, we propose\n\\textbf{Grad-CL}, a novel source-free domain adaptation framework that\nleverages a pre-trained source model and unlabeled target data to robustly\nadapt segmentation performance without requiring access to the original source\ndata. Grad-CL combines a gradient-guided pseudolabel refinement module with a\ncosine similarity-based contrastive learning strategy. In the first stage,\nsalient class-specific features are extracted via a gradient-based mechanism,\nenabling more accurate uncertainty quantification and robust prototype\nestimation for refining noisy pseudolabels. In the second stage, a contrastive\nloss based on cosine similarity is employed to explicitly enforce inter-class\nseparability between the gradient-informed features of the optic cup and disc.\nExtensive experiments on challenging cross-domain fundus imaging datasets\ndemonstrate that Grad-CL outperforms state-of-the-art unsupervised and\nsource-free domain adaptation methods, achieving superior segmentation accuracy\nand improved boundary delineation. Project and code are available at\nhttps://visdomlab.github.io/GCL/.", "AI": {"tldr": "Grad-CL is a source-free domain adaptation framework for optic disc/cup segmentation that uses gradient-guided pseudolabel refinement and cosine-similarity contrastive learning to improve cross-domain performance and boundary accuracy, outperforming SOTA methods; code available.", "motivation": "Domain shift across fundus imaging datasets degrades segmentation performance when models trained on one dataset are applied to another. Source-free domain adaptation is desirable because it does not require access to the original source data, which can be sensitive or large. Accurate optic disc and cup segmentation is essential for glaucoma diagnosis and management.", "method": "Grad-CL comprises two stages. Stage 1 uses a gradient-based mechanism to extract salient class-specific features, enabling improved uncertainty quantification and robust prototype estimation to refine noisy pseudolabels. Stage 2 applies a cosine-similarity-based contrastive loss to enforce inter-class separability between gradient-informed features of the optic cup and disc. The approach leverages a pre-trained source model and unlabeled target data without accessing source data.", "result": "Extensive cross-domain experiments on challenging fundus imaging datasets show Grad-CL outperforms state-of-the-art unsupervised and source-free domain adaptation methods, achieving superior segmentation accuracy and better boundary delineation.", "conclusion": "Grad-CL is an effective source-free domain adaptation framework for optic disc/cup segmentation, demonstrating strong robustness across domain shifts. The authors provide project and code at the given URL, facilitating replication and adoption."}}
{"id": "2509.10303", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10303", "abs": "https://arxiv.org/abs/2509.10303", "authors": ["Jesse van Remmerden", "Zaharah Bukhsh", "Yingqian Zhang"], "title": "Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data", "comment": null, "summary": "The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling\nProblem (FJSP), are canonical combinatorial optimization problems with\nwide-ranging applications in industrial operations. In recent years, many\nonline reinforcement learning (RL) approaches have been proposed to learn\nconstructive heuristics for JSP and FJSP. Although effective, these online RL\nmethods require millions of interactions with simulated environments that may\nnot capture real-world complexities, and their random policy initialization\nleads to poor sample efficiency. To address these limitations, we introduce\nConservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL\nalgorithm that learns effective scheduling policies directly from historical\ndata, eliminating the need for costly online interactions, while maintaining\nthe ability to improve upon suboptimal training data. CDQAC couples a\nquantile-based critic with a delayed policy update, estimating the return\ndistribution of each machine-operation pair rather than selecting pairs\noutright. Our extensive experiments demonstrate CDQAC's remarkable ability to\nlearn from diverse data sources. CDQAC consistently outperforms the original\ndata-generating heuristics and surpasses state-of-the-art offline and online RL\nbaselines. In addition, CDQAC is highly sample efficient, requiring only 10-20\ntraining instances to learn high-quality policies. Surprisingly, we find that\nCDQAC performs better when trained on data generated by a random heuristic than\nwhen trained on higher-quality data from genetic algorithms and priority\ndispatching rules.", "AI": {"tldr": "Offline RL method CDQAC for JSP/FJSP learns from historical data to produce high-quality scheduling policies with strong sample efficiency, outperforming heuristics and both offline/online baselines; data quality effects can be counterintuitive.", "motivation": "Online RL for JSP/FJSP requires extensive environment interaction and suffers from sample inefficiency and misalignment with real-world data. An offline approach that can learn from historical logs aims to overcome these limitations while still improving upon the training data.", "method": "Conservative Discrete Quantile Actor-Critic (CDQAC) is an offline RL algorithm that uses a quantile-based critic to estimate the return distribution for each machine-operation pair and employs a delayed policy update to ensure conservatism. It learns directly from historical data and can improve upon suboptimal training data by leveraging distributional (quantile) estimates.", "result": "CDQAC consistently outperforms the original data-generating heuristics and surpasses state-of-the-art offline and online RL baselines across diverse data sources. It is highly sample-efficient, requiring only 10-20 training instances. Notably, performance improves when trained on data from a random heuristic rather than higher-quality data from genetic algorithms and priority dispatch rules.", "conclusion": "CDQAC offers a practical, data-efficient offline RL approach for JSP/FJSP, capable of achieving high-quality policies from historical data and reducing reliance on online interactions. The finding that random-heuristic data can outperform higher-quality data highlights nuanced data utility in offline RL for scheduling."}}
{"id": "2509.10140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10140", "abs": "https://arxiv.org/abs/2509.10140", "authors": ["Yifan Chang", "Jie Qin", "Limeng Qiao", "Xiaofeng Wang", "Zheng Zhu", "Lin Ma", "Xingang Wang"], "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization", "comment": null, "summary": "Vector quantization (VQ) is a key component in discrete tokenizers for image\ngeneration, but its training is often unstable due to straight-through\nestimation bias, one-step-behind updates, and sparse codebook gradients, which\nlead to suboptimal reconstruction performance and low codebook usage. In this\nwork, we analyze these fundamental challenges and provide a simple yet\neffective solution. To maintain high codebook usage in VQ networks (VQN) during\nlearning annealing and codebook size expansion, we propose VQBridge, a robust,\nscalable, and efficient projector based on the map function method. VQBridge\noptimizes code vectors through a compress-process-recover pipeline, enabling\nstable and effective codebook training. By combining VQBridge with learning\nannealing, our VQN achieves full (100%) codebook usage across diverse codebook\nconfigurations, which we refer to as FVQ (FullVQ). Through extensive\nexperiments, we demonstrate that FVQ is effective, scalable, and generalizable:\nit attains 100% codebook usage even with a 262k-codebook, achieves\nstate-of-the-art reconstruction performance, consistently improves with larger\ncodebooks, higher vector channels, or longer training, and remains effective\nacross different VQ variants. Moreover, when integrated with LlamaGen, FVQ\nsignificantly enhances image generation performance, surpassing visual\nautoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,\nhighlighting the importance of high-quality tokenizers for strong\nautoregressive image generation.", "AI": {"tldr": "FVQ (FullVQ) uses VQBridge to achieve 100% codebook usage and improved reconstruction in VQ networks, enabling scalable performance and improved image generation.", "motivation": "To address instability and underutilization in vector-quantized tokenizers caused by straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, which hurt reconstruction and codebook usage.", "method": "Introduce VQBridge, a robust, scalable projector based on the map function with a compress-process-recover pipeline to train code vectors. Combine with learning annealing and codebook size expansion, evaluate across diverse configurations, and test integration with LlamaGen.", "result": "Achieves 100% codebook usage across configurations (including a 262k-codebook), FVQ delivers state-of-the-art reconstruction, scales with larger codebooks, higher vector channels, and longer training, remains effective across VQ variants, and improves image generation performance when integrated with LlamaGen (outperforming VAR by 0.5 rFID and DiT by 0.2 rFID).", "conclusion": "High-quality tokenizers are crucial for strong autoregressive image generation; FVQ provides a robust, scalable, and generalizable VQ training solution that yields full codebook utilization and improved generation performance."}}
{"id": "2509.10308", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10308", "abs": "https://arxiv.org/abs/2509.10308", "authors": ["Joshua Dimasaka", "Christian Gei\u00df", "Robert Muir-Wood", "Emily So"], "title": "GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction", "comment": "Accepted full paper at the 8th International Disaster and Risk\n  Conference, IDRC 2025 | Keywords: weakly supervised, graph deep learning,\n  categorical distribution, physical vulnerability, remote sensing,\n  spatiotemporal disaster risk, transition matrix | The data and code are\n  respectively available at https://doi.org/10.5281/zenodo.16656471 and\n  https://github.com/riskaudit/GraphCSVAE", "summary": "In the aftermath of disasters, many institutions worldwide face challenges in\ncontinually monitoring changes in disaster risk, limiting the ability of key\ndecision-makers to assess progress towards the UN Sendai Framework for Disaster\nRisk Reduction 2015-2030. While numerous efforts have substantially advanced\nthe large-scale modeling of hazard and exposure through Earth observation and\ndata-driven methods, progress remains limited in modeling another equally\nimportant yet challenging element of the risk equation: physical vulnerability.\nTo address this gap, we introduce Graph Categorical Structured Variational\nAutoencoder (GraphCSVAE), a novel probabilistic data-driven framework for\nmodeling physical vulnerability by integrating deep learning, graph\nrepresentation, and categorical probabilistic inference, using time-series\nsatellite-derived datasets and prior expert belief systems. We introduce a\nweakly supervised first-order transition matrix that reflects the changes in\nthe spatiotemporal distribution of physical vulnerability in two\ndisaster-stricken and socioeconomically disadvantaged areas: (1) the\ncyclone-impacted coastal Khurushkul community in Bangladesh and (2) the\nmudslide-affected city of Freetown in Sierra Leone. Our work reveals\npost-disaster regional dynamics in physical vulnerability, offering valuable\ninsights into localized spatiotemporal auditing and sustainable strategies for\npost-disaster risk reduction.", "AI": {"tldr": "A novel GraphCSVAE framework models physical vulnerability for post-disaster risk assessment, revealing localized spatiotemporal dynamics to support post-disaster risk reduction.", "motivation": "There is a critical gap in modeling physical vulnerability within disaster risk assessment. Current large-scale hazard/exposure modeling lacks robust data-driven methods for physical vulnerability. The work proposes a probabilistic, graph-based, data-driven approach using satellite time-series data and expert beliefs to inform risk reduction aligned with the UN Sendai Framework.", "method": "GraphCSVAE (Graph-Categorical Structured Variational Autoencoder) integrates deep learning, graph representation, and categorical probabilistic inference. It employs a weakly supervised first-order transition matrix to capture changes in the spatiotemporal distribution of physical vulnerability, demonstrated on time-series satellite data from two disaster-affected, socioeconomically disadvantaged areas: Khurushkul, Bangladesh, and Freetown, Sierra Leone.", "result": "The approach reveals post-disaster regional dynamics in physical vulnerability, enabling localized spatiotemporal auditing and informing sustainable strategies for post-disaster risk reduction.", "conclusion": "The GraphCSVAE framework provides a new, data-driven tool for modeling physical vulnerability, advancing risk assessment capabilities in line with the Sendai Framework and supporting targeted, evidence-based post-disaster risk reduction in vulnerable communities."}}
{"id": "2509.10156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10156", "abs": "https://arxiv.org/abs/2509.10156", "authors": ["Goker Erdogan", "Nikhil Parthasarathy", "Catalin Ionescu", "Drew Hudson", "Alexander Lerchner", "Andrew Zisserman", "Mehdi Sajjadi", "Joao Carreira"], "title": "LayerLock: Non-collapsing Representation Learning with Progressive Freezing", "comment": "ICCV 2025", "summary": "We introduce LayerLock, a simple yet effective approach for self-supervised\nvisual representation learning, that gradually transitions from pixel to latent\nprediction through progressive layer freezing. First, we make the observation\nthat during training of video masked-autoencoding (MAE) models, ViT layers\nconverge in the order of their depth: shallower layers converge early, deeper\nlayers converge late. We then show that this observation can be exploited to\naccelerate standard MAE by progressively freezing the model according to an\nexplicit schedule, throughout training. Furthermore, this same schedule can be\nused in a simple and scalable approach to latent prediction that does not\nsuffer from \"representation collapse\". We apply our proposed approach,\nLayerLock, to large models of up to 4B parameters with results surpassing those\nof non-latent masked prediction on the 4DS perception suite.", "AI": {"tldr": "LayerLock progressively freezes ViT layers during training to shift from pixel-guided to latent prediction, accelerating MAE and enabling scalable latent prediction, achieving strong results on the 4DS perception suite.", "motivation": "Observations in video MAE training show shallow layers converge early while deeper layers converge later. Leveraging this schedule could speed up training, prevent representation collapse, and enable effective latent-prediction objectives at scale.", "method": "Introduce a freezing schedule that transitions from pixel-level prediction to latent prediction across ViT layers. Apply this schedule to standard MAE to accelerate training, and reuse the same schedule for a latent-prediction framework that avoids representation collapse. Demonstrate scalability to models up to 4B parameters.", "result": "LayerLock-enabled training yields results that surpass non-latent masked prediction on the 4DS perception suite, even for very large models (up to 4B parameters).", "conclusion": "A simple, scalable scheduling strategy (LayerLock) can accelerate MAE and enable robust latent-prediction self-supervision at scale, achieving superior performance on challenging perception benchmarks."}}
{"id": "2509.10324", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10324", "abs": "https://arxiv.org/abs/2509.10324", "authors": ["Myung Jin Kim", "YeongHyeon Park", "Il Dong Yun"], "title": "ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting", "comment": null, "summary": "This paper proposes a simple yet effective convolutional module for long-term\ntime series forecasting. The proposed block, inspired by the Auto-Regressive\nIntegrated Moving Average (ARIMA) model, consists of two convolutional\ncomponents: one for capturing the trend (autoregression) and the other for\nrefining local variations (moving average). Unlike conventional ARIMA, which\nrequires iterative multi-step forecasting, the block directly performs\nmulti-step forecasting, making it easily extendable to multivariate settings.\nExperiments on nine widely used benchmark datasets demonstrate that our method\nARMA achieves competitive accuracy, particularly on datasets exhibiting strong\ntrend variations, while maintaining architectural simplicity. Furthermore,\nanalysis shows that the block inherently encodes absolute positional\ninformation, suggesting its potential as a lightweight replacement for\npositional embeddings in sequential models.", "AI": {"tldr": "ARMA: a lean ARIMA-inspired convolutional module for long-term forecasting that directly outputs multi-step forecasts and may serve as a lightweight positional embedding replacement.", "motivation": "Need for accurate long-horizon forecasts using a simple, extensible architecture that captures both global trends and local variations, avoids iterative multi-step forecasting, and can extend to multivariate settings.", "method": "A two-branch convolutional block: one autoregression-like path to capture trend and one moving-average-like path to refine local variations; directly performs multi-step forecasting; claims to encode absolute positional information, potentially replacing positional embeddings.", "result": "Competitive accuracy on nine benchmark datasets, especially for series with strong trend variations, while maintaining a simple architectural design and ease of extension to multivariate tasks.", "conclusion": "The ARMA block offers a simple yet effective approach for long-term forecasting with potential broader impact as a lightweight positional embedding alternative in sequential models."}}
{"id": "2509.10241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10241", "abs": "https://arxiv.org/abs/2509.10241", "authors": ["Elias De Smijter", "Renaud Detry", "Christophe De Vleeschouwer"], "title": "On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints", "comment": "9 pages, 3 figures, to be presented at ASTRA25,", "summary": "We present the first systematic comparison of implicit and explicit Novel\nView Synthesis methods for space-based 3D object reconstruction, evaluating the\nrole of appearance embeddings. While embeddings improve photometric fidelity by\nmodeling lighting variation, we show they do not translate into meaningful\ngains in geometric accuracy - a critical requirement for space robotics\napplications. Using the SPEED+ dataset, we compare K-Planes, Gaussian\nSplatting, and Convex Splatting, and demonstrate that embeddings primarily\nreduce the number of primitives needed for explicit methods rather than\nenhancing geometric fidelity. Moreover, convex splatting achieves more compact\nand clutter-free representations than Gaussian splatting, offering advantages\nfor safety-critical applications such as interaction and collision avoidance.\nOur findings clarify the limits of appearance embeddings for geometry-centric\ntasks and highlight trade-offs between reconstruction quality and\nrepresentation efficiency in space scenarios.", "AI": {"tldr": "Appearance embeddings improve lighting realism but do not improve geometric accuracy in space-based novel view synthesis; convex splatting offers more compact, clutter-free representations and generally better safety-critical suitability; embeddings mainly reduce primitive count for explicit methods rather than enhancing geometry.", "motivation": "To understand whether appearance embeddings aid geometry in space-based novel view synthesis and to compare implicit vs explicit methods (K-Planes, Gaussian Splatting, Convex Splatting) on the SPEED+ dataset, with relevance to space robotics safety-critical tasks.", "method": "Systematic empirical comparison of implicit and explicit NVS methods on the SPEED+ dataset, evaluating geometric accuracy and photometric fidelity with and without appearance embeddings; compare K-Planes, Gaussian Splatting, and Convex Splatting; assess embedding impact on primitive counts and representation efficiency.", "result": "Embeddings improve photometric fidelity by modeling lighting variation but do not produce meaningful gains in geometric accuracy; embeddings mainly reduce the number of primitives needed for explicit methods; convex splatting yields more compact and clutter-free representations than Gaussian splatting; these findings clarify the limits of appearance embeddings for geometry-centric tasks and highlight trade-offs between reconstruction quality and representation efficiency in space scenarios.", "conclusion": "For geometry-centric tasks in space contexts, appearance embeddings have limited value; prioritize representation choices that optimize geometry fidelity and efficiency\u2014convex splatting is advantageous for safety-critical applications like interaction and collision avoidance, while embeddings mainly aid photometric realism rather than geometry."}}
{"id": "2509.10363", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.10363", "abs": "https://arxiv.org/abs/2509.10363", "authors": ["Benjamin David Shaffer", "Brooks Kinch", "Joseph Klobusicky", "M. Ani Hsieh", "Nathaniel Trask"], "title": "Physics-informed sensor coverage through structure preserving machine learning", "comment": null, "summary": "We present a machine learning framework for adaptive source localization in\nwhich agents use a structure-preserving digital twin of a coupled\nhydrodynamic-transport system for real-time trajectory planning and data\nassimilation. The twin is constructed with conditional neural Whitney forms\n(CNWF), coupling the numerical guarantees of finite element exterior calculus\n(FEEC) with transformer-based operator learning. The resulting model preserves\ndiscrete conservation, and adapts in real time to streaming sensor data. It\nemploys a conditional attention mechanism to identify: a reduced Whitney-form\nbasis; reduced integral balance equations; and a source field, each compatible\nwith given sensor measurements. The induced reduced-order environmental model\nretains the stability and consistency of standard finite-element simulation,\nyielding a physically realizable, regular mapping from sensor data to the\nsource field. We propose a staggered scheme that alternates between evaluating\nthe digital twin and applying Lloyd's algorithm to guide sensor placement, with\nanalysis providing conditions for monotone improvement of a coverage\nfunctional. Using the predicted source field as an importance function within\nan optimal-recovery scheme, we demonstrate recovery of point sources under\ncontinuity assumptions, highlighting the role of regularity as a sufficient\ncondition for localization. Experimental comparisons with physics-agnostic\ntransformer architectures show improved accuracy in complex geometries when\nphysical constraints are enforced, indicating that structure preservation\nprovides an effective inductive bias for source identification.", "AI": {"tldr": "A structure-preserving ML framework uses conditional neural Whitney forms to build a real-time digital twin for adaptive source localization, achieving stable, physically consistent source recovery and outperforming physics-agnostic transformers in complex geometries.", "motivation": "To enable real-time, data-assimilating source localization in coupled hydrodynamic-transport systems while preserving physical conservation laws via a structure-preserving neural approach.", "method": "Develop a conditional neural Whitney form (CNWF) that combines finite element exterior calculus (FEEC) guarantees with transformer-based operator learning. Use a conditional attention mechanism to identify a reduced Whitney-basis, reduced balance equations, and a source field compatible with sensor data. Employ a staggered scheme alternating digital-twin evaluation and Lloyd-based sensor-placement updates. Use the predicted source field as an importance function in an optimal-recovery framework for recovering point sources under continuity assumptions.", "result": "The reduced-order model preserves discrete conservation and remains stable and consistent with standard FE simulations. It yields a physically realizable mapping from sensor data to the source field. Experiments show improved accuracy over physics-agnostic transformers in complex geometries when physical constraints are enforced.", "conclusion": "Structure-preserving neural modeling provides a strong inductive bias for realistic source identification in dynamic, geometry-rich environments, enabling real-time data assimilation and adaptive sensor placement."}}
{"id": "2509.10250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10250", "abs": "https://arxiv.org/abs/2509.10250", "authors": ["Haozhen Yan", "Yan Hong", "Suning Lang", "Jiahui Zhan", "Yikun Ji", "Yujie Gao", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang"], "title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection", "comment": "11 pages, 5 figures", "summary": "With generative models becoming increasingly sophisticated and diverse,\ndetecting AI-generated images has become increasingly challenging. While\nexisting AI-genereted Image detectors achieve promising performance on\nin-distribution generated images, their generalization to unseen generative\nmodels remains limited. This limitation is largely attributed to their reliance\non generation-specific artifacts, such as stylistic priors and compression\npatterns. To address these limitations, we propose GAMMA, a novel training\nframework designed to reduce domain bias and enhance semantic alignment. GAMMA\nintroduces diverse manipulation strategies, such as inpainting-based\nmanipulation and semantics-preserving perturbations, to ensure consistency\nbetween manipulated and authentic content. We employ multi-task supervision\nwith dual segmentation heads and a classification head, enabling pixel-level\nsource attribution across diverse generative domains. In addition, a reverse\ncross-attention mechanism is introduced to allow the segmentation heads to\nguide and correct biased representations in the classification branch. Our\nmethod achieves state-of-the-art generalization performance on the GenImage\nbenchmark, imporving accuracy by 5.8%, but also maintains strong robustness on\nnewly released generative model such as GPT-4o.", "AI": {"tldr": "GAMMA improves generalization of AI-generated image detectors by reducing domain bias with diverse manipulations and multi-task learning, achieving state-of-the-art results on GenImage and robustness to GPT-4o.", "motivation": "Current detectors rely on generation-specific artifacts, which harms generalization to unseen generative models. A domain-invariant and semantically aligned detection approach is needed to reliably distinguish AI-generated content across diverse sources.", "method": "GAMMA introduces diverse manipulation strategies (inpainting-based manipulation and semantics-preserving perturbations) to enforce content consistency between manipulated and authentic images. It employs multi-task supervision with dual segmentation heads and a classification head for pixel-level source attribution across generative domains, and a reverse cross-attention mechanism whereby segmentation guides and corrects the biased representations in the classification branch.", "result": "The method achieves state-of-the-art generalization on the GenImage benchmark, with an accuracy improvement of 5.8 percentage points, and demonstrates strong robustness on newly released models such as GPT-4o.", "conclusion": "GAMMA reduces domain bias and enhances semantic alignment across generative domains, enabling more reliable detection of AI-generated images even from unseen models."}}
{"id": "2509.10367", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10367", "abs": "https://arxiv.org/abs/2509.10367", "authors": ["Tong Chen", "Raghavendra Selvan"], "title": "A Discrepancy-Based Perspective on Dataset Condensation", "comment": "30 pages, 4 tables, 1 figure", "summary": "Given a dataset of finitely many elements $\\mathcal{T} = \\{\\mathbf{x}_i\\}_{i\n= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic\ndataset $\\mathcal{S} = \\{\\tilde{\\mathbf{x}}_j\\}_{j = 1}^M$ which is\nsignificantly smaller ($M \\ll N$) such that a model trained from scratch on\n$\\mathcal{S}$ achieves comparable or even superior generalization performance\nto a model trained on $\\mathcal{T}$. Recent advances in DC reveal a close\nconnection to the problem of approximating the data distribution represented by\n$\\mathcal{T}$ with a reduced set of points. In this work, we present a unified\nframework that encompasses existing DC methods and extend the task-specific\nnotion of DC to a more general and formal definition using notions of\ndiscrepancy, which quantify the distance between probability distribution in\ndifferent regimes. Our framework broadens the objective of DC beyond\ngeneralization, accommodating additional objectives such as robustness,\nprivacy, and other desirable properties.", "AI": {"tldr": "A unified, discrepancy-based framework for dataset condensation that formalizes and broadens DC beyond generalization, enabling robustness, privacy, and other objectives by approximating the data distribution with a small synthetic set.", "motivation": "Condense finite datasets into a much smaller synthetic set without sacrificing performance, and provide a general, formal objective for DC that subsumes existing methods while supporting goals like robustness and privacy through distribution discrepancies.", "method": "Proposes a unified framework that uses discrepancy measures to quantify the distance between the empirical distribution of the original data and that of the synthetic set, unifying existing DC methods under a common formalism and enabling task-specific objectives beyond accuracy.", "result": "Demonstrates that the framework subsumes existing DC approaches and can accommodate multi-objective goals (robustness, privacy, etc.); reframes DC as a distribution-approximation problem rather than solely optimizing generalization.", "conclusion": "The discrepancy-based, unified DC framework broadens the objectives of dataset condensation beyond generalization, providing a flexible, principled approach to constructing small synthetic datasets with desirable properties."}}
{"id": "2509.10257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10257", "abs": "https://arxiv.org/abs/2509.10257", "authors": ["Ema Masterl", "Tina Vipotnik Vesnaver", "\u017diga \u0160piclin"], "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI", "comment": "Accepted at the PIPPI Workshop of MICCAI 2025", "summary": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce\nmotion artifacts caused by fetal movement. However, these stacks are typically\nlow resolution, may suffer from motion corruption, and do not adequately\ncapture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to\naddress these limitations by combining slice-to-volume registration and\nsuper-resolution techniques to generate high-resolution (HR) 3D volumes. While\nseveral SRR methods have been proposed, their comparative performance -\nparticularly in pathological cases - and their influence on downstream\nvolumetric analysis and diagnostic tasks remain underexplored. In this study,\nwe applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to\n140 fetal brain MRI scans, including both healthy controls (HC) and\npathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was\nsegmented using the BoUNTi algorithm to extract volumes of nine principal brain\nstructures. We evaluated visual quality, SRR success rates, volumetric\nmeasurement agreement, and diagnostic classification performance. NeSVoR\ndemonstrated the highest and most consistent reconstruction success rate (>90%)\nacross both HC and PC groups. Although significant differences in volumetric\nestimates were observed between SRR methods, classification performance for VM\nwas not affected by the choice of SRR method. These findings highlight NeSVoR's\nrobustness and the resilience of diagnostic performance despite SRR-induced\nvolumetric variability.", "AI": {"tldr": "NeSVoR yields the highest, most consistent HR reconstructions (>90%) in fetal brain MRI across healthy and ventriculomegaly cases. Different SRR methods change volumetric estimates, but VM classification remains robust to SRR choice.", "motivation": "Assess how state-of-the-art SRR methods perform on fetal brain MRI, including in pathology, and their impact on downstream volumetric analysis and diagnostic tasks.", "method": "Apply three SRR methods (NiftyMIC, SVRTK, NeSVoR) to 140 fetal brain scans (healthy and VM pathology). Generate HR volumes, segment nine brain structures with BoUNTi. Evaluate visual quality, SRR success, volumetric agreement, and VM classification performance.", "result": "NeSVoR achieved the highest and most consistent reconstruction success rate (>90%) across both HC and PC. Significant differences in volumetric estimates exist between SRR methods, yet VM classification performance is unaffected by SRR choice, indicating robustness of diagnostic outcome to SRR-induced variability.", "conclusion": "NeSVoR is robust for fetal SRR, and VM diagnostic performance is resilient to differences in SRR-induced volumetric estimates. Method selection may thus prioritize reconstruction success and downstream volumetric reliability, but further work is needed to understand clinical implications across more pathologies and datasets."}}
{"id": "2509.10369", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2509.10369", "abs": "https://arxiv.org/abs/2509.10369", "authors": ["Gul Rukh Khattak", "Konstantinos Patlatzoglou", "Joseph Barker", "Libor Pastika", "Boroumand Zeidaabadi", "Ahmed El-Medany", "Hesham Aggour", "Yixiu Liang", "Antonio H. Ribeiro", "Jeffrey Annis", "Antonio Luiz Pinho Ribeiro", "Junbo Ge", "Daniel B. Kramer", "Jonathan W. Waks", "Evan Brittain", "Nicholas Peters", "Fu Siong Ng", "Arunashis Sau"], "title": "Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms", "comment": "Currently under review at npj Digital Medicine", "summary": "Contrastive learning is a widely adopted self-supervised pretraining\nstrategy, yet its dependence on cohort composition remains underexplored. We\npresent Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation\nmodel and pretrain on four cohorts (n = 5,203,352), from diverse populations\nacross three continents (North America, South America, Asia). We systematically\nassess how cohort demographics, health status, and population diversity\ninfluence the downstream performance for prediction tasks also including two\nadditional cohorts from another continent (Europe). We find that downstream\nperformance depends on the distributional properties of the pretraining cohort,\nincluding demographics and health status. Moreover, while pretraining with a\nmulti-centre, demographically diverse cohort improves in-distribution accuracy,\nit reduces out-of-distribution (OOD) generalisation of our contrastive approach\nby encoding cohort-specific artifacts. To address this, we propose the\nIn-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency\nduring pretraining and enhances OOD robustness. This work provides important\ninsights for developing clinically fair and generalisable foundation models.", "AI": {"tldr": "Contrasting by Patient Augmented ECGs (CAPE) studies how pretraining cohort composition shapes contrastive self-supervised learning for ECG data; diverse multi-centre pretraining improves in-distribution accuracy but harms OOD generalization, and the In-Distribution Batch (IDB) strategy mitigates this to enhance OOD robustness, with implications for fairness and generalisability.", "motivation": "Understand the influence of demographics and health status on self-supervised ECG pretraining and address cross-cohort generalisation and fairness in foundation models.", "method": "Pretrain CAPE on four large cohorts (over 5.2 million patients) spanning three continents; assess downstream tasks, including two more European cohorts; analyze distributional effects; propose IDB to preserve intra-cohort consistency during pretraining.", "result": "Pretraining cohort distribution significantly affects downstream performance; multi-centre diverse data improves in-distribution accuracy but worsens OOD generalization due to cohort-specific artifacts; IDB improves OOD robustness.", "conclusion": "Cohort composition matters for clinically deployed foundation models; IDB offers a practical approach to balance in-distribution performance with OOD generalisation and fairness considerations."}}
{"id": "2509.10259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10259", "abs": "https://arxiv.org/abs/2509.10259", "authors": ["Hua Yuan", "Jin Yuan", "Yicheng Jiang", "Yao Zhang", "Xin Geng", "Yong Rui"], "title": "Mask Consistency Regularization in Object Removal", "comment": null, "summary": "Object removal, a challenging task within image inpainting, involves\nseamlessly filling the removed region with content that matches the surrounding\ncontext. Despite advancements in diffusion models, current methods still face\ntwo critical challenges. The first is mask hallucination, where the model\ngenerates irrelevant or spurious content inside the masked region, and the\nsecond is mask-shape bias, where the model fills the masked area with an object\nthat mimics the mask's shape rather than surrounding content. To address these\nissues, we propose Mask Consistency Regularization (MCR), a novel training\nstrategy designed specifically for object removal tasks. During training, our\napproach introduces two mask perturbations: dilation and reshape, enforcing\nconsistency between the outputs of these perturbed branches and the original\nmask. The dilated masks help align the model's output with the surrounding\ncontent, while reshaped masks encourage the model to break the mask-shape bias.\nThis combination of strategies enables MCR to produce more robust and\ncontextually coherent inpainting results. Our experiments demonstrate that MCR\nsignificantly reduces hallucinations and mask-shape bias, leading to improved\nperformance in object removal.", "AI": {"tldr": "Mask Consistency Regularization (MCR) is a training strategy for object removal in image inpainting that uses two mask perturbations\u2014dilation and reshape\u2014to enforce output consistency, reduce hallucinations, and break mask-shape bias, yielding more contextually coherent results.", "motivation": "Current diffusion-based inpainting struggles with mask hallucination (irrelevant content inside the masked region) and mask-shape bias (content that mirrors the mask shape rather than surrounding context). A training-time regularization that enforces consistency across perturbed masks aims to align the inpainted region with surrounding content while avoiding shape bias.", "method": "During training, apply two mask perturbations to create parallel branches: dilation and reshape. Enforce consistency between the outputs of these perturbed branches and the original mask. Dilated masks encourage alignment with surrounding content; reshaped masks encourage breaking the mask-shape bias, together regularizing the model.", "result": "The approach substantially reduces hallucinations and mask-shape bias and improves object removal performance on benchmark tasks, demonstrating more robust and contextually coherent inpainting.", "conclusion": "MCR provides an effective training strategy for object removal in image inpainting, yielding outputs that better integrate with surrounding content and exhibit fewer mask-driven artifacts."}}
{"id": "2509.10384", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.10384", "abs": "https://arxiv.org/abs/2509.10384", "authors": ["Jianxin Zhang", "Clayton Scott"], "title": "Flow Straight and Fast in Hilbert Space: Functional Rectified Flow", "comment": null, "summary": "Many generative models originally developed in finite-dimensional Euclidean\nspace have functional generalizations in infinite-dimensional settings.\nHowever, the extension of rectified flow to infinite-dimensional spaces remains\nunexplored. In this work, we establish a rigorous functional formulation of\nrectified flow in an infinite-dimensional Hilbert space. Our approach builds\nupon the superposition principle for continuity equations in an\ninfinite-dimensional space. We further show that this framework extends\nnaturally to functional flow matching and functional probability flow ODEs,\ninterpreting them as nonlinear generalizations of rectified flow. Notably, our\nextension to functional flow matching removes the restrictive measure-theoretic\nassumptions in the existing theory of \\citet{kerrigan2024functional}.\nFurthermore, we demonstrate experimentally that our method achieves superior\nperformance compared to existing functional generative models.", "AI": {"tldr": "A functional generalization of rectified flow to infinite-dimensional Hilbert spaces using a rigorous formulation based on the infinite-dimensional continuity equation, enabling functional flow matching and functional probability flow ODEs, removing restrictive assumptions, with empirical performance gains over prior functional generative models.", "motivation": "Finite-dimensional rectified flow has strong generative capabilities, but many modern data arise as functions or fields living in infinite-dimensional spaces. A rigorous functional framework is needed to extend rectified flow to such settings, address gaps in prior theory (notably measure-theoretic restrictions), and unlock functional flow matching and functional probability flow ODEs.", "method": "Ground the approach in the superposition principle for continuity equations in infinite dimensions to derive a functional formulation of rectified flow on a Hilbert space. Extend the framework to functional flow matching and functional probability flow ODEs, and show that this avoids restrictive measure-theoretic assumptions present in earlier functional theories. Provide experimental validation comparing to existing functional generative models.", "result": "A rigorous functional formulation of rectified flow in infinite-dimensional Hilbert spaces is established. The framework naturally extends to functional flow matching and functional probability flow ODEs as nonlinear generalizations of rectified flow. It eliminates certain restrictive measure-theoretic assumptions and demonstrates superior empirical performance relative to prior functional generative models.", "conclusion": "The work supplies a principled infinite-dimensional extension of rectified flow and its functional analogs, enabling more powerful functional generative modeling and inviting further research into functional flow dynamics and their applications."}}
{"id": "2509.10260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10260", "abs": "https://arxiv.org/abs/2509.10260", "authors": ["Jia Wang", "Jie Hu", "Xiaoqi Ma", "Hanghang Ma", "Yanbing Zeng", "Xiaoming Wei"], "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation", "comment": null, "summary": "Text-to-image (T2I) generation has achieved remarkable progress in\ninstruction following and aesthetics. However, a persistent challenge is the\nprevalence of physical artifacts, such as anatomical and structural flaws,\nwhich severely degrade perceptual quality and limit application. Given the\ndiversity and complexity of these artifacts, a systematic and fine-grained\nevaluation framework is required, which is lacking in current benchmarks. To\nfill this gap, we introduce MagicMirror, a comprehensive framework for\nartifacts assessment. We first establish a detailed taxonomy of generated image\nartifacts. Guided by this taxonomy, we manually annotate MagicData340K, the\nfirst human-annotated large-scale dataset of 340K generated images with\nfine-grained artifact labels. Building on this dataset, we train MagicAssessor,\na Vision-Language Model (VLM) that provides detailed assessments and\ncorresponding labels. To overcome challenges like class imbalance and reward\nhacking, we design a novel data sampling strategy and a multi-level reward\nsystem for Group Relative Policy Optimization (GRPO). Finally, we leverage\nMagicAssessor to construct MagicBench, an automated benchmark for evaluating\nthe image artifacts of current T2I models. Our evaluation with MagicBench\nreveals that despite their widespread adoption, even top-tier models like\nGPT-image-1 are consistently plagued by significant artifacts, highlighting\nartifact reduction as a critical frontier for future T2I development. Project\npage: https://wj-inf.github.io/MagicMirror-page/.", "AI": {"tldr": "Introduces MagicMirror, a comprehensive framework to assess image artifacts in text-to-image generation, including a taxonomy, a large labeled dataset (MagicData340K), a Vision-Language Model (MagicAssessor), a data sampling and GRPO-based training strategy, and MagicBench benchmark; demonstrates that artifacts persist in state-of-the-art models.", "motivation": "To address the lack of fine-grained, systematic evaluation of artifacts in T2I generation, which harms perceptual quality and applicability.", "method": "Develop taxonomy of artifacts; manually annotate 340K generated images; train MagicAssessor VLM; design data sampling strategy and multi-level reward for Group Relative Policy Optimization (GRPO); construct MagicBench benchmark.", "result": "MagicAssessor provides detailed artifact assessments and labels; MagicBench enables automated evaluation; experiments show even top models like GPT-image-1 have significant artifacts.", "conclusion": "Artifact reduction is a key frontier for future T2I work; the MagicMirror suite offers a scalable path for evaluation and benchmarking."}}
{"id": "2509.10390", "categories": ["cs.LG", "cs.IT", "math.IT", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2509.10390", "abs": "https://arxiv.org/abs/2509.10390", "authors": ["Quan Nguyen", "Adji Bousso Dieng"], "title": "Vendi Information Gain for Active Learning and its Application to Ecology", "comment": null, "summary": "While monitoring biodiversity through camera traps has become an important\nendeavor for ecological research, identifying species in the captured image\ndata remains a major bottleneck due to limited labeling resources. Active\nlearning -- a machine learning paradigm that selects the most informative data\nto label and train a predictive model -- offers a promising solution, but\ntypically focuses on uncertainty in the individual predictions without\nconsidering uncertainty across the entire dataset. We introduce a new active\nlearning policy, Vendi information gain (VIG), that selects images based on\ntheir impact on dataset-wide prediction uncertainty, capturing both\ninformativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG\nachieves impressive predictive accuracy close to full supervision using less\nthan 10% of the labels. It consistently outperforms standard baselines across\nmetrics and batch sizes, collecting more diverse data in the feature space. VIG\nhas broad applicability beyond ecology, and our results highlight its value for\nbiodiversity monitoring in data-limited environments.", "AI": {"tldr": "A new active-learning policy, VIG, uses dataset-wide prediction uncertainty to guide labeling, balancing informativeness and diversity; it achieves near full-supervision accuracy on Snapshot Serengeti with under 10% labels and outperforms baselines, suggesting broad applicability for data-limited biodiversity monitoring.", "motivation": "Labeling scarcity in camera-trap biodiversity data; existing active learning often focuses on single-sample uncertainty and ignores global dataset uncertainty and data diversity.", "method": "Introduce Vendi Information Gain (VIG), an active-learning policy that selects images based on their impact on dataset-wide prediction uncertainty, thus capturing informativeness and diversity. Applied to camera-trap data (Snapshot Serengeti).", "result": "VIG achieves predictive accuracy close to full supervision using less than 10% of labels; consistently outperforms standard baselines across metrics and batch sizes; collects more diverse data in the feature space.", "conclusion": "VIG is broadly applicable beyond ecology and offers a valuable tool for biodiversity monitoring in data-limited environments."}}
{"id": "2509.10266", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10266", "abs": "https://arxiv.org/abs/2509.10266", "authors": ["Wenfang Wu", "Tingting Yuan", "Yupeng Li", "Daling Wang", "Xiaoming Fu"], "title": "SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion", "comment": null, "summary": "Sign language translation (SLT) aims to translate natural language from sign\nlanguage videos, serving as a vital bridge for inclusive communication. While\nrecent advances leverage powerful visual backbones and large language models,\nmost approaches mainly focus on manual signals (hand gestures) and tend to\noverlook non-manual cues like mouthing. In fact, mouthing conveys essential\nlinguistic information in sign languages and plays a crucial role in\ndisambiguating visually similar signs. In this paper, we propose SignClip, a\nnovel framework to improve the accuracy of sign language translation. It fuses\nmanual and non-manual cues, specifically spatial gesture and lip movement\nfeatures. Besides, SignClip introduces a hierarchical contrastive learning\nframework with multi-level alignment objectives, ensuring semantic consistency\nacross sign-lip and visual-text modalities. Extensive experiments on two\nbenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our\napproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip\nsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from\n24.32 to 24.71, and ROUGE from 46.57 to 48.38.", "AI": {"tldr": "SignClip improves sign language translation by fusing manual gestures with mouthing cues and using hierarchical contrastive learning to align sign, lip, and text modalities, achieving state-of-the-art BLEU-4 and ROUGE on PHOENIX14T in gloss-free settings.", "motivation": "Non-manual cues like mouthing carry essential linguistic information and help disambiguate visually similar signs; current SLT approaches underutilize lip movements beyond hand gestures.", "method": "Fuse spatial gesture (manual) and lip movement (non-manual) features; propose a hierarchical contrastive learning framework with multi-level alignment objectives across sign-lip and visual-text modalities.", "result": "Outperforms prior SOTA SpaMo on PHOENIX14T and How2Sign; in gloss-free PHOENIX14T, BLEU-4 improves from 24.32 to 24.71 and ROUGE from 46.57 to 48.38.", "conclusion": "SignClip effectively enhances SLT accuracy by integrating mouthing cues and cross-modal contrastive learning, underlining the importance of non-manual signals for disambiguation."}}
{"id": "2509.10396", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10396", "abs": "https://arxiv.org/abs/2509.10396", "authors": ["Siyan Zhao", "Mengchen Liu", "Jing Huang", "Miao Liu", "Chenyu Wang", "Bo Liu", "Yuandong Tian", "Guan Pang", "Sean Bell", "Aditya Grover", "Feiyu Chen"], "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "comment": "preprint; 21 pages", "summary": "Masked diffusion large language models (dLLMs) are emerging as promising\nalternatives to autoregressive LLMs, offering competitive performance while\nsupporting unique generation capabilities such as inpainting. We explore how\ninpainting can inform RL algorithm design for dLLMs. Aligning LLMs with\nreinforcement learning faces an exploration challenge: sparse reward signals\nand sample waste when models fail to discover correct solutions. While this\ninefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their\ninpainting ability can guide exploration. We introduce IGPO (Inpainting Guided\nPolicy Optimization), an RL framework that strategically inserts partial\nground-truth reasoning traces during online sampling. Unlike providing full\nsolutions, inpainting steers exploration toward promising trajectory spaces\nwhile preserving self-generated reasoning, bridging supervised fine-tuning and\nreinforcement learning. We apply IGPO to group-based optimization methods such\nas GRPO, where exploration failures cause zero advantages and gradients. IGPO\nrestores meaningful gradients while improving sample efficiency. We also\npropose supervised fine-tuning on synthetically rewritten concise traces that\nbetter align with dLLM generation patterns. With additional techniques\nincluding entropy-based filtering, our training recipe yields substantial gains\nacross three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new\nstate-of-the-art results for full-attention masked dLLMs.", "AI": {"tldr": "IGPO (Inpainting Guided Policy Optimization) uses inpainting in masked diffusion LLMs to guide exploration during online sampling via partial ground-truth reasoning traces, restoring gradients and improving sample efficiency, achieving state-of-the-art results on GSM8K, Math500, and AMC.", "motivation": "Reinforcement learning for LLMs suffers from sparse rewards and wasted samples; masked diffusion LLMs offer inpainting as a unique exploration tool to steer search without compromising self-generated reasoning.", "method": "Introduce IGPO by inserting partial ground-truth reasoning traces during online sampling; apply to group-based optimization like GRPO to avoid zero-gradient issues; combine with supervised fine-tuning on synthetically rewritten concise traces; employ entropy-based filtering to refine training.", "result": "IGPO restores meaningful gradients, improves sample efficiency, and yields substantial gains across GSM8K, Math500, and AMC, achieving new state-of-the-art results for full-attention masked dLLMs.", "conclusion": "Inpainting-guided exploration bridges supervised fine-tuning and reinforcement learning for dLLMs, providing an effective avenue to enhance learning efficiency and performance."}}
{"id": "2509.10278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10278", "abs": "https://arxiv.org/abs/2509.10278", "authors": ["Vidit Vidit", "Pavel Korshunov", "Amir Mohammadi", "Christophe Ecabert", "Ketan Kotwal", "S\u00e9bastien Marcel"], "title": "Detecting Text Manipulation in Images using Vision Language Models", "comment": "Accepted in Synthetic Realities and Biometric Security Workshop\n  BMVC-2025. For paper page see https://www.idiap.ch/paper/textvlmdet/", "summary": "Recent works have shown the effectiveness of Large Vision Language Models\n(VLMs or LVLMs) in image manipulation detection. However, text manipulation\ndetection is largely missing in these studies. We bridge this knowledge gap by\nanalyzing closed- and open-source VLMs on different text manipulation datasets.\nOur results suggest that open-source models are getting closer, but still\nbehind closed-source ones like GPT- 4o. Additionally, we benchmark image\nmanipulation detection-specific VLMs for text manipulation detection and show\nthat they suffer from the generalization problem. We benchmark VLMs for\nmanipulations done on in-the-wild scene texts and on fantasy ID cards, where\nthe latter mimic a challenging real-world misuse.", "AI": {"tldr": "The abstract reports a systematic evaluation of large vision-language models (LVLMs) for text manipulation detection. Open-source LVLMs are approaching the performance of closed models like GPT-4o but remain behind. Image-manipulation-detection-specific VLMs struggle with generalization to text manipulation tasks. The study also tests these models on in-the-wild scene texts and on fantasy ID cards to assess practical misuse scenarios.", "motivation": "Address the gap in text manipulation detection using LVLMs, comparing open- and closed-source models, and evaluating their robustness and generalization to real-world and synthetic misuse scenarios.", "method": "Benchmark closed- and open-source LVLMs on text manipulation datasets; benchmark image-manipulation-detection-specific VLMs for text manipulation detection; evaluate on in-the-wild scene texts and fantasy ID card scenarios to probe generalization and misuse potential.", "result": "Open-source LVLMs are converging toward but still lagging behind closed models like GPT-4o in text manipulation tasks. Text-specific VLMs exhibit generalization problems, failing to consistently transfer to new text manipulation contexts such as in-the-wild scene text and fantasy ID cards.", "conclusion": "There is notable progress in leveraging LVLMs for text manipulation detection, but robustness and generalization remain key challenges. Further work is needed to close the gap with strong closed models and to improve generalization across diverse, real-world text manipulation scenarios."}}
{"id": "2509.10406", "categories": ["cs.LG", "68W25, 68T50 (primary) 68W40, 68T07 (secondary)", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.10406", "abs": "https://arxiv.org/abs/2509.10406", "authors": ["Rupert Mitchell", "Kristian Kersting"], "title": "Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining", "comment": null, "summary": "We present Multipole Semantic Attention (MuSe), an efficient approximation of\nsoftmax attention that combines semantic clustering with multipole expansions\nfrom computational physics. Our method addresses the quadratic computational\ncomplexity of transformers in the context length by clustering queries and keys\nseparately in their learned representation spaces, enabling a hierarchical\ntwo-stage attention mechanism. Unlike prior clustering approaches that group\nonly keys or use unified clustering, we maintain separate clusterings that\nrespect attention's asymmetric treatment of these spaces. We augment\ncentroid-based (monopole) approximations with dipole corrections that capture\ndirectional variance within clusters, preserving richer information during\ntraining. The method operates as a drop-in replacement for standard attention,\nrequiring only hyperparameter specification without architectural\nmodifications. Our approach achieves $\\mathcal{O}(NCD)$ complexity for acausal\nattention with $C$ clusters and $\\mathcal{O}(NCD \\log N)$ for causal attention.\nOn isolated attention layers, we demonstrate $3\\times$ speedup over CUDNN Flash\nAttention at 8k context length, with relative squared errors below 20%. For\ncausal attention, we develop a hierarchical block decomposition that combines\nexact local computation with efficient long-range approximation. In end-to-end\npretraining of a 30M parameter model on book-length texts with 16k context, we\nachieve 12.2% runtime reduction with only 0.36% loss degradation, establishing\nthe viability of multipole approximations for efficient transformer\npretraining.", "AI": {"tldr": "MuSe introduces a hierarchical, multipole-based fast attention by clustering queries and keys separately, using monopole (centroid) approximations augmented with dipole corrections to capture directional variance. It achieves O(NCD) acausal and O(NCD log N) causal complexity, serving as a drop-in replacement with modest accuracy loss; demonstrates speedups in isolated layers and practical gains in pretraining.", "motivation": "Transformers suffer quadratic complexity in sequence length; long-context modeling demands efficient attention. Prior clustering methods either group only keys or use joint clustering and do not respect the asymmetry between queries and keys. MuSe aims to reduce compute while preserving attention dynamics, enabling efficient pretraining with long context.", "method": "Two-stage hierarchical attention with separate clustering of queries and keys in their learned representations. Centroid-based (monopole) approximations augmented with dipole corrections to capture directional variance within clusters. Drop-in replacement requiring only hyperparameters; for causal attention, a hierarchical block decomposition that combines exact local computation with efficient long-range approximation.", "result": "In isolated attention layers, MuSe achieves ~3\u00d7 speedup over CUDNN Flash Attention at 8k context with relative squared error < 20%. For causal attention, it uses a hierarchical block decomposition to combine exact local and efficient long-range computation. In end-to-end pretraining of a 30M-parameter model on book-length texts with 16k context, MuSe achieves 12.2% runtime reduction with only 0.36% loss degradation.", "conclusion": "Multipole-based approximations are viable for efficient transformer pretraining and long-context modeling, offering substantial speedups with modest accuracy loss and requiring minimal architectural changes."}}
{"id": "2509.10282", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10282", "abs": "https://arxiv.org/abs/2509.10282", "authors": ["Gang Li", "Tianjiao Chen", "Mingle Zhou", "Min Li", "Delong Han", "Jin Wan"], "title": "MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection", "comment": "Page 14, 5 pictures", "summary": "Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects\nwithout relying on labeled training data, making it especially valuable in\nscenarios constrained by data scarcity, privacy, or high annotation cost.\nHowever, most existing methods focus exclusively on point clouds, neglecting\nthe rich semantic cues available from complementary modalities such as RGB\nimages and texts priors. This paper introduces MCL-AD, a novel framework that\nleverages multimodal collaboration learning across point clouds, RGB images,\nand texts semantics to achieve superior zero-shot 3D anomaly detection.\nSpecifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that\nenhances the intra-modal representation capability and inter-modal\ncollaborative learning by introducing an object-agnostic decoupled text prompt\nand a multimodal contrastive loss. In addition, a collaborative modulation\nmechanism (CMM) is proposed to fully leverage the complementary representations\nof point clouds and RGB images by jointly modulating the RGB image-guided and\npoint cloud-guided branches. Extensive experiments demonstrate that the\nproposed MCL-AD framework achieves state-of-the-art performance in ZS-3D\nanomaly detection.", "AI": {"tldr": "Proposes MCL-AD, a multimodal framework for zero-shot 3D anomaly detection using point clouds, RGB images, and text semantics. It introduces MPLM (multimodal prompt learning) with an object-agnostic decoupled text prompt and a multimodal contrastive loss, plus CMM (collaborative modulation) to fuse RGB-guided and point-cloud-guided branches, achieving state-of-the-art results on ZS-3D anomaly detection.", "motivation": "Zero-shot 3D anomaly detection struggles with data scarcity and privacy, and existing work mainly uses point clouds. Leveraging complementary modalities (RGB images and text priors) can enrich semantic understanding and improve detection without labeled data.", "method": "MCL-AD combines MPLM to enhance intra- and inter-modal learning via an object-agnostic decoupled text prompt and a multimodal contrastive loss, and a Collaborative Modulation Mechanism (CMM) to jointly modulate RGB-guided and point-cloud-guided branches for better multimodal fusion.", "result": "Extensive experiments show MCL-AD achieves state-of-the-art performance in zero-shot 3D anomaly detection across benchmarks, validating the effectiveness of multimodal collaboration.", "conclusion": "Multimodal collaboration across point clouds, RGB images, and texts Semantics via MPLM and CMM substantially improves zero-shot 3D anomaly detection, offering a strong direction for future multimodal anomaly detection research."}}
{"id": "2509.10419", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10419", "abs": "https://arxiv.org/abs/2509.10419", "authors": ["Francesco Vitale", "Tommaso Zoppi", "Francesco Flammini", "Nicola Mazzocca"], "title": "Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining", "comment": "Accepted to the 6th International Conference on Reliability, Safety,\n  and Security of Railway Systems (RSSRail2025)", "summary": "Ensuring the resilience of computer-based railways is increasingly crucial to\naccount for uncertainties and changes due to the growing complexity and\ncriticality of those systems. Although their software relies on strict\nverification and validation processes following well-established best-practices\nand certification standards, anomalies can still occur at run-time due to\nresidual faults, system and environmental modifications that were unknown at\ndesign-time, or other emergent cyber-threat scenarios. This paper explores\nrun-time control-flow anomaly detection using process mining to enhance the\nresilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European\nTrain Control System Level 2). Process mining allows learning the actual\ncontrol flow of the system from its execution traces, thus enabling run-time\nmonitoring through online conformance checking. In addition, anomaly\nlocalization is performed through unsupervised machine learning to link\nrelevant deviations to critical system components. We test our approach on a\nreference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its\ncapability to detect and localize anomalies with high accuracy, efficiency, and\nexplainability.", "AI": {"tldr": "A runtime anomaly-detection framework for ERTMS/ETCS L2 using process mining and unsupervised learning to detect and localize control-flow deviations in real time, demonstrated on the RBC Handover with high accuracy, efficiency, and explainability.", "motivation": "Enhance resilience of critical railway software against run-time faults, unknown design-time changes, and cyber threats, by enabling real-time monitoring beyond existing verification and certification.", "method": "Apply process mining to learn the system's actual control flow from execution traces; perform online conformance checking for runtime monitoring; use unsupervised machine learning to localize anomalies by linking deviations to critical components; validate on a reference ERTMS/ETCS L2 scenario (RBC/Handover).", "result": "The approach can detect and localize anomalies with high accuracy, efficiency, and explainability.", "conclusion": "Process mining-based run-time anomaly detection is a promising avenue to improve resilience of ERTMS/ETCS L2 rail systems, enabling real-time monitoring and component-level localization."}}
{"id": "2509.10298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10298", "abs": "https://arxiv.org/abs/2509.10298", "authors": ["Laith Nayal", "Mahmoud Mousatat", "Bader Rasheed"], "title": "Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks", "comment": "8 pages, 2 tables", "summary": "Deep neural networks and Vision Transformers achieve state-of-the-art\nperformance in computer vision but are highly vulnerable to adversarial\nperturbations. Standard defenses often incur high computational cost or lack\nformal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)\nmethod, where drop probabilities increase with depth to control the effective\nLipschitz constant of the network. This approach regularizes deeper layers,\nimproving robustness while preserving clean accuracy and reducing computation.\nExperiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent\nschedule maintains near-baseline clean accuracy, enhances robustness under\nFGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to\nbaseline and linear DropPath schedules.", "AI": {"tldr": "A Lipschitz-guided, depth-dependent DropPath regularizes networks to improve adversarial robustness with reduced FLOPs, maintaining clean accuracy on CIFAR-10 ViT-Tiny under various attacks.", "motivation": "Adversarial perturbations threaten DNNs/ViTs; existing defenses are costly or lack guarantees. A lightweight regularization that controls the network's Lipschitz constant could improve robustness while preserving accuracy and efficiency.", "method": "Introduce depth-dependent drop probabilities (Lipschitz-guided stochastic depth) that increase with depth to regulate the effective Lipschitz constant. Apply a custom depth schedule in DropPath, reducing computation. Evaluate on CIFAR-10 using ViT-Tiny, testing against FGSM, PGD-20, and AutoAttack.", "result": "Maintains near-baseline clean accuracy; improves robustness against specified attacks; significantly reduces FLOPs compared to both baseline and a linear DropPath schedule.", "conclusion": "Depth-dependent DropPath provides a practical, efficient defense by constraining the Lipschitz behavior of deep networks, achieving robustness gains with lower computation and without sacrificing clean accuracy."}}
{"id": "2509.10439", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.10439", "abs": "https://arxiv.org/abs/2509.10439", "authors": ["Ahmed Khaled", "Satyen Kale", "Arthur Douillard", "Chi Jin", "Rob Fergus", "Manzil Zaheer"], "title": "Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration", "comment": null, "summary": "Modern machine learning often requires training with large batch size,\ndistributed data, and massively parallel compute hardware (like mobile and\nother edge devices or distributed data centers). Communication becomes a major\nbottleneck in such settings but methods like Local Stochastic Gradient Descent\n(Local SGD) show great promise in reducing this additional communication\noverhead. Local SGD consists of three parts: a local optimization process, an\naggregation mechanism, and an outer optimizer that uses the aggregated updates\nfrom the nodes to produce a new model. While there exists an extensive\nliterature on understanding the impact of hyperparameters in the local\noptimization process, the choice of outer optimizer and its hyperparameters is\nless clear. We study the role of the outer optimizer in Local SGD, and prove\nnew convergence guarantees for the algorithm. In particular, we show that\ntuning the outer learning rate allows us to (a) trade off between optimization\nerror and stochastic gradient noise variance, and (b) make up for ill-tuning of\nthe inner learning rate. Our theory suggests that the outer learning rate\nshould sometimes be set to values greater than $1$. We extend our results to\nsettings where we use momentum in the outer optimizer, and we show a similar\nrole for the momentum-adjusted outer learning rate. We also study acceleration\nin the outer optimizer and show that it improves the convergence rate as a\nfunction of the number of communication rounds, improving upon the convergence\nrate of prior algorithms that apply acceleration locally. Finally, we also\nintroduce a novel data-dependent analysis of Local SGD that yields further\ninsights on outer learning rate tuning. We conduct comprehensive experiments\nwith standard language models and various outer optimizers to validate our\ntheory.", "AI": {"tldr": "Outer optimization in Local SGD is crucial: tuning the outer learning rate can balance optimization error and gradient noise, and may exceed 1 to compensate inner learning-rate issues; adding momentum or acceleration to the outer optimizer further improves convergence and reduces communication rounds, with data-dependent insights supporting these choices.", "motivation": "Distributed training with large batches and limited communication suffers from bottlenecks. Local SGD helps but the outer optimizer and its hyperparameters are under-studied. Understanding and guiding outer-optimizer design can improve convergence and reduce communication costs.", "method": "The paper provides a theoretical convergence analysis for Local SGD incorporating various outer optimizers (including momentum) and their hyperparameters. It shows how the outer learning rate interacts with inner learning rates and noise, extends the theory to momentum and accelerated variants, and offers a data-dependent analysis. It also backs the theory with experiments on standard language models using different outer optimizers.", "result": "Convergence guarantees are established for Local SGD with flexible outer optimizers. The outer learning rate should sometimes exceed 1 to achieve favorable trade-offs, and momentum-adjusted outer rates behave similarly. Outer acceleration can improve the convergence rate with respect to communication rounds, outperforming methods that apply acceleration locally. Data-dependent analyses yield further guidance for tuning the outer learning rate. Empirical validation on language models confirms the theoretical insights.", "conclusion": "Outer-optimizer design is pivotal for Local SGD performance. Properly tuning the outer learning rate (potentially >1), incorporating momentum, and using outer-optimizer acceleration can substantially improve convergence and reduce communication burden. The results provide both theoretical guarantees and practical guidance for selecting outer-optimizer hyperparameters in distributed training."}}
{"id": "2509.10334", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10334", "abs": "https://arxiv.org/abs/2509.10334", "authors": ["Jordan Sassoon", "Michal Szczepanski", "Martyna Poreba"], "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation", "comment": null, "summary": "Vision Transformers (ViTs) have recently achieved strong results in semantic\nsegmentation, yet their deployment on resource-constrained devices remains\nlimited due to their high memory footprint and computational cost. Quantization\noffers an effective strategy to improve efficiency, but ViT-based segmentation\nmodels are notoriously fragile under low precision, as quantization errors\naccumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the\nfirst fully integer-only ViT segmentation framework. Building on the Segmenter\narchitecture, I-Segmenter systematically replaces floating-point operations\nwith integer-only counterparts. To further stabilize both training and\ninference, we propose $\\lambda$-ShiftGELU, a novel activation function that\nmitigates the limitations of uniform quantization in handling long-tailed\nactivation distributions. In addition, we remove the L2 normalization layer and\nreplace bilinear interpolation in the decoder with nearest neighbor upsampling,\nensuring integer-only execution throughout the computational graph. Extensive\nexperiments show that I-Segmenter achieves accuracy within a reasonable margin\nof its FP32 baseline (5.1 % on average), while reducing model size by up to\n3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,\neven in one-shot PTQ with a single calibration image, I-Segmenter delivers\ncompetitive accuracy, underscoring its practicality for real-world deployment.", "AI": {"tldr": "I-Segmenter is the first fully integer-only ViT-based semantic segmentation framework, achieving competitive accuracy with substantial efficiency gains and enabling practical deployment on resource-constrained devices.", "motivation": "ViT-based segmentation delivers strong accuracy but suffers from high memory and compute, and is fragile under quantization; there is a need for an all-integer pipeline to enable deployment on constrained hardware.", "method": "Extend the Segmenter architecture by replacing all floating-point operations with integer-only equivalents, introduce lambda-ShiftGELU activation to better handle long-tailed activations under quantization, remove L2 normalization, and replace bilinear upsampling with nearest-neighbor to ensure integer-only execution; demonstrate one-shot PTQ with a single calibration image.", "result": "Achieves accuracy within ~5.1 percentage points of the FP32 baseline, while reducing model size by up to 3.8x and enabling up to 1.2x faster inference with optimized runtimes; one-shot PTQ remains competitive.", "conclusion": "Fully integer-only ViT segmentation is feasible with modest accuracy loss and significant efficiency gains, enabling practical deployment on resource-limited devices."}}
{"id": "2509.10310", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10310", "abs": "https://arxiv.org/abs/2509.10310", "authors": ["Evan Murphy", "Marco Viola", "Vladimir A. Krylov"], "title": "A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments", "comment": "Accepted for publication in the Proceedings of the 27th Irish Machine\n  Vision and Image Processing Conference (IMVIP 2025)", "summary": "In this paper we address the problem of precise geolocation of street\nfurniture in complex urban environments, which is a critical task for effective\nmonitoring and maintenance of public infrastructure by local authorities and\nprivate stakeholders. To this end, we propose a probabilistic framework based\non energy maps that encode the spatial likelihood of object locations.\nRepresenting the energy in a map-based geopositioned format allows the\noptimisation process to seamlessly integrate external geospatial information,\nsuch as GIS layers, road maps, or placement constraints, which improves\ncontextual awareness and localisation accuracy. A stochastic birth-and-death\noptimisation algorithm is introduced to infer the most probable configuration\nof assets. We evaluate our approach using a realistic simulation informed by a\ngeolocated dataset of street lighting infrastructure in Dublin city centre,\ndemonstrating its potential for scalable and accurate urban asset mapping. The\nimplementation of the algorithm will be made available in the GitHub repository\nhttps://github.com/EMurphy0108/SBD_Street_Furniture.", "AI": {"tldr": "Proposes a probabilistic, energy-map based framework for precise street furniture geolocation using a stochastic birth-and-death optimization, integrating GIS data; evaluated in Dublin via realistic simulation; code to be released on GitHub.", "motivation": "Accurate urban asset localization is crucial for monitoring and maintenance of public infrastructure in complex city environments; leveraging external geospatial information can improve localization accuracy.", "method": "Energy maps encode the spatial likelihood of asset locations and are used for map-based geopositioning. A stochastic birth-and-death optimization algorithm infers the most probable configuration of assets while seamlessly integrating GIS layers, road maps, and placement constraints.", "result": "Evaluated with a realistic Dublin city centre-inspired simulation using a geolocated dataset of street lighting; demonstrates potential for scalable and accurate urban asset mapping; GitHub implementation to be released.", "conclusion": "The framework enables seamless integration of external geospatial information into localization, supporting scalable urban asset mapping. The authors plan to release the code on GitHub."}}
{"id": "2509.10344", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10344", "abs": "https://arxiv.org/abs/2509.10344", "authors": ["Yuexi Du", "Lihui Chen", "Nicha C. Dvornek"], "title": "GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography", "comment": "Accepted by MICCAI 2025", "summary": "Mammography screening is an essential tool for early detection of breast\ncancer. The speed and accuracy of mammography interpretation have the potential\nto be improved with deep learning methods. However, the development of a\nfoundation visual language model (VLM) is hindered by limited data and domain\ndifferences between natural and medical images. Existing mammography VLMs,\nadapted from natural images, often ignore domain-specific characteristics, such\nas multi-view relationships in mammography. Unlike radiologists who analyze\nboth views together to process ipsilateral correspondence, current methods\ntreat them as independent images or do not properly model the multi-view\ncorrespondence learning, losing critical geometric context and resulting in\nsuboptimal prediction. We propose GLAM: Global and Local Alignment for\nMulti-view mammography for VLM pretraining using geometry guidance. By\nleveraging the prior knowledge about the multi-view imaging process of\nmammograms, our model learns local cross-view alignments and fine-grained local\nfeatures through joint global and local, visual-visual, and visual-language\ncontrastive learning. Pretrained on EMBED [14], one of the largest open\nmammography datasets, our model outperforms baselines across multiple datasets\nunder different settings.", "AI": {"tldr": "GLAM introduces a geometry-guided multi-view alignment strategy for pretraining a visual-language model on mammography, capturing local cross-view correspondences and global context to better model ipsilateral view relationships; pretrained on EMBED, it outperforms baselines on multiple datasets.", "motivation": "Mammography VLMs suffer from limited domain-specific data and differences from natural images; crucial multi-view relationships between the two mammogram views are not properly encoded by prior methods, hindering performance.", "method": "Global and Local Alignment for Multi-view mammography (GLAM) leverages geometry guidance to learn local cross-view alignments and fine-grained features via joint global/local, visual-visual, and visual-language contrastive learning. Pretrained on EMBED.", "result": "The GLAM model outperforms baselines across multiple datasets under various settings.", "conclusion": "Incorporating geometry-guided multi-view alignment improves VLM pretraining for mammography by preserving ipsilateral geometric context and cross-view relationships."}}
{"id": "2509.10312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10312", "abs": "https://arxiv.org/abs/2509.10312", "authors": ["Zhixin Zheng", "Xinyu Wang", "Chang Zou", "Shaobo Wang", "Linfeng Zhang"], "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching", "comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration", "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.", "AI": {"tldr": "ClusCa introduces Cluster-Driven Feature Caching for diffusion transformers, using per-timestep spatial clustering to reduce tokens and compute only one token per cluster, enabling substantial speedups (up to 4.96x on FLUX) with minimal performance loss, plug-and-play for training-free integration.", "motivation": "Despite progress with temporal feature caching, spatial redundancy in token representations is overlooked. Diffusion models are expensive due to iterative denoising; exploiting spatial similarity alongside temporal similarity can further accelerate generation.", "method": "Apply spatial clustering to tokens at each diffusion timestep, compute a single token per cluster, and propagate its information to the rest; reduces tokens by >90%; can be used as a complement to existing caching; training-free; applicable to DiT, FLUX, HunyuanVideo.", "result": "Empirically improves speed for text-to-image and text-to-video on multiple diffusion transformers; reported 4.96x acceleration on FLUX with ImageReward 99.49% (0.51% higher than baseline).", "conclusion": "ClusCa provides an orthogonal, plug-in acceleration technique for diffusion transformers that leverages spatial redundancy and requires no retraining, with broad applicability across diffusion models."}}
{"id": "2509.10345", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10345", "abs": "https://arxiv.org/abs/2509.10345", "authors": ["Georgios Pantazopoulos", "Eda B. \u00d6zyi\u011fit"], "title": "Towards Understanding Visual Grounding in Visual Language Models", "comment": null, "summary": "Visual grounding refers to the ability of a model to identify a region within\nsome visual input that matches a textual description. Consequently, a model\nequipped with visual grounding capabilities can target a wide range of\napplications in various domains, including referring expression comprehension,\nanswering questions pertinent to fine-grained details in images or videos,\ncaption visual context by explicitly referring to entities, as well as low and\nhigh-level control in simulated and real environments. In this survey paper, we\nreview representative works across the key areas of research on modern\ngeneral-purpose vision language models (VLMs). We first outline the importance\nof grounding in VLMs, then delineate the core components of the contemporary\nparadigm for developing grounded models, and examine their practical\napplications, including benchmarks and evaluation metrics for grounded\nmultimodal generation. We also discuss the multifaceted interrelations among\nvisual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,\nwe analyse the challenges inherent to visual grounding and suggest promising\ndirections for future research.", "AI": {"tldr": "A survey of visual grounding in modern VLMs, outlining importance, components, applications, benchmarks, reasoning relations, challenges, and future directions.", "motivation": "Visual grounding enables models to pinpoint image regions matching natural language, enabling tasks like referring expression comprehension, fine-grained QA, captioning with explicit entity references, and controllable behavior in simulators and real environments. A unified survey helps map the field and guide future research.", "method": "Literature survey of representative works across grounding areas in VLMs; synthesize core components; categorize applications and benchmarks; discuss relations to multimodal reasoning and chain-of-thought; identify challenges and propose future directions.", "result": "A cohesive framework including taxonomy of grounding components, summaries of benchmarks/metrics, analysis of interactions with reasoning, and a set of challenges plus recommended research directions.", "conclusion": "Grounded VLMs are central to versatile vision-language intelligence; progress is evident but challenges remain in evaluation, generalization, and integration of reasoning; future work should advance benchmarks, efficiency, and cross-domain grounding capabilities."}}
{"id": "2509.10341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10341", "abs": "https://arxiv.org/abs/2509.10341", "authors": ["Botond Fazekas", "Thomas Pinetz", "Guilherme Aresta", "Taha Emre", "Hrvoje Bogunovic"], "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT", "comment": null, "summary": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing\nand monitoring retinal diseases. However, OCT images are inherently degraded by\nspeckle noise, which obscures fine details and hinders accurate interpretation.\nWhile numerous denoising methods exist, many struggle to balance noise\nreduction with the preservation of crucial anatomical structures. This paper\nintroduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel\ndeep learning approach for OCT image despeckling that leverages the strengths\nof diffusion probabilistic models. Unlike conventional diffusion models that\nassume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more\naccurately reflect the statistical properties of speckle. Furthermore, we\nintroduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,\nless-noisy image to guide the denoising process. This crucial addition prevents\nthe reintroduction of high-frequency noise. We accelerate the inference process\nby adapting the Denoising Diffusion Implicit Model framework to our Gamma-based\nmodel. Experiments on a dataset with paired noisy and less-noisy OCT B-scans\ndemonstrate that GARD significantly outperforms traditional denoising methods\nand state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.\nQualitative results confirm that GARD produces sharper edges and better\npreserves fine anatomical details.", "AI": {"tldr": "A diffusion-based OCT denoising method (GARD) with gamma-distributed noise modelling and a noise-reduced fidelity term, achieving superior despeckling and edge preservation.", "motivation": "Speckle noise in OCT degrades detail and hinders interpretation; many denoisers fail to suppress noise while preserving fine anatomical structures; modeling speckle with a Gamma distribution and guiding denoising with a cleaner image could improve restoration.", "method": "Introduce Denoising Diffusion Gamma Model (DDGM) for despeckling by aligning diffusion steps with Gamma noise statistics; incorporate a Noise-Reduced Fidelity Term that uses a pre-processed, less-noisy image to guide denoising and prevent reintroduction of high-frequency noise; adapt Denoising Diffusion Implicit Model (DDIM) for faster inference; train/evaluate on paired noisy/less-noisy OCT B-scans.", "result": "GARD outperforms traditional denoising methods and state-of-the-art DL models in PSNR, SSIM, and MSE; qualitative results show sharper edges and better preservation of fine anatomical details.", "conclusion": "Gamma-based diffusion despeckling with a guided fidelity term and accelerated inference provides effective OCT despeckling that preserves anatomical structures, with potential for improved clinical interpretation."}}
{"id": "2509.10408", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10408", "abs": "https://arxiv.org/abs/2509.10408", "authors": ["Iacopo Curti", "Pierluigi Zama Ramirez", "Alioscia Petrelli", "Luigi Di Stefano"], "title": "Multimodal SAM-adapter for Semantic Segmentation", "comment": null, "summary": "Semantic segmentation, a key task in computer vision with broad applications\nin autonomous driving, medical imaging, and robotics, has advanced\nsubstantially with deep learning. Nevertheless, current approaches remain\nvulnerable to challenging conditions such as poor lighting, occlusions, and\nadverse weather. To address these limitations, multimodal methods that\nintegrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,\nproviding complementary information that enhances robustness. In this work, we\npresent MM SAM-adapter, a novel framework that extends the capabilities of the\nSegment Anything Model (SAM) for multimodal semantic segmentation. The proposed\nmethod employs an adapter network that injects fused multimodal features into\nSAM's rich RGB features. This design enables the model to retain the strong\ngeneralization ability of RGB features while selectively incorporating\nauxiliary modalities only when they contribute additional cues. As a result, MM\nSAM-adapter achieves a balanced and efficient use of multimodal information. We\nevaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,\nwhere MM SAM-adapter delivers state-of-the-art performance. To further analyze\nmodality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard\nsubsets. Results consistently demonstrate that our framework outperforms\ncompeting methods in both favorable and adverse conditions, highlighting the\neffectiveness of multimodal adaptation for robust scene understanding. The code\nis available at the following link:\nhttps://github.com/iacopo97/Multimodal-SAM-Adapter.", "AI": {"tldr": "MM SAM-adapter introduces an adapter to fuse multimodal features into SAM for semantic segmentation, achieving state-of-the-art results on DeLiVER, FMB, and MUSES while robustly leveraging auxiliary modalities when beneficial.", "motivation": "Current semantic segmentation methods struggle under challenging conditions (e.g., poor lighting, occlusions, adverse weather). Multimodal data from sensors like LiDAR or infrared can provide complementary cues, but extending powerful RGB-based models like SAM to multimodal inputs without losing RGB generalization remains challenging.", "method": "An adapter network fuses multimodal features and injects them into SAM's RGB feature stream, enabling selective use of auxiliary modalities only when they contribute additional cues and preserving RGB generalization.", "result": "State-of-the-art performance on three benchmarks (DeLiVER, FMB, MUSES); consistent improvements on RGB-easy and RGB-hard subsets, under both favorable and adverse conditions.", "conclusion": "Demonstrates the effectiveness of multimodal adaptation for robust scene understanding; code is released for reuse and experimentation."}}
{"id": "2509.10359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10359", "abs": "https://arxiv.org/abs/2509.10359", "authors": ["Matteo Trippodo", "Federico Becattini", "Lorenzo Seidenari"], "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention", "comment": "Accepted as Regular Paper at ACM Multimedia 2025", "summary": "Recent advances in text-based image editing have enabled fine-grained\nmanipulation of visual content guided by natural language. However, such\nmethods are susceptible to adversarial attacks. In this work, we propose a\nnovel attack that targets the visual component of editing methods. We introduce\nAttention Attack, which disrupts the cross-attention between a textual prompt\nand the visual representation of the image by using an automatically generated\ncaption of the source image as a proxy for the edit prompt. This breaks the\nalignment between the contents of the image and their textual description,\nwithout requiring knowledge of the editing method or the editing prompt.\nReflecting on the reliability of existing metrics for immunization success, we\npropose two novel evaluation strategies: Caption Similarity, which quantifies\nsemantic consistency between original and adversarial edits, and semantic\nIntersection over Union (IoU), which measures spatial layout disruption via\nsegmentation masks. Experiments conducted on the TEDBench++ benchmark\ndemonstrate that our attack significantly degrades editing performance while\nremaining imperceptible.", "AI": {"tldr": "Proposes Attention Attack, a novel adversarial attack on text-based image editing that disrupts cross-attention by using an automatically generated image caption as a proxy edit prompt; introduces new evaluation metrics and demonstrates effectiveness on TEDBench++ with imperceptible perturbations.", "motivation": "Text-based image editing systems are vulnerable to adversarial manipulation, and current metrics for immunization success may be unreliable. There is a need to understand how adversarial cues can affect alignment between image content and its textual description and to develop robust evaluation tools.", "method": "The Attack targets the cross-attention between textual prompts and image representations, using an automatically generated caption of the source image as a proxy for the edit prompt to disrupt alignment. It does not require knowledge of the specific editing method or the edit prompt. The authors also introduce two evaluation strategies: Caption Similarity (semantic consistency between original and adversarial edits) and semantic Intersection over Union (IoU) (spatial disruption via segmentation masks).", "result": "Empirical results on the TEDBench++ benchmark show that Attention Attack significantly degrades editing performance while remaining imperceptible to observers.", "conclusion": "The work highlights potential reliability issues in existing immunization metrics and offers new evaluation tools (Caption Similarity and semantic IoU) for assessing adversarial robustness in text-guided image editing, underscoring the need for more robust defenses."}}
{"id": "2509.10366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10366", "abs": "https://arxiv.org/abs/2509.10366", "authors": ["Fabien Allemand", "Attilio Fiandrotti", "Sumanta Chaudhuri", "Alaa Eddine Mazouz"], "title": "Efficient Learned Image Compression Through Knowledge Distillation", "comment": "19 pages, 21 figures", "summary": "Learned image compression sits at the intersection of machine learning and\nimage processing. With advances in deep learning, neural network-based\ncompression methods have emerged. In this process, an encoder maps the image to\na low-dimensional latent space, which is then quantized, entropy-coded into a\nbinary bitstream, and transmitted to the receiver. At the receiver end, the\nbitstream is entropy-decoded, and a decoder reconstructs an approximation of\nthe original image. Recent research suggests that these models consistently\noutperform conventional codecs. However, they require significant processing\npower, making them unsuitable for real-time use on resource-constrained\nplatforms, which hinders their deployment in mainstream applications. This\nstudy aims to reduce the resource requirements of neural networks used for\nimage compression by leveraging knowledge distillation, a training paradigm\nwhere smaller neural networks, partially trained on the outputs of larger, more\ncomplex models, can achieve better performance than when trained independently.\nOur work demonstrates that knowledge distillation can be effectively applied to\nimage compression tasks: i) across various architecture sizes, ii) to achieve\ndifferent image quality/bit rate tradeoffs, and iii) to save processing and\nenergy resources. This approach introduces new settings and hyperparameters,\nand future research could explore the impact of different teacher models, as\nwell as alternative loss functions. Knowledge distillation could also be\nextended to transformer-based models. The code is publicly available at:\nhttps://github.com/FABallemand/PRIM .", "AI": {"tldr": "Knowledge distillation enables smaller neural image compression models to match or exceed independently trained counterparts while reducing compute and energy, across different architectures and rate-quality tradeoffs; code available.", "motivation": "Address high resource requirements of neural image compression to enable real-time deployment on constrained devices.", "method": "Train smaller networks via knowledge distillation using outputs of larger teacher models across multiple architecture sizes; explore different loss functions and hyperparameters; discuss extension to transformer-based models.", "result": "Demonstrates that distillation can be effectively applied to image compression tasks; smaller models can achieve better performance or comparable results with less resources; saves processing and energy; provides new hyperparameters and settings; lacks numerical metrics in abstract.", "conclusion": "Knowledge distillation is a viable route for resource-efficient neural image compression; future work includes exploring teacher models, loss functions, and transformer-based architectures; code is publicly available."}}
{"id": "2509.10388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10388", "abs": "https://arxiv.org/abs/2509.10388", "authors": ["Zeqing Leo Yuan", "Mani Ramanagopal", "Aswin C. Sankaranarayanan", "Srinivasa G. Narasimhan"], "title": "Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition", "comment": null, "summary": "Decomposing an image into its intrinsic photometric factors--shading and\nreflectance--is a long-standing challenge due to the lack of extensive\nground-truth data for real-world scenes. Recent methods rely on synthetic data\nor sparse annotations for limited indoor and even fewer outdoor scenes. We\nintroduce a novel training-free approach for intrinsic image decomposition\nusing only a pair of visible and thermal images. We leverage the principle that\nlight not reflected from an opaque surface is absorbed and detected as heat by\na thermal camera. This allows us to relate the ordinalities between visible and\nthermal image intensities to the ordinalities of shading and reflectance, which\ncan densely self-supervise an optimizing neural network to recover shading and\nreflectance. We perform quantitative evaluations with known reflectance and\nshading under natural and artificial lighting, and qualitative experiments\nacross diverse outdoor scenes. The results demonstrate superior performance\nover recent learning-based models and point toward a scalable path to curating\nreal-world ordinal supervision, previously infeasible via manual labeling.", "AI": {"tldr": "A training-free intrinsic image decomposition method using paired visible and thermal images to infer shading and reflectance, leveraging a physical heat-emission principle to self-supervise a neural network, achieving superior performance and enabling scalable real-world supervision.", "motivation": "Intrinsic image decomposition is hampered by scarce real-world ground-truth data; synthetic data and sparse annotations are inadequate for broad outdoor scenes; there is a need for scalable, real-world supervision that does not rely on manual labeling.", "method": "Use only a visible-thermal image pair and a physical principle: light not reflected by opaque surfaces is absorbed and converted to heat detected by a thermal camera. Relate the ordinal relations between visible and thermal intensities to the ordinalities of shading and reflectance, enabling a densely self-supervised optimization of a neural network to recover shading and reflectance without training data (training-free).", "result": "Quantitative evaluations with known shading/reflectance under natural and artificial lighting and qualitative outdoor scene experiments; the method shows superior performance compared to recent learning-based models.", "conclusion": "Demonstrates a scalable path to acquiring real-world ordinal supervision for intrinsic decomposition without manual labeling, reducing dependence on synthetic data and annotations and enabling broader applicability."}}
{"id": "2509.10407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10407", "abs": "https://arxiv.org/abs/2509.10407", "authors": ["Xiem HoangVan", "Dang BuiDinh", "Sang NguyenQuang", "Wen-Hsiao Peng"], "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards", "comment": null, "summary": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment.", "AI": {"tldr": "Taxonomy, benchmarking framework, and trade-off analysis for CVQE to standardize evaluation across codecs and architectures.", "motivation": "Existing surveys fail to map CVQE methods to specific codecs/artifacts, compare architectural paradigms across coding types, and provide robust benchmarking.", "method": "Introduce a three-part framework: (1) a taxonomy of CVQE methods by architectural paradigm, coding standard, and compressed-domain features; (2) a unified benchmarking framework aligned with modern codecs and test sequences; (3) a systematic analysis of reconstruction quality vs. computational complexity.", "result": "Proposes a taxonomy, benchmarking framework, and trade-off insights; enables fair cross-method comparisons and multi-criteria evaluation.", "conclusion": "Lays the groundwork for consistent CVQE assessment and informed model selection for research and deployment."}}
{"id": "2509.10441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10441", "abs": "https://arxiv.org/abs/2509.10441", "authors": ["Tao Han", "Wanghan Xu", "Junchao Gong", "Xiaoyu Yue", "Song Guo", "Luping Zhou", "Lei Bai"], "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis", "comment": "Accepted by ICCV 2025", "summary": "Arbitrary resolution image generation provides a consistent visual experience\nacross devices, having extensive applications for producers and consumers.\nCurrent diffusion models increase computational demand quadratically with\nresolution, causing 4K image generation delays over 100 seconds. To solve this,\nwe explore the second generation upon the latent diffusion models, where the\nfixed latent generated by diffusion models is regarded as the content\nrepresentation and we propose to decode arbitrary resolution images with a\ncompact generated latent using a one-step generator. Thus, we present the\n\\textbf{InfGen}, replacing the VAE decoder with the new generator, for\ngenerating images at any resolution from a fixed-size latent without retraining\nthe diffusion models, which simplifies the process, reducing computational\ncomplexity and can be applied to any model using the same latent space.\nExperiments show InfGen is capable of improving many models into the arbitrary\nhigh-resolution era while cutting 4K image generation time to under 10 seconds.", "AI": {"tldr": "InfGen introduces a one-step generator to decode a fixed-size latent from diffusion models into arbitrary-resolution images, replacing the VAE decoder, enabling 4K generation in under 10 seconds without retraining diffusion models; it reduces computational complexity and is model-agnostic within the same latent space.", "motivation": "Diffusion models incur quadratic compute with image resolution, making high-resolution generation (e.g., 4K) slow. A need exists for fast, scalable, arbitrary-resolution image generation without retraining or heavy recomputation.", "method": "Use a fixed latent produced by diffusion models as content representation and train a compact, one-step generator to decode it into images at arbitrary resolutions. Replace the VAE decoder with this generator. Keep diffusion models unchanged; apply across models that share the same latent space.", "result": "The approach enables arbitrary high-resolution generation across models and significantly reduces generation time (e.g., 4K under 10 seconds). It reduces computational complexity and can generate high-resolution outputs without retraining diffusion systems.", "conclusion": "InfGen offers a simple, efficient pathway to arbitrary-resolution image generation by decoupling latent diffusion from the final high-res decoding, broadening applicability and speeding up high-resolution pipelines."}}
{"id": "2509.10453", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10453", "abs": "https://arxiv.org/abs/2509.10453", "authors": ["Emily Kaczmarek", "Justin Szeto", "Brennan Nichyporuk", "Tal Arbel"], "title": "SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets", "comment": null, "summary": "Alzheimer's disease is a progressive, neurodegenerative disorder that causes\nmemory loss and cognitive decline. While there has been extensive research in\napplying deep learning models to Alzheimer's prediction tasks, these models\nremain limited by lack of available labeled data, poor generalization across\ndatasets, and inflexibility to varying numbers of input scans and time\nintervals between scans. In this study, we adapt three state-of-the-art\ntemporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,\nand add novel extensions designed to handle variable-length inputs and learn\nrobust spatial features. We aggregate four publicly available datasets\ncomprising 3,161 patients for pre-training, and show the performance of our\nmodel across multiple Alzheimer's prediction tasks including diagnosis\nclassification, conversion detection, and future conversion prediction.\nImportantly, our SSL model implemented with temporal order prediction and\ncontrastive learning outperforms supervised learning on six out of seven\ndownstream tasks. It demonstrates adaptability and generalizability across\ntasks and number of input images with varying time intervals, highlighting its\ncapacity for robust performance across clinical applications. We release our\ncode and model publicly at https://github.com/emilykaczmarek/SSL-AD.", "AI": {"tldr": "A self-supervised temporal 3D MRI learning framework for Alzheimer's prediction that uses temporal order prediction and contrastive learning to handle variable input lengths and time intervals, achieving strong generalization and outperforming supervised learning on most tasks.", "motivation": "Overcoming limited labeled data, poor cross-dataset generalization, and the need to handle varying numbers of input scans and irregular time intervals in longitudinal MRI for Alzheimer\u2019s disease prediction.", "method": "Adapt three state-of-the-art temporal SSL methods for 3D brain MRI, add extensions for variable-length inputs and robust spatial features, and pretrain on 4 public datasets (n=3,161) before evaluating on downstream tasks. The SSL model leverages temporal order prediction and contrastive learning.", "result": "The SSL model outperforms supervised learning on 6 of 7 downstream tasks and shows strong adaptability to different numbers of input images and varying time intervals, indicating robust generalization across clinical settings.", "conclusion": "Temporal self-supervision with temporal order and contrastive objectives effectively addresses data scarcity and variability in longitudinal MRI, improving Alzheimer's prediction and generalizability; code and model released."}}
