<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 145]
- [cs.RO](#cs.RO) [Total: 54]
- [cs.AI](#cs.AI) [Total: 50]
- [cs.LG](#cs.LG) [Total: 152]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Real-Time Diminished Reality Approach to Privacy in MR Collaboration](https://arxiv.org/abs/2509.10466)
*Christian Fane*

Main category: cs.CV

TL;DR: A real-time diminished reality system for privacy control in shared MR meetings, enabling selective removal of objects from the secondary observer’s viewpoint with inpainting, using YOLOv11 for detection and a modified DSTT for high-quality video inpainting, achieving >20 fps at 720p without fixed secondary viewpoints or prior 3D data.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in shared-space MR environments by allowing a primary user to hide personal or sensitive objects from other participants in real-time, without requiring a fixed secondary camera setup or prior 3D scans.

Method: Semantic segmentation and precise object selection identify targets, followed by real-time inpainting from the secondary observer’s viewpoint. Implementation uses a mobile ZED 2i depth camera, YOLOv11 for detection, and a modified Decoupled Spatial-Temporal Transformer (DSTT) for inpainting. The pipeline runs at 720p with >20 fps and does not require a fixed secondary viewpoint or prior environment scanning.

Result: Demonstrates real-time diminished reality feasibility for privacy-preserving MR, achieving >20 fps at 720p and offering portability and robustness by not relying on a fixed secondary viewpoint or pre-scanned environments.

Conclusion: The approach shows promise for practical privacy-preserving MR deployments, combining object-level removal with high-quality inpainting. Future work could address broader object categories, robustness under complex scenes, and optimization for lower latency and energy efficiency.

Abstract: Diminished reality (DR) refers to the digital removal of real-world objects
by compositing background content in their place. This thesis presents a
real-time, inpainting-based DR system designed to enable privacy control in
shared-space mixed reality (MR) meetings. The system allows a primary headset
user to selectively remove personal or sensitive items from their environment,
ensuring that those objects are no longer visible to other participants.
Removal is achieved through semantic segmentation and precise object selection,
followed by real-time inpainting from the viewpoint of a secondary observer,
implemented using a mobile ZED 2i depth camera. The solution is designed to be
portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D
scanning of the environment. The system utilises YOLOv11 for object detection
and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for
high-quality video inpainting. At 720p resolution, the pipeline sustains frame
rates exceeding 20 fps, demonstrating the feasibility of real-time diminished
reality for practical privacy-preserving MR applications.

</details>


### [2] [SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning](https://arxiv.org/abs/2509.10555)
*Alejandra Perez,Chinedu Nwoye,Ramtin Raji Kermani,Omid Mohareri,Muhammad Abdullah Jamal*

Main category: cs.CV

TL;DR: SurgLaVi provides a large, hierarchical surgical VLP dataset, enabling SurgCLIP-style models that outperform previous methods.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of existing surgical VLP datasets (scale, diversity, semantic quality, hierarchy) to enable robust foundation models and cross-task transfer with minimal labeling.

Method: Developed an automated pipeline to transcribe and segment videos into phase/step units, applied dual-modality filtering to ensure quality captions; released SurgLaVi-beta from public data; implemented SurgCLIP with dual encoders for video-text contrastive learning.

Result: Surpass prior SOTA on phase, step, action, and tool recognition; SurgLaVi is largest and most diverse surgical VLP dataset to date; open-source derivative expands accessibility; large-scale, semantically rich data improves representations and generalization.

Conclusion: SurgLaVi constitutes a foundational resource for surgical foundation models; hierarchical, high-quality data translates into stronger, more generalizable models; fosters further research and practical deployment.

Abstract: Vision-language pre-training (VLP) offers unique advantages for surgery by
aligning language with surgical videos, enabling workflow understanding and
transfer across tasks without relying on expert-labeled datasets. However,
progress in surgical VLP remains constrained by the limited scale, procedural
diversity, semantic quality, and hierarchical structure of existing datasets.
In this work, we present SurgLaVi, the largest and most diverse surgical
vision-language dataset to date, comprising nearly 240k clip-caption pairs from
more than 200 procedures, and comprising hierarchical levels at phase-, step-,
and task-level. At the core of SurgLaVi lies a fully automated pipeline that
systematically generates fine-grained transcriptions of surgical videos and
segments them into coherent procedural units. To ensure high-quality
annotations, it applies dual-modality filtering to remove irrelevant and noisy
samples. Within this framework, the resulting captions are enriched with
contextual detail, producing annotations that are both semantically rich and
easy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, an
open-source derivative of 113k clip-caption pairs constructed entirely from
public data, which is over four times larger than existing surgical VLP
datasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,
a CLIP-style video-text contrastive framework with dual encoders, as a
representative base model. SurgCLIP achieves consistent improvements across
phase, step, action, and tool recognition, surpassing prior state-of-the-art
methods, often by large margins. These results validate that large-scale,
semantically rich, and hierarchically structured datasets directly translate
into stronger and more generalizable representations, establishing SurgLaVi as
a key resource for developing surgical foundation models.

</details>


### [3] [Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses](https://arxiv.org/abs/2509.10620)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: A high-resolution 3D brain MRI SSL foundation model based on SimCLR trained on 18,759 patients (44,958 scans) across 11 datasets; outperforms MAE and supervised baselines on four downstream tasks, even with limited labeled data (20%); code released for broad clinical use.


<details>
  <summary>Details</summary>
Motivation: To address the lack of generalizable foundation models for 3D brain MRI and the limitations of task-specific models. SSL enables leveraging large unlabeled datasets across diverse health states, supporting robust performance across tasks and populations.

Method: Self-supervised learning using a SimCLR-based framework trained on high-resolution 3D brain structural MRI from 11 public datasets (18,759 patients, 44,958 scans). Compared against Masked Autoencoders (MAE) and two supervised baselines on four downstream tasks, evaluated in-distribution and out-of-distribution, with finetuning using varying labeled data (notably as little as 20%).

Result: The finetuned SimCLR model outperforms MAE and supervised baselines across all four downstream tasks in both in-distribution and out-of-distribution settings. It remains superior when fine-tuned with only 20% of labeled data for Alzheimer's disease prediction, indicating strong generalization and data efficiency.

Conclusion: A general, high-resolution 3D brain MRI foundation model is feasible and effective, offering broad applicability for clinical brain MRI analysis across diverse diseases and tasks. The work provides publicly available code and data access to facilitate community adoption.

Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly
acquired in clinical settings to monitor a wide range of neurological
conditions, including neurodegenerative disorders and stroke. While deep
learning models have shown promising results analyzing 3D MRI across a number
of brain imaging tasks, most are highly tailored for specific tasks with
limited labeled data, and are not able to generalize across tasks and/or
populations. The development of self-supervised learning (SSL) has enabled the
creation of large medical foundation models that leverage diverse, unlabeled
datasets ranging from healthy to diseased data, showing significant success in
2D medical imaging applications. However, even the very few foundation models
for 3D brain MRI that have been developed remain limited in resolution, scope,
or accessibility. In this work, we present a general, high-resolution
SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on
18,759 patients (44,958 scans) from 11 publicly available datasets spanning
diverse neurological diseases. We compare our model to Masked Autoencoders
(MAE), as well as two supervised baselines, on four diverse downstream
prediction tasks in both in-distribution and out-of-distribution settings. Our
fine-tuned SimCLR model outperforms all other models across all tasks. Notably,
our model still achieves superior performance when fine-tuned using only 20% of
labeled training samples for predicting Alzheimer's disease. We use publicly
available code and data, and release our trained model at
https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly
applicable and accessible foundation model for clinical brain MRI analysis.

</details>


### [4] [USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction](https://arxiv.org/abs/2509.10651)
*Xiaoyang Ma,Yiyang Chai,Xinran Qu,Hong Sun*

Main category: cs.CV

TL;DR: RGB-to-HSI reconstruction from a single RGB image using a physics-grounded inverse problem with learnable transform-domain nuclear norm regularization, explicit CSS/illumination estimation, and a data-adaptive low-rank SVT operator within a deep-unfolding solver (USCTNet); shows improved reconstruction accuracy over RGB-based methods.


<details>
  <summary>Details</summary>
Motivation: Reconstructing hyperspectral images from RGB is highly ill-posed and can become physically inconsistent when camera spectral sensitivity and scene illumination are misestimated. A method that jointly estimates the forward physics and enforces low-rank structure in a learnable transform domain aims to improve consistency and accuracy.

Method: Formulate RGB→HSI as a physics-guided inverse problem regularized by a nuclear norm in a learnable transform domain. Explicitly estimate CSS and illumination to define the forward operator within each iteration. Replace costly full SVDs in SVT with a data-adaptive low-rank subspace SVT. Build USCTNet, a deep unfolding solver that couples a parameter estimation module with learnable proximal updates.

Result: Extensive experiments on standard benchmarks show consistent improvements over state-of-the-art RGB-based methods in reconstruction accuracy.

Conclusion: The approach yields physically consistent RGB→HSI reconstructions with improved accuracy, leveraging explicit physics estimation and efficient low-rank proximal updates within a deep-unfolding framework. Code is provided by the authors.

Abstract: Reconstructing hyperspectral images (HSIs) from a single RGB image is
ill-posed and can become physically inconsistent when the camera spectral
sensitivity (CSS) and scene illumination are misspecified. We formulate
RGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by
a nuclear norm in a learnable transform domain, and we explicitly estimate CSS
and illumination to define the forward operator embedded in each iteration,
ensuring colorimetric consistency. To avoid the cost and instability of full
singular-value decompositions (SVDs) required by singular-value thresholding
(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on
these components, we develop USCTNet, a deep unfolding solver tailored to HSI
that couples a parameter estimation module with learnable proximal updates.
Extensive experiments on standard benchmarks show consistent improvements over
state-of-the-art RGB-based methods in reconstruction accuracy. Code:
https://github.com/psykheXX/USCTNet-Code-Implementation.git

</details>


### [5] [A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI](https://arxiv.org/abs/2509.10683)
*Felicia Liu,Jay J. Yoo,Farzad Khalvati*

Main category: cs.CV

TL;DR: Vision-language LLMs underperform CNNs on glioma classification and segmentation in BraTS 2020; even after fine-tuning, LLMs show limited spatial understanding and high misclassification of Low-Grade tumors.


<details>
  <summary>Details</summary>
Motivation: To assess whether general purpose vision-language LLMs (e.g., LLaMA 3.2 Instruct) can handle image-based medical tasks and how they compare to traditional CNN baselines using the BraTS 2020 dataset.

Method: Evaluate a general-purpose vision-language LLM before and after fine-tuning on BraTS 2020 multi-modal brain MRIs, comparing to custom 3D CNNs. Classification (Low-Grade vs High-Grade) with metrics like accuracy, precision, recall, and specificity. Segmentation tested with three output modalities: center point, bounding box, and polygon extraction. Analyze spatial accuracy and outputs.

Result: Classification: CNN achieved 80% accuracy with balanced precision/recall; LLM reached 76% accuracy with only 18% specificity initially (misclassifying Low-Grade tumors). Fine-tuning raised specificity to 55% but reduced overall accuracy to ~72%. Segmentation: CNNs localized gliomas well; LLMs clustered predictions near image center with no regard to tumor size or location; fine-tuning improved output formatting but not spatial accuracy; bounding polygon outputs were random/unstructured. Overall, CNNs outperformed LLMs in both tasks; LLMs show limited spatial understanding and minimal gains from fine-tuning.

Conclusion: Current vision-language LLMs are not well-suited for image-based medical tasks like glioma classification and segmentation; more aggressive fine-tuning or alternative training strategies are needed to improve robustness, spatial understanding, and clinical utility.

Abstract: Large Language Models (LLMs) have shown strong performance in text-based
healthcare tasks. However, their utility in image-based applications remains
unexplored. We investigate the effectiveness of LLMs for medical imaging tasks,
specifically glioma classification and segmentation, and compare their
performance to that of traditional convolutional neural networks (CNNs). Using
the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a
general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after
fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma
classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and
balanced precision and recall. The general LLM reached 76% accuracy but
suffered from a specificity of only 18%, often misclassifying Low-Grade tumors.
Fine-tuning improved specificity to 55%, but overall performance declined
(e.g., accuracy dropped to 72%). For segmentation, three methods - center
point, bounding box, and polygon extraction, were implemented. CNNs accurately
localized gliomas, though small tumors were sometimes missed. In contrast, LLMs
consistently clustered predictions near the image center, with no distinction
of glioma size, location, or placement. Fine-tuning improved output formatting
but failed to meaningfully enhance spatial accuracy. The bounding polygon
method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in
both tasks. LLMs showed limited spatial understanding and minimal improvement
from fine-tuning, indicating that, in their current form, they are not
well-suited for image-based tasks. More rigorous fine-tuning or alternative
training strategies may be needed for LLMs to achieve better performance,
robustness, and utility in the medical space.

</details>


### [6] [Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation](https://arxiv.org/abs/2509.10687)
*Hao Zhang,Chun-Han Yao,Simon Donné,Narendra Ahuja,Varun Jampani*

Main category: cs.CV

TL;DR: A diffusion-based, dual-branch framework (SP4D) that generates synchronized RGB videos and kinematic part maps from monocular inputs; uses spatial color encoding to share latent space, BiDiFuse for cross-branch consistency, and 2D-to-3D lifting, evaluated on KinematicParts20K with strong generalization.


<details>
  <summary>Details</summary>
Motivation: Conventional appearance-based part segmentation struggles to reliably capture true kinematic structure across views and time; there is a need for kinematic-aware parts aligned with articulation to enable robust animation and motion tasks.

Method: Dual-branch diffusion model generating RGB frames and corresponding part segmentation maps. Introduces a spatial color encoding that maps part masks to continuous RGB-like images, enabling the segmentation branch to share the latent VAE with the RGB branch. Part segmentation is recovered via post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enforces cross-branch consistency, aided by a contrastive part consistency loss to align spatially and temporally. 2D part maps can be lifted to 3D to obtain skeletal structures and harmonic skinning weights with minimal manual adjustment. Training/evaluation on KinematicParts20K, a dataset of 20K rigged objects from Objaverse XL with multi-view RGB and part video sequences.

Result: Experiments show strong generalization to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses. Outputs are kinematic-aware and suitable for downstream animation and motion tasks; 2D part maps can be lifted to 3D with modest manual tweaks.

Conclusion: SP4D provides a stable 4D generative framework for producing kinematic parts from monocular inputs, enabling effective downstream animation. Key contributions include the spatial color encoding, BiDiFuse fusion, contrastive part consistency loss, and a large-scale KinematicParts20K dataset.

Abstract: We present Stable Part Diffusion 4D (SP4D), a framework for generating paired
RGB and kinematic part videos from monocular inputs. Unlike conventional part
segmentation methods that rely on appearance-based semantic cues, SP4D learns
to produce kinematic parts - structural components aligned with object
articulation and consistent across views and time. SP4D adopts a dual-branch
diffusion model that jointly synthesizes RGB frames and corresponding part
segmentation maps. To simplify the architecture and flexibly enable different
part counts, we introduce a spatial color encoding scheme that maps part masks
to continuous RGB-like images. This encoding allows the segmentation branch to
share the latent VAE from the RGB branch, while enabling part segmentation to
be recovered via straightforward post-processing. A Bidirectional Diffusion
Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a
contrastive part consistency loss to promote spatial and temporal alignment of
part predictions. We demonstrate that the generated 2D part maps can be lifted
to 3D to derive skeletal structures and harmonic skinning weights with few
manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,
a curated dataset of over 20K rigged objects selected and processed from
Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part
video sequences. Experiments show that SP4D generalizes strongly to diverse
scenarios, including real-world videos, novel generated objects, and rare
articulated poses, producing kinematic-aware outputs suitable for downstream
animation and motion-related tasks.

</details>


### [7] [SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition](https://arxiv.org/abs/2509.10710)
*Sven Schreiber,Noha Sarhan,Simone Frintrop,Christian Wilms*

Main category: cs.CV

TL;DR: SegSLR combines pose-guided rough localization with promptable zero-shot video segmentation to precisely segment hands and body, preserving hand shape/orientation, and achieves state-of-the-art ISLR on ChaLearn249 IsoGD.


<details>
  <summary>Details</summary>
Motivation: ISLR methods relying on RGB or pose alone often lose crucial details (e.g., hand shape and orientation) due to imprecise bounding boxes; a method that preserves fine-grained spatial details while leveraging pose cues is needed.

Method: Propose SegSLR: use rough pose localization to identify hand/body regions, apply promptable zero-shot video segmentation to obtain precise segmentations of these parts, and then process the RGB data within these segments for sign recognition; ablation studies show the benefit of focusing on hands and body.

Result: On ChaLearn249 IsoGD, SegSLR outperforms state-of-the-art methods; ablation studies indicate strong gains when the model focuses on the signer’s body and hands.

Conclusion: SegSLR demonstrates that segmentation-based fusion of RGB and pose information preserves crucial shape details and yields significant improvements for ISLR, validating the design choice of segmenting relevant body parts before RGB processing.

Abstract: Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB
data or signer pose information. However, combining these modalities often
results in the loss of crucial details, such as hand shape and orientation, due
to imprecise representations like bounding boxes. Therefore, we propose the
ISLR system SegSLR, which combines RGB and pose information through promptable
zero-shot video segmentation. Given the rough localization of the hands and the
signer's body from pose information, we segment the respective parts through
the video to maintain all relevant shape information. Subsequently, the
segmentations focus the processing of the RGB data on the most relevant body
parts for ISLR. This effectively combines RGB and pose information. Our
evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR
outperforms state-of-the-art methods. Furthermore, ablation studies indicate
that SegSLR strongly benefits from focusing on the signer's body and hands,
justifying our design choices.

</details>


### [8] [SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation](https://arxiv.org/abs/2509.10748)
*Jecia Z. Y. Mao,Francis X Creighton,Russell H Taylor,Manish Sahu*

Main category: cs.CV

TL;DR: A speech-guided collaborative perception (SCOPE) framework blends LLM reasoning with open-set vision foundation models to enable live, open-set segmentation, labeling, and tracking of surgical instruments and anatomy via natural speech-based human–machine collaboration, reducing reliance on labeled data.


<details>
  <summary>Details</summary>
Motivation: Current supervised, domain-specific models require labeled data and struggle to generalize to new intraoperative scenarios; open-set zero-shot segmentation exists but often needs manual cues, limiting hands-free intraoperative deployment.

Method: Introduce SCOPE with a collaborative perception agent that merges LLM reasoning with open-set vision foundation models. It generates top segmentation candidates and incorporates clinician speech feedback to guide segmentation of instruments; instruments themselves act as interactive labels to annotate other scene elements; evaluation on Cataract1k subset and an in-house ex-vivo skull-base dataset; live, mock ex-vivo demonstration.

Result: Demonstrates on-the-fly segmentation and tracking of surgical scene and a dynamic, hands-free surgeon-centric workflow; validated on public and in-house datasets and through a live mock experiment, illustrating feasibility.

Conclusion: SCOPE enables adaptable, hands-free, surgeon-centric intraoperative tools by leveraging human-AI collaboration and open-set perception, paving the way for context-aware adaptive surgical assistance in dynamic operating rooms.

Abstract: Accurate segmentation and tracking of relevant elements of the surgical scene
is crucial to enable context-aware intraoperative assistance and decision
making. Current solutions remain tethered to domain-specific, supervised models
that rely on labeled data and required domain-specific data to adapt to new
surgical scenarios and beyond predefined label categories. Recent advances in
prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot
segmentation across heterogeneous medical images. However, dependence of these
models on manual visual or textual cues restricts their deployment in
introperative surgical settings. We introduce a speech-guided collaborative
perception (SCOPE) framework that integrates reasoning capabilities of large
language model (LLM) with perception capabilities of open-set VFMs to support
on-the-fly segmentation, labeling and tracking of surgical instruments and
anatomy in intraoperative video streams. A key component of this framework is a
collaborative perception agent, which generates top candidates of VFM-generated
segmentation and incorporates intuitive speech feedback from clinicians to
guide the segmentation of surgical instruments in a natural human-machine
collaboration paradigm. Afterwards, instruments themselves serve as interactive
pointers to label additional elements of the surgical scene. We evaluated our
proposed framework on a subset of publicly available Cataract1k dataset and an
in-house ex-vivo skull-base dataset to demonstrate its potential to generate
on-the-fly segmentation and tracking of surgical scene. Furthermore, we
demonstrate its dynamic capabilities through a live mock ex-vivo experiment.
This human-AI collaboration paradigm showcase the potential of developing
adaptable, hands-free, surgeon-centric tools for dynamic operating-room
environments.

</details>


### [9] [Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation](https://arxiv.org/abs/2509.10759)
*Yi-Ruei Liu,You-Zhe Xie,Yu-Hsiang Hsu,I-Sheng Fang,Yu-Lun Liu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: A two-stage pipeline (4D-GRT) combines 4D Gaussian Splatting with physically-based ray tracing to simulate camera effects (e.g., fisheye, rolling shutter) on dynamic scenes. It reconstructs scenes from multi-view videos and then renders with controllable, physically accurate camera effects. Claims faster rendering with quality on par or better than baselines; also provides an 8-scene synthetic benchmark across 4 effects.


<details>
  <summary>Details</summary>
Motivation: Current CV systems typically assume ideal pinhole cameras and fail under real-world camera effects. Data generation methods suffer from high cost, sim-to-real gaps, or poor camera-effect modeling.

Method: Stage 1: reconstruct dynamic scenes from multi-view videos using 4D Gaussian Splatting. Stage 2: use physically-based ray tracing to render videos with controllable camera effects. Evaluation includes comparison to baselines and an 8-scene synthetic benchmark with four camera effects.

Result: Faster rendering than baselines while achieving similar or better visual quality. A practical pipeline for generating camera-effect videos and a new synthetic benchmark.

Conclusion: 4D-GRT provides an effective, efficient framework for simulating camera effects in dynamic scenes and offers a valuable benchmark for evaluating camera-effect data generation.

Abstract: Common computer vision systems typically assume ideal pinhole cameras but
fail when facing real-world camera effects such as fisheye distortion and
rolling shutter, mainly due to the lack of learning from training data with
camera effects. Existing data generation approaches suffer from either high
costs, sim-to-real gaps or fail to accurately model camera effects. To address
this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage
pipeline that combines 4D Gaussian Splatting with physically-based ray tracing
for camera effect simulation. Given multi-view videos, 4D-GRT first
reconstructs dynamic scenes, then applies ray tracing to generate videos with
controllable, physically accurate camera effects. 4D-GRT achieves the fastest
rendering speed while performing better or comparable rendering quality
compared to existing baselines. Additionally, we construct eight synthetic
dynamic scenes in indoor environments across four camera effects as a benchmark
to evaluate generated videos with camera effects.

</details>


### [10] [EditDuet: A Multi-Agent System for Video Non-Linear Editing](https://arxiv.org/abs/2509.10761)
*Marcelo Sandoval-Castaneda,Bryan Russell,Josef Sivic,Gregory Shakhnarovich,Fabian Caba Heilbron*

Main category: cs.CV

TL;DR: A multi-agent framework automates video editing driven by natural language, with Editor and Critic agents, plus LLM-based evaluation, achieving superior performance in user studies.


<details>
  <summary>Details</summary>
Motivation: Address the gap where prior work focused on retrieval/UI rather than editing; automate the core video editing task as sequential decision making.

Method: Editor and Critic agents collaborate through learning-based communication to edit video clips guided by natural language instructions; Editor uses standard video editing tools to produce sequences; Critic provides natural language feedback or renders the result; includes an LLM-based judge for evaluation.

Result: Qualitative and quantitative evaluation via a user study shows system vastly outperforms existing approaches in coverage, time constraint satisfaction, and human preference.

Conclusion: Proposes a scalable, language-driven automated video editing framework with cross-agent collaboration and an LLM-based evaluation metric.

Abstract: Automated tools for video editing and assembly have applications ranging from
filmmaking and advertisement to content creation for social media. Previous
video editing work has mainly focused on either retrieval or user interfaces,
leaving actual editing to the user. In contrast, we propose to automate the
core task of video editing, formulating it as sequential decision making
process. Ours is a multi-agent approach. We design an Editor agent and a Critic
agent. The Editor takes as input a collection of video clips together with
natural language instructions and uses tools commonly found in video editing
software to produce an edited sequence. On the other hand, the Critic gives
natural language feedback to the editor based on the produced sequence or
renders it if it is satisfactory. We introduce a learning-based approach for
enabling effective communication across specialized agents to address the
language-driven video editing task. Finally, we explore an LLM-as-a-judge
metric for evaluating the quality of video editing system and compare it with
general human preference. We evaluate our system's output video sequences
qualitatively and quantitatively through a user study and find that our system
vastly outperforms existing approaches in terms of coverage, time constraint
satisfaction, and human preference.

</details>


### [11] [Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging](https://arxiv.org/abs/2509.10767)
*Sajad Amiri,Shahram Taeb,Sara Gharibi,Setareh Dehghanfard,Somayeh Sadat Mehrnia,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Main category: cs.CV

TL;DR: A stability-aware ML framework predicts glioma MRI contrast enhancement from non-contrast MRI across multicenter datasets, achieving high external accuracy and robustness; MI with ETr pipeline often yielded best balance of accuracy and stability, suggesting improved generalizability and reduced GBCA reliance.


<details>
  <summary>Details</summary>
Motivation: Gadolinium-based contrast agents (GBCAs) pose safety, cost, and access concerns. Predicting contrast enhancement from non-contrast MRI could reduce GBCA use while preserving clinical decision support. Robust, generalizable models are needed across scanners and cohorts.

Method: Non-contrast T1-weighted MRI input; enhancement derived from paired post-contrast T1WI. Radiomic features extracted using PyRadiomics per IBSI (108 features). Combined with 48 dimensionality reduction methods and 25 classifiers, creating 1,200 pipelines. Rotational validation: train on three multicenter datasets and test on the fourth. Evaluation included CV accuracy (0.91–0.96) and external testing accuracies (0.87, 0.98, 0.95) with an average of 0.93; stability metrics (F1/precision/recall 0.87–0.96); ROC-AUC more variable (0.50–0.82) due to cohort heterogeneity. The MI (mutual information) paired with the ETr pipeline consistently ranked top.

Result: External test accuracies were high across cohorts (0.87–0.98, avg 0.93). F1, precision, and recall remained stable (0.87–0.96). ROC-AUC varied substantially (0.50–0.82), reflecting heterogeneity among cohorts. The MI + ETr pipeline most frequently ranked highest, indicating a favorable balance of accuracy and stability across centers.

Conclusion: Stability-aware model selection enables reliable prediction of gadolinium-free contrast enhancement from non-contrast glioma MRI and generalizes across centers, offering a scalable framework for reproducible ML in neuro-oncology and beyond, potentially reducing GBCA use.

Abstract: Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but
raise safety, cost, and accessibility concerns. Predicting contrast enhancement
from non-contrast MRI using machine learning (ML) offers a safer alternative,
as enhancement reflects tumor aggressiveness and informs treatment planning.
Yet scanner and cohort variability hinder robust model selection. We propose a
stability-aware framework to identify reproducible ML pipelines for multicenter
prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases
from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG).
Non-contrast T1WI served as input, with enhancement derived from paired
post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were
extracted and combined with 48 dimensionality reduction methods and 25
classifiers, yielding 1,200 pipelines. Rotational validation was trained on
three datasets and tested on the fourth. Cross-validation prediction accuracies
ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM),
0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1,
precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more
widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr
pipeline consistently ranked highest, balancing accuracy and stability. This
framework demonstrates that stability-aware model selection enables reliable
prediction of contrast enhancement from non-contrast glioma MRI, reducing
reliance on GBCAs and improving generalizability across centers. It provides a
scalable template for reproducible ML in neuro-oncology and beyond.

</details>


### [12] [Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection](https://arxiv.org/abs/2509.10779)
*Yilun Xiao*

Main category: cs.CV

TL;DR: Detector-agnostic post-processing converts overlap redundancy into group evidence to recover missed dense small objects in UAV imagery using tiling, Spatial Gate (DBSCAN on box centroids), and Semantic Gate (DBSCAN on ResNet-18 embeddings), followed by reweighting and class-aware NMS; improves recall on VisDrone with a precision trade-off; no retraining required; latency ~0.095 s per image.


<details>
  <summary>Details</summary>
Motivation: Dense small object detection in UAV imagery is hampered by long-range viewpoints, occlusion, and clutter, causing missed detections. A detector-agnostic post-processing step that leverages geometric and appearance coherence can boost recall without retraining the backbone.

Method: 1) Apply overlapping tiling to reveal low-confidence candidates. 2) Spatial Gate: perform DBSCAN on bounding-box centroids to form Groups with geometric consistency. 3) Semantic Gate: perform DBSCAN on ResNet-18 embeddings for appearance coherence. 4) Validate groups and apply confidence reweighting. 5) Fuse with the baseline using class-aware NMS. 6) Detector-agnostic; no retraining; ablations confirm tiling, spatial/semantic clustering, and reweighting; latency ~0.095 s/image.

Result: VisDrone results show recall 0.685 -> 0.778 (+0.093); precision 0.801 -> 0.595; F1=0.669. Post-processing latency ~0.095 s per image.

Conclusion: The framework yields a recall-first improvement with a controlled precision trade-off, integrates with modern detectors without retraining, and is supported by ablations validating tiling, spatial clustering, semantic clustering, and reweighting. Future work targets reducing semantic-gating cost and incorporating temporal cues.

Abstract: Dense small objects in UAV imagery are often missed due to long-range
viewpoints, occlusion, and clutter[cite: 5]. This paper presents a
detector-agnostic post-processing framework that converts overlap-induced
redundancy into group evidence[cite: 6]. Overlapping tiling first recovers
low-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)
and a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group
evidence[cite: 7]. Validated groups receive controlled confidence reweighting
before class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall
increase from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to
0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per
image[cite: 10]. These results indicate recall-first, precision-trade-off
behavior that benefits recall-sensitive applications such as far-field counting
and monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,
spatial clustering stabilizes geometry, semantic clustering enforces appearance
coherence, and reweighting provides calibrated integration with the
baseline[cite: 11]. The framework requires no retraining and integrates with
modern detectors[cite: 12]. Future work will reduce semantic gating cost and
extend the approach with temporal cues[cite: 13].

</details>


### [13] [InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts](https://arxiv.org/abs/2509.10813)
*Weipeng Zhong,Peizhou Cao,Yichen Jin,Li Luo,Wenzhe Cai,Jingli Lin,Hanqing Wang,Zhaoyang Lyu,Tai Wang,Bo Dai,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: InternScenes proposes a large-scale simulatable indoor scene dataset that integrates real-world scans, procedurally generated, and designer-created scenes to produce about 40k diverse scenes with 1.96M objects across 15 scene types and 288 classes, preserving many small items for realism. It includes a data pipeline to create real-to-sim replicas, add interactive objects, and resolve collisions via physics, and demonstrates utility on scene layout generation and point-goal navigation, aiming to scale model training and is open-sourced.


<details>
  <summary>Details</summary>
Motivation: Existing embodied AI datasets suffer from limited scale, limited diversity, sanitized layouts that omit small items, and object collisions. A larger, more realistic, and simulatable dataset is needed to train and benchmark embodied agents in complex environments.

Method: Integrate three scene sources (real-world scans, procedurally generated scenes, designer-created scenes) to form ~40k scenes and 1.96M objects (15 scene types, 288 object classes). Preserve small items to create realistic layouts (average 41.5 objects per region). Build a data processing pipeline that (1) creates real-to-sim replicas for scans, (2) inserts interactive objects to enhance interactivity, and (3) resolves object collisions via physical simulations.

Result: The work demonstrates value via two benchmarks—scene layout generation and point-goal navigation—highlighting the challenges of complex, realistic layouts and showing the dataset’s potential to scale training; they plan to open-source data, models, and benchmarks.

Conclusion: InternScenes provides a scalable, realistic, and interactive indoor scene dataset that addresses prior limitations and facilitates training of embodied AI across generation and navigation tasks; it is intended to be open-sourced to benefit the community.

Abstract: The advancement of Embodied AI heavily relies on large-scale, simulatable 3D
scene datasets characterized by scene diversity and realistic layouts. However,
existing datasets typically suffer from limitations in data scale or diversity,
sanitized layouts lacking small items, and severe object collisions. To address
these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale
simulatable indoor scene dataset comprising approximately 40,000 diverse scenes
by integrating three disparate scene sources, real-world scans, procedurally
generated scenes, and designer-created scenes, including 1.96M 3D objects and
covering 15 common scene types and 288 object classes. We particularly preserve
massive small items in the scenes, resulting in realistic and complex layouts
with an average of 41.5 objects per region. Our comprehensive data processing
pipeline ensures simulatability by creating real-to-sim replicas for real-world
scans, enhances interactivity by incorporating interactive objects into these
scenes, and resolves object collisions by physical simulations. We demonstrate
the value of InternScenes with two benchmark applications: scene layout
generation and point-goal navigation. Both show the new challenges posed by the
complex and realistic layouts. More importantly, InternScenes paves the way for
scaling up the model training for both tasks, making the generation and
navigation in such complex scenes possible. We commit to open-sourcing the
data, models, and benchmarks to benefit the whole community.

</details>


### [14] [Well-Conditioned Polynomial Representations for Mathematical Handwriting Recognition](https://arxiv.org/abs/2509.10815)
*Robert M. Corless,Deepak Singh Kalhan,Stephen M. Watt*

Main category: cs.CV

TL;DR: Investigates basis choices (Legendre, Legendre-Sobolev, Chebyshev, Chebyshev-Sobolev) for parameterized plane-curve handwriting models, analyzing conditioning and norms to balance accuracy and computational cost.


<details>
  <summary>Details</summary>
Motivation: Handwriting modeling using compact polynomial representations requires stable evaluation and meaningful measures of symbol variation; choosing the right basis and degree can reduce cost while maintaining fidelity.

Method: The study analyzes the condition number of polynomial evaluation across bases and derives bounds on norms for symbol variations using inner products; compares existing Legendre-based representations with Chebyshev variants; both theoretical analysis and preliminary results are reported.

Result: Bounds and insights on how basis choice affects conditioning and variation norms; trade-offs between accuracy and computation are characterized, suggesting methods to select basis and degree for efficient modeling.

Conclusion: A framework that guides basis and degree selection for handwriting models, indicating Chebyshev-based bases may offer favorable conditioning in certain regimes; further empirical validation recommended.

Abstract: Previous work has made use of a parameterized plane curve polynomial
representation for mathematical handwriting, with the polynomials represented
in a Legendre or Legendre-Sobolev graded basis. This provides a compact
geometric representation for the digital ink. Preliminary results have also
been shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the
trade-offs between basis choice and polynomial degree to achieve accurate
modeling with a low computational cost. To do this, we consider the condition
number for polynomial evaluation in these bases and bound how the various inner
products give norms for the variations between symbols.

</details>


### [15] [Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression](https://arxiv.org/abs/2509.10824)
*Aghiles Kebaili,Romain Modzelewski,Jérôme Lapuyade-Lahorgue,Maxime Fontanilles,Sébastien Thureau,Su Ruan*

Main category: cs.CV

TL;DR: A multitask diffusion-based framework for time-agnostic glioma progression: synthesizes future FLAIR, outputs probabilistic tumor evolution maps via SDFs, uses a pretrained deformation module for arbitrary-interval dynamics, augmentations for data scarcity, and a radiotherapy-weighted loss; shows promising results on public and private datasets.


<details>
  <summary>Details</summary>
Motivation: Glioma progression prediction is hindered by sparse, irregular longitudinal MRI data, causing data imbalance and unreliable modeling. A time-agnostic, uncertainty-aware approach is needed to predict progression at arbitrary future times with limited data.

Method: A multitask diffusion model that (1) generates future FLAIR at any chosen time point, (2) estimates spatial probabilistic tumor evolution maps via signed distance fields (SDFs) for uncertainty quantification, (3) incorporates a pretrained deformation module to model inter-scan changes with deformation fields across arbitrary intervals, (4) employs targeted augmentation to synthesize complete three-follow-up sequences and impute missing modalities, and (5) uses a radiotherapy-weighted focal loss leveraging dose maps to emphasize clinically important regions.

Result: Trained on a public dataset and evaluated on an internal private dataset, the method achieved promising results in both settings, aided by the augmentation pipeline for stability and accuracy and by uncertainty quantification through SDF-based maps.

Conclusion: The framework enables flexible, time-dependent, uncertainty-aware prediction of glioma progression from limited early data, integrating diffusion-based synthesis, deformation-driven temporal modeling, and dose-informed training to improve clinical applicability and potential radiotherapy planning.

Abstract: Glioma, an aggressive brain malignancy characterized by rapid progression and
its poor prognosis, poses significant challenges for accurate evolution
prediction. These challenges are exacerbated by sparse, irregularly acquired
longitudinal MRI data in clinical practice, where incomplete follow-up
sequences create data imbalances and make reliable modeling difficult. In this
paper, we present a multitask diffusion framework for time-agnostic, pixel-wise
prediction of glioma progression. The model simultaneously generates future
FLAIR sequences at any chosen time point and estimates spatial probabilistic
tumor evolution maps derived using signed distance fields (SDFs), allowing
uncertainty quantification. To capture temporal dynamics of tumor evolution
across arbitrary intervals, we integrate a pretrained deformation module that
models inter-scan changes using deformation fields. Regarding the common
clinical limitation of data scarcity, we implement a targeted augmentation
pipeline that synthesizes complete sequences of three follow-up scans and
imputes missing MRI modalities from available patient studies, improving the
stability and accuracy of predictive models. Based on merely two follow-up
scans at earlier timepoints, our framework produces flexible time-depending
probability maps, enabling clinicians to interrogate tumor progression risks at
any future temporal milestone. We further introduce a radiotherapy-weighted
focal loss term that leverages radiation dose maps, as these highlight regions
of greater clinical importance during model training. The proposed method was
trained on a public dataset and evaluated on an internal private dataset,
achieving promising results in both cases

</details>


### [16] [Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios](https://arxiv.org/abs/2509.10841)
*Simone Mosco,Daniel Fusaro,Wanmeng Li,Emanuele Menegatti,Alberto Pretto*

Main category: cs.CV

TL;DR: Introduces 3PNet that learns from 2D projections (point-plane) of LiDAR data to enhance point-based LiDAR semantic segmentation using only LiDAR data; adds geometry-aware augmentation; strong in data-scarce regimes and competitive on SemanticKITTI and PandaSet; code released.


<details>
  <summary>Details</summary>
Motivation: LiDAR-only semantic segmentation often suffers under limited training data and high computational/data demands, and prior methods rely on multi-sensor data or diverse representations. This work aims to improve generalization and performance in data-scarce scenarios by leveraging informative 2D representations and geometry-consistent augmentation while staying LiDAR-only.

Method: Project the LiDAR point cloud onto multiple informative 2D representations via point-plane projections to learn complementary features; fuse these 2D features with 3D point features for improved segmentation; introduce a geometry-aware data augmentation that aligns with LiDAR sensor properties to mitigate class imbalance and data scarcity.

Result: Experiments show significant improvements in limited-data scenarios and competitive performance on standard datasets SemanticKITTI and PandaSet; code available at the provided GitHub link.

Conclusion: The combination of 2D projection-based feature learning and geometry-aware data augmentation enhances LiDAR-only point cloud segmentation, especially in data-scarce settings, while maintaining competitive results on public benchmarks.

Abstract: LiDAR point cloud semantic segmentation is essential for interpreting 3D
environments in applications such as autonomous driving and robotics. Recent
methods achieve strong performance by exploiting different point cloud
representations or incorporating data from other sensors, such as cameras or
external datasets. However, these approaches often suffer from high
computational complexity and require large amounts of training data, limiting
their generalization in data-scarce scenarios. In this paper, we improve the
performance of point-based methods by effectively learning features from 2D
representations through point-plane projections, enabling the extraction of
complementary information while relying solely on LiDAR data. Additionally, we
introduce a geometry-aware technique for data augmentation that aligns with
LiDAR sensor properties and mitigates class imbalance. We implemented and
evaluated our method that applies point-plane projections onto multiple
informative 2D representations of the point cloud. Experiments demonstrate that
this approach leads to significant improvements in limited-data scenarios,
while also achieving competitive results on two publicly available standard
datasets, as SemanticKITTI and PandaSet. The code of our method is available at
https://github.com/SiMoM0/3PNet

</details>


### [17] [OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds](https://arxiv.org/abs/2509.10842)
*Chongyu Wang,Kunlei Jing,Jihua Zhu,Di Wang*

Main category: cs.CV

TL;DR: OpenUrban3D introduces a 3D open-vocabulary segmentation framework for large-scale urban scenes that operates without aligned multi-view images, pre-trained point cloud models, or manual annotations. It uses multi-view, multi-granularity rendering combined with mask-level vision-language features and sample-balanced fusion, then distills the results into a 3D backbone to enable zero-shot segmentation for arbitrary text queries. It demonstrates strong cross-scene generalization and improved segmentation accuracy on SensatUrban and SUM.


<details>
  <summary>Details</summary>
Motivation: The work targets two main gaps in 3D urban understanding: (1) the lack of high-quality, aligned multi-view imagery for large-scale urban point cloud datasets, and (2) the poor generalization of existing 3D segmentation methods across diverse urban environments with varying geometry, scale, and appearance. OpenUrban3D aims to enable flexible, language-driven segmentation and robust cross-scene performance for urban analytics and digital twins.

Method: 1) Generate robust semantic features directly from raw point clouds via multi-view, multi-granularity rendering. 2) Extract mask-level vision-language features from these renderings. 3) Fuse features with a sample-balanced strategy across views/granularities. 4) Distill the fused representations into a 3D backbone model. 5) Enable zero-shot segmentation for arbitrary text prompts, leveraging semantic richness and geometric priors.

Result: Extensive experiments on SensatUrban and SUM show significant improvements in segmentation accuracy and cross-scene generalization over existing methods, indicating strong open-vocabulary capability and robustness to diverse urban settings.

Conclusion: OpenUrban3D establishes a first-of-its-kind 3D open-vocabulary segmentation framework for large-scale urban scenes that does not rely on aligned multi-view data, pre-trained 3D models, or manual labels. It demonstrates strong zero-shot performance and generalization, presenting a scalable solution for 3D urban scene understanding and urban analytics.

Abstract: Open-vocabulary semantic segmentation enables models to recognize and segment
objects from arbitrary natural language descriptions, offering the flexibility
to handle novel, fine-grained, or functionally defined categories beyond fixed
label sets. While this capability is crucial for large-scale urban point clouds
that support applications such as digital twins, smart city management, and
urban analytics, it remains largely unexplored in this domain. The main
obstacles are the frequent absence of high-quality, well-aligned multi-view
imagery in large-scale urban point cloud datasets and the poor generalization
of existing three-dimensional (3D) segmentation pipelines across diverse urban
environments with substantial variation in geometry, scale, and appearance. To
address these challenges, we present OpenUrban3D, the first 3D open-vocabulary
semantic segmentation framework for large-scale urban scenes that operates
without aligned multi-view images, pre-trained point cloud segmentation
networks, or manual annotations. Our approach generates robust semantic
features directly from raw point clouds through multi-view, multi-granularity
rendering, mask-level vision-language feature extraction, and sample-balanced
fusion, followed by distillation into a 3D backbone model. This design enables
zero-shot segmentation for arbitrary text queries while capturing both semantic
richness and geometric priors. Extensive experiments on large-scale urban
benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves
significant improvements in both segmentation accuracy and cross-scene
generalization over existing methods, demonstrating its potential as a flexible
and scalable solution for 3D urban scene understanding.

</details>


### [18] [AutoOEP -- A Multi-modal Framework for Online Exam Proctoring](https://arxiv.org/abs/2509.10887)
*Aryan Kashyap Naveen,Bhuvanesh Singla,Raajan Wankhade,Shreesha M,Ramu S,Ram Mohana Reddy Guddeti*

Main category: cs.CV

TL;DR: A multi-modal AI prototype (AutoOEP) for automated online proctoring using dual cameras, face/hand analyses, and temporal modeling to detect cheating in real-time, achieving solid accuracy with low resource usage.


<details>
  <summary>Details</summary>
Motivation: Scalable, less intrusive means to maintain academic integrity in remote exams; existing proctoring is either labor-intensive or intrusive and limited in cheating coverage.

Method: Dual-camera setup; Face Module with ArcFace-based identity verification, head pose, gaze, and mouth movement analysis; Hand Module with fine-tuned YOLOv11 to detect prohibited items and monitor hand proximity; feature fusion fed into an LSTM to output a real-time cheating probability score; evaluated on a custom dataset; 2.4 FPS without GPU.

Result: Classification accuracy for suspicious activities: 90.7%; prohibited-item detection mAP@0.5: 0.57; system runs at ~2.4 frames per second without GPU.

Conclusion: AutoOEP demonstrates an effective, resource-efficient automated proctoring solution that can reduce human oversight and strengthen integrity of online assessments.

Abstract: The burgeoning of online education has created an urgent need for robust and
scalable systems to ensure academic integrity during remote examinations.
Traditional human proctoring is often not feasible at scale, while existing
automated solutions can be intrusive or fail to detect a wide range of cheating
behaviors. This paper introduces AutoOEP (Automated Online Exam Proctoring), a
comprehensive, multi-modal framework that leverages computer vision and machine
learning to provide effective, automated proctoring. The system utilizes a
dual-camera setup to capture both a frontal view of the examinee and a side
view of the workspace, minimizing blind spots. Our approach integrates several
parallel analyses: the Face Module performs continuous identity verification
using ArcFace, along with head pose estimation, gaze tracking, and mouth
movement analysis to detect suspicious cues. Concurrently, the Hand Module
employs a fine-tuned YOLOv11 model for detecting prohibited items (e.g., mobile
phones, notes) and tracks hand proximity to these objects. Features from these
modules are aggregated and fed into a Long Short-Term Memory (LSTM) network
that analyzes temporal patterns to calculate a real-time cheating probability
score. We evaluate AutoOEP on a custom-collected dataset simulating diverse
exam conditions. Our system achieves an accuracy of 90.7% in classifying
suspicious activities. The object detection component obtains a mean Average
Precision (mAP@.5) of 0.57 for prohibited items, and the entire framework
processes video streams at approximately 2.4 frames per second without a GPU.
The results demonstrate that AutoOEP is an effective and resource-efficient
solution for automated proctoring, significantly reducing the need for human
intervention and enhancing the integrity of online assessments.

</details>


### [19] [Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System](https://arxiv.org/abs/2509.10897)
*Weiqiang Zhao,Tianzhu Liu,Yuzhe Gui,Yanfeng Gu*

Main category: cs.CV

TL;DR: Proposes a dual-camera CASSI reconstruction framework using TV subgradient theory with adaptive RGB/panchromatic references to produce an interpretable, robust solution for spectral imaging.


<details>
  <summary>Details</summary>
Motivation: To overcome the trade-offs in spectral imaging (spectral, spatial, temporal resolution) and address ill-posed reconstructions in high-compression CASSI, while avoiding black-box deep learning by providing a physics-informed, optimization-based method.

Method: Develops an end-to-end SD-CASSI model, introduces a dynamic regularization strategy with normalized gradient constraints from RGB/panchromatic references to form a TV subgradient similarity function with convex guarantees, and uses spatial priors from auxiliary cameras for adaptive reference generation within an ADMM framework.

Result: Demonstrates preserved spatial-spectral structural consistency and robust performance across diverse reconstruction scenarios, with a mathematically grounded framework that enhances interpretability; code is available for reproduction.

Conclusion: The work provides an interpretable mathematical foundation for computational spectral imaging via a dual-camera CASSI approach with TV subgradient guidance and adaptive references, showing strong performance and broad applicability.

Abstract: Spectral imaging technology has long-faced fundamental challenges in
balancing spectral, spatial, and temporal resolutions. While compressive
sensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this
trade-off through optical encoding, high compression ratios result in ill-posed
reconstruction problems. Traditional model-based methods exhibit limited
performance due to reliance on handcrafted inherent image priors, while deep
learning approaches are constrained by their black-box nature, which
compromises physical interpretability. To address these limitations, we propose
a dual-camera CASSI reconstruction framework that integrates total variation
(TV) subgradient theory. By establishing an end-to-end SD-CASSI mathematical
model, we reduce the computational complexity of solving the inverse problem
and provide a mathematically well-founded framework for analyzing multi-camera
systems. A dynamic regularization strategy is introduced, incorporating
normalized gradient constraints from RGB/panchromatic-derived reference images,
which constructs a TV subgradient similarity function with strict convex
optimization guarantees. Leveraging spatial priors from auxiliary cameras, an
adaptive reference generation and updating mechanism is designed to provide
subgradient guidance. Experimental results demonstrate that the proposed method
effectively preserves spatial-spectral structural consistency. The theoretical
framework establishes an interpretable mathematical foundation for
computational spectral imaging, demonstrating robust performance across diverse
reconstruction scenarios. The source code is available at
https://github.com/bestwishes43/ADMM-TVDS.

</details>


### [20] [Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation](https://arxiv.org/abs/2509.10919)
*Mohanad Albughdadi*

Main category: cs.CV

TL;DR: A compact, metadata-aware MoE-MAE (2.5M params) achieves competitive transfer performance with strong metadata conditioning, demonstrating that metadata-aware pretraining boosts transfer and label efficiency in small EO models.


<details>
  <summary>Details</summary>
Motivation: Large Earth Observation foundation models are computationally expensive. There is a need for compact, general-purpose EO models that are efficient and reusable for downstream tasks.

Method: Introduce Metadata-aware Mixture-of-Experts Masked Autoencoder (MoE-MAE) with 2.5M parameters. Uses sparse expert routing and geo-temporal conditioning, incorporating imagery with latitude/longitude and seasonal/daily cyclic encodings. Pretrained on BigEarthNet-Landsat; evaluate frozen encoder embeddings with linear probes.

Result: Despite its small size, the model competes with much larger architectures. Metadata-aware pretraining improves transfer and label efficiency. On EuroSAT-Landsat (which lacks explicit metadata), the model remains competitive with models having hundreds of millions of parameters.

Conclusion: Compact, metadata-aware MoE-MAEs are an efficient and scalable step toward future EO foundation models.

Abstract: Recent advances in Earth Observation have focused on large-scale foundation
models. However, these models are computationally expensive, limiting their
accessibility and reuse for downstream tasks. In this work, we investigate
compact architectures as a practical pathway toward smaller general-purpose EO
models. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder
(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing
with geo-temporal conditioning, incorporating imagery alongside
latitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE
on the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen
encoder using linear probes. Despite its small size, the model competes with
much larger architectures, demonstrating that metadata-aware pretraining
improves transfer and label efficiency. To further assess generalization, we
evaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and
still observe competitive performance compared to models with hundreds of
millions of parameters. These results suggest that compact, metadata-aware
MoE-MAEs are an efficient and scalable step toward future EO foundation models.

</details>


### [21] [Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging](https://arxiv.org/abs/2509.10961)
*Farhan Sadik,Christopher L. Newman,Stuart J. Warden,Rachel K. Surowiec*

Main category: cs.CV

TL;DR: The paper develops a deep learning framework (ESWGAN-GP) to correct HR-pQCT motion artifacts by training on synthetic motion-degraded data and real-world scans, using edge-enhancing, self-attention, and perceptual losses; it achieves notable improvements in SNR, SSIM, and VIF on both source and target datasets, though the motion model is simplified.


<details>
  <summary>Details</summary>
Motivation: Motion artifacts in HR-pQCT (bone streaking, trabecular smearing) obstruct in vivo microstructure assessment. There is no standardized degradation model or effective motion correction method, hindering widespread adoption. Creating paired, artifact-ground-truth data enables supervised learning for correction and evaluation.

Method: Simulate motion artifacts using an optimized sinogram-based degradation to produce paired data (corrupted and ground truth). Train Edge-enhanced Self-attention Wasserstein GAN with Gradient Penalty (ESWGAN-GP) on source (synthetic) and target (real) data. Key components include edge-enhancing skip connections to preserve trabecular edges, self-attention to capture long-range dependencies, and a VGG-based perceptual loss for fine micro-structure reconstruction.

Result: On the source dataset: SNR 26.78, SSIM 0.81, VIF 0.76. On the target dataset: SNR 29.31, SSIM 0.87, VIF 0.81. The approach yields improved performance on real-world data, but the authors acknowledge that the motion model may not fully capture in vivo complexity and that this remains an initial step.

Conclusion: The ESWGAN-GP framework represents a promising, first-step approach to deep-learning-based motion correction in HR-pQCT by combining synthetic artifacts for supervised training with architecture choices that preserve micro-structural features; further work is needed to model real-world motion more comprehensively and validate on diverse in vivo data.

Abstract: Rigid-motion artifacts, such as cortical bone streaking and trabecular
smearing, hinder in vivo assessment of bone microstructures in high-resolution
peripheral quantitative computed tomography (HR-pQCT). Despite various motion
grading techniques, no motion correction methods exist due to the lack of
standardized degradation models. We optimize a conventional sinogram-based
method to simulate motion artifacts in HR-pQCT images, creating paired datasets
of motion-corrupted images and their corresponding ground truth, which enables
seamless integration into supervised learning frameworks for motion correction.
As such, we propose an Edge-enhanced Self-attention Wasserstein Generative
Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion
artifacts in both simulated (source) and real-world (target) datasets. The
model incorporates edge-enhancing skip connections to preserve trabecular edges
and self-attention mechanisms to capture long-range dependencies, facilitating
motion correction. A visual geometry group (VGG)-based perceptual loss is used
to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean
signal-to-noise ratio (SNR) of 26.78, structural similarity index measure
(SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source
dataset, while showing improved performance on the target dataset with an SNR
of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a
simplified representation of real-world motion that may not fully capture the
complexity of in vivo motion artifacts. Nevertheless, because motion artifacts
present one of the foremost challenges to more widespread adoption of this
modality, these methods represent an important initial step toward implementing
deep learning-based motion correction in HR-pQCT.

</details>


### [22] [Gaze Authentication: Factors Influencing Authentication Performance](https://arxiv.org/abs/2509.10969)
*Dillon Lohr,Michael J Proulx,Mehedi Hasan Raju,Oleg V Komogortsev*

Main category: cs.CV

TL;DR: The study analyzes how calibration depth, fusion of calibrated/non-calibrated gaze, and signal quality affect gaze-based authentication using a large-scale 8,849-subject dataset; a simple three-sample moving average filter can slightly hurt performance, with some exceptions.


<details>
  <summary>Details</summary>
Motivation: To identify practical factors that influence gaze-based authentication performance and guide system design, using a large-scale in-house dataset and state-of-the-art neural networks.

Method: Experiments with a state-of-the-art neural network architecture on an in-house dataset of 8,849 subjects, collected with Meta Quest Pro–equivalent hardware at 72 Hz, using a video oculography-based gaze estimation pipeline. Systematically varied eye-tracking calibration aspects, calibration target depth, fusion of calibrated and non-calibrated gaze, and applied a three-sample moving average filter to raw gaze.

Result: Key findings: using the same calibration target depth for calibration improves performance; fusing calibrated and non-calibrated gaze improves performance; higher eye-tracking signal quality improves authentication performance; a three-sample moving average filter generally reduces performance, though there are some exceptions.

Conclusion: Calibration strategy, gaze fusion, and signal quality are critical levers for gaze-based authentication performance; simple filtering may be detrimental; results suggest targeted calibration and fusion strategies can yield better authentication, with some nuanced exceptions.

Abstract: This paper examines the key factors that influence the performance of
state-of-the-art gaze-based authentication. Experiments were conducted on a
large-scale, in-house dataset comprising 8,849 subjects collected with Meta
Quest Pro equivalent hardware running a video oculography-driven gaze
estimation pipeline at 72Hz. The state-of-the-art neural network architecture
was employed to study the influence of the following factors on authentication
performance: eye tracking signal quality, various aspects of eye tracking
calibration, and simple filtering on estimated raw gaze. We found that using
the same calibration target depth for eye tracking calibration, fusing
calibrated and non-calibrated gaze, and improving eye tracking signal quality
all enhance authentication performance. We also found that a simple
three-sample moving average filter slightly reduces authentication performance
in general. While these findings hold true for the most part, some exceptions
were noted.

</details>


### [23] [TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation](https://arxiv.org/abs/2509.10980)
*Haoming Lu*

Main category: cs.CV

TL;DR: TrueSkin introduces a 7,299-image, 6-class skin tone dataset to benchmark and improve skin tone recognition and generation, revealing biases in LMMs and generative models; training/fine-tuning on TrueSkin improves accuracy and fidelity, underscoring dataset's value for fairness.


<details>
  <summary>Details</summary>
Motivation: Address data gaps and biases in skin-tone recognition and synthesis, which impact fairness in AI systems, healthcare, and generative tasks.

Method: Collect TrueSkin under diverse lighting, camera angles, and capture settings; benchmark current recognition and generation models; train a recognition model on TrueSkin; fine-tune generation models with TrueSkin.

Result: LMMs misclassify intermediate tones as lighter; generation models struggle to produce specified tones due to prompt-attribute biases; training on TrueSkin improves recognition accuracy by >20%; fine-tuning improves skin-tone fidelity in generation.

Conclusion: Comprehensive, well-curated datasets like TrueSkin are essential benchmarks and training resources to improve fairness and accuracy in skin tone recognition and generation.

Abstract: Skin tone recognition and generation play important roles in model fairness,
healthcare, and generative AI, yet they remain challenging due to the lack of
comprehensive datasets and robust methodologies. Compared to other human image
analysis tasks, state-of-the-art large multimodal models (LMMs) and image
generation models struggle to recognize and synthesize skin tones accurately.
To address this, we introduce TrueSkin, a dataset with 7299 images
systematically categorized into 6 classes, collected under diverse lighting
conditions, camera angles, and capture settings. Using TrueSkin, we benchmark
existing recognition and generation approaches, revealing substantial biases:
LMMs tend to misclassify intermediate skin tones as lighter ones, whereas
generative models struggle to accurately produce specified skin tones when
influenced by inherent biases from unrelated attributes in the prompts, such as
hairstyle or environmental context. We further demonstrate that training a
recognition model on TrueSkin improves classification accuracy by more than
20\% compared to LMMs and conventional approaches, and fine-tuning with
TrueSkin significantly improves skin tone fidelity in image generation models.
Our findings highlight the need for comprehensive datasets like TrueSkin, which
not only serves as a benchmark for evaluating existing models but also provides
a valuable training resource to enhance fairness and accuracy in skin tone
recognition and generation tasks.

</details>


### [24] [Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring](https://arxiv.org/abs/2509.10995)
*Nisha Pillai,Aditi Virupakshaiah,Harrison W. Smith,Amanda J. Ashworth,Prasanna Gowda,Phillip R. Owens,Adam R. Rivers,Bindu Nanduri,Mahalingam Ramkumar*

Main category: cs.CV

TL;DR: An RL-based transfer learning framework using UCB to automatically select pre-trained models for UAV-based animal detection with limited data, achieving higher detection rates with less compute than traditional model-selection methods.


<details>
  <summary>Details</summary>
Motivation: Limited labeled data for wildlife detection and a plethora of pre-trained models; need an automated, efficient way to choose the best model to improve performance without excessive computation.

Method: Formulates model selection as a reinforcement learning problem and employs the Upper Confidence Bound (UCB) algorithm to evaluate and rank candidate pre-trained networks, enabling automated transfer learning for animal detection tasks.

Result: Experimental results show the framework achieves higher detection rates while substantially reducing computational time compared with traditional model-selection approaches.

Conclusion: RL-based UCB-driven model selection is effective for transfer learning in wildlife monitoring under data-scarce conditions, enabling faster and more accurate animal detection.

Abstract: Animal health monitoring and population management are critical aspects of
wildlife conservation and livestock management that increasingly rely on
automated detection and tracking systems. While Unmanned Aerial Vehicle (UAV)
based systems combined with computer vision offer promising solutions for
non-invasive animal monitoring across challenging terrains, limited
availability of labeled training data remains an obstacle in developing
effective deep learning (DL) models for these applications. Transfer learning
has emerged as a potential solution, allowing models trained on large datasets
to be adapted for resource-limited scenarios such as those with limited data.
However, the vast landscape of pre-trained neural network architectures makes
it challenging to select optimal models, particularly for researchers new to
the field. In this paper, we propose a reinforcement learning (RL)-based
transfer learning framework that employs an upper confidence bound (UCB)
algorithm to automatically select the most suitable pre-trained model for
animal detection tasks. Our approach systematically evaluates and ranks
candidate models based on their performance, streamlining the model selection
process. Experimental results demonstrate that our framework achieves a higher
detection rate while requiring significantly less computational time compared
to traditional methods.

</details>


### [25] [Improving Fungi Prototype Representations for Few-Shot Classification](https://arxiv.org/abs/2509.11020)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: Prototypical networks enable few-shot fungal classification and outperform the baseline by over 30pp Recall@5 on FungiCLEF 2025 PB/PR leaderboards.


<details>
  <summary>Details</summary>
Motivation: Address highly imbalanced, real-world field data and the scarcity of training samples for rare fungal species to support accurate biodiversity monitoring.

Method: A robust deep learning method based on prototypical networks with enhanced prototype representations for few-shot fungal classification.

Result: Recall@5 on both public and private leaderboards improved by more than 30 percentage points over the competition baseline.

Conclusion: Demonstrates strong potential for accurate identification of both common and rare fungal species, aligning with FungiCLEF 2025 objectives.

Abstract: The FungiCLEF 2025 competition addresses the challenge of automatic fungal
species recognition using realistic, field-collected observational data.
Accurate identification tools support both mycologists and citizen scientists,
greatly enhancing large-scale biodiversity monitoring. Effective recognition
systems in this context must handle highly imbalanced class distributions and
provide reliable performance even when very few training samples are available
for many species, especially rare and under-documented taxa that are often
missing from standard training sets. According to competition organizers, about
20\% of all verified fungi observations, representing nearly 20,000 instances,
are associated with these rarely recorded species. To tackle this challenge, we
propose a robust deep learning method based on prototypical networks, which
enhances prototype representations for few-shot fungal classification. Our
prototypical network approach exceeds the competition baseline by more than 30
percentage points in Recall@5 on both the public (PB) and private (PR)
leaderboards. This demonstrates strong potential for accurately identifying
both common and rare fungal species, supporting the main objectives of
FungiCLEF 2025.

</details>


### [26] [Cluster-Level Sparse Multi-Instance Learning for Whole-Slide Images](https://arxiv.org/abs/2509.11034)
*Yuedi Zhang,Zhixiang Xia,Guosheng Yin,Bin Liu*

Main category: cs.CV

TL;DR: Cluster-level Sparse MIL (csMIL) introduces a global/local clustering and cluster-sparsity framework to prune non-informative instances in MIL, yielding robust, interpretable predictions with reduced computation and state-of-the-art results on histopathology benchmarks.


<details>
  <summary>Details</summary>
Motivation: MIL on weakly labeled data (e.g., WSIs) suffers from redundancy of instances, presence of non-informative patches, and limited interpretability. There is a need for mechanisms to discard irrelevant instances and clusters while maintaining diagnostic relevance.

Method:  csMIL performs global clustering across bags to obtain K cluster centers, then does local clustering within each bag to assign cluster labels. Attention is computed within each cluster, followed by sparse regularization on cluster weights to retain only diagnostically relevant clusters and discard the rest. This yields robustness to noise, better interpretability (identifying critical regions), and lower computational burden.

Result: Theoretical analysis shows csMIL requires O(s log K) bags to recover s relevant clusters, consistent with compressed sensing. Empirically, csMIL achieves state-of-the-art performance on CAMELYON16 and TCGA-NSCLC histopathology benchmarks.

Conclusion: csMIL provides a robust, interpretable, and efficient MIL solution by integrating global-local clustering and cluster-level sparsity, with solid theoretical guarantees and strong empirical performance on public datasets.

Abstract: Multi-Instance Learning (MIL) is pivotal for analyzing complex, weakly
labeled datasets, such as whole-slide images (WSIs) in computational pathology,
where bags comprise unordered collections of instances with sparse diagnostic
relevance. Traditional MIL approaches, including early statistical methods and
recent attention-based frameworks, struggle with instance redundancy and lack
explicit mechanisms for discarding non-informative instances, limiting their
robustness and interpretability. We propose Cluster-level Sparse MIL (csMIL), a
novel framework that integrates global-local instance clustering,
within-cluster attention, and cluster-level sparsity induction to address these
challenges. Our csMIL first performs global clustering across all bags to
establish $K$ cluster centers, followed by local clustering within each bag to
assign cluster labels. Attention scores are computed within each cluster, and
sparse regularization is applied to cluster weights, enabling the selective
retention of diagnostically relevant clusters while discarding irrelevant ones.
This approach enhances robustness to noisy instances, improves interpretability
by identifying critical regions, and reduces computational complexity.
Theoretical analysis demonstrates that csMIL requires $O(s log K)$ bags to
recover $s$ relevant clusters, aligning with compressed sensing principles.
Empirically, csMIL achieves state-of-the-art performance on two public
histopathology benchmarks (CAMELYON16, TCGA-NSCLC).

</details>


### [27] [Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection](https://arxiv.org/abs/2509.11058)
*Canhui Tang,Sanping Zhou,Haoyue Shi,Le Wang*

Main category: cs.CV

TL;DR: A skeleton-based zero-shot video anomaly detection framework that uses language-guided semantic typicality modeling and test-time context uniqueness analysis to derive scene-adaptive boundaries, achieving state-of-the-art results on four large-scale VAD datasets without target-domain training data.


<details>
  <summary>Details</summary>
Motivation: Zero-shot VAD is crucial for privacy and new surveillance deployments. Skeleton representations help reduce background and appearance domain gaps, but prior methods rely on low-level features and fixed normality boundaries, limiting generalization to unseen scenes with varying behaviors.

Method: Two modules: (1) language-guided semantic typicality modeling that projects skeleton snippets into an action semantic space and leverages LLM knowledge to model typical normal/abnormal behavior during training. (2) test-time context uniqueness analysis that measures spatio-temporal differences between skeleton snippets to derive scene-adaptive decision boundaries.

Result: Without target-domain training data, the method achieves state-of-the-art results among skeleton-based VAD methods on four large-scale datasets (ShanghaiTech, UBnormal, NWPU, UCF-Crime) spanning over 100 unseen surveillance scenes.

Conclusion: The approach demonstrates that enriching skeleton-based ZS-VAD with semantic typicality and context-unique analysis improves generalization across diverse unseen scenes, offering a privacy-preserving, adaptable anomaly detection framework.

Abstract: Zero-Shot Video Anomaly Detection (ZS-VAD) requires temporally localizing
anomalies without target domain training data, which is a crucial task due to
various practical concerns, e.g., data privacy or new surveillance deployments.
Skeleton-based approach has inherent generalizable advantages in achieving
ZS-VAD as it eliminates domain disparities both in background and human
appearance. However, existing methods only learn low-level skeleton
representation and rely on the domain-limited normality boundary, which cannot
generalize well to new scenes with different normal and abnormal behavior
patterns. In this paper, we propose a novel zero-shot video anomaly detection
framework, unlocking the potential of skeleton data via action typicality and
uniqueness learning. Firstly, we introduce a language-guided semantic
typicality modeling module that projects skeleton snippets into action semantic
space and distills LLM's knowledge of typical normal and abnormal behaviors
during training. Secondly, we propose a test-time context uniqueness analysis
module to finely analyze the spatio-temporal differences between skeleton
snippets and then derive scene-adaptive boundaries. Without using any training
samples from the target domain, our method achieves state-of-the-art results
against skeleton-based methods on four large-scale VAD datasets: ShanghaiTech,
UBnormal, NWPU, and UCF-Crime, featuring over 100 unseen surveillance scenes.

</details>


### [28] [Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos](https://arxiv.org/abs/2509.11063)
*Xiaoyu Huang,Lauren M Maxson,Trang Nguyen,Cheng Jack Song,Yuankai Huo*

Main category: cs.CV

TL;DR: Organoid Tracker is an open-source GUI tool that uses Segment Anything Model 2 (SAM2) for zero-shot segmentation to analyze kidney organoid video data, enabling pixel-level, longitudinal analysis of PKD-relevant phenotypes (cyst formation, growth, morphology) without coding.


<details>
  <summary>Details</summary>
Motivation: Manual analysis of organoid video data is coarse and misses granular, time-resolved information; scalable, automated analysis is needed to extract detailed metrics for PKD modeling and therapeutic discovery.

Method: A modular, plugin-based GUI platform built on SAM2 for zero-shot segmentation to process spatial-temporal microscopy videos, extract metrics (e.g., cyst formation rate, growth velocity, morphological changes), and generate reports; open-source implementation available.

Result: Provides quantitative metrics on cyst formation rate, growth velocity, and morphological changes, with automated reporting; extensible framework enabling researchers to analyze PKD organoids without programming.

Conclusion: Organoid Tracker offers a powerful, open-source solution to accelerate kidney organoid PKD research and drug discovery by enabling detailed, pixel-level, longitudinal analysis without coding.

Abstract: Recent advances in organoid models have revolutionized the study of human
kidney disease mechanisms and drug discovery by enabling scalable,
cost-effective research without the need for animal sacrifice. Here, we present
a kidney organoid platform optimized for efficient screening in polycystic
kidney disease (PKD). While these systems generate rich spatial-temporal
microscopy video datasets, current manual approaches to analysis remain limited
to coarse classifications (e.g., hit vs. non-hit), often missing valuable
pixel-level and longitudinal information. To help overcome this bottleneck, we
developed Organoid Tracker, a graphical user interface (GUI) platform designed
with a modular plugin architecture, which empowers researchers to extract
detailed, quantitative metrics without programming expertise. Built on the
cutting-edge vision foundation model Segment Anything Model 2 (SAM2), Organoid
Tracker enables zero-shot segmentation and automated analysis of
spatial-temporal microscopy videos. It quantifies key metrics such as cyst
formation rate, growth velocity, and morphological changes, while generating
comprehensive reports. By providing an extensible, open-source framework,
Organoid Tracker offers a powerful solution for improving and accelerating
research in kidney development, PKD modeling, and therapeutic discovery. The
platform is publicly available as open-source software at
https://github.com/hrlblab/OrganoidTracker.

</details>


### [29] [The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge](https://arxiv.org/abs/2509.11071)
*Jinghan Peng,Jingwen Wang,Xing Yu,Dehui Du*

Main category: cs.CV

TL;DR: Vision-language driven approach for the CVPR 2024 Driving with Language track using DriveLM-nuScenes, fine-tuned LLaVA with LoRA/DoRA, augmented by depth estimates, and Chain-of-Thought reasoning for MC/Yes-No questions; achieved 0.7799 on validation and ranked 1st.


<details>
  <summary>Details</summary>
Motivation: To advance autonomous driving by integrating strong vision-language models with depth cues and efficient fine-tuning, enabling reliable reasoning in multi-modal driving tasks while leveraging a specialized driving dataset (DriveLM-nuScenes).

Method: Base model: LLaVA-family; fine-tuned with LoRA and DoRA on the DriveLM-nuScenes dataset; depth information incorporated from open-source depth estimation models to enrich training and inference; inference for multiple-choice and yes/no questions employs Chain-of-Thought reasoning to improve accuracy.

Result: Achieved a top validation score of 0.7799, ranking 1st on the Driving with Language leaderboard.

Conclusion: The approach demonstrates that combining vision-language models with depth cues and Chain-of-Thought reasoning yields state-of-the-art performance for autonomous driving tasks on the CVPR Autonomous Grand Challenge, highlighting the benefits of modular fine-tuning, depth augmentation, and explicit reasoning in multi-modal settings.

Abstract: This report outlines our approach using vision language model systems for the
Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We
have exclusively utilized the DriveLM-nuScenes dataset for training our models.
Our systems are built on the LLaVA models, which we enhanced through
fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated
depth information from open-source depth estimation models to enrich the
training and inference processes. For inference, particularly with
multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning
approach to improve the accuracy of the results. This comprehensive methodology
enabled us to achieve a top score of 0.7799 on the validation set leaderboard,
ranking 1st on the leaderboard.

</details>


### [30] [Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation](https://arxiv.org/abs/2509.11082)
*Zongwu Xie,Kaijie Yun,Yang Liu,Yiming Ji,Han Li*

Main category: cs.CV

TL;DR: A robust, self-supervised, multi-modal BEV costmap predictor for planetary rovers using camera+LiDAR with IMU-based labeling; demonstrates geometry-driven robustness and highlights reproducible simulation, IMU labeling, and strong multi-modal fusion; notes domain generalization and data diversity as future work.


<details>
  <summary>Details</summary>
Motivation: Need accurate traversability costs for planetary rovers without heavy hand-labeled data; leverage self-supervision from IMU signals; show robustness to sensor ablations; emphasize reproducible simulation.

Method: BEV costmap regression using a DINOv3 image encoder, FiLM-based fusion of camera and LiDAR, and a loss combining Huber and smoothness terms; training is self-supervised via IMU-derived labels; includes ablation tests (color removal, occlusion, added noise) and evaluates MAE/MSE.

Result: Ablations show only minor performance changes; e.g., MAE rises from ~0.0775 to 0.0915 when LiDAR is sparsified, indicating geometry dominates the learned cost; the model is robust to color loss, occlusions, and noise; performance attributed to geometry-focused IMU labeling and limited data diversity.

Conclusion: Contributions include a high-fidelity, reproducible simulation environment; a self-supervised IMU-based labeling pipeline; and a strong multi-modal BEV costmap predictor. Limitations include domain generalization and dataset expansion; future work to broaden generalization and data diversity.

Abstract: We present a robust multi-modal framework for predicting traversability
costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce
a bird's-eye-view (BEV) terrain costmap, trained self-supervised using
IMU-derived labels. Key updates include a DINOv3-based image encoder,
FiLM-based sensor fusion, and an optimization loss combining Huber and
smoothness terms. Experimental ablations (removing image color, occluding
inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases
from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry
dominates the learned cost and the model is highly robust. We attribute the
small performance differences to the IMU labeling primarily reflecting terrain
geometry rather than semantics and to limited data diversity. Unlike prior work
claiming large gains, we emphasize our contributions: (1) a high-fidelity,
reproducible simulation environment; (2) a self-supervised IMU-based labeling
pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss
limitations and future work such as domain generalization and dataset
expansion.

</details>


### [31] [End-to-End Visual Autonomous Parking via Control-Aided Attention](https://arxiv.org/abs/2509.11090)
*Chao Chen,Shunyu Yao,Yuanwu He,Tao Feng,Ruojing Song,Yuliang Guo,Xinyu Huang,Chenxu Wu,Ren Liu,Chen Feng*

Main category: cs.CV

TL;DR: CAA-Policy: an end-to-end imitation-learning system for precise parking that uses Control-Aided Attention (CAA) to steer visual attention with control signals, trained in a self-supervised manner, and bolstered by short-horizon waypoint prediction and a motion-prediction module; it outperforms end-to-end baselines and modular pipelines in CARLA.


<details>
  <summary>Details</summary>
Motivation: End-to-end systems aim to map perception directly to control for precise parking, but pure transformer self-attention can be unstable and temporally inconsistent, hindering reliable policy decisions.

Method: Introduce CAA-Policy with Control-Aided Attention, where attention is guided by control signals via backprop from control outputs (self-supervised) rather than training loss alone. Add short-horizon waypoint prediction as an auxiliary task and a separately trained motion-prediction module to robustly track the target over time.

Result: In CARLA experiments, CAA-Policy consistently surpasses both end-to-end baselines and modular BEV segmentation + hybrid A* pipelines in accuracy, robustness, and interpretability; code released.

Conclusion: Control-guided attention with self-supervised training plus auxiliary temporal predictions yields a more stable, generalizable end-to-end policy for precise parking, addressing the perception-control synergy gap.

Abstract: Precise parking requires an end-to-end system where perception adaptively
provides policy-relevant details-especially in critical areas where fine
control decisions are essential. End-to-end learning offers a unified framework
by directly mapping sensor inputs to control actions, but existing approaches
lack effective synergy between perception and control. We find that
transformer-based self-attention, when used alone, tends to produce unstable
and temporally inconsistent spatial attention, which undermines the reliability
of downstream policy decisions over time. Instead, we propose CAA-Policy, an
end-to-end imitation learning system that allows control signal to guide the
learning of visual attention via a novel Control-Aided Attention (CAA)
mechanism. For the first time, we train such an attention module in a
self-supervised manner, using backpropagated gradients from the control outputs
instead of from the training loss. This strategy encourages the attention to
focus on visual features that induce high variance in action outputs, rather
than merely minimizing the training loss-a shift we demonstrate leads to a more
robust and generalizable policy. To further enhance stability, CAA-Policy
integrates short-horizon waypoint prediction as an auxiliary task, and
introduces a separately trained motion prediction module to robustly track the
target spot over time. Extensive experiments in the CARLA simulator show that
\titlevariable~consistently surpasses both the end-to-end learning baseline and
the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,
robustness, and interpretability. Code is released at
https://github.com/Joechencc/CAAPolicy.

</details>


### [32] [PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation](https://arxiv.org/abs/2509.11092)
*Zeyu Dong,Yuyang Yin,Yuqi Li,Eric Li,Hao-Xiang Guo,Yikai Wang*

Main category: cs.CV

TL;DR: A data-efficient, adaptation-based method for 360° panoramic video generation using LoRA to adapt a perspective-view diffusion model, achieving state-of-the-art results with ~1k videos.


<details>
  <summary>Details</summary>
Motivation: Panoramic video generation faces projection differences and inefficiencies of existing architectures. LoRA’s success in style transfer inspires treating panorama generation as adapting perspective-view models, with a theoretical basis that LoRA can model projection transformations when its rank exceeds the task’s degrees of freedom.

Method: Formulate panoramic generation as an adaptation problem. Use Low-Rank Adaptation (LoRA) to fine-tune a pretrained video diffusion model to produce 360° panoramas from perspective inputs. Provide theoretical analysis showing that LoRA can capture the projection transformation when its rank exceeds the DOF. Train/fine-tune on roughly 1,000 videos, preserving projection geometry and achieving efficient adaptation.

Result: The approach yields high-quality panoramic videos, maintains proper projection geometry, and surpasses previous state-of-the-art in visual quality, left–right consistency, and motion diversity.

Conclusion: LoRA-based cross-projection adaptation is an effective, data-efficient strategy for panoramic video generation, enabling scalable improvement over prior methods with limited training data and without overhauling the core diffusion model.

Abstract: Generating high-quality 360{\deg} panoramic videos remains a significant
challenge due to the fundamental differences between panoramic and traditional
perspective-view projections. While perspective videos rely on a single
viewpoint with a limited field of view, panoramic content requires rendering
the full surrounding environment, making it difficult for standard video
generation models to adapt. Existing solutions often introduce complex
architectures or large-scale training, leading to inefficiency and suboptimal
results. Motivated by the success of Low-Rank Adaptation (LoRA) in style
transfer tasks, we propose treating panoramic video generation as an adaptation
problem from perspective views. Through theoretical analysis, we demonstrate
that LoRA can effectively model the transformation between these projections
when its rank exceeds the degrees of freedom in the task. Our approach
efficiently fine-tunes a pretrained video diffusion model using only
approximately 1,000 videos while achieving high-quality panoramic generation.
Experimental results demonstrate that our method maintains proper projection
geometry and surpasses previous state-of-the-art approaches in visual quality,
left-right consistency, and motion diversity.

</details>


### [33] [SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing](https://arxiv.org/abs/2509.11093)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: SMILE: a super-resolution guided multitask framework for hyperspectral unmixing that provides theoretical guarantees (task affinity and convergence) and shows improved results on synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral unmixing is hindered by low spatial resolution; incorporating super-resolution could help, but requires verified task relationships and convergence guarantees to be reliable.

Method: Propose SMILE, a super-resolution guided multi-task learning framework that learns shared and task-specific representations; provide theoretical analysis including relationship and existence theorems to show positive guidance from super-resolution; provide an accessibility theorem to guarantee convergence by characterizing the optimal solution of unmixing.

Result: Theoretical analysis validates feasibility of multitask learning and task affinity; the framework transfers positive information from SR to unmixing; experiments on synthetic and real datasets demonstrate usefulness of the approach.

Conclusion: SMILE offers progressive theoretical support and a novel framework for unmixing under SR guidance, with empirical validation supporting its effectiveness.

Abstract: The performance of hyperspectral unmixing may be constrained by low spatial
resolution, which can be enhanced using super-resolution in a multitask
learning way. However, integrating super-resolution and unmixing directly may
suffer two challenges: Task affinity is not verified, and the convergence of
unmixing is not guaranteed. To address the above issues, in this paper, we
provide theoretical analysis and propose super-resolution guided multi-task
learning method for hyperspectral unmixing (SMILE). The provided theoretical
analysis validates feasibility of multitask learning way and verifies task
affinity, which consists of relationship and existence theorems by proving the
positive guidance of super-resolution. The proposed framework generalizes
positive information from super-resolution to unmixing by learning both shared
and specific representations. Moreover, to guarantee the convergence, we
provide the accessibility theorem by proving the optimal solution of unmixing.
The major contributions of SMILE include providing progressive theoretical
support, and designing a new framework for unmixing under the guidance of
super-resolution. Our experiments on both synthetic and real datasets have
substantiate the usefulness of our work.

</details>


### [34] [A Copula-Guided Temporal Dependency Method for Multitemporal Hyperspectral Images Unmixing](https://arxiv.org/abs/2509.11096)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: Proposes Cog-TD: a copula-guided temporal dependency approach for multitemporal hyperspectral unmixing that explicitly models temporal dependencies via copulas, with two modules and theoretical support; validated on synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: MTHU aims to capture variable endmembers and dynamical abundances, but existing methods struggle to model temporal dependency. Copula theory can explicitly represent dependency structures, offering a principled way to encode temporal evolution in unmixing.

Method: Redefine the MTHU problem to incorporate temporal dependency using copula theory. Build a copula-guided framework with two key modules: (1) copula function estimation and (2) temporal dependency guidance, to estimate dynamical endmembers and abundances guided by temporal correlations. Provide theoretical support proving validity of the estimated copula and the existence of temporal dependency in hyperspectral data.

Result: Experimental results on synthetic and real-world datasets demonstrate the utility of the proposed Cog-TD method, indicating improved capability to model temporal dynamics in MTHU.

Conclusion: Cog-TD offers a principled way to model temporal dependency in multitemporal hyperspectral unmixing by embedding copula-based dependency modeling into the unmixing process, with theoretical guarantees and empirical validation on both synthetic and real data.

Abstract: Multitemporal hyperspectral unmixing (MTHU) aims to model variable endmembers
and dynamical abundances, which emphasizes the critical temporal information.
However, existing methods have limitations in modeling temporal dependency,
thus fail to capture the dynamical material evolution. Motivated by the ability
of copula theory in modeling dependency structure explicitly, in this paper, we
propose a copula-guided temporal dependency method (Cog-TD) for multitemporal
hyperspectral unmixing. Cog-TD defines new mathematical model, constructs
copula-guided framework and provides two key modules with theoretical support.
The mathematical model provides explicit formulations for MTHU problem
definition, which describes temporal dependency structure by incorporating
copula theory. The copula-guided framework is constructed for utilizing copula
function, which estimates dynamical endmembers and abundances with temporal
dependency. The key modules consist of copula function estimation and temporal
dependency guidance, which computes and employs temporal information to guide
unmixing process. Moreover, the theoretical support demonstrates that estimated
copula function is valid and the represented temporal dependency exists in
hyperspectral images. The major contributions of this paper include redefining
MTHU problem with temporal dependency, proposing a copula-guided framework,
developing two key modules and providing theoretical support. Our experimental
results on both synthetic and real-world datasets demonstrate the utility of
the proposed method.

</details>


### [35] [3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment](https://arxiv.org/abs/2509.11097)
*Nhut Le,Ehsan Karimi,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: A new 3D semantic segmentation benchmark, 3DAeroRelief, for post-disaster assessment using UAV-derived point clouds, filling a gap in disaster-focused 3D datasets; baseline models reveal challenges and opportunities for real-world disaster response.


<details>
  <summary>Details</summary>
Motivation: Existing work on disaster analysis relies on 2D imagery lacking depth and context; current 3D benchmarks focus on urban/indoor scenes and ignore disaster-affected outdoor areas. There is a need for a large-scale, real-world disaster dataset with dense 3D geometry and fine-grained damage annotations to improve 3D perception for post-disaster response.

Method: Collect data with low-cost UAVs over hurricane-damaged regions. Build dense 3D point clouds via Structure-from-Motion and Multi-View Stereo. Generate semantic labels by manual 2D labeling and projection into 3D space. Evaluate state-of-the-art 3D segmentation models on the dataset to assess challenges and opportunities.

Result: The dataset demonstrates utility for 3D disaster scene understanding. Benchmark experiments on several SOTA 3D segmentation models reveal the dataset’s challenges (e.g., large outdoor scales, fine-grained damage, occlusions, variable point density) and opportunities for robust post-disaster perception.

Conclusion: 3DAeroRelief provides a valuable resource for advancing robust 3D vision systems in real-world post-disaster scenarios, enabling more effective disaster response and recovery through improved 3D understanding.

Abstract: Timely assessment of structural damage is critical for disaster response and
recovery. However, most prior work in natural disaster analysis relies on 2D
imagery, which lacks depth, suffers from occlusions, and provides limited
spatial context. 3D semantic segmentation offers a richer alternative, but
existing 3D benchmarks focus mainly on urban or indoor scenes, with little
attention to disaster-affected areas. To address this gap, we present
3DAeroRelief--the first 3D benchmark dataset specifically designed for
post-disaster assessment. Collected using low-cost unmanned aerial vehicles
(UAVs) over hurricane-damaged regions, the dataset features dense 3D point
clouds reconstructed via Structure-from-Motion and Multi-View Stereo
techniques. Semantic annotations were produced through manual 2D labeling and
projected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3D
large-scale outdoor environments with fine-grained structural damage in
real-world disaster contexts. UAVs enable affordable, flexible, and safe data
collection in hazardous areas, making them particularly well-suited for
emergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluate
several state-of-the-art 3D segmentation models on the dataset to highlight
both the challenges and opportunities of 3D scene understanding in disaster
response. Our dataset serves as a valuable resource for advancing robust 3D
vision systems in real-world applications for post-disaster scenarios.

</details>


### [36] [Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.11102)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: GEMMNet: a generative-enhanced multi-modal network for robust remote-sensing semantic segmentation under missing modalities, integrating HyFEx, HyFMA, and CoLoss; outperforms AE, cGAN, mmformer, and shaspec on Vaihingen/Potsdam; code available.


<details>
  <summary>Details</summary>
Motivation: In real-world multimodal remote sensing, sensor failures and adverse conditions cause missing modalities, and existing generative approaches struggle with heterogeneity and semantic context, bias toward dominant modalities, and limited robustness.

Method: GEMMNet comprises three components: (1) Hybrid Feature Extractor (HyFEx) for modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness (HyFMA) to capture cross-modality semantic context at multiple scales, and (3) Complementary Loss (CoLoss) to enforce cross-modality and cross-task consistency and reduce bias.

Result: On two challenging semantic segmentation datasets (Vaihingen and Potsdam), GEMMNet outperforms generative baselines AE and cGAN and non-generative baselines mmformer and shaspec; source code is released.

Conclusion: The proposed GEMMNet demonstrates improved robustness to missing modalities by explicitly modeling modality-specific features, multiscale cross-modal fusion, and alignment losses, contributing a practical solution for robust remote-sensing segmentation under incomplete multimodal data.

Abstract: Multimodal learning has shown significant performance boost compared to
ordinary unimodal models across various domains. However, in real-world
scenarios, multimodal signals are susceptible to missing because of sensor
failures and adverse weather conditions, which drastically deteriorates models'
operation and performance. Generative models such as AutoEncoder (AE) and
Generative Adversarial Network (GAN) are intuitive solutions aiming to
reconstruct missing modality from available ones. Yet, their efficacy in remote
sensing semantic segmentation remains underexplored. In this paper, we first
examine the limitations of existing generative approaches in handling the
heterogeneity of multimodal remote sensing data. They inadequately capture
semantic context in complex scenes with large intra-class and small inter-class
variation. In addition, traditional generative models are susceptible to heavy
dependence on the dominant modality, introducing bias that affects model
robustness under missing modality conditions. To tackle these limitations, we
propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with
three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn
modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness
(HyFMA) to capture modality-synergistic semantic context across scales and (3)
Complementary Loss (CoLoss) scheme to alleviate the inherent bias by
encouraging consistency across modalities and tasks. Our method, GEMMNet,
outperforms both generative baselines AE, cGAN (conditional GAN), and
state-of-the-art non-generative approaches - mmformer and shaspec - on two
challenging semantic segmentation remote sensing datasets (Vaihingen and
Potsdam). Source code is made available.

</details>


### [37] [WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild](https://arxiv.org/abs/2509.11114)
*Yuqiu Liu,Jialin Song,Manolis Savva,Wuyang Chen*

Main category: cs.CV

TL;DR: A pipeline that reconstructs dynamic 3D smoke from a single wild video, with background removal, particle/camera initialization, and multi-view inference, enabling interactive smoke editing and 4D asset creation; achieves higher quality on in-the-wild data and facilitates editable fluid simulations.


<details>
  <summary>Details</summary>
Motivation: Reconstructing 3D smoke from real-world videos is challenging due to cluttered backgrounds, lighting variation, and uncontrolled camera motion. Current approaches excel mainly in controlled settings; there is a need for robust, end-to-end pipelines that can produce editable 4D smoke assets from wild videos.

Method: Proposes a pipeline with (1) smoke extraction with background removal, (2) initialization of smoke particles and camera poses, and (3) inferring multi-view videos, followed by integration with interactive simulation to allow editing of fluid dynamics. Demonstrates improved reconstruction quality (PSNR gain) on wild videos and provides 4D smoke assets.

Result: Outperforms prior reconstruction and generation methods on wild videos, achieving +2.22 average PSNR. Produces high-quality smoke reconstructions and enables diverse, realistic editing of fluid dynamics via simulation. Releases models, data, and 4D smoke assets at the stated URL.

Conclusion: The approach closes the gap between lab-grade reconstructions and real-world footage by enabling robust, editable 4D smoke assets from single in-the-wild videos, with practical workflows for smoke design and editing.

Abstract: We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from
a single in-the-wild video, and further integrate interactive simulation for
smoke design and editing. Recent developments in 3D vision have significantly
improved reconstructing and rendering fluid dynamics, supporting realistic and
temporally consistent view synthesis. However, current fluid reconstructions
rely heavily on carefully controlled clean lab environments, whereas real-world
videos captured in the wild are largely underexplored. We pinpoint three key
challenges of reconstructing smoke in real-world videos and design targeted
techniques, including smoke extraction with background removal, initialization
of smoke particles and camera poses, and inferring multi-view videos. Our
method not only outperforms previous reconstruction and generation methods with
high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but
also enables diverse and realistic editing of fluid dynamics by simulating our
smoke assets. We provide our models, data, and 4D smoke assets at
[https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke).

</details>


### [38] [SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting](https://arxiv.org/abs/2509.11116)
*Ashkan Taghipour,Vahid Naghshin,Benjamin Southwell,Farid Boussaid,Hamid Laga,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: SVR-GS introduces a per-pixel spatial regularizer for 3D Gaussian Splatting that targets low-contribution Gaussians using per-ray masks, enabling sparse pruning aligned with view-specific quality while preserving performance.


<details>
  <summary>Details</summary>
Motivation: Mask-based pruning (e.g., MaskGS) uses a global mask mean that does not align with per-pixel, per-ray image quality, leading to suboptimal sparsification and inefficiency.

Method: Propose a spatially variant regularizer that generates a per-pixel spatial mask from each Gaussian's effective contribution along rays. Explore three spatial-mask aggregation strategies, implement in CUDA, and provide a gradient analysis to justify the design.

Result: SVR-GS reduces the number of Gaussians by 1.79x compared to MaskGS and 5.63x compared to vanilla 3DGS on average across Tanks&Temples, Deep Blending, and Mip-NeRF360, with PSNR drops of 0.50 dB and 0.40 dB respectively.

Conclusion: By aligning sparsity pressure with per-ray importance, SVR-GS yields smaller, faster, and more memory-efficient 3D Gaussian Splatting models suitable for real-time applications such as robotics, AR/VR, and mobile perception.

Abstract: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis
but typically relies on densification followed by pruning to optimize the
number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes
the global mean of the mask, which is misaligned with the local per-pixel
(per-ray) reconstruction loss that determines image quality along individual
camera rays. This paper introduces SVR-GS, a spatially variant regularizer that
renders a per-pixel spatial mask from each Gaussian's effective contribution
along the ray, thereby applying sparsity pressure where it matters: on
low-importance Gaussians. We explore three spatial-mask aggregation strategies,
implement them in CUDA, and conduct a gradient analysis to motivate our final
design. Extensive experiments on Tanks\&Temples, Deep Blending, and Mip-NeRF360
datasets demonstrate that, on average across the three datasets, the proposed
SVR-GS reduces the number of Gaussians by 1.79\(\times\) compared to MaskGS and
5.63\(\times\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR
drops, respectively. These gains translate into significantly smaller, faster,
and more memory-efficient models, making them well-suited for real-time
applications such as robotics, AR/VR, and mobile perception.

</details>


### [39] [No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images](https://arxiv.org/abs/2509.11164)
*Diego Eustachio Farchione,Ramzi Idoughi,Peter Wonka*

Main category: cs.CV

TL;DR: A lightweight DL framework estimates coral volume and surface area from sparse multi-view RGB images using VGGT-based point extraction, merged cloud with per-view confidence, and two DGCNN decoder heads for volume and surface area with uncertainty, trained with Gaussian NLL losses in real and log domains; claims competitive accuracy and generalization to unseen morphologies.


<details>
  <summary>Details</summary>
Motivation: Quantifying coral growth requires 3D metrics (volume and surface area) that are hard due to coral morphology; scalable, automated methods from limited imagery are needed for reef monitoring.

Method: Use a pretrained VGGT module to extract dense point maps from each view, merge into unified per-view-confidence-enriched point cloud; two parallel DGCNN decoder heads predict volume, surface area, and their uncertainties; train with a composite Gaussian negative log-likelihood loss in real and log domains to stabilize predictions.

Result: The approach achieves competitive accuracy and generalizes to unseen morphologies; demonstrates potential for efficient and scalable coral geometry estimation from a sparse image set.

Conclusion: Provides a scalable framework for coral geometry estimation from sparse multi-view images, enabling improved coral growth analysis and reef monitoring.

Abstract: Effective reef monitoring requires the quantification of coral growth via
accurate volumetric and surface area estimates, which is a challenging task due
to the complex morphology of corals. We propose a novel, lightweight, and
scalable learning framework that addresses this challenge by predicting the 3D
volume and surface area of coral-like objects from 2D multi-view RGB images.
Our approach utilizes a pre-trained module (VGGT) to extract dense point maps
from each view; these maps are merged into a unified point cloud and enriched
with per-view confidence scores. The resulting cloud is fed to two parallel
DGCNN decoder heads, which jointly output the volume and the surface area of
the coral, as well as their corresponding confidence estimate. To enhance
prediction stability and provide uncertainty estimates, we introduce a
composite loss function based on Gaussian negative log-likelihood in both real
and log domains. Our method achieves competitive accuracy and generalizes well
to unseen morphologies. This framework paves the way for efficient and scalable
coral geometry estimation directly from a sparse set of images, with potential
applications in coral growth analysis and reef monitoring.

</details>


### [40] [Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic](https://arxiv.org/abs/2509.11165)
*Waikit Xiu,Qiang Lu,Xiying Li,Chen Hu,Shengbo Sun*

Main category: cs.CV

TL;DR: Traffic-MLLM is a multimodal large language model for fine-grained traffic analysis, built on Qwen2.5-VL with LoRA fine-tuning, plus a Chain-of-Thought + Retrieval-Augmented Generation knowledge prompting module, achieving SOTA on TrafficQA/DriveQA and showing strong zero-shot and cross-scenario generalization.


<details>
  <summary>Details</summary>
Motivation: Existing traffic video understanding struggles to accurately model spatiotemporal causality and to incorporate domain-specific knowledge, limiting performance in complex scenarios. There is a need for models that can handle continuous spatiotemporal features and integrate traffic regulations and domain knowledge efficiently.

Method: Fine-tune the Qwen2.5-VL backbone with LoRA on high-quality traffic-specific multimodal datasets. Introduce a knowledge prompting module that fuses Chain-of-Thought reasoning with Retrieval-Augmented Generation to inject detailed traffic regulations and domain knowledge into inference, enabling better logical reasoning and knowledge adaptation.

Result: Traffic-MLLM achieves state-of-the-art performance on TrafficQA and DriveQA. It exhibits strong zero-shot reasoning and cross-scenario generalization on multimodal traffic data.

Conclusion: Traffic-MLLM demonstrates effective multimodal traffic data understanding with enhanced spatiotemporal reasoning and domain knowledge integration, offering robust performance and generalization across traffic scenarios.

Abstract: As intelligent transportation systems advance, traffic video understanding
plays an increasingly pivotal role in comprehensive scene perception and causal
analysis. Yet, existing approaches face notable challenges in accurately
modeling spatiotemporal causality and integrating domain-specific knowledge,
limiting their effectiveness in complex scenarios. To address these
limitations, we propose Traffic-MLLM, a multimodal large language model
tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,
our model leverages high-quality traffic-specific multimodal datasets and uses
Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing
its capacity to model continuous spatiotemporal features in video sequences.
Furthermore, we introduce an innovative knowledge prompting module fusing
Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),
enabling precise injection of detailed traffic regulations and domain knowledge
into the inference process. This design markedly boosts the model's logical
reasoning and knowledge adaptation capabilities. Experimental results on
TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art
performance, validating its superior ability to process multimodal traffic
data. It also exhibits remarkable zero-shot reasoning and cross-scenario
generalization capabilities.

</details>


### [41] [Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields](https://arxiv.org/abs/2509.11169)
*Hong Zhang,Fei Guo,Zihan Xie,Dizhao Yao*

Main category: cs.CV

TL;DR: Introduces Multispectral-NeRF to enable 6-band multispectral 3D reconstruction within a NeRF framework by expanding latent space, refining spectral discrepancy residuals, and adjusting data compression; achieves high-precision geometry with faithful spectral preservation.


<details>
  <summary>Details</summary>
Motivation: Current NeRF variants are designed for 3-band RGB data and do not exploit additional spectral bands; multispectral 3D reconstruction methods suffer from cost, accuracy, and geometric issues; a NeRF-based approach that fuses multispectral information is needed.

Method: Modify NeRF architecture: (1) increase hidden layer dimensionality to 6-band input; (2) redesign residual functions to optimize spectral discrepancy between reconstructed and reference multispectral images; (3) adapt data compression modules for higher bit-depth imagery; train on multispectral data; evaluate spectral fidelity and 3D reconstruction quality.

Result: Demonstrates Multispectral-NeRF can process multi-band spectral features and preserve spectral characteristics of scenes with high fidelity; shows improved spectral preservation and reconstruction quality compared to RGB-based baselines on multispectral data.

Conclusion: Multispectral-NeRF effectively integrates multispectral information into the NeRF framework, addressing prior limitations and enabling high-quality, spectrally faithful 6-band 3D reconstructions.

Abstract: 3D reconstruction technology generates three-dimensional representations of
real-world objects, scenes, or environments using sensor data such as 2D
images, with extensive applications in robotics, autonomous vehicles, and
virtual reality systems. Traditional 3D reconstruction techniques based on 2D
images typically relies on RGB spectral information. With advances in sensor
technology, additional spectral bands beyond RGB have been increasingly
incorporated into 3D reconstruction workflows. Existing methods that integrate
these expanded spectral data often suffer from expensive scheme prices, low
accuracy and poor geometric features. Three - dimensional reconstruction based
on NeRF can effectively address the various issues in current multispectral 3D
reconstruction methods, producing high - precision and high - quality
reconstruction results. However, currently, NeRF and some improved models such
as NeRFacto are trained on three - band data and cannot take into account the
multi - band information. To address this problem, we propose
Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can
effectively integrates multispectral information. Our technical contributions
comprise threefold modifications: Expanding hidden layer dimensionality to
accommodate 6-band spectral inputs; Redesigning residual functions to optimize
spectral discrepancy calculations between reconstructed and reference images;
Adapting data compression modules to address the increased bit-depth
requirements of multispectral imagery. Experimental results confirm that
Multispectral-NeRF successfully processes multi-band spectral features while
accurately preserving the original scenes' spectral characteristics.

</details>


### [42] [SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion](https://arxiv.org/abs/2509.11171)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: SPHERE combines voxel and Gaussian scene representations for camera-based SSC, using SGI to initialize Gaussians and PHE with semantic spherical harmonics to enhance physical realism and semantic-geometry consistency, achieving improved accuracy on SemanticKITTI and KITTI-360 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between voxel-based/plane-based SSC which capture geometry but lack realistic physical details, and neural reconstructions like NeRF which are accurate but slow for large-scale driving scenes; aim for efficient, semantically-rich, physically-informed SSC.

Method: Two-stage representation: SGI for initialization; dual-branch 3D representations locate focal voxels to seed Gaussian distributions; PHE module uses semantic spherical harmonics to model context and align focal distributions to improve semantic-geometry consistency; integrates voxel and Gaussian representations into a joint framework for SSC.

Result: Validated on SemanticKITTI and SSCBench-KITTI-360; performance improvements in semantic accuracy and geometric details; publicly available code.

Conclusion: SPHERE offers a practical, scalable approach to camera-based SSC, achieving realistic details while maintaining efficiency; demonstrates benefit of combining semantic-guided Gaussian initialization and physics-aware spherical harmonics.

Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task in
autonomous driving systems, assessing voxel-level geometry and semantics for
holistic scene perception. While existing voxel-based and plane-based SSC
methods have achieved considerable progress, they struggle to capture physical
regularities for realistic geometric details. On the other hand, neural
reconstruction methods like NeRF and 3DGS demonstrate superior physical
awareness, but suffer from high computational cost and slow convergence when
handling large-scale, complex autonomous driving scenes, leading to inferior
semantic accuracy. To address these issues, we propose the Semantic-PHysical
Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel
and Gaussian representations for joint exploitation of semantic and physical
information. First, the Semantic-guided Gaussian Initialization (SGI) module
leverages dual-branch 3D scene representations to locate focal voxels as
anchors to guide efficient Gaussian initialization. Then, the Physical-aware
Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to
model physical-aware contextual details and promote semantic-geometry
consistency through focal distribution alignment, generating SSC results with
realistic details. Extensive experiments and analyses on the popular
SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of
SPHERE. The code is available at
https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.

</details>


### [43] [StegOT: Trade-offs in Steganography via Optimal Transport](https://arxiv.org/abs/2509.11178)
*Chengde Lin,Xuezhu Gong,Shuxue Ding,Mingzhe Yang,Xijun Lu,Chengjun Mo*

Main category: cs.CV

TL;DR: StegOT uses an autoencoder-based steganography framework with a multiple channel optimal transport (MCOT) module to address mode collapse in representation learning, balancing the cover and secret information and improving the quality of both stego and recovered images.


<details>
  <summary>Details</summary>
Motivation: GAN/VAEs-based steganography often suffers from mode collapse, causing information imbalance between cover and secret images and degraded extraction.

Method: Introduce MCOT within an autoencoder-based steganography model to transform multi-peak feature distributions into a single peak, achieving a better information trade-off.

Result: Empirical results show improved balance between cover and secret images and higher quality of both stego and recovery images; code to be released.

Conclusion: MCOT effectively mitigates mode collapse in steganography and enhances information balance and image quality.

Abstract: Image hiding is often referred to as steganography, which aims to hide a
secret image in a cover image of the same resolution. Many steganography models
are based on genera-tive adversarial networks (GANs) and variational
autoencoders (VAEs). However, most existing models suffer from mode collapse.
Mode collapse will lead to an information imbalance between the cover and
secret images in the stego image and further affect the subsequent extraction.
To address these challenges, this paper proposes StegOT, an autoencoder-based
steganography model incorporating optimal transport theory. We designed the
multiple channel optimal transport (MCOT) module to transform the feature
distribution, which exhibits multiple peaks, into a single peak to achieve the
trade-off of information. Experiments demonstrate that we not only achieve a
trade-off between the cover and secret images but also enhance the quality of
both the stego and recovery images. The source code will be released on
https://github.com/Rss1124/StegOT.

</details>


### [44] [The Impact of Skin Tone Label Granularity on the Performance and Fairness of AI Based Dermatology Image Classification Models](https://arxiv.org/abs/2509.11184)
*Partha Shah,Durva Sankhe,Maariyah Rashid,Zakaa Khaled,Esther Puyol-Antón,Tiarna Lee,Maram Alqarni,Sweta Rai,Andrew P. King*

Main category: cs.CV

TL;DR: Investigates how Fitzpatrick Skin Tone (FST) granularity affects AI skin-lesion classification. Finds that using three FST groups (1/2, 3/4, 5/6) often outperforms a general, FST-balanced model, while finer granularity (1/2/3/4) can harm performance. Suggests moving away from FST toward alternative scales that better represent skin-tone diversity.


<details>
  <summary>Details</summary>
Motivation: Fairness and bias concerns in AI dermatology: the Fitzpatrick scale’s uneven granularity and its impact on model performance; clarify whether coarser or finer FST representations help or hurt, and whether to abandon FST in favor of better scales.

Method: Train multiple AI models to classify benign vs malignant skin lesions using FST-specific data with differing granularity (three groups vs four groups) and compare to a model trained on FST-balanced data; evaluate performance and potential bias.

Result: Models trained on FST-specific data with three groups generally outperform the general FST-balanced model. Increasing granularity to four groups can degrade performance. Findings underscore the importance of how skin-tone granularity is defined for fairness and accuracy in lesion classification.

Conclusion: The study argues against continued reliance on the FST scale in fair AI dermatology research and advocates adopting alternative scales that better capture the diversity of human skin tones.

Abstract: Artificial intelligence (AI) models to automatically classify skin lesions
from dermatology images have shown promising performance but also
susceptibility to bias by skin tone. The most common way of representing skin
tone information is the Fitzpatrick Skin Tone (FST) scale. The FST scale has
been criticised for having greater granularity in its skin tone categories for
lighter-skinned subjects. This paper conducts an investigation of the impact
(on performance and bias) on AI classification models of granularity in the FST
scale. By training multiple AI models to classify benign vs. malignant lesions
using FST-specific data of differing granularity, we show that: (i) when
training models using FST-specific data based on three groups (FST 1/2, 3/4 and
5/6), performance is generally better for models trained on FST-specific data
compared to a general model trained on FST-balanced data; (ii) reducing the
granularity of FST scale information (from 1/2 and 3/4 to 1/2/3/4) can have a
detrimental effect on performance. Our results highlight the importance of the
granularity of FST groups when training lesion classification models. Given the
question marks over possible human biases in the choice of categories in the
FST scale, this paper provides evidence for a move away from the FST scale in
fair AI research and a transition to an alternative scale that better
represents the diversity of human skin tones.

</details>


### [45] [Scaling Up Forest Vision with Synthetic Data](https://arxiv.org/abs/2509.11201)
*Yihang She,Andrew Blake,David Coomes,Srinivasan Keshav*

Main category: cs.CV

TL;DR: Synthetic 3D forest data with physics-based LiDAR enables pretraining that substantially reduces the need for labeled real data in tree segmentation; fine-tuning on a single small real plot yields competitive performance versus models trained on full real data.


<details>
  <summary>Details</summary>
Motivation: Public 3D forest datasets are too small to train robust tree segmentation models; synthetic data has shown promise in other domains, prompting exploration in forestry.

Method: Developed a synthetic data generation pipeline using game engines and physics-based LiDAR simulation; pretrained a state-of-the-art tree segmentation model on synthetic data; fine-tuned on a single real forest plot (<0.1 ha); evaluated on real data with a popular segmentation algorithm; provided a comprehensive annotated 3D forest dataset.

Result: Synthetic data substantially reduces the need for labeled real data; after fine-tuning on a very small real plot, the model achieves segmentations competitive with a model trained on the full real dataset; factors critical to success include physics realism, data diversity, and dataset scale.

Conclusion: Physics, diversity, and scale are key to successful synthetic-data-based forest vision; the approach paves the way for more robust 3D forest segmentation and vision systems, with the pipeline and dataset publicly available.

Abstract: Accurate tree segmentation is a key step in extracting individual tree
metrics from forest laser scans, and is essential to understanding ecosystem
functions in carbon cycling and beyond. Over the past decade, tree segmentation
algorithms have advanced rapidly due to developments in AI. However existing,
public, 3D forest datasets are not large enough to build robust tree
segmentation systems. Motivated by the success of synthetic data in other
domains such as self-driving, we investigate whether similar approaches can
help with tree segmentation. In place of expensive field data collection and
annotation, we use synthetic data during pretraining, and then require only
minimal, real forest plot annotation for fine-tuning.
  We have developed a new synthetic data generation pipeline to do this for
forest vision tasks, integrating advances in game-engines with physics-based
LiDAR simulation. As a result, we have produced a comprehensive, diverse,
annotated 3D forest dataset on an unprecedented scale. Extensive experiments
with a state-of-the-art tree segmentation algorithm and a popular real dataset
show that our synthetic data can substantially reduce the need for labelled
real data. After fine-tuning on just a single, real, forest plot of less than
0.1 hectare, the pretrained model achieves segmentations that are competitive
with a model trained on the full scale real data. We have also identified
critical factors for successful use of synthetic data: physics, diversity, and
scale, paving the way for more robust 3D forest vision systems in the future.
Our data generation pipeline and the resulting dataset are available at
https://github.com/yihshe/CAMP3D.git.

</details>


### [46] [Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation](https://arxiv.org/abs/2509.11213)
*Yufei Tang,Daiheng Gao,Pingyu Wu,Wenbo Zhou,Bang Zhang,Weiming Zhang*

Main category: cs.CV

TL;DR: Beyond Sliders is a framework that merges GANs and diffusion models to enable advanced image manipulation across diverse image categories, addressing limitations of concept sliders for no-AIGC, real-world images. It uses fine-grained textual and visual guidance in an adversarial setup to improve image quality and realism, with strong experimental validation across applications.


<details>
  <summary>Details</summary>
Motivation: There is a growing demand for realistic, customizable image edits for real-world images, and existing concept slider methods struggle with no-AIGC or real-world settings. A cross-category, high-fidelity editing framework is needed.

Method: A framework that integrates GANs and diffusion models to enable sophisticated image manipulation. It refines images through fine-grained guidance, both textual and visual, in an adversarial manner, improving image quality and realism across diverse image categories.

Result: Demonstrates marked improvements in image quality and realism. The approach is robust and versatile across a spectrum of applications, as confirmed by extensive experimental validation.

Conclusion: Beyond Sliders provides a versatile, cross-domain image-editing framework that surpasses concept sliders for no-AIGC images, with strong empirical support and broad applicability, paving the way for future enhancements.

Abstract: In the realm of image generation, the quest for realism and customization has
never been more pressing. While existing methods like concept sliders have made
strides, they often falter when it comes to no-AIGC images, particularly images
captured in real world settings. To bridge this gap, we introduce Beyond
Sliders, an innovative framework that integrates GANs and diffusion models to
facilitate sophisticated image manipulation across diverse image categories.
Improved upon concept sliders, our method refines the image through fine
grained guidance both textual and visual in an adversarial manner, leading to a
marked enhancement in image quality and realism. Extensive experimental
validation confirms the robustness and versatility of Beyond Sliders across a
spectrum of applications.

</details>


### [47] [Geometrically Constrained and Token-Based Probabilistic Spatial Transformers](https://arxiv.org/abs/2509.11218)
*Johann Schmidt,Sebastian Stober*

Main category: cs.CV

TL;DR: A probabilistic, component-wise Spatial Transformer Network improves transformer-based FGVC robustness to geometric variability by decomposing affine transforms into rotation, scaling, and shear, modeling each with Gaussian posteriors, and using a sampling-based canonicalization and a novel alignment loss; validated on challenging moth benchmarks.


<details>
  <summary>Details</summary>
Motivation: Fine-grained visual classification is highly sensitive to geometric variability (arbitrary orientation, scale, perspective). While equivariant architectures exist, they are costly and constrain the hypothesis space. The paper revisits Spatial Transformer Networks as a flexible, backbone-agnostic canonicalization tool for vision transformers, aiming to boost robustness without architectural constraints.

Method: Decompose affine transforms into rotation, scaling, and shearing components; regress each component under geometric constraints using a shared localization encoder; model each component with a Gaussian variational posterior; perform sampling-based canonicalization during inference; introduce a component-wise alignment loss that uses augmentation parameters to guide spatial alignment.

Result: Experiments on challenging moth classification benchmarks show consistent robustness improvements compared with other STN variants in transformer-based pipelines.

Conclusion: A probabilistic, component-wise STN extension provides flexible, backbone-agnostic canonicalization for vision transformers, yielding improved robustness to geometric variability in FGVC without imposing restrictive architectural constraints.

Abstract: Fine-grained visual classification (FGVC) remains highly sensitive to
geometric variability, where objects appear under arbitrary orientations,
scales, and perspective distortions. While equivariant architectures address
this issue, they typically require substantial computational resources and
restrict the hypothesis space. We revisit Spatial Transformer Networks (STNs)
as a canonicalization tool for transformer-based vision pipelines, emphasizing
their flexibility, backbone-agnostic nature, and lack of architectural
constraints. We propose a probabilistic, component-wise extension that improves
robustness. Specifically, we decompose affine transformations into rotation,
scaling, and shearing, and regress each component under geometric constraints
using a shared localization encoder. To capture uncertainty, we model each
component with a Gaussian variational posterior and perform sampling-based
canonicalization during inference.A novel component-wise alignment loss
leverages augmentation parameters to guide spatial alignment. Experiments on
challenging moth classification benchmarks demonstrate that our method
consistently improves robustness compared to other STNs.

</details>


### [48] [CCoMAML: Efficient Cattle Identification Using Cooperative Model-Agnostic Meta-Learning](https://arxiv.org/abs/2509.11219)
*Rabin Dulal,Lihong Zheng,Ashad Kabir*

Main category: cs.CV

TL;DR: A few-shot meta-learning approach (CCoMAML + MHAFF) for cattle muzzle-pattern identification that adapts quickly with limited data, outperforming existing methods with high F1 scores.


<details>
  <summary>Details</summary>
Motivation: RFID-based cattle tagging is prone to loss, damage, tampering, and external attacks; biometric muzzle patterns offer a robust alternative but traditional deep learning needs abundant data and retraining; need data-efficient, adaptable methods for dynamic herds.

Method: Introduce Cooperative Model-Agnostic Meta-Learning (CCoMAML) paired with a Multi-Head Attention Feature Fusion (MHAFF) extractor to enable real-time, few-shot identification of cattle muzzle patterns without retraining; evaluate against state-of-the-art few-shot approaches.

Result: Achieves high identification performance with F1 scores of 98.46% and 97.91%, outperforming competing few-shot methods in cattle identification.

Conclusion: CCoMAML with MHAFF provides strong adaptability to new data and changing herd compositions, enabling robust, real-time cattle identification without extensive retraining, offering a viable alternative to RFID-based systems.

Abstract: Cattle identification is critical for efficient livestock farming management,
currently reliant on radio-frequency identification (RFID) ear tags. However,
RFID-based systems are prone to failure due to loss, damage, tampering, and
vulnerability to external attacks. As a robust alternative, biometric
identification using cattle muzzle patterns similar to human fingerprints has
emerged as a promising solution. Deep learning techniques have demonstrated
success in leveraging these unique patterns for accurate identification. But
deep learning models face significant challenges, including limited data
availability, disruptions during data collection, and dynamic herd compositions
that require frequent model retraining. To address these limitations, this
paper proposes a novel few-shot learning framework for real-time cattle
identification using Cooperative Model-Agnostic Meta-Learning (CCoMAML) with
Multi-Head Attention Feature Fusion (MHAFF) as a feature extractor model. This
model offers great model adaptability to new data through efficient learning
from few data samples without retraining. The proposed approach has been
rigorously evaluated against current state-of-the-art few-shot learning
techniques applied in cattle identification. Comprehensive experimental results
demonstrate that our proposed CCoMAML with MHAFF has superior cattle
identification performance with 98.46% and 97.91% F1 scores.

</details>


### [49] [ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification](https://arxiv.org/abs/2509.11220)
*Gao Yu Lee,Tanmoy Dam,Md Meftahul Ferdaus,Daniel Puiu Poenar,Vu N. Duong*

Main category: cs.CV

TL;DR: ANROT-HELANet introduces a Hellinger-distance based aggregation and a Hellinger Similarity contrastive loss to improve robustness in few-shot learning, achieving state-of-the-art robustness to adversarial and Gaussian noise and gains on miniImageNet (1-shot +1.20%, 5-shot +1.40%), plus better image reconstruction (FID 2.75 vs 3.43/3.38).


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of Bayesian KL-based FSL methods to adversarial and natural perturbations and to improve reconstruction quality while preserving few-shot performance.

Method: Proposes ANROT-HELANet with Hellinger distance-based feature aggregation, attention mechanisms, and a novel Hellinger Similarity contrastive loss that generalizes cosine similarity for variational few-shot inference; demonstrates robustness to adversarial perturbations up to ε=0.30 and Gaussian noise up to σ=0.30; reports improved reconstruction fidelity (FID 2.75).

Result: Achieves state-of-the-art performance under robust/few-shot settings across four benchmark datasets, with gains of 1.20% (1-shot) and 1.40% (5-shot) on miniImageNet; FID improvements over VAE (3.43) and WAE (3.38).

Conclusion: Introduces a novel robust FSL framework combining Hellinger-based aggregation and a Hellinger Similarity loss; demonstrates superior robustness to perturbations and improved reconstruction, with code available at the provided repository.

Abstract: Few-Shot Learning (FSL), which involves learning to generalize using only a
few data samples, has demonstrated promising and superior performances to
ordinary CNN methods. While Bayesian based estimation approaches using
Kullback-Leibler (KL) divergence have shown improvements, they remain
vulnerable to adversarial attacks and natural noises. We introduce
ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation
Network that significantly advances the state-of-the-art in FSL robustness and
performance. Our approach implements an adversarially and naturally robust
Hellinger distance-based feature class aggregation scheme, demonstrating
resilience to adversarial perturbations up to $\epsilon=0.30$ and Gaussian
noise up to $\sigma=0.30$. The network achieves substantial improvements across
benchmark datasets, including gains of 1.20\% and 1.40\% for 1-shot and 5-shot
scenarios on miniImageNet respectively. We introduce a novel Hellinger
Similarity contrastive loss function that generalizes cosine similarity
contrastive loss for variational few-shot inference scenarios. Our approach
also achieves superior image reconstruction quality with a FID score of 2.75,
outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive
experiments conducted on four few-shot benchmarked datasets verify that
ANROT-HELANet's combination of Hellinger distance-based feature aggregation,
attention mechanisms, and our novel loss function establishes new
state-of-the-art performance while maintaining robustness against both
adversarial and natural perturbations. Our code repository will be available at
https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main.

</details>


### [50] [MIS-LSTM: Multichannel Image-Sequence LSTM for Sleep Quality and Stress Prediction](https://arxiv.org/abs/2509.11232)
*Seongwan Park,Jieun Woo,Siheon Yang*

Main category: cs.CV

TL;DR: Hybrid MIS-LSTM fuses multi-channel lifelog images and discrete event encodings via CNNs and an attention-fused LSTM to predict daily sleep quality and stress; enhanced with an uncertainty-aware ensemble (UALRE); achieves Macro-F1 0.647 on the 2025 ETRI Lifelog Challenge, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: The paper targets robust day-level sleep quality and stress prediction from multimodal lifelog data, addressing long-range temporal dependencies and ensemble uncertainty.

Method: Partition sensor streams into N-hour blocks and render as multi-channel images; encode sparse discrete events with a dedicated 1D-CNN; fuse modalities with a Convolutional Block Attention Module (CBAM) to produce refined block embeddings; feed embeddings into an LSTM to capture long-range temporal dependencies; introduce UALRE, an uncertainty-aware ensemble that overrides low-confidence votes with high-confidence predictions.

Result: Base MIS-LSTM achieves Macro-F1 0.615; with UALRE ensemble, Macro-F1 rises to 0.647, outperforming strong LSTM, 1D-CNN, and CNN baselines. Ablations show (i) multi-channel imaging outperforms stacked-vertical imaging, (ii) 4-hour block granularity is beneficial, (iii) modality-specific discrete encoding is advantageous.

Conclusion: The architecture benefits from multi-channel imaging, suitable block granularity, and modality-specific encoding; the UALRE ensemble further improves robustness and overall performance on the Lifelog Challenge dataset.

Abstract: This paper presents MIS-LSTM, a hybrid framework that joins CNN encoders with
an LSTM sequence model for sleep quality and stress prediction at the day level
from multimodal lifelog data. Continuous sensor streams are first partitioned
into N-hour blocks and rendered as multi-channel images, while sparse discrete
events are encoded with a dedicated 1D-CNN. A Convolutional Block Attention
Module fuses the two modalities into refined block embeddings, which an LSTM
then aggregates to capture long-range temporal dependencies. To further boost
robustness, we introduce UALRE, an uncertainty-aware ensemble that overrides
lowconfidence majority votes with high-confidence individual predictions.
Experiments on the 2025 ETRI Lifelog Challenge dataset show that Our base
MISLSTM achieves Macro-F1 0.615; with the UALRE ensemble, the score improves to
0.647, outperforming strong LSTM, 1D-CNN, and CNN baselines. Ablations confirm
(i) the superiority of multi-channel over stacked-vertical imaging, (ii) the
benefit of a 4-hour block granularity, and (iii) the efficacy of
modality-specific discrete encoding.

</details>


### [51] [Contextualized Multimodal Lifelong Person Re-Identification in Hybrid Clothing States](https://arxiv.org/abs/2509.11247)
*Robert Long,Rongxin Jiang,Mingrui Yan*

Main category: cs.CV

TL;DR: A CLIP-based continual learning framework (CMLReID) for both same-cloth and clothing-changing ReID (LReID-Hybrid), with Context-Aware Semantic Prompts (CASP) and Adaptive Knowledge Fusion & Projection (AKFP); achieves SOTA performance and robust generalization under clothing variation in continual learning.


<details>
  <summary>Details</summary>
Motivation: ReID in realistic surveillance suffers from clothing changes and the need for continual learning; existing methods focus on single-cloth or CC as separate subproblems, leading to forgetting and misalignment across tasks.

Method: Proposes LReID-Hybrid and CMLReID: CASP generates adaptive prompts with context to align multi-grained visual cues to semantic text; AKFP uses a dual-path learner and Clothing-State-Aware Projection Loss to form robust SC/CC prototypes; CLIP-based framework.

Result: Empirical results on diverse datasets show CMLReID outperforms state-of-the-art methods, with strong robustness to clothing variations and effective sequential learning.

Conclusion: CMLReID provides a unified solution for SC and CC in continual learning for ReID, enhancing robustness and generalization in realistic surveillance settings.

Abstract: Person Re-Identification (ReID) has several challenges in real-world
surveillance systems due to clothing changes (CCReID) and the need for
maintaining continual learning (LReID). Previous existing methods either
develop models specifically for one application, which is mostly a same-cloth
(SC) setting or treat CCReID as its own separate sub-problem. In this work, we
will introduce the LReID-Hybrid task with the goal of developing a model to
achieve both SC and CC while learning in a continual setting. Mismatched
representations and forgetting from one task to the next are significant
issues, we address this with CMLReID, a CLIP-based framework composed of two
novel tasks: (1) Context-Aware Semantic Prompt (CASP) that generates adaptive
prompts, and also incorporates context to align richly multi-grained visual
cues with semantic text space; and (2) Adaptive Knowledge Fusion and Projection
(AKFP) which produces robust SC/CC prototypes through the use of a dual-path
learner that aligns features with our Clothing-State-Aware Projection Loss.
Experiments performed on a wide range of datasets and illustrate that CMLReID
outperforms all state-of-the-art methods with strong robustness and
generalization despite clothing variations and a sophisticated process of
sequential learning.

</details>


### [52] [Cross-Domain Attribute Alignment with CLIP: A Rehearsal-Free Approach for Class-Incremental Unsupervised Domain Adaptation](https://arxiv.org/abs/2509.11264)
*Kerun Mi,Guoliang Kang,Guangyu Li,Lin Zhao,Tao Zhou,Chen Gong*

Main category: cs.CV

TL;DR: A rehearsal-free CI-UDA method using CLIP-derived attributes and cross-domain attribute alignment to preserve knowledge and mitigate domain shift.


<details>
  <summary>Details</summary>
Motivation: To tackle catastrophic forgetting and unbounded memory growth in CI-UDA by exploiting domain-invariant, class-agnostic knowledge instead of memory-based rehearsal or per-step shared-class alignment.

Method: Extract class-agnostic attributes via CLIP and store them as key-value pairs (visual prototype -> textual prompt) in two domain-specific dictionaries. Perform cross-domain attribute alignment to reduce domain shift through visual-attention consistency and prediction-consistency losses, achieving rehearsal-free CI-UDA.

Result: Outperforms state-of-the-art on three CI-UDA benchmarks and effectively alleviates catastrophic forgetting without rehearsal; code available at the provided repository.

Conclusion: Attribute modeling and cross-domain attribute alignment offer an effective, rehearsal-free solution for CI-UDA by preserving domain-invariant knowledge while mitigating domain shift.

Abstract: Class-Incremental Unsupervised Domain Adaptation (CI-UDA) aims to adapt a
model from a labeled source domain to an unlabeled target domain, where the
sets of potential target classes appearing at different time steps are disjoint
and are subsets of the source classes. The key to solving this problem lies in
avoiding catastrophic forgetting of knowledge about previous target classes
during continuously mitigating the domain shift. Most previous works
cumbersomely combine two technical components. On one hand, they need to store
and utilize rehearsal target sample from previous time steps to avoid
catastrophic forgetting; on the other hand, they perform alignment only between
classes shared across domains at each time step. Consequently, the memory will
continuously increase and the asymmetric alignment may inevitably result in
knowledge forgetting. In this paper, we propose to mine and preserve
domain-invariant and class-agnostic knowledge to facilitate the CI-UDA task.
Specifically, via using CLIP, we extract the class-agnostic properties which we
name as "attribute". In our framework, we learn a "key-value" pair to represent
an attribute, where the key corresponds to the visual prototype and the value
is the textual prompt. We maintain two attribute dictionaries, each
corresponding to a different domain. Then we perform attribute alignment across
domains to mitigate the domain shift, via encouraging visual attention
consistency and prediction consistency. Through attribute modeling and
cross-domain alignment, we effectively reduce catastrophic knowledge forgetting
while mitigating the domain shift, in a rehearsal-free way. Experiments on
three CI-UDA benchmarks demonstrate that our method outperforms previous
state-of-the-art methods and effectively alleviates catastrophic forgetting.
Code is available at https://github.com/RyunMi/VisTA.

</details>


### [53] [Synthetic Dataset Evaluation Based on Generalized Cross Validation](https://arxiv.org/abs/2509.11273)
*Zhihang Song,Dingyi Yao,Ruibo Ming,Lihui Peng,Danya Yao,Yi Zhang*

Main category: cs.CV

TL;DR: A framework using generalized cross-validation and domain transfer learning to evaluate synthetic data quality; builds a cross-performance matrix by training task models on synthetic and real data, then derives two metrics (simulation quality and transfer quality); validated on Virtual KITTI.


<details>
  <summary>Details</summary>
Motivation: There is no universally accepted, principled framework to evaluate synthetic datasets. A scalable, comparable evaluation framework is needed to drive improvements in data generation and usage.

Method: Train task-specific models (e.g., YOLOv5s) on synthetic datasets and multiple real benchmarks (e.g., KITTI, BDD100K) to form a cross-performance matrix. Normalize this matrix to construct a Generalized Cross-Validation (GCV) matrix that quantifies domain transferability. Introduce two metrics: (1) simulation quality—similarity between synthetic and real data; (2) transfer quality—diversity/coverage of synthetic data across real-world scenarios.

Result: Experimental validation on Virtual KITTI demonstrates the framework and metrics effectively assess synthetic data fidelity, providing a scalable, quantifiable evaluation solution.

Conclusion: The work offers a principled framework to guide synthetic dataset optimization in AI research by enabling generalizable and comparable assessments of synthetic data quality.

Abstract: With the rapid advancement of synthetic dataset generation techniques,
evaluating the quality of synthetic data has become a critical research focus.
Robust evaluation not only drives innovations in data generation methods but
also guides researchers in optimizing the utilization of these synthetic
resources. However, current evaluation studies for synthetic datasets remain
limited, lacking a universally accepted standard framework. To address this,
this paper proposes a novel evaluation framework integrating generalized
cross-validation experiments and domain transfer learning principles, enabling
generalizable and comparable assessments of synthetic dataset quality. The
framework involves training task-specific models (e.g., YOLOv5s) on both
synthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K),
forming a cross-performance matrix. Following normalization, a Generalized
Cross-Validation (GCV) Matrix is constructed to quantify domain
transferability. The framework introduces two key metrics. One measures the
simulation quality by quantifying the similarity between synthetic data and
real-world datasets, while another evaluates the transfer quality by assessing
the diversity and coverage of synthetic data across various real-world
scenarios. Experimental validation on Virtual KITTI demonstrates the
effectiveness of our proposed framework and metrics in assessing synthetic data
fidelity. This scalable and quantifiable evaluation solution overcomes
traditional limitations, providing a principled approach to guide synthetic
dataset optimization in artificial intelligence research.

</details>


### [54] [ROSGS: Relightable Outdoor Scenes With Gaussian Splatting](https://arxiv.org/abs/2509.11275)
*Lianjun Liao,Chunhui Zhang,Tong Wu,Henglei Lv,Bailin Deng,Lin Gao*

Main category: cs.CV

TL;DR: ROSGS presents a two-stage outdoor relighting pipeline using Gaussian Splatting: first reconstructs geometry with 2D Gaussian Splatting aided by monocular normals, then decomposes texture and lighting with a hybrid model (spherical Gaussian for sun + SH-based radiance transfer for skylight) to achieve state-of-the-art relighting and rendering efficiency.


<details>
  <summary>Details</summary>
Motivation: Outdoor scenes exhibit unbounded geometry and highly dynamic, high-frequency lighting. Traditional NeRF/3DGS approaches suffer from heavy computational costs and low-frequency lighting representations, limiting relighting quality and speed.

Method: Stage 1: geometry reconstruction using compact 2D Gaussian Splatting with monocular normal priors to obtain an efficient geometric foundation. Stage 2: texture/lighting decomposition with a hybrid lighting model: directional high-frequency sunlight captured by a spherical Gaussian function, and remaining low-frequency skylight modeled via radiance transfer using spherical harmonic coefficients.

Result: Quantitative metrics and qualitative comparisons indicate state-of-the-art relighting performance for outdoor scenes, with superior relighting accuracy and rendering efficiency compared to prior approaches.

Conclusion: ROSGS provides an efficient, high-quality relighting pipeline for outdoor scenes by decoupling geometry and lighting using Gaussian Splatting, leveraging monocular normals and a hybrid lighting model to capture both high- and low-frequency illumination.

Abstract: Image data captured outdoors often exhibit unbounded scenes and
unconstrained, varying lighting conditions, making it challenging to decompose
them into geometry, reflectance, and illumination. Recent works have focused on
achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D
Gaussian Splatting (3DGS) representation but remain hindered by two key
limitations: the high computational overhead associated with neural networks of
NeRF and the use of low-frequency lighting representations, which often result
in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS,
a two-stage pipeline designed to efficiently reconstruct relightable outdoor
scenes using the Gaussian Splatting representation. By leveraging monocular
normal priors, ROSGS first reconstructs the scene's geometry with the compact
2D Gaussian Splatting (2DGS) representation, providing an efficient and
accurate geometric foundation. Building upon this reconstructed geometry, ROSGS
then decomposes the scene's texture and lighting through a hybrid lighting
model. This model effectively represents typical outdoor lighting by employing
a spherical Gaussian function to capture the directional, high-frequency
components of sunlight, while learning a radiance transfer function via
Spherical Harmonic coefficients to model the remaining low-frequency skylight
comprehensively. Both quantitative metrics and qualitative comparisons
demonstrate that ROSGS achieves state-of-the-art performance in relighting
outdoor scenes and highlight its ability to deliver superior relighting
accuracy and rendering efficiency.

</details>


### [55] [Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking](https://arxiv.org/abs/2509.11453)
*BaiChen Fan,Sifan Zhou,Jian Li,Shibo Zhao,Muqing Cao,Qin Wang*

Main category: cs.CV

TL;DR: TrajTrack proposes a trajectory-based paradigm for LiDAR 3D SOT, bridging two-frame efficiency and sequence-based robustness by learning motion continuity from historical trajectories to refine a fast motion proposal, achieving state-of-the-art on NuScenes at 56 FPS.


<details>
  <summary>Details</summary>
Motivation: Two-frame methods are efficient but lack long-term context; sequence-based methods are robust but costly; there is a need for a method that is both fast and robust in sparse/occluded scenes.

Method: A lightweight framework that augments a base two-frame tracker with an implicit motion modeling module trained on historical bbox trajectories. It first generates a fast explicit motion proposal based on current frames, then predicts future trajectory to refine and correct the proposal, using only bounding box history and without additional point clouds.

Result: On NuScenes, TrajTrack achieves state-of-the-art tracking precision, with a 4.48% improvement over a strong baseline, and runs at 56 FPS. It also generalizes well across different base trackers.

Conclusion: The trajectory-based paradigm successfully balances accuracy and efficiency, offering robustness in challenging scenes and broad generalizability without extra point cloud input.

Abstract: LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics
and autonomous systems. Existing methods typically follow frame-wise motion
estimation or a sequence-based paradigm. However, the two-frame methods are
efficient but lack long-term temporal context, making them vulnerable in sparse
or occluded scenes, while sequence-based methods that process multiple point
clouds gain robustness at a significant computational cost. To resolve this
dilemma, we propose a novel trajectory-based paradigm and its instantiation,
TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame
tracker by implicitly learning motion continuity from historical bounding box
trajectories alone-without requiring additional, costly point cloud inputs. It
first generates a fast, explicit motion proposal and then uses an implicit
motion modeling module to predict the future trajectory, which in turn refines
and corrects the initial proposal. Extensive experiments on the large-scale
NuScenes benchmark show that TrajTrack achieves new state-of-the-art
performance, dramatically improving tracking precision by 4.48% over a strong
baseline while running at 56 FPS. Besides, we also demonstrate the strong
generalizability of TrajTrack across different base trackers. Video is
available at https://www.bilibili.com/video/BV1ahYgzmEWP.

</details>


### [56] [Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations](https://arxiv.org/abs/2509.11287)
*Yifan Lu,Ziqi Zhang,Chunfeng Yuan,Jun Gao,Congxuan Zhang,Xiaojuan Qi,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: APASI: a self-injection, no-external-data method to mitigate LVLM hallucinations via self-generated dis-preferred responses and curriculum-aligned training; achieves competitive results across six benchmarks on three baselines.


<details>
  <summary>Details</summary>
Motivation: LVLMs hallucinate and current mitigation relies on expensive human or auxiliary data; need self-contained, scalable approach.

Method: Self-injected hallucination generation: the LVLM produces a pair of responses, one preferred and one dis-preferred; dis-preferred crafted from three observations of hallucinations; training uses iterative alignment with curriculum to update preference data.

Result: Outperforms or matches external-alignment baselines across six benchmarks on three base models; demonstrates generalization; code released.

Conclusion: APASI shows generalizable, dependency-free hallucination mitigation with stable, scalable improvements.

Abstract: Large Vision-Language Models (LVLMs) suffer from serious hallucination
problems, where the model-generated responses are inconsistent with the visual
inputs. Existing hallucination mitigation methods are mainly based on
preference alignment and require external human annotations or auxiliary models
for preference data collection, which increase costs and limit sustainable
improvement. To tackle these challenges, we propose Autonomous Preference
Alignment via Self-Injection (APASI), a novel and generalizable method that
mitigates hallucinations without external dependencies. APASI leverages the
target LVLM to self-inject hallucinations into a generated response, creating a
pair of responses with varying preference levels. During the self-injection
process, the dis-preferred response is generated based on three key
observations of hallucinations, ensuring it simulates real hallucination
patterns. This fidelity offers an accurate learning signal for hallucination
mitigation. Moreover, APASI incorporates an iterative alignment training
strategy combined with curriculum learning to periodically update the
preference data with increasing challenge, enabling stable and continuous
enhancement of the LVLM. Extensive experiments across six benchmarks show that
APASI not only effectively mitigates hallucinations for three baseline models
but also achieves comparable or even superior performance to alignment-based
methods with external dependency, thereby demonstrating its effectiveness and
generalization capability. The code is available at
https://github.com/davidluciolu/APASI.

</details>


### [57] [Leveraging Geometric Priors for Unaligned Scene Change Detection](https://arxiv.org/abs/2509.11292)
*Ziling Liu,Ziwei Chen,Mingqi Gao,Jinyu Yang,Feng Zheng*

Main category: cs.CV

TL;DR: A training-free, geometry-prior-guided unaligned SCD framework that leverages a Geometric Foundation Model with a visual foundation model to robustly detect scene changes under viewpoint misalignment, addressing 2D-only cues and occlusion, with strong results on PSCD, ChangeSim and PASLCD.


<details>
  <summary>Details</summary>
Motivation: Unaligned Scene Change Detection relies on 2D visual cues to establish cross-image correspondences. Large viewpoint changes can break appearance-based matching, and scarce 2D change masks from small SCD datasets limit learning of multi-view knowledge, hindering overlap identification and occlusion handling. The lack of explicit geometric reasoning is a key limitation.

Method: Proposes to leverage geometric priors from a Geometric Foundation Model and build a training-free framework that integrates these priors with visual foundation-model representations to enable reliable change detection under viewpoint misalignment. The approach aims to identify visual overlaps, establish robust correspondences, and perform explicit occlusion detection.

Result: Extensive evaluation on PSCD, ChangeSim, and PASLCD datasets shows superior and robust performance compared to baselines.

Conclusion: First to exploit geometric priors for unaligned SCD; a training-free framework that combines geometric priors with visual foundation-model features to enhance cross-view change detection and occlusion handling; code will be released.

Abstract: Unaligned Scene Change Detection aims to detect scene changes between image
pairs captured at different times without assuming viewpoint alignment. To
handle viewpoint variations, current methods rely solely on 2D visual cues to
establish cross-image correspondence to assist change detection. However, large
viewpoint changes can alter visual observations, causing appearance-based
matching to drift or fail. Additionally, supervision limited to 2D change masks
from small-scale SCD datasets restricts the learning of generalizable
multi-view knowledge, making it difficult to reliably identify visual overlaps
and handle occlusions. This lack of explicit geometric reasoning represents a
critical yet overlooked limitation. In this work, we are the first to leverage
geometric priors from a Geometric Foundation Model to address the core
challenges of unaligned SCD, including reliable identification of visual
overlaps, robust correspondence establishment, and explicit occlusion
detection. Building on these priors, we propose a training-free framework that
integrates them with the powerful representations of a visual foundation model
to enable reliable change detection under viewpoint misalignment. Through
extensive evaluation on the PSCD, ChangeSim, and PASLCD datasets, we
demonstrate that our approach achieves superior and robust performance. Our
code will be released at https://github.com/ZilingLiu/GeoSCD.

</details>


### [58] [Learning to Generate 4D LiDAR Sequences](https://arxiv.org/abs/2509.11959)
*Ao Liang,Youquan Liu,Yu Yang,Dongyue Lu,Linfeng Li,Lingdong Kong,Huaici Zhao,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: Language-guided 4D LiDAR generation with editable outputs via scene-graph + diffusion pipelines, plus EvalSuite for fair evaluation.


<details>
  <summary>Details</summary>
Motivation: LiDAR data generation is underexplored despite its importance; 4D LiDAR synthesis demands controllability, temporal stability, and robust evaluation for simulation and augmentation.

Method: Parse instructions into ego-centric scene graphs; a tri-branch diffusion model yields object layouts, trajectories, and shapes; a range-image diffusion model creates the initial scan; an autoregressive module extends to temporally coherent sequences; an explicit layout enables object-level editing (insert/relocate); EvalSuite benchmarks scene-, object-, and sequence-level metrics.

Result: On nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and temporal consistency; introduces EvalSuite for fair assessment.

Conclusion: Proposes a unified framework for language-guided LiDAR generation with editable 4D outputs and a standardized evaluation suite, enabling LiDAR-based simulation and data augmentation.

Abstract: While generative world models have advanced video and occupancy-based data
synthesis, LiDAR generation remains underexplored despite its importance for
accurate 3D perception. Extending generation to 4D LiDAR data introduces
challenges in controllability, temporal stability, and evaluation. We present
LiDARCrafter, a unified framework that converts free-form language into
editable LiDAR sequences. Instructions are parsed into ego-centric scene
graphs, which a tri-branch diffusion model transforms into object layouts,
trajectories, and shapes. A range-image diffusion model generates the initial
scan, and an autoregressive module extends it into a temporally coherent
sequence. The explicit layout design further supports object-level editing,
such as insertion or relocation. To enable fair assessment, we provide
EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On
nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and
temporal consistency, offering a foundation for LiDAR-based simulation and data
augmentation.

</details>


### [59] [UnLoc: Leveraging Depth Uncertainties for Floorplan Localization](https://arxiv.org/abs/2509.11301)
*Matthias Wüest,Francis Engelmann,Ondrej Miksik,Marc Pollefeys,Daniel Barath*

Main category: cs.CV

TL;DR: UnLoc provides uncertainty-aware sequential camera localization in floorplans by modeling depth as probabilistic distributions and using off-the-shelf monocular depth models, eliminating per-environment depth networks and improving robustness and accuracy on long and short sequences.


<details>
  <summary>Details</summary>
Motivation: Address the lack of depth uncertainty modeling and the need for per-environment depth networks, aiming for better generalization to unseen spaces through probabilistic depth predictions and off-the-shelf depth models.

Method: Introduce a probabilistic model that treats depth predictions as explicit probability distributions to capture uncertainty, and rely on pre-trained monocular depth models rather than training environment-specific depth networks.

Result: On large-scale synthetic and real datasets, UnLoc achieves significant gains over prior methods, including 2.7x higher localization recall on 100-frame sequences and 16.7x on 15-frame sequences on the LaMAR HGE dataset.

Conclusion: UnLoc is efficient, generalizes to unseen spaces, reduces dependence on per-environment depth networks, and delivers substantial improvements in sequential camera localization within floorplans.

Abstract: We propose UnLoc, an efficient data-driven solution for sequential camera
localization within floorplans. Floorplan data is readily available, long-term
persistent, and robust to changes in visual appearance. We address key
limitations of recent methods, such as the lack of uncertainty modeling in
depth predictions and the necessity for custom depth networks trained for each
environment. We introduce a novel probabilistic model that incorporates
uncertainty estimation, modeling depth predictions as explicit probability
distributions. By leveraging off-the-shelf pre-trained monocular depth models,
we eliminate the need to rely on per-environment-trained depth networks,
enhancing generalization to unseen spaces. We evaluate UnLoc on large-scale
synthetic and real-world datasets, demonstrating significant improvements over
existing methods in terms of accuracy and robustness. Notably, we achieve $2.7$
times higher localization recall on long sequences (100 frames) and $16.7$
times higher on short ones (15 frames) than the state of the art on the
challenging LaMAR HGE dataset.

</details>


### [60] [Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding](https://arxiv.org/abs/2509.11323)
*Jian Song,Wei Mei,Yunfeng Xu,Qiang Fu,Renke Kou,Lina Bu,Yucheng Long*

Main category: cs.CV

TL;DR: A learning-aided Kalman-like filter, SIKNet, improves MOT motion estimation by encoding state vectors with a Semantic-Independent Encoder, outperforming traditional KF and other learning-aided filters; validated on a semi-simulated dataset; code available.


<details>
  <summary>Details</summary>
Motivation: KF with linear constant-velocity models struggles when parameters are mismatched or objects exhibit non-stationary motion. A learning-aided approach can adapt to such dynamics to improve robustness and accuracy in MOT motion estimation.

Method: Introduce Semantic-Independent KalmanNet (SIKNet) that encodes the state vector using a Semantic-Independent Encoder (SIE). SIE first applies a 1D convolution with kernel size 1 across homogeneous-semantic elements of different state vectors to extract independent semantic information, then uses a fully-connected layer and nonlinear activation to capture nonlinear and cross-dependency information among heterogeneous elements. Trainable to estimate motion dynamics; evaluate on a semi-simulated dataset constructed from open MOT datasets.

Result: SIKNet outperforms the traditional Kalman filter and achieves superior robustness and accuracy compared to existing learning-aided filters.

Conclusion: SIKNet provides an effective, learning-aided alternative to KF for MOT motion estimation, with strong robustness and accuracy improvements; code is publicly available.

Abstract: Motion estimation is a crucial component in multi-object tracking (MOT).
  It predicts the trajectory of objects by analyzing the changes in their
positions in consecutive frames of images, reducing tracking failures and
identity switches.
  The Kalman filter (KF) based on the linear constant-velocity model is one of
the most commonly used methods in MOT.
  However, it may yield unsatisfactory results when KF's parameters are
mismatched and objects move in non-stationary.
  In this work, we utilize the learning-aided filter to handle the motion
estimation of MOT.
  In particular, we propose a novel method named Semantic-Independent KalmanNet
(SIKNet), which encodes the state vector (the input feature) using a
Semantic-Independent Encoder (SIE) by two steps.
  First, the SIE uses a 1D convolution with a kernel size of 1, which convolves
along the dimension of homogeneous-semantic elements across different state
vectors to encode independent semantic information.
  Then it employs a fully-connected layer and a nonlinear activation layer to
encode nonlinear and cross-dependency information between
heterogeneous-semantic elements.
  To independently evaluate the performance of the motion estimation module in
MOT, we constructed a large-scale semi-simulated dataset from several
open-source MOT datasets.
  Experimental results demonstrate that the proposed SIKNet outperforms the
traditional KF and achieves superior robustness and accuracy than existing
learning-aided filters.
  The code is available at (https://github.com/SongJgit/filternet and
https://github.com/SongJgit/TBDTracker).

</details>


### [61] [Toward Next-generation Medical Vision Backbones: Modeling Finer-grained Long-range Visual Dependency](https://arxiv.org/abs/2509.11328)
*Mingyuan Meng*

Main category: cs.CV

TL;DR: MLP-based visual models effectively capture fine-grained long-range dependencies in high-resolution medical images, outperforming transformers and CNNs and paving the way for next-generation MIC backbones.


<details>
  <summary>Details</summary>
Motivation: Medical imaging requires both global context and local细细 details; transformers offer long-range modeling but are computationally heavy at high resolution; CNNs are local; MLPs offer computation/memory efficiency for long-range modeling, motivating investigation in MIC.

Method: The work first applies transformers for pixel- and image-wise medical vision tasks, then pioneers MLP-based visual models to capture fine-grained long-range dependencies in medical images, with extensive experiments comparing against transformers and CNNs across multiple MIC tasks.

Result: Extensive experiments confirm that modeling long-range dependencies is critical; MLP-based models feasibly capture finer-grained long-range dependencies at high resolution and consistently improve performance across diverse medical vision tasks compared with CNN/transformer baselines.

Conclusion: MLP-based visual models emerge as a superior paradigm for modeling fine-grained long-range dependencies in medical imaging, offering a promising direction for next-generation MIC backbones.

Abstract: Medical Image Computing (MIC) is a broad research topic covering both
pixel-wise (e.g., segmentation, registration) and image-wise (e.g.,
classification, regression) vision tasks. Effective analysis demands models
that capture both global long-range context and local subtle visual
characteristics, necessitating fine-grained long-range visual dependency
modeling. Compared to Convolutional Neural Networks (CNNs) that are limited by
intrinsic locality, transformers excel at long-range modeling; however, due to
the high computational loads of self-attention, transformers typically cannot
process high-resolution features (e.g., full-scale image features before
downsampling or patch embedding) and thus face difficulties in modeling
fine-grained dependency among subtle medical image details. Concurrently,
Multi-layer Perceptron (MLP)-based visual models are recognized as
computation/memory-efficient alternatives in modeling long-range visual
dependency but have yet to be widely investigated in the MIC community. This
doctoral research advances deep learning-based MIC by investigating effective
long-range visual dependency modeling. It first presents innovative use of
transformers for both pixel- and image-wise medical vision tasks. The focus
then shifts to MLPs, pioneeringly developing MLP-based visual models to capture
fine-grained long-range visual dependency in medical images. Extensive
experiments confirm the critical role of long-range dependency modeling in MIC
and reveal a key finding: MLPs provide feasibility in modeling finer-grained
long-range dependency among higher-resolution medical features containing
enriched anatomical/pathological details. This finding establishes MLPs as a
superior paradigm over transformers/CNNs, consistently enhancing performance
across various medical vision tasks and paving the way for next-generation
medical vision backbones.

</details>


### [62] [Dual Band Video Thermography Near Ambient Conditions](https://arxiv.org/abs/2509.11334)
*Sriram Narayanan,Mani Ramanagopal,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: Dual-band thermography method that separates reflected and emitted infrared components in videos from two cameras with different spectral sensitivities, enabling estimation of emissivity and dynamic temperature while isolating background.


<details>
  <summary>Details</summary>
Motivation: Near-ambient conditions cause both reflected and emitted IR components to be significant and time-varying; existing methods assume dominance or constant background; need to infer emissivity, temperature, reflectance, shape for CV applications.

Method: Derives a dual-band image formation model for two spectral bands; develops algorithms to estimate surface emissivity and its time-varying temperature; isolates a dynamic background; uses carefully calibrated emissivities across materials; evaluates quantitatively and demonstrates qualitative results on everyday scenes.

Result: Quantitative evaluation with calibrated emissivities for multiple materials; qualitative results on complex scenes such as glass with hot liquid and people with moving background.

Conclusion: First method to separate reflected and emitted components in videos from dual-band thermal cameras; enables reliable estimation of emissivity and temperature in near-ambient conditions and enhancement of scene understanding for computer vision tasks.

Abstract: Long-wave infrared radiation captured by a thermal camera consists of two
components: (a) light from the environment reflected or transmitted by a
surface, and (b) light emitted by the surface after undergoing heat transport
through the object and exchanging heat with the surrounding environment.
Separating these components is essential for understanding object properties
such as emissivity, temperature, reflectance and shape. Previous thermography
studies often assume that only one component is dominant (e.g., in welding) or
that the second component is constant and can be subtracted. However, in
near-ambient conditions, which are most relevant to computer vision
applications, both components are typically comparable in magnitude and vary
over time. We introduce the first method that separates reflected and emitted
components of light in videos captured by two thermal cameras with different
spectral sensitivities. We derive a dual-band thermal image formation model and
develop algorithms to estimate the surface's emissivity and its time-varying
temperature while isolating a dynamic background. We quantitatively evaluate
our approach using carefully calibrated emissivities for a range of materials
and show qualitative results on complex everyday scenes, such as a glass filled
with hot liquid and people moving in the background.

</details>


### [63] [Beyond Instance Consistency: Investigating View Diversity in Self-supervised Learning](https://arxiv.org/abs/2509.11344)
*Huaiyuan Qin,Muli Yang,Siyuan Hu,Peng Hu,Yu Zhang,Chen Gong,Hongyuan Zhu*

Main category: cs.CV

TL;DR: SSL can work without strict instance consistency; increasing view diversity helps performance up to an optimal range; moderate mutual information between views (measured by EMD) correlates with better learning; excessive diversity degrades results.


<details>
  <summary>Details</summary>
Motivation: Instance-consistency assumptions in SSL fail for non-iconic data; it is important to understand robustness of SSL to violations and how view diversity impacts learning; using EMD to quantify mutual information between views can guide SSL design.

Method: Extensive ablation studies on view configurations (zero overlap, smaller crop scales) and their impact on classification and dense prediction; use Earth Mover's Distance (EMD) as an estimator of mutual information between views; validate robustness across diverse data sources.

Result: SSL can learn meaningful representations even when positive pairs lack strict instance consistency; moderate view diversity improves downstream performance; excessive diversity reduces effectiveness; moderate EMD values correlate with better SSL learning.

Conclusion: Provide design guidelines for SSL frameworks regarding view diversity; advocate EMD as a practical proxy for mutual information between views; demonstrate robustness and applicability across diverse datasets.

Abstract: Self-supervised learning (SSL) conventionally relies on the instance
consistency paradigm, assuming that different views of the same image can be
treated as positive pairs. However, this assumption breaks down for non-iconic
data, where different views may contain distinct objects or semantic
information. In this paper, we investigate the effectiveness of SSL when
instance consistency is not guaranteed. Through extensive ablation studies, we
demonstrate that SSL can still learn meaningful representations even when
positive pairs lack strict instance consistency. Furthermore, our analysis
further reveals that increasing view diversity, by enforcing zero overlapping
or using smaller crop scales, can enhance downstream performance on
classification and dense prediction tasks. However, excessive diversity is
found to reduce effectiveness, suggesting an optimal range for view diversity.
To quantify this, we adopt the Earth Mover's Distance (EMD) as an estimator to
measure mutual information between views, finding that moderate EMD values
correlate with improved SSL learning, providing insights for future SSL
framework design. We validate our findings across a range of settings,
highlighting their robustness and applicability on diverse data sources.

</details>


### [64] [Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness](https://arxiv.org/abs/2509.11355)
*Robin Narsingh Ranabhat,Longwei Wang,Amit Kumar Patel,KC santosh*

Main category: cs.CV

TL;DR: Two regularization methods—frequency-consistency loss and supervised contrastive learning—help CNNs become more shape-biased and robust to corruptions without sacrificing clean accuracy, demonstrated on CIFAR-10-C.


<details>
  <summary>Details</summary>
Motivation: CNNs tend to rely on local textures rather than object shapes, making them fragile to common corruptions; humans leverage shapes more robustly. Shifting representations toward shape cues could improve robustness while preserving performance.

Method: 1) Introduce an auxiliary loss that enforces feature consistency between the original inputs and their low-frequency filtered versions, reducing dependence on high-frequency textures. 2) Use supervised contrastive learning to structure the feature space so class-consistent, shape-relevant representations cluster together.

Result: Both methods improve corruption robustness on CIFAR-10-C without degrading clean accuracy.

Conclusion: Loss-level regularization can steer CNNs toward shape-aware, more resilient representations; combining frequency-based consistency with contrastive objectives appears promising for improving robustness without harming clean accuracy.

Abstract: Convolutional Neural Networks (CNNs) excel at image classification but remain
vulnerable to common corruptions that humans handle with ease. A key reason for
this fragility is their reliance on local texture cues rather than global
object shapes -- a stark contrast to human perception. To address this, we
propose two complementary regularization strategies designed to encourage
shape-biased representations and enhance robustness. The first introduces an
auxiliary loss that enforces feature consistency between original and
low-frequency filtered inputs, discouraging dependence on high-frequency
textures. The second incorporates supervised contrastive learning to structure
the feature space around class-consistent, shape-relevant representations.
Evaluated on the CIFAR-10-C benchmark, both methods improve corruption
robustness without degrading clean accuracy. Our results suggest that
loss-level regularization can effectively steer CNNs toward more shape-aware,
resilient representations.

</details>


### [65] [GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration](https://arxiv.org/abs/2509.11360)
*Wan Xu,Feng Zhu,Yihan Zeng,Yuanfan Guo,Ming Liu,Hang Xu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: A global-local aligned video captioning framework (GLaVE-Cap) with TrackFusion and CaptionBridge, plus new benchmarks/dataset (GLaVE-Bench, GLaVE-1.2M); achieves state-of-the-art results and will be open-sourced.


<details>
  <summary>Details</summary>
Motivation: Current local-to-global captioning struggles with detail and contextual coherence due to lack of fine-grained mechanisms and weak local-global interaction.

Method: Two core modules: TrackFusion (local caption generation using vision experts to obtain cross-frame visual prompts with a dual-stream design) and CaptionBridge (local-global interaction guiding local captioning with global context and adaptive summarization). Also datasets: GLaVE-Bench (expanded, more queries) and GLaVE-1.2M (16k high-quality fine-grained captions and 1.2M QA pairs).

Result: Extensive experiments on four benchmarks show state-of-the-art performance; ablations validate effectiveness; analysis of student models; contribution of the large GLaVE-1.2M dataset; plan to open-source code, weights, benchmark, and dataset.

Conclusion: The framework improves detail and coherence in video captions; provides robust evaluation benchmarks and training data to advance video understanding; sources will be released to the community.

Abstract: Video detailed captioning aims to generate comprehensive video descriptions
to facilitate video understanding. Recently, most efforts in the video detailed
captioning community have been made towards a local-to-global paradigm, which
first generates local captions from video clips and then summarizes them into a
global caption. However, we find this paradigm leads to less detailed and
contextual-inconsistent captions, which can be attributed to (1) no mechanism
to ensure fine-grained captions, and (2) weak interaction between local and
global captions. To remedy the above two issues, we propose GLaVE-Cap, a
Global-Local aligned framework with Vision Expert integration for Captioning,
which consists of two core modules: TrackFusion enables comprehensive local
caption generation, by leveraging vision experts to acquire cross-frame visual
prompts, coupled with a dual-stream structure; while CaptionBridge establishes
a local-global interaction, by using global context to guide local captioning,
and adaptively summarizing local captions into a coherent global caption.
Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmark
featuring 5X more queries per video than existing benchmarks, covering diverse
visual dimensions to facilitate reliable evaluation. We further provide a
training dataset GLaVE-1.2M containing 16K high-quality fine-grained video
captions and 1.2M related question-answer pairs. Extensive experiments on four
benchmarks show that our GLaVE-Cap achieves state-of-the-art performance.
Besides, the ablation studies and student model analyses further validate the
effectiveness of the proposed modules and the contribution of GLaVE-1.2M to the
video understanding community. The source code, model weights, benchmark, and
dataset will be open-sourced.

</details>


### [66] [In-Vivo Skin 3-D Surface Reconstruction and Wrinkle Depth Estimation using Handheld High Resolution Tactile Sensing](https://arxiv.org/abs/2509.11385)
*Akhil Padmanabha,Arpit Agarwal,Catherine Li,Austin Williams,Dinesh K. Patel,Sankalp Chopkar,Achu Wilson,Ahmet Ozkan,Wenzhen Yuan,Sonal Choudhary,Arash Mostaghimi,Zackory Erickson,Carmel Majidi*

Main category: cs.CV

TL;DR: A portable 3D skin reconstruction device using GelSight tactile imaging with a custom gel and a learning-based reconstruction algorithm achieves ~12.55 μm mean absolute error in wrinkle height, validated across body regions in 15 participants, and detects significant wrinkle reductions after moisturizer application, enabling clinical/cosmetic skin analysis.


<details>
  <summary>Details</summary>
Motivation: There is a need for portable, high-resolution, depth-reconstruction tools that work across various body locations and are validated for dermatological use. Existing devices lack portability, depth accuracy, or broad regional validation.

Method: A handheld GelSight-based probe with a custom elastic gel and a learning-based reconstruction algorithm for micron-level wrinkle height estimation; force sensing ensures consistent contact. Validation includes wrinkle-like test objects (MAE 12.55 μm) and a study with 15 participants without skin disorders to establish wrinkle-depth metrics across multiple body regions; a moisturizer application trial assesses cosmetic efficacy.

Result: Mean absolute error of 12.55 μm on wrinkle-like test objects. Established validated wrinkle depth metrics across multiple body regions in humans. Demonstrated statistically significant reductions in wrinkle height at three locations after moisturizer application.

Conclusion: The work delivers a validated, portable device for clinical and cosmetic skin analysis with potential to aid diagnosis, treatment monitoring, and skincare efficacy evaluation; supports broader adoption of objective, region-wide wrinkle depth assessment.

Abstract: Three-dimensional (3-D) skin surface reconstruction offers promise for
objective and quantitative dermatological assessment, but no portable,
high-resolution device exists that has been validated and used for depth
reconstruction across various body locations. We present a compact 3-D skin
reconstruction probe based on GelSight tactile imaging with a custom elastic
gel and a learning-based reconstruction algorithm for micron-level wrinkle
height estimation. Our probe, integrated into a handheld probe with force
sensing for consistent contact, achieves a mean absolute error of 12.55 micron
on wrinkle-like test objects. In a study with 15 participants without skin
disorders, we provide the first validated wrinkle depth metrics across multiple
body regions. We further demonstrate statistically significant reductions in
wrinkle height at three locations following over-the-counter moisturizer
application. Our work offers a validated tool for clinical and cosmetic skin
analysis, with potential applications in diagnosis, treatment monitoring, and
skincare efficacy evaluation.

</details>


### [67] [MixANT: Observation-dependent Memory Propagation for Stochastic Dense Action Anticipation](https://arxiv.org/abs/2509.11394)
*Syed Talal Wasim,Hamid Suleman,Olga Zatsarynna,Muzammal Naseer,Juergen Gall*

Main category: cs.CV

TL;DR: MixANT introduces a mixture-of-experts approach to dynamically select A matrices in state-space models for long-term stochastic anticipation of human activities, achieving state-of-the-art results on 50Salads, Breakfast, and Assembly101.


<details>
  <summary>Details</summary>
Motivation: Static forget-gate (A matrix) in recent SSMs limits temporal memory and predictive power for diverse real-world human activities; an input-adaptive memory mechanism is needed.

Method: Extend SSMs with a mixture of experts that choose context-relevant A matrices based on input features, enabling dynamic memory control without sacrificing efficiency; aims to improve long-term dense activity anticipation.

Result: Empirical evaluation on 50Salads, Breakfast, and Assembly101 shows MixANT consistently outperforming state-of-the-art methods across all evaluation settings.

Conclusion: Input-dependent forget-gate mechanisms are crucial for reliable human-behavior prediction; MixANT demonstrates that dynamic A matrices yield better representations and performance in real-world scenarios.

Abstract: We present MixANT, a novel architecture for stochastic long-term dense
anticipation of human activities. While recent State Space Models (SSMs) like
Mamba have shown promise through input-dependent selectivity on three key
parameters, the critical forget-gate ($\textbf{A}$ matrix) controlling temporal
memory remains static. We address this limitation by introducing a mixture of
experts approach that dynamically selects contextually relevant $\textbf{A}$
matrices based on input features, enhancing representational capacity without
sacrificing computational efficiency. Extensive experiments on the 50Salads,
Breakfast, and Assembly101 datasets demonstrate that MixANT consistently
outperforms state-of-the-art methods across all evaluation settings. Our
results highlight the importance of input-dependent forget-gate mechanisms for
reliable prediction of human behavior in diverse real-world scenarios.

</details>


### [68] [No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data](https://arxiv.org/abs/2509.11406)
*Christoph Fürböck,Paul Weiser,Branko Mitic,Philipp Seeböck,Thomas Helbich,Georg Langs*

Main category: cs.CV

TL;DR: Hypernetwork dynamically generates modality-conditioned task classifiers to handle partially missing multi-modal medical data, enabling robust training and inference across all modality configurations.


<details>
  <summary>Details</summary>
Motivation: In real-world clinical settings, multi-modal imaging data often arrives incomplete. Standard remedies (discarding samples, imputation, dropout) hurt robustness and generalizability. The paper proposes a hypernetwork approach to adapt the classifier to available modalities without discarding data.

Method: A hypernetwork predicts the parameters of a task-specific model conditioned on the set of available modalities. Trains on all samples to handle incomplete data. Comparisons against models trained only on complete data, channel dropout, and imputation on artificially incomplete datasets.

Result: The method achieves superior adaptability, with up to 8% absolute accuracy gain over baselines when training on data with only 25% completeness (i.e., 75% missing modalities).

Conclusion: A single modality-agnostic model can generalize across modality configurations, offering an efficient, robust solution for real-world multi-modal medical data analysis.

Abstract: In real world clinical environments, training and applying deep learning
models on multi-modal medical imaging data often struggles with partially
incomplete data. Standard approaches either discard missing samples, require
imputation or repurpose dropout learning schemes, limiting robustness and
generalizability. To address this, we propose a hypernetwork-based method that
dynamically generates task-specific classification models conditioned on the
set of available modalities. Instead of training a fixed model, a hypernetwork
learns to predict the parameters of a task model adapted to available
modalities, enabling training and inference on all samples, regardless of
completeness. We compare this approach with (1) models trained only on complete
data, (2) state of the art channel dropout methods, and (3) an imputation-based
method, using artificially incomplete datasets to systematically analyze
robustness to missing modalities. Results demonstrate superior adaptability of
our method, outperforming state of the art approaches with an absolute increase
in accuracy of up to 8% when trained on a dataset with 25% completeness (75% of
training data with missing modalities). By enabling a single model to
generalize across all modality configurations, our approach provides an
efficient solution for real-world multi-modal medical data analysis.

</details>


### [69] [On the Skinning of Gaussian Avatars](https://arxiv.org/abs/2509.11411)
*Nikolaos Zioulis,Nikolaos Kotarelas,Georgios Albanis,Spyridon Thermos,Anargyros Chatzitofis*

Main category: cs.CV

TL;DR: Quaternion-averaged weighted rotation blending for Gaussian-based radiance-field avatars enables fast, artifact-free animation using forward skinning and any Gaussian rasterizer.


<details>
  <summary>Details</summary>
Motivation: Neural radiance fields enable avatar reconstruction but suffer from slow rendering and backward mapping; Gaussian splatting accelerates training and rendering and simplifies implementation via forward skinning, yet linear skinning misrepresents non-linear Gaussian rotations, leading to artifacts. A simple, robust solution is needed to integrate into engines and scale to real-time use.

Method: Introduce a weighted rotation blending approach that uses quaternion averaging to handle Gaussian rotations within vertex-based Gaussians. This replaces or augments linear blend skinning and requires only modifying the skinning step, compatible with any Gaussian rasterizer.

Result: The approach yields simpler, efficiently animated vertex-based Gaussians and enables fast training/rendering with straightforward engine integration, while mitigating rotation artifacts without resorting to mesh-based rotation schemes or corrective offset predictions.

Conclusion: Weighted quaternion-based rotation blending provides a practical, easy-to-integrate improvement for Gaussian-based radiance-field avatars, supporting real-time animation workflows and broad adoption in graphics pipelines.

Abstract: Radiance field-based methods have recently been used to reconstruct human
avatars, showing that we can significantly downscale the systems needed for
creating animated human avatars. Although this progress has been initiated by
neural radiance fields, their slow rendering and backward mapping from the
observation space to the canonical space have been the main challenges. With
Gaussian splatting overcoming both challenges, a new family of approaches has
emerged that are faster to train and render, while also straightforward to
implement using forward skinning from the canonical to the observation space.
However, the linear blend skinning required for the deformation of the
Gaussians does not provide valid results for their non-linear rotation
properties. To address such artifacts, recent works use mesh properties to
rotate the non-linear Gaussian properties or train models to predict corrective
offsets. Instead, we propose a weighted rotation blending approach that
leverages quaternion averaging. This leads to simpler vertex-based Gaussians
that can be efficiently animated and integrated in any engine by only modifying
the linear blend skinning technique, and using any Gaussian rasterizer.

</details>


### [70] [Disentanglement of Biological and Technical Factors via Latent Space Rotation in Clinical Imaging Improves Disease Pattern Discovery](https://arxiv.org/abs/2509.11436)
*Jeanny Pan,Philipp Seeböck,Christoph Fürböck,Svitlana Pochepnia,Jennifer Straub,Lucian Beer,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: A label-free, post-hoc latent-space rotation method disentangles biological from technical factors in medical imaging, yielding stable tissue-type clusters across acquisition settings, outperforming harmonization methods and improving survival prediction.


<details>
  <summary>Details</summary>
Motivation: Domain shifts from imaging hardware and reconstruction parameters hinder learning of biologically meaningful patterns and hinder multi-center biomarker discovery; robust, acquisition-invariant representations are needed without requiring labels.

Method: Post-hoc rotation of latent representations to actively learn and remove acquisition-related factors, disentangling biological from technical factors. The approach is evaluated on real-world heterogeneous clinical data, comparing against four state-of-the-art harmonization methods, using clustering stability metrics (ARI, NMI, Dice) and downstream Cox survival analysis. Code is released on GitHub.

Result: Cluster consistency improves significantly over the entangled representation: ARI +19.01%, NMI +16.85%, Dice +12.39%. The method outperforms four state-of-the-art harmonization methods. When clusters quantify tissue composition in idiopathic pulmonary fibrosis, the learned profiles enhance Cox survival prediction.

Conclusion: A label-free framework enabling disentangled representations via latent-space rotation yields stable tissue-type clusters across different acquisition settings and improves downstream prognostic modeling, supporting biomarker discovery in multi-center imaging studies; code available on GitHub.

Abstract: Identifying new disease-related patterns in medical imaging data with the
help of machine learning enlarges the vocabulary of recognizable findings. This
supports diagnostic and prognostic assessment. However, image appearance varies
not only due to biological differences, but also due to imaging technology
linked to vendors, scanning- or re- construction parameters. The resulting
domain shifts impedes data representation learning strategies and the discovery
of biologically meaningful cluster appearances. To address these challenges, we
introduce an approach to actively learn the domain shift via post-hoc rotation
of the data latent space, enabling disentanglement of biological and technical
factors. Results on real-world heterogeneous clinical data showcase that the
learned disentangled representation leads to stable clusters representing
tissue-types across different acquisition settings. Cluster consistency is
improved by +19.01% (ARI), +16.85% (NMI), and +12.39% (Dice) compared to the
entangled representation, outperforming four state-of-the-art harmonization
methods. When using the clusters to quantify tissue composition on idiopathic
pulmonary fibrosis patients, the learned profiles enhance Cox survival
prediction. This indicates that the proposed label-free framework facilitates
biomarker discovery in multi-center routine imaging data. Code is available on
GitHub https://github.com/cirmuw/latent-space-rotation-disentanglement.

</details>


### [71] [MultiMAE for Brain MRIs: Robustness to Missing Inputs Using Multi-Modal Masked Autoencoder](https://arxiv.org/abs/2509.11442)
*Ayhan Can Erdur,Christian Beischl,Daniel Scholz,Jiazhen Pan,Benedikt Wiestler,Daniel Rueckert,Jan C Peeken*

Main category: cs.CV

TL;DR: Masked autoencoder for multi-modal 3D brain MRI that handles missing sequences via late-fusion transformer encoding and per-modality decoders, achieving robust representations and improved downstream segmentation/classification when inputs are incomplete.


<details>
  <summary>Details</summary>
Motivation: Missing input sequences in medical imaging impede deep learning; a robust pretraining strategy that can infer missing modalities is needed, inspired by MultiMAE.

Method: Treat each MRI sequence as a separate modality; use a late-fusion transformer encoder to integrate modalities; have separate decoders per modality for multi-task reconstruction; pretrain to enable cross-sequence reasoning and missing-input inference.

Result: Outperforms MAE-ViT baseline for downstream segmentation and classification with missing inputs; absolute improvements: 10.1 in Dice score and 0.46 in MCC; demonstrates robustness and generalizability.

Conclusion: Provides a flexible, generalizable brain MRI encoder capable of inferring missing sequences and adapting to various downstream tasks; code is released.

Abstract: Missing input sequences are common in medical imaging data, posing a
challenge for deep learning models reliant on complete input data. In this
work, inspired by MultiMAE [2], we develop a masked autoencoder (MAE) paradigm
for multi-modal, multi-task learning in 3D medical imaging with brain MRIs. Our
method treats each MRI sequence as a separate input modality, leveraging a
late-fusion-style transformer encoder to integrate multi-sequence information
(multi-modal) and individual decoder streams for each modality for multi-task
reconstruction. This pretraining strategy guides the model to learn rich
representations per modality while also equipping it to handle missing inputs
through cross-sequence reasoning. The result is a flexible and generalizable
encoder for brain MRIs that infers missing sequences from available inputs and
can be adapted to various downstream applications. We demonstrate the
performance and robustness of our method against an MAE-ViT baseline in
downstream segmentation and classification tasks, showing absolute improvement
of $10.1$ overall Dice score and $0.46$ MCC over the baselines with missing
input sequences. Our experiments demonstrate the strength of this pretraining
strategy. The implementation is made available.

</details>


### [72] [Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision](https://arxiv.org/abs/2509.11476)
*Tianyao Sun,Dawei Xiang,Tianqi Ding,Xiang Fang,Yijiashun Qi,Zunduo Zhao*

Main category: cs.CV

TL;DR: FusionNet fuses infrared and visible images using modality-aware attention, pixel-wise alpha blending, and target-aware supervision to preserve semantics, achieving interpretable, high-quality fusion on M3FD.


<details>
  <summary>Details</summary>
Motivation: Leverage complementary infrared and visible cues to preserve important semantic structures (e.g., pedestrians, vehicles) while maintaining perceptual quality and interpretability for downstream tasks.

Method: An end-to-end fusion framework with (1) modality-aware attention to adaptively weight infrared vs. visible features by discriminative capacity, (2) a pixel-wise alpha blending module for spatially varying fusion weights, (3) a target-aware loss using weak ROI supervision to preserve semantic consistency in important regions.

Result: On the public M3FD dataset, FusionNet improves semantic preservation, perceptual quality, and interpretability of fused images, and is positioned as a general, extensible semantic-aware fusion framework.

Conclusion: FusionNet provides a flexible framework for semantic-aware multi-modal image fusion, with potential benefits for downstream tasks such as object detection and scene understanding, by explicitly modeling inter-modality interaction and targeting important regions.

Abstract: Infrared and visible image fusion (IVIF) is a fundamental task in multi-modal
perception that aims to integrate complementary structural and textural cues
from different spectral domains. In this paper, we propose FusionNet, a novel
end-to-end fusion framework that explicitly models inter-modality interaction
and enhances task-critical regions. FusionNet introduces a modality-aware
attention mechanism that dynamically adjusts the contribution of infrared and
visible features based on their discriminative capacity. To achieve
fine-grained, interpretable fusion, we further incorporate a pixel-wise alpha
blending module, which learns spatially-varying fusion weights in an adaptive
and content-aware manner. Moreover, we formulate a target-aware loss that
leverages weak ROI supervision to preserve semantic consistency in regions
containing important objects (e.g., pedestrians, vehicles). Experiments on the
public M3FD dataset demonstrate that FusionNet generates fused images with
enhanced semantic preservation, high perceptual quality, and clear
interpretability. Our framework provides a general and extensible solution for
semantic-aware multi-modal image fusion, with benefits for downstream tasks
such as object detection and scene understanding.

</details>


### [73] [Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis](https://arxiv.org/abs/2509.11526)
*Wenhao Tang,Sheng Huang,Heng Fang,Fengtao Zhou,Bo Liu,Qingshan Liu*

Main category: cs.CV

TL;DR: Introduces MHIM-MIL, a masked hard instance mining MIL framework for gigapixel WSIs that uses a Siamese network with EMA teacher and randomized masking to mine hard instances, achieving superior performance and efficiency on cancer diagnosis, subtyping, and survival across 12 benchmarks; code available.


<details>
  <summary>Details</summary>
Motivation: In WSI MIL for computational pathology, attention-based MIL tends to focus on easy, salient instances and neglect hard ones, which are crucial for modeling discriminative boundaries. There is a need to explicitly mine hard instances to improve discriminative power. Existing methods often lack diversity in hard samples and stability during training.

Method: Proposes MHIM-MIL: a Siamese structure with a consistency constraint and a class-aware instance probability. A momentum teacher masks salient instances to implicitly mine hard instances for the student. Large-scale random masking plus a global recycle network ensures diverse, non-redundant hard samples and feature preservation. The student updates the teacher via exponential moving average (EMA), enabling continual discovery of hard instances. Training is designed for stability and efficiency across 12 benchmarks.

Result: Empirical results show that MHIM-MIL outperforms state-of-the-art MIL methods in cancer diagnosis, subtyping, and survival analysis tasks across 12 benchmarks, with improved performance and efficiency. Code is released at the provided GitHub link.

Conclusion: MHIM-MIL effectively mines hard instances in MIL for WSIs, addressing the bias toward easy samples and producing robust, efficient performance gains across multiple cancer-related tasks. The approach is reproducible via the released code.

Abstract: Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has
opened new avenues for Computational Pathology (CPath). As positive tissue
comprises only a small fraction of gigapixel WSIs, existing Multiple Instance
Learning (MIL) methods typically focus on identifying salient instances via
attention mechanisms. However, this leads to a bias towards easy-to-classify
instances while neglecting challenging ones. Recent studies have shown that
hard examples are crucial for accurately modeling discriminative boundaries.
Applying such an idea at the instance level, we elaborate a novel MIL framework
with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure
with a consistency constraint to explore the hard instances. Using a
class-aware instance probability, MHIM-MIL employs a momentum teacher to mask
salient instances and implicitly mine hard instances for training the student
model. To obtain diverse, non-redundant hard instances, we adopt large-scale
random masking while utilizing a global recycle network to mitigate the risk of
losing key features. Furthermore, the student updates the teacher using an
exponential moving average, which identifies new hard instances for subsequent
training iterations and stabilizes optimization. Experimental results on cancer
diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate
that MHIM-MIL outperforms the latest methods in both performance and
efficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.

</details>


### [74] [SFGNet: Semantic and Frequency Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2509.11539)
*Dezhen Wang,Haixiang Zhao,Xiang Shen,Sheng Miao*

Main category: cs.CV

TL;DR: Introduces SFGNet for camouflaged object detection that combines semantic prompts with frequency-domain features, augmented by a Multi-Band Fourier Module and an Interactive Structure Enhancement Block; achieves state-of-the-art results on three COD benchmarks; code released.


<details>
  <summary>Details</summary>
Motivation: Camouflaged objects are hard to segment due to blending with surroundings. Existing COD approaches often ignore semantic distinctions among textual prompts for different targets and overlook fine-grained frequency information, which can impair boundary perception and segmentation accuracy.

Method: Proposes Semantic and Frequency Guided Network (SFGNet). It integrates semantic prompts with frequency-domain features to detect camouflaged objects. Key components include Multi-Band Fourier Module (MBFM) to capture diverse frequency information for complex backgrounds and blurred boundaries, and Interactive Structure Enhancement Block (ISEB) to maintain structural integrity and refine boundary details in predictions.

Result: Extensive experiments on three COD benchmark datasets show significant improvements over state-of-the-art methods, indicating the effectiveness of combining semantic prompts with Fourier-domain features for COD.

Conclusion: SFGNet effectively fuses semantic and frequency cues to enhance camouflaged object detection. The MBFM and ISEB modules contribute to improved boundary precision and structural consistency. The approach achieves state-of-the-art performance on COD benchmarks, and the code is publicly available.

Abstract: Camouflaged object detection (COD) aims to segment objects that blend into
their surroundings. However, most existing studies overlook the semantic
differences among textual prompts of different targets as well as fine-grained
frequency features. In this work, we propose a novel Semantic and Frequency
Guided Network (SFGNet), which incorporates semantic prompts and
frequency-domain features to capture camouflaged objects and improve boundary
perception. We further design Multi-Band Fourier Module(MBFM) to enhance the
ability of the network in handling complex backgrounds and blurred boundaries.
In addition, we design an Interactive Structure Enhancement Block (ISEB) to
ensure structural integrity and boundary details in the predictions. Extensive
experiments conducted on three COD benchmark datasets demonstrate that our
method significantly outperforms state-of-the-art approaches. The core code of
the model is available at the following link:
https://github.com/winter794444/SFGNetICASSP2026.

</details>


### [75] [How Auxiliary Reasoning Unleashes GUI Grounding in VLMs](https://arxiv.org/abs/2509.11548)
*Weiming Li,Yan Shao,Jing Yang,Yujing Lu,Ling Zhong,Yuhan Wang,Manni Duan*

Main category: cs.CV

TL;DR: VLMs possess latent spatial grounding capability (as shown by Pointing Game) but struggle to output explicit coordinates for GUI grounding; introducing zero-shot spatial cues (axes, grids, labeled intersections) into the input image substantially improves GUI grounding across multiple benchmarks and models without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: There is a gap between latent grounding ability of vision–language models and their ability to generate explicit coordinate outputs for GUI grounding. Fine-tuning approaches are data- and annotation-heavy, so a low-cost, zero-shot solution is desirable to unlock spatial reasoning for GUI tasks.

Method: Introduce three zero-shot auxiliary reasoning methods by embedding explicit spatial cues into the input image: axes, grids, and labeled intersections. These cues provide explicit spatial references to help VLMs articulate their implicit spatial understanding. Evaluate on four GUI grounding benchmarks across seven open-source and proprietary VLMs.

Result: The proposed zero-shot spatial-cue methods substantially improve GUI grounding performance across the tested benchmarks and models, indicating that augmenting inputs with explicit spatial references helps VLMs translate latent spatial reasoning into explicit coordinate outputs without fine-tuning.

Conclusion: Zero-shot augmentation with explicit spatial cues effectively taps into latent spatial grounding in VLMs for GUI tasks, reducing data/annotation costs and offering a promising direction for robust GUI grounding. Further work could explore additional cues, robustness across varied GUIs, and integration with downstream GUI automation tasks.

Abstract: Graphical user interface (GUI) grounding is a fundamental task for building
GUI agents. However, general vision-language models (VLMs) struggle with this
task due to a lack of specific optimization. We identify a key gap in this
paper: while VLMs exhibit significant latent grounding potential, as
demonstrated by their performance measured by Pointing Game, they underperform
when tasked with outputting explicit coordinates. To address this discrepancy,
and bypass the high data and annotation costs of current fine-tuning
approaches, we propose three zero-shot auxiliary reasoning methods. By
providing explicit spatial cues such as axes, grids and labeled intersections
as part of the input image, these methods enable VLMs to articulate their
implicit spatial understanding capabilities. We evaluate these methods on four
GUI grounding benchmarks across seven open-source and proprietary VLMs. The
evaluation results demonstrate that the proposed methods substantially improve
the performance of GUI grounding.

</details>


### [76] [Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps](https://arxiv.org/abs/2509.11574)
*Zhexi Peng,Kun Zhou,Tianjia Shao*

Main category: cs.CV

TL;DR: Gaussian-SDF hybrid representation for RGB-D SLAM enables real-time reconstruction by combining a colorized SDF with 3D Gaussians; achieves over 150 fps with 50% fewer Gaussians and 75% fewer optimization iterations, maintaining comparable quality.


<details>
  <summary>Details</summary>
Motivation: Gaussian-based SLAM methods offer photorealistic reconstruction but are computationally heavy due to modeling scenes with many Gaussians and iterative optimization, resulting in subreal-time fps. There is a need to close the gap with geometry-centric methods and achieve real-time performance.

Method: Introduce a Gaussian-SDF hybrid representation: use a colorized Signed Distance Field (SDF) for smooth geometry and appearance, and 3D Gaussians to capture underrepresented details. Build the SDF via RGB-D fusion as in geometry-centric methods; iteratively optimize the Gaussians for appearance details. Reduce Gaussian modeling by avoiding full-scene Gaussian counts and accelerate optimization with targeted refinement.

Result: GPS-SLAM achieves real-time reconstruction at over 150 fps on real-world Azure Kinect sequences, delivering an order-of-magnitude speedup over state-of-the-art Gaussian-based SLAM while maintaining comparable reconstruction quality.

Conclusion: The Gaussian-SDF hybrid approach enables practical real-time Gaussian-based SLAM, combining efficient geometry with targeted appearance refinement. The authors plan to release source code and data to facilitate further research.

Abstract: While recent Gaussian-based SLAM methods achieve photorealistic
reconstruction from RGB-D data, their computational performance remains a
critical bottleneck. State-of-the-art techniques operate at less than 20 fps,
significantly lagging behind geometry-centric approaches like KinectFusion
(hundreds of fps). This limitation stems from the heavy computational burden:
modeling scenes requires numerous Gaussians and complex iterative optimization
to fit RGB-D data, where insufficient Gaussian counts or optimization
iterations cause severe quality degradation. To address this, we propose a
Gaussian-SDF hybrid representation, combining a colorized Signed Distance Field
(SDF) for smooth geometry and appearance with 3D Gaussians to capture
underrepresented details. The SDF is efficiently constructed via RGB-D fusion
(as in geometry-centric methods), while Gaussians undergo iterative
optimization. Our representation enables drastic Gaussian reduction (50% fewer)
by avoiding full-scene Gaussian modeling, and efficient Gaussian optimization
(75% fewer iterations) through targeted appearance refinement. Building upon
this representation, we develop GPS-SLAM (Gaussian-Plus-SDF SLAM), a real-time
3D reconstruction system achieving over 150 fps on real-world Azure Kinect
sequences -- delivering an order-of-magnitude speedup over state-of-the-art
techniques while maintaining comparable reconstruction quality. We will release
the source code and data to facilitate future research.

</details>


### [77] [Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2509.11587)
*Haonan Shi,Yubin Wang,De Cheng,Lingfeng He,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: A hierarchical identity learning framework with multi-centre contrastive learning and bidirectional pseudo-label matching for unsupervised visible-infrared Re-ID, achieving state-of-the-art results on SYSU-MM01 and RegDB.


<details>
  <summary>Details</summary>
Motivation: Current USVI-ReID methods rely on single-cluster representations that overlook fine-grained intra-cluster variations and cross-modal discrepancies. This work aims to capture finer-grained identities, enhance intra-modal clustering, and improve cross-modal alignment without labels.

Method: 1) Hierarchical Identity Learning (HIL): generate multiple memories per coarse cluster via secondary clustering to form sub-clusters; 2) Multi-Center Contrastive Learning (MCCL): refine representations using multiple centers to reduce intra-cluster variation and cross-modal gap; 3) Bidirectional Reverse Selection Transmission (BRST): bidirectional pseudo-label matching to establish reliable cross-modal correspondences.

Result: Experiments on SYSU-MM01 and RegDB show the proposed method outperforms existing approaches for USVI-ReID.

Conclusion: The combination of hierarchical identities, multi-center learning, and bidirectional pseudo-label matching improves intra-cluster discrimination and cross-modal matching, advancing unsupervised VI-ReID performance; source code is provided.

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
learn modality-invariant image features from unlabeled cross-modal person
datasets by reducing the modality gap while minimizing reliance on costly
manual annotations. Existing methods typically address USVI-ReID using
cluster-based contrastive learning, which represents a person by a single
cluster center. However, they primarily focus on the commonality of images
within each cluster while neglecting the finer-grained differences among them.
To address the limitation, we propose a Hierarchical Identity Learning (HIL)
framework. Since each cluster may contain several smaller sub-clusters that
reflect fine-grained variations among images, we generate multiple memories for
each existing coarse-grained cluster via a secondary clustering. Additionally,
we propose Multi-Center Contrastive Learning (MCCL) to refine representations
for enhancing intra-modal clustering and minimizing cross-modal discrepancies.
To further improve cross-modal matching quality, we design a Bidirectional
Reverse Selection Transmission (BRST) mechanism, which establishes reliable
cross-modal correspondences by performing bidirectional matching of
pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB
datasets demonstrate that the proposed method outperforms existing approaches.
The source code is available at: https://github.com/haonanshi0125/HIL.

</details>


### [78] [Optimizing Class Distributions for Bias-Aware Multi-Class Learning](https://arxiv.org/abs/2509.11588)
*Mirco Felske,Stefan Stiene*

Main category: cs.CV

TL;DR: BiCDO is a data-centric, Pareto-optimized method to tailor per-class image counts in multi-class classification, prioritizing chosen classes to improve reliability and reduce bias/variance, and is plug-in compatible with common pipelines. It is validated on CIFAR-10 and iNaturalist21 across multiple architectures with improved balanced performance.


<details>
  <summary>Details</summary>
Motivation: Uniform class distributions can propagate bias and reliability gaps in multi-class models, especially when safety-critical or fairness considerations require prioritizing certain classes. BiCDO aims to give practitioners explicit control over class distribution to balance performance and bias.

Method: Iteratively identify Pareto-optimal per-class image counts to optimize a multi-objective objective (reliability, bias, variance, and perhaps per-class performance). Integrates into existing training pipelines with minimal code changes and supports any labelled multi-class dataset; validated with EfficientNet, ResNet, and ConvNeXt on CIFAR-10 and iNaturalist21.

Result: BiCDO yields improved, balanced model performance by optimizing data distribution, achieving better trade-offs among classes without changing the learning algorithm itself; demonstrated across multiple architectures and datasets.

Conclusion: BiCDO is a practical, plug-in data distribution optimizer that enables bias-aware prioritization in multi-class classification and generalizes across architectures and datasets.

Abstract: We propose BiCDO (Bias-Controlled Class Distribution Optimizer), an
iterative, data-centric framework that identifies Pareto optimized class
distributions for multi-class image classification. BiCDO enables performance
prioritization for specific classes, which is useful in safety-critical
scenarios (e.g. prioritizing 'Human' over 'Dog'). Unlike uniform distributions,
BiCDO determines the optimal number of images per class to enhance reliability
and minimize bias and variance in the objective function. BiCDO can be
incorporated into existing training pipelines with minimal code changes and
supports any labelled multi-class dataset. We have validated BiCDO using
EfficientNet, ResNet and ConvNeXt on CIFAR-10 and iNaturalist21 datasets,
demonstrating improved, balanced model performance through optimized data
distribution.

</details>


### [79] [MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment](https://arxiv.org/abs/2509.11589)
*Yanyun Pu,Kehan Li,Zeyi Huang,Zhijie Zhong,Kaixiang Yang*

Main category: cs.CV

TL;DR: MVQA-68K introduces a multi-dimensional, 68k-video VQA dataset with seven quality dimensions and chain-of-thought annotations, improving multimodal LLM VQA capabilities and zero-shot generalization; code/data released.


<details>
  <summary>Details</summary>
Motivation: Traditional VQA for videos yields single scores and limited interpretability. There's a need for a comprehensive, interpretable assessment of video quality to guide data curation and model selection in large-scale video pretraining.

Method: Create MVQA-68K with 68,000 annotated videos across seven dimensions: overall aesthetics, camera movement, dynamic degree, texture detail, composition, visual quality, and factual consistency, plus detailed chain-of-thought reasoning per annotation; evaluate MLLMs on VQA tasks using these annotations; demonstrate improved zero-shot generalization with explicit reasoning during training.

Result: Achieves state-of-the-art results on internal test sets and public benchmarks (LSVQ-test, LSVQ-1080p, LIVE-VQC) for VQA; explicit reasoning during training substantially boosts zero-shot generalization; code and dataset will be released on GitHub.

Conclusion: MVQA-68K provides a valuable, interpretable, multi-dimensional framework for video quality assessment that enhances VQA performance of multimodal models and generalization, with broad applicability to pretraining data curation; release planned at GitHub.

Abstract: With the rapid advancement of video generation models such as Sora, video
quality assessment (VQA) is becoming increasingly crucial for selecting
high-quality videos from large-scale datasets used in pre-training. Traditional
VQA methods, typically producing single numerical scores, often lack
comprehensiveness and interpretability. To address these challenges, we
introduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over
68,000 carefully annotated videos, covering seven essential quality dimensions:
overall aesthetics, camera movement, dynamic degree, texture detail,
composition, visual quality, and factual consistency. Each annotation includes
detailed chain-of-thought reasoning to facilitate interpretability and
comprehensive understanding. Extensive experiments demonstrate that MVQA-68K
significantly enhances the performance of various multimodal large language
models (MLLMs) on the VQA task, achieving state-of-the-art results not only on
our internal test set (Fig.1) but also on public benchmarks including
LSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning
process during VQA training substantially boosts the zero-shot generalization.
Code and dataset will be available at github:
https://github.com/Controller01-ai/MVQA-68K

</details>


### [80] [Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework](https://arxiv.org/abs/2509.11598)
*Siming Fu,Sijun Dong,Xiaoliang Meng*

Main category: cs.CV

TL;DR: Identifies shortcut learning as a fundamental SSL flaw and introduces HyGDL, a hybrid generative-discriminative framework that disentangles content and style via invariance pre-training with a single encoder.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning generalizes poorly due to reliance on superficial cues (e.g., texture), a problem that persists in generative (e.g., MAE) and discriminative models and underlies poor performance on unseen domains. Existing domain-alignment methods address symptoms rather than the root learning mechanism that fosters shortcut dependency.

Method: HyGDL uses the Invariance Pre-training Principle: vary input bias (style) while keeping supervision constant to force learning of an invariant content essence. It defines style as the component of a representation orthogonal to content via vector projection, and operates with a single encoder. It combines generative and discriminative objectives to explicitly disentangle content and style.

Result: The abstract asserts that HyGDL achieves explicit content-style disentanglement and targets the root cause of domain-generalization failures; it also presents experimental verification of shortcut learning as a systemic flaw. The abstract does not provide quantitative results.

Conclusion: HyGDL offers a principled route to overcome shortcut learning in SSL by reframing learning around invariant content representations through a hybrid objective and a projection-based style definition, potentially improving generalization to unseen domains.

Abstract: Despite the remarkable success of Self-Supervised Learning (SSL), its
generalization is fundamentally hindered by Shortcut Learning, where models
exploit superficial features like texture instead of intrinsic structure. We
experimentally verify this flaw within the generative paradigm (e.g., MAE) and
argue it is a systemic issue also affecting discriminative methods, identifying
it as the root cause of their failure on unseen domains. While existing methods
often tackle this at a surface level by aligning or separating domain-specific
features, they fail to alter the underlying learning mechanism that fosters
shortcut dependency. To address this at its core, we propose HyGDL (Hybrid
Generative-Discriminative Learning Framework), a hybrid framework that achieves
explicit content-style disentanglement. Our approach is guided by the
Invariance Pre-training Principle: forcing a model to learn an invariant
essence by systematically varying a bias (e.g., style) at the input while
keeping the supervision signal constant. HyGDL operates on a single encoder and
analytically defines style as the component of a representation that is
orthogonal to its style-invariant content, derived via vector projection.

</details>


### [81] [DUAL-VAD: Dual Benchmarks and Anomaly-Focused Sampling for Video Anomaly Detection](https://arxiv.org/abs/2509.11605)
*Seoik Jung,Taekyung Song,Joshua Jordan Daniel,JinYoung Lee,SungJun Lee*

Main category: cs.CV

TL;DR: Proposes anomaly-focused, softmax-based frame allocation for video anomaly detection, builds two complementary benchmarks (image-based and video-based), and shows improvements on UCF-Crime via anomaly-focused sampling over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing VAD benchmarks are limited to frame-level or video-level tasks, hindering a holistic assessment of model generalization across temporal scales and sampling strategies. There is a need for sampling methods that balance full-video coverage with emphasis on anomaly-dense segments.

Method: Introduce a softmax-based frame allocation strategy that prioritizes anomaly-dense segments while preserving full-video coverage for balanced sampling across temporal scales. Construct two benchmarks: (1) an image-based frame-level benchmark evaluating reasoning on representative frames, and (2) a video-based benchmark extending to temporally localized segments with an abnormality scoring task.

Result: On the UCF-Crime dataset, the approach yields improvements at both frame- and video-level evaluations. Ablation studies indicate anomaly-focused sampling outperforms uniform and random baselines.

Conclusion: Anomaly-focused sampling via softmax-based frame allocation provides a more effective and generalizable framework for VAD, and the dual benchmarks offer a more holistic evaluation of model performance across temporal scales.

Abstract: Video Anomaly Detection (VAD) is critical for surveillance and public safety.
However, existing benchmarks are limited to either frame-level or video-level
tasks, restricting a holistic view of model generalization. This work first
introduces a softmax-based frame allocation strategy that prioritizes
anomaly-dense segments while maintaining full-video coverage, enabling balanced
sampling across temporal scales. Building on this process, we construct two
complementary benchmarks. The image-based benchmark evaluates frame-level
reasoning with representative frames, while the video-based benchmark extends
to temporally localized segments and incorporates an abnormality scoring
task.Experiments on UCF-Crime demonstrate improvements at both the frame and
video levels, and ablation studies confirm clear advantages of anomaly-focused
sampling over uniform and random baselines.

</details>


### [82] [A Controllable 3D Deepfake Generation Framework with Gaussian Splatting](https://arxiv.org/abs/2509.11624)
*Wending Liu,Siyun Liang,Huy H. Nguyen,Isao Echizen*

Main category: cs.CV

TL;DR: A 3D Gaussian Splatting-based deepfake framework enabling identity-preserving, multi-view face manipulation with background separation and a repair module; matches 2D SOTA in identity control and pose/expression consistency, while surpassing in multi-view rendering and 3D consistency, highlighting potential misuse risks.


<details>
  <summary>Details</summary>
Motivation: 2D deepfakes suffer geometric inconsistencies and poor generalization to novel viewpoints; there is a need for true 3D-consistent, controllable face manipulation in a scene, along with addressing editing challenges in point-based representations; however, this advancement also raises manipulation risk using 3D Gaussian Splatting.

Method: Integrates a parametric head model with dynamic 3D Gaussian representations; explicitly separates head and background Gaussians; leverages pre-trained 2D guidance to optimize facial regions across views; includes a repair module to improve visual consistency under extreme poses and expressions; evaluated on NeRSemble and additional videos.

Result: Performance is comparable to state-of-the-art 2D methods for identity preservation and pose/expression consistency; substantially better in multi-view rendering quality and 3D consistency.

Conclusion: Represents a bridge between 3D modeling and deepfake synthesis, enabling scene-aware, controllable forgeries; simultaneously highlights the threat of 3D Gaussian Splatting-based manipulation attacks and the need for safeguards.

Abstract: We propose a novel 3D deepfake generation framework based on 3D Gaussian
Splatting that enables realistic, identity-preserving face swapping and
reenactment in a fully controllable 3D space. Compared to conventional 2D
deepfake approaches that suffer from geometric inconsistencies and limited
generalization to novel view, our method combines a parametric head model with
dynamic Gaussian representations to support multi-view consistent rendering,
precise expression control, and seamless background integration. To address
editing challenges in point-based representations, we explicitly separate the
head and background Gaussians and use pre-trained 2D guidance to optimize the
facial region across views. We further introduce a repair module to enhance
visual consistency under extreme poses and expressions. Experiments on
NeRSemble and additional evaluation videos demonstrate that our method achieves
comparable performance to state-of-the-art 2D approaches in identity
preservation, as well as pose and expression consistency, while significantly
outperforming them in multi-view rendering quality and 3D consistency. Our
approach bridges the gap between 3D modeling and deepfake synthesis, enabling
new directions for scene-aware, controllable, and immersive visual forgeries,
revealing the threat that emerging 3D Gaussian Splatting technique could be
used for manipulation attacks.

</details>


### [83] [IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed](https://arxiv.org/abs/2509.11638)
*Yongzhe Lyu,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: A training-free diffusion-inpainting method using initial seeds from unmasked regions (IS-Diff) with dynamic refinement to harmonize results and improve consistency, outperforming state-of-the-art on standard and large masks.


<details>
  <summary>Details</summary>
Motivation: Address mismatch between masked region semantics and diffusion noise causing low consistency and coherence in inpainting. Avoid retraining by leveraging seed distribution from unmasked regions and dynamic refinement.

Method: Initial seeds sampled from unmasked areas to imitate masked data distribution; dynamic selective refinement to detect unharmonious intermediate results and adapt initialization strength during diffusion; completely training-free.

Result: Validated on CelebA-HQ, ImageNet, Places2 across standard and large-mask inpainting tasks; improvements across metrics relative to SOTA inpainting methods.

Conclusion: IS-Diff offers a training-free pathway to harmonize inpaintings by seeding diffusion with distribution-consistent seeds and adaptively refining initialization strength, achieving better consistency and coherence.

Abstract: Diffusion models have shown promising results in free-form inpainting. Recent
studies based on refined diffusion samplers or novel architectural designs led
to realistic results and high data consistency. However, random initialization
seed (noise) adopted in vanilla diffusion process may introduce mismatched
semantic information in masked regions, leading to biased inpainting results,
e.g., low consistency and low coherence with the other unmasked area. To
address this issue, we propose the Initial Seed refined Diffusion Model
(IS-Diff), a completely training-free approach incorporating distributional
harmonious seeds to produce harmonious results. Specifically, IS-Diff employs
initial seeds sampled from unmasked areas to imitate the masked data
distribution, thereby setting a promising direction for the diffusion
procedure. Moreover, a dynamic selective refinement mechanism is proposed to
detect severe unharmonious inpaintings in intermediate latent and adjust the
strength of our initialization prior dynamically. We validate our method on
both standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet,
and Places2 datasets, demonstrating its effectiveness across all metrics
compared to state-of-the-art inpainting methods.

</details>


### [84] [WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration](https://arxiv.org/abs/2509.11642)
*Qiyuan Guan,Qianfeng Yang,Xiang Chen,Tianyu Song,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: Proposes WeatherBench, a real-world all-in-one adverse weather restoration benchmark with aligned degraded/clean image pairs across rain, snow, and haze, enabling supervised learning and rigorous evaluation; dataset released publicly to foster robust all-in-one restoration.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap caused by reliance on mixed single-weather synthetic datasets, which introduce strong domain differences and hinder fair evaluation and training of unified restoration models; a large-scale real-world dataset is needed.

Method: Constructs a real-world dataset with precisely aligned degraded and clean image pairs captured under varied weather conditions (rain, snow, haze) and diverse outdoor scenes and illumination; provides publicly released data and conducts experiments by benchmarking task-specific, task-general, and all-in-one restoration methods.

Result: Comprehensive benchmarking of multiple restoration approaches on the dataset; demonstrates the dataset’s utility for supervised learning and fair evaluation, highlights domain gaps, and establishes baseline performances across method types.

Conclusion: The dataset establishes a foundation for robust, real-world all-in-one weather restoration and is publicly released to accelerate research and fair benchmarking.

Abstract: Existing all-in-one image restoration approaches, which aim to handle
multiple weather degradations within a single framework, are predominantly
trained and evaluated using mixed single-weather synthetic datasets. However,
these datasets often differ significantly in resolution, style, and domain
characteristics, leading to substantial domain gaps that hinder the development
and fair evaluation of unified models. Furthermore, the lack of a large-scale,
real-world all-in-one weather restoration dataset remains a critical bottleneck
in advancing this field. To address these limitations, we present a real-world
all-in-one adverse weather image restoration benchmark dataset, which contains
image pairs captured under various weather conditions, including rain, snow,
and haze, as well as diverse outdoor scenes and illumination settings. The
resulting dataset provides precisely aligned degraded and clean images,
enabling supervised learning and rigorous evaluation. We conduct comprehensive
experiments by benchmarking a variety of task-specific, task-general, and
all-in-one restoration methods on our dataset. Our dataset offers a valuable
foundation for advancing robust and practical all-in-one image restoration in
real-world scenarios. The dataset has been publicly released and is available
at https://github.com/guanqiyuan/WeatherBench.

</details>


### [85] [Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba](https://arxiv.org/abs/2509.11649)
*Chuang Liu,Nan Guo*

Main category: cs.CV

TL;DR: A unified Joint-OCTAMamba framework for OCTA vessel and FAZ segmentation that combines RVMamba and FAZMamba with a Mamba state-space backbone, achieving state-of-the-art results on OCTA-500 and releasing code.


<details>
  <summary>Details</summary>
Motivation: Current 2D retinal vessel segmentation methods lack accuracy, and existing joint OCTA models exhibit task imbalance between FAZ and other segmentation tasks, limiting clinical utility.

Method: Develop RVMamba with multi-feature extraction modules and a Mamba state-space model for retinal vessel segmentation; introduce FAZMamba for FAZ segmentation; unify in Joint-OCTAMamba for joint learning and balanced performance; evaluate on OCTA-500; code at GitHub.

Result: Joint-OCTAMamba outperforms existing models across evaluation metrics on OCTA-500.

Conclusion: The proposed framework provides improved accuracy and balanced performance for OCTA segmentation tasks and is openly available, supporting improved diagnosis/monitoring of retinal diseases.

Abstract: OCTA is a crucial non-invasive imaging technique for diagnosing and
monitoring retinal diseases like diabetic retinopathy, age-related macular
degeneration, and glaucoma. Current 2D-based methods for retinal vessel (RV)
segmentation offer insufficient accuracy. To address this, we propose RVMamba,
a novel architecture integrating multiple feature extraction modules with the
Mamba state-space model. Moreover, existing joint segmentation models for OCTA
data exhibit performance imbalance between different tasks. To simultaneously
improve the segmentation of the foveal avascular zone (FAZ) and mitigate this
imbalance, we introduce FAZMamba and a unified Joint-OCTAMamba framework.
Experimental results on the OCTA-500 dataset demonstrate that Joint-OCTAMamba
outperforms existing models across evaluation metrics.The code is available at
https://github.com/lc-sfis/Joint-OCTAMamba.

</details>


### [86] [DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition](https://arxiv.org/abs/2509.11661)
*Lifei Hao,Yue Cheng,Baoqi Huang,Bing Jia,Xuandong Zhao*

Main category: cs.CV

TL;DR: A diffusion-model-based few-shot data augmentation (DTGen) with LoRA fine-tuning and CLIP filtering to enable fine-grained dirty tableware recognition, enabling virtually unlimited high-quality samples and deployment in embedded dishwashers.


<details>
  <summary>Details</summary>
Motivation: Industrial-scale dirty tableware recognition faces coarse-grained labels and scarce few-shot data; a scalable data augmentation approach is needed to meet deployment constraints in smart kitchens and dishwashers.

Method: DTGen uses diffusion models with LoRA for domain specialization, structured prompts for diverse dirty patterns, and CLIP-based cross-modal filtering; designed for extremely limited real data and lightweight deployment.

Result: Significant improvement in classifier performance under extremely limited data; generation of large volumes of high-quality samples; supports integration with energy and detergent optimization in cleaning programs.

Conclusion: DTGen demonstrates practicality of generative AI for few-shot industrial vision and outlines deployment pathways for automated cleaning and food safety monitoring.

Abstract: Intelligent tableware cleaning is a critical application in food safety and
smart homes, but existing methods are limited by coarse-grained classification
and scarcity of few-shot data, making it difficult to meet industrialization
requirements. We propose DTGen, a few-shot data augmentation scheme based on
generative diffusion models, specifically designed for fine-grained dirty
tableware recognition. DTGen achieves efficient domain specialization through
LoRA, generates diverse dirty images via structured prompts, and ensures data
quality through CLIP-based cross-modal filtering. Under extremely limited real
few-shot conditions, DTGen can synthesize virtually unlimited high-quality
samples, significantly improving classifier performance and supporting
fine-grained dirty tableware recognition. We further elaborate on lightweight
deployment strategies, promising to transfer DTGen's benefits to embedded
dishwashers and integrate with cleaning programs to intelligently regulate
energy consumption and detergent usage. Research results demonstrate that DTGen
not only validates the value of generative AI in few-shot industrial vision but
also provides a feasible deployment path for automated tableware cleaning and
food safety monitoring.

</details>


### [87] [MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs](https://arxiv.org/abs/2509.11662)
*Feilong Chen,Yijiang Liu,Yi Huang,Hao Wang,Miren Tian,Ya-Qi Yu,Minghui Liao,Jihao Wu*

Main category: cs.CV

TL;DR: MindVL is a high-resolution multimodal LLM trained on Ascend NPUs that matches Qwen2.5-VL using roughly 10% of its data, with strong OCR and document/table understanding thanks to native-resolution Vision Transformers and targeted training tricks.


<details>
  <summary>Details</summary>
Motivation: To enable efficient, high-quality multimodal LLMs capable of processing variable-resolution images without tiling artifacts, while reducing training data needs and leveraging Ascend NPUs through a tailored distributed framework and optimization techniques.

Method: 1) Use native-resolution Vision Transformers to handle images at their original resolutions. 2) Develop Mindspeed-MLLM, a distributed multimodal training framework for Ascend NPUs with operator-equivalent replacements to maintain accuracy. 3) Three-stage training: warm-up, multitask pretraining, and supervised instruction tuning. 4) Multimodal data packaging and hybrid parallelism to speed training. 5) Test-time resolution search and model weight averaging to boost performance.

Result: MindVL achieves performance comparable to Qwen2.5-VL while using about one-tenth of the training data; it also leads in OCR benchmarks and delivers strong results on general multimodal understanding and document/table comprehension.

Conclusion: MindVL demonstrates that high-resolution, modality-rich LLMs can be trained efficiently on Ascend NPUs with competitive performance, and it highlights the benefits of native-resolution ViTs, specialized training infrastructure, and targeted optimization techniques for dense visual content.

Abstract: We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.
Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,
which enables it to process images at their original variable resolutions. This
design avoids the degradation caused by fixed-resolution tiling while
preserving fine-grained details and global layouts, which is crucial for
visually dense content such as complex charts and diagrams. To ensure the
smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a
distributed multimodal training framework tailored for Ascend NPUs. To maintain
training accuracy, we implement equivalent replacements for certain operators.
MindVL undergoes a three-phase training process, namely the warm-up phase,
multitask training phase, and supervised instruction tuning phase, to gradually
enhance its capabilities. This process starts with basic visual and multimodal
pre-training, followed by large-scale multiask trainging and instruction
tuning. We also adopt multimodal data packaging and hybrid parallelism
techniques, which significantly improve end-to-end training speed. To further
boost model performance, we specifically introduce test-time resolution search
and model weight averaging. Notably, despite using about 1/10 of the training
data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL
in evaluations of general multimodal understanding and document/table
comprehension. Beyond overall scores, MindVL also delivers leading performance
in OCR assessments.

</details>


### [88] [RouteExtract: A Modular Pipeline for Extracting Routes from Paper Maps](https://arxiv.org/abs/2509.11674)
*Bjoern Kremser,Yusuke Matsui*

Main category: cs.CV

TL;DR: A pipeline to extract navigable trails from scanned maps by georeferencing, U‑Net segmentation, graph construction, and routing refinement, enabling practical GPS routes.


<details>
  <summary>Details</summary>
Motivation: Paper maps contain curated trails and locally relevant annotations that are often missing from digital navigation apps; converting these maps into navigable data expands GPS routing capabilities.

Method: Georeference scanned maps; perform U‑Net-based binary segmentation to detect trails; construct a graph from the segmentation; iteratively refine the network using a routing engine; evaluate end-to-end and per-component performance across diverse map styles.

Result: The approach robustly recovers trail networks from diverse map styles and generates GPS routes suitable for practical use.

Conclusion: The pipeline demonstrates feasibility of digitizing scanned map trail data for real-world navigation and can generalize across varied map styles.

Abstract: Paper maps remain widely used for hiking and sightseeing because they contain
curated trails and locally relevant annotations that are often missing from
digital navigation applications such as Google Maps. We propose a pipeline to
extract navigable trails from scanned maps, enabling their use in GPS-based
navigation. Our method combines georeferencing, U-Net-based binary
segmentation, graph construction, and an iterative refinement procedure using a
routing engine. We evaluate the full end-to-end pipeline as well as individual
components, showing that the approach can robustly recover trail networks from
diverse map styles and generate GPS routes suitable for practical use.

</details>


### [89] [IMD: A 6-DoF Pose Estimation Benchmark for Industrial Metallic Objects](https://arxiv.org/abs/2509.11680)
*Ruimin Ma,Sebastian Zudaire,Zhen Li,Chi Zhang*

Main category: cs.CV

TL;DR: Proposes Industrial Metallic Dataset (IMD) to evaluate 6D pose estimation and segmentation for metallic/textureless industrial objects; demonstrates current SOTA models struggle on industrial data; provides baselines for three tasks.


<details>
  <summary>Details</summary>
Motivation: Industrial robotics require robust perception for metallic, textureless, reflective objects. Existing benchmarks mostly cover textured household items, leading to a gap in generalization to industrial settings.

Method: Collect 45 true-to-scale industrial components; RGB-D data captured under natural indoor lighting with varied object arrangements; establish a benchmark with three tasks: video object segmentation, 6D pose tracking, and one-shot 6D pose estimation; evaluate state-of-the-art models (XMem and SAM2 for segmentation; BundleTrack and BundleSDF for pose estimation).

Result: The dataset is more challenging than existing household datasets; current models show performance gaps on industrial data; baseline results establish the difficulty and underscore the need for methods robust to metallic, textureless, and reflective objects.

Conclusion: IMD provides a baseline benchmark for developing and comparing segmentation and 6D pose estimation algorithms that generalize to industrial robotics, guiding future research toward more robust industrial perception.

Abstract: Object 6DoF (6D) pose estimation is essential for robotic perception,
especially in industrial settings. It enables robots to interact with the
environment and manipulate objects. However, existing benchmarks on object 6D
pose estimation primarily use everyday objects with rich textures and
low-reflectivity, limiting model generalization to industrial scenarios where
objects are often metallic, texture-less, and highly reflective. To address
this gap, we propose a novel dataset and benchmark namely \textit{Industrial
Metallic Dataset (IMD)}, tailored for industrial applications. Our dataset
comprises 45 true-to-scale industrial components, captured with an RGB-D camera
under natural indoor lighting and varied object arrangements to replicate
real-world conditions. The benchmark supports three tasks, including video
object segmentation, 6D pose tracking, and one-shot 6D pose estimation. We
evaluate existing state-of-the-art models, including XMem and SAM2 for
segmentation, and BundleTrack and BundleSDF for pose estimation, to assess
model performance in industrial contexts. Evaluation results show that our
industrial dataset is more challenging than existing household object datasets.
This benchmark provides the baseline for developing and comparing segmentation
and pose estimation algorithms that better generalize to industrial robotics
scenarios.

</details>


### [90] [Uncertainty-Aware Retinal Vessel Segmentation via Ensemble Distillation](https://arxiv.org/abs/2509.11689)
*Jeremiah Fadugba,Petru Manescu,Bolanle Oladejo,Delmiro Fernandez-Reyes,Philipp Berens*

Main category: cs.CV

TL;DR: Ensemble Distillation distills multiple retinal vessel segmentation models into a single model to estimate uncertainty efficiently; on DRIVE and FIVES, it matches deep ensembles in calibration and segmentation quality with reduced compute.


<details>
  <summary>Details</summary>
Motivation: Uncertainty estimation is essential for reliable medical image segmentation and retinal vessel analysis, but deep ensembles—though accurate—are computationally expensive. An efficient alternative is needed for clinical deployment.

Method: Train multiple ensemble models for retinal vessel segmentation and distill their knowledge into a single model. Evaluate how well the distilled model calibrates uncertainty and preserves segmentation performance on DRIVE and FIVES datasets.

Result: Ensemble Distillation achieves comparable calibration and segmentation metrics to deep ensembles while significantly reducing computational complexity.

Conclusion: Ensemble Distillation is a promising and efficient approach for uncertainty estimation in retinal vessel segmentation, offering reduced compute without sacrificing performance.

Abstract: Uncertainty estimation is critical for reliable medical image segmentation,
particularly in retinal vessel analysis, where accurate predictions are
essential for diagnostic applications. Deep Ensembles, where multiple networks
are trained individually, are widely used to improve medical image segmentation
performance. However, training and testing costs increase with the number of
ensembles. In this work, we propose Ensemble Distillation as a robust
alternative to commonly used uncertainty estimation techniques by distilling
the knowledge of multiple ensemble models into a single model. Through
extensive experiments on the DRIVE and FIVES datasets, we demonstrate that
Ensemble Distillation achieves comparable performance via calibration and
segmentation metrics, while significantly reducing computational complexity.
These findings suggest that Ensemble distillation provides an efficient and
reliable approach for uncertainty estimation in the segmentation of the retinal
vessels, making it a promising tool for medical imaging applications.

</details>


### [91] [The Quest for Universal Master Key Filters in DS-CNNs](https://arxiv.org/abs/2509.11711)
*Zahra Babaiee,Peyman M. Kiassari,Daniela Rus,Radu Grosu*

Main category: cs.CV

TL;DR: A single set of 8 universal filters governs depthwise separable CNNs; most learned filters are linear shifts of these, and freezing them yields strong performance, implying a fundamental, transferable set of spatial operators.


<details>
  <summary>Details</summary>
Motivation: To determine whether depthwise convolutional layers rely on a small universal filter set that underpins generalization and transfer across architectures and datasets.

Method: Systematic unsupervised search to extract core filter patterns across various DS-CNN architectures and datasets; compare learned filters to a fixed 8-filter set; evaluate performance when these 8 filters are frozen; analyze similarity to DoG/Gaussian and receptive fields.

Result: The learned filter banks collapse to 8 universal patterns, with many filters being linear shifts of these; networks initialized with these 8 frozen filters achieve >80% ImageNet accuracy and can outperform models with thousands of trainable parameters on smaller datasets; the universal filters resemble DoGs, Gaussians, and derivatives, akin to mammalian visual receptive fields.

Conclusion: Depthwise convolutional layers naturally converge toward a compact, universal set of spatial operators, offering new insights into generalization and transfer learning, and suggesting a universal language of master key filters across tasks and architectures.

Abstract: A recent study has proposed the "Master Key Filters Hypothesis" for
convolutional neural network filters. This paper extends this hypothesis by
radically constraining its scope to a single set of just 8 universal filters
that depthwise separable convolutional networks inherently converge to. While
conventional DS-CNNs employ thousands of distinct trained filters, our analysis
reveals these filters are predominantly linear shifts (ax+b) of our discovered
universal set. Through systematic unsupervised search, we extracted these
fundamental patterns across different architectures and datasets. Remarkably,
networks initialized with these 8 unique frozen filters achieve over 80%
ImageNet accuracy, and even outperform models with thousands of trainable
parameters when applied to smaller datasets. The identified master key filters
closely match Difference of Gaussians (DoGs), Gaussians, and their derivatives,
structures that are not only fundamental to classical image processing but also
strikingly similar to receptive fields in mammalian visual systems. Our
findings provide compelling evidence that depthwise convolutional layers
naturally gravitate toward this fundamental set of spatial operators regardless
of task or architecture. This work offers new insights for understanding
generalization and transfer learning through the universal language of these
master key filters.

</details>


### [92] [Advanced Layout Analysis Models for Docling](https://arxiv.org/abs/2509.11720)
*Nikolaos Livathinos,Christoph Auer,Ahmed Nassar,Rafael Teixeira de Lima,Maksym Lysak,Brown Ebouky,Cesar Berrospi,Michele Dolfi,Panagiotis Vagenas,Matteo Omenetti,Kasper Dinkla,Yusik Kim,Valery Weber,Lucas Morin,Ingmar Meijer,Viktor Kuropiatnyk,Tim Strohmeyer,A. Said Gurbuz,Peter W. J. Staar*

Main category: cs.CV

TL;DR: Five new document layout models built on RT-DETR/DFINE improve Docling's layout analysis by ~20.6–23.9% mAP over the previous baseline; the top model heron-101 achieves 78% mAP with 28 ms/image on an A100, with open-source release of checkpoints and code.


<details>
  <summary>Details</summary>
Motivation: Improve accuracy and speed of document layout analysis to enhance document conversion pipelines. The authors target a heterogeneous, large-scale corpus and real-world runtimes across CPUs and GPUs, addressing limitations of prior baselines in Docling.

Method: Train several state-of-the-art detectors (RT-DETR, RT-DETRv2, DFINE) on a dataset of ~150k documents (openly available and proprietary). Apply post-processing tailored to document-conversion tasks. Evaluate on multiple benchmarks and measure runtime across CPU and GPUs (NVIDIA, Apple). Propose five new layout models, with the top model 'heron-101' highlighted.

Result: Achieved 20.6%–23.9% mAP improvement over Docling’s prior baseline, with comparable or better runtime. The best model, 'heron-101', attains 78% mAP with 28 ms/image inference on NVIDIA A100. Provided extensive quantitative/qualitative experiments and best-practice guidance. All resources released on HuggingFace.

Conclusion: The work delivers state-of-the-art doc-layout detectors with practical deployment guidance for the document-conversion community, demonstrating clear gains in accuracy and enabling efficient deployment across diverse hardware; it also advocates a open, reproducible release of checkpoints, code, and documentation.

Abstract: This technical report documents the development of novel Layout Analysis
models integrated into the Docling document-conversion pipeline. We trained
several state-of-the-art object detectors based on the RT-DETR, RT-DETRv2 and
DFINE architectures on a heterogeneous corpus of 150,000 documents (both openly
available and proprietary). Post-processing steps were applied to the raw
detections to make them more applicable to the document conversion task. We
evaluated the effectiveness of the layout analysis on various document
benchmarks using different methodologies while also measuring the runtime
performance across different environments (CPU, Nvidia and Apple GPUs). We
introduce five new document layout models achieving 20.6% - 23.9% mAP
improvement over Docling's previous baseline, with comparable or better
runtime. Our best model, "heron-101", attains 78% mAP with 28 ms/image
inference time on a single NVIDIA A100 GPU. Extensive quantitative and
qualitative experiments establish best practices for training, evaluating, and
deploying document-layout detectors, providing actionable guidance for the
document conversion community. All trained checkpoints, code, and documentation
are released under a permissive license on HuggingFace.

</details>


### [93] [Microsurgical Instrument Segmentation for Robot-Assisted Surgery](https://arxiv.org/abs/2509.11727)
*Tae Kyeong Jeong,Garam Kim,Juyoun Park*

Main category: cs.CV

TL;DR: MISRA is a segmentation framework for microsurgery instruments that enhances thin-structure segmentation by adding luminance information, skip-attention, and an iterative feedback module, plus a new fine-grained dataset. It reports a +5.37% mean IoU gain over baselines and more stable predictions at contacts/overlaps.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of thin microsurgical instruments is essential for robotic assistance but is hard due to resolution loss, low contrast, and class imbalance. A reliable method and a representative dataset are needed to enable robust scene parsing in microsurgery.

Method: Augment RGB input with luminance channels; use skip-attention to preserve elongated features; introduce an Iterative Feedback Module (IFM) to restore continuity across multiple passes; provide a fine-grained annotated microsurgical instrument dataset for benchmarking.

Result: MISRA achieves a mean class IoU improvement of 5.37% over competing methods and delivers more stable predictions during instrument contacts and overlaps.

Conclusion: MISRA is a promising advancement toward reliable scene parsing in computer-assisted and robotic microsurgery, supported by a dedicated benchmark dataset for robust evaluation.

Abstract: Accurate segmentation of thin structures is critical for microsurgical scene
understanding but remains challenging due to resolution loss, low contrast, and
class imbalance. We propose Microsurgery Instrument Segmentation for Robotic
Assistance(MISRA), a segmentation framework that augments RGB input with
luminance channels, integrates skip attention to preserve elongated features,
and employs an Iterative Feedback Module(IFM) for continuity restoration across
multiple passes. In addition, we introduce a dedicated microsurgical dataset
with fine-grained annotations of surgical instruments including thin objects,
providing a benchmark for robust evaluation Dataset available at
https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstrate
that MISRA achieves competitive performance, improving the mean class IoU by
5.37% over competing methods, while delivering more stable predictions at
instrument contacts and overlaps. These results position MISRA as a promising
step toward reliable scene parsing for computer-assisted and robotic
microsurgery.

</details>


### [94] [Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference](https://arxiv.org/abs/2509.11731)
*Yudong Shen,Wenyu Wu,Jiali Mao,Yixiao Tong,Guoping Liu,Chaoya Wang*

Main category: cs.CV

TL;DR: DGMap combines dual decoding with global context for trajectory-based map inference, using multi-scale grid encoding and keypoint extraction plus global relation prediction to reduce sparsity fragmentation and dense-region false connections, achieving ~5% APLS gains across three real datasets (including Didi Chuxing).


<details>
  <summary>Details</summary>
Motivation: Uneven trajectory density causes fragmented roads in sparse areas and redundant segments in dense regions; existing methods struggle to reconcile global context with local geometry, limiting accuracy.

Method: Introduce DGMap: dual-decoding framework with Multi-scale Grid Encoding, Mask-enhanced Keypoint Extraction, and Global Context-aware Relation Prediction to fuse global semantic context with local features and model long-range trajectory patterns.

Result: Outperforms state-of-the-art by about 5% in APLS on three real-world datasets, with notable gains on Didi Chuxing data.

Conclusion: DGMap effectively mitigates fragmentation and false connections by integrating global context with local geometry, offering robust performance across varying trajectory densities.

Abstract: Trajectory data has become a key resource for automated map in-ference due to
its low cost, broad coverage, and continuous availability. However, uneven
trajectory density often leads to frag-mented roads in sparse areas and
redundant segments in dense regions, posing significant challenges for existing
methods. To address these issues, we propose DGMap, a dual-decoding framework
with global context awareness, featuring Multi-scale Grid Encoding,
Mask-enhanced Keypoint Extraction, and Global Context-aware Relation
Prediction. By integrating global semantic context with local geometric
features, DGMap improves keypoint detection accuracy to reduce road
fragmentation in sparse-trajectory areas. Additionally, the Global
Context-aware Relation Prediction module suppresses false connections in
dense-trajectory regions by modeling long-range trajectory patterns.
Experimental results on three real-world datasets show that DGMap outperforms
state-of-the-art methods by 5% in APLS, with notable performance gains on
trajectory data from the Didi Chuxing platform

</details>


### [95] [A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications](https://arxiv.org/abs/2509.11752)
*Hongyuan Zhang,Yuheng Wu,Mingyang Zhao,Zhiwei Chen,Rebecca Li,Fei Zhu,Haohan Zhao,Xiaohua Yuan,Meng Yang,Chunli Qiu,Xiang Cong,Haiyan Chen,Lina Luan,Randolph H. L. Wong,Huai Liao,Colin A Graham,Shi Chang,Guowei Tao,Dong Yi,Zhen Lei,Nassir Navab,Sebastien Ourselin,Jiebo Luo,Hongbin Liu,Gaofeng Meng*

Main category: cs.CV

TL;DR: EchoCare is a self-supervised ultrasound foundation model trained on a large, diverse dataset, using a hierarchical classifier to jointly learn pixel- and representation-level features; it achieves superior performance across 10 benchmarks and is open-source.


<details>
  <summary>Details</summary>
Motivation: Overcome the lack of large labeled clinical ultrasound datasets and the limited generalizability of task-specific models by creating a generalizable, multi-source ultrasound foundation model.

Method: Pretrain EchoCare with self-supervised learning on EchoCareData (4.5M ultrasound images from 23 countries, 5 continents, multiple devices). Introduce a hierarchical classifier to jointly learn pixel-level and representation-level features, enabling global anatomical context and local ultrasound characteristics. Release code and pretrained model for fine-tuning and local adaptation.

Result: With minimal training, EchoCare outperforms state-of-the-art models across 10 ultrasound benchmarks spanning disease diagnosis, lesion segmentation, organ detection, landmark prediction, quantitative regression, imaging enhancement, and report generation.

Conclusion: EchoCare provides a fully open, generalizable ultrasound foundation model that facilitates fine-tuning and local adaptation, supporting extensibility to additional clinical ultrasound applications.

Abstract: Artificial intelligence (AI) that can effectively learn ultrasound
representations by integrating multi-source data holds significant promise for
advancing clinical care. However, the scarcity of large labeled datasets in
real-world clinical environments and the limited generalizability of
task-specific models have hindered the development of generalizable clinical AI
models for ultrasound applications. In this study, we present EchoCare, a novel
ultrasound foundation model for generalist clinical use, developed via
self-supervised learning on our curated, publicly available, large-scale
dataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images,
sourced from over 23 countries across 5 continents and acquired via a diverse
range of distinct imaging devices, thus encompassing global cohorts that are
multi-center, multi-device, and multi-ethnic. Unlike prior studies that adopt
off-the-shelf vision foundation model architectures, we introduce a
hierarchical classifier into EchoCare to enable joint learning of pixel-level
and representation-level features, capturing both global anatomical contexts
and local ultrasound characteristics. With minimal training, EchoCare
outperforms state-of-the-art comparison models across 10 representative
ultrasound benchmarks of varying diagnostic difficulties, spanning disease
diagnosis, lesion segmentation, organ detection, landmark prediction,
quantitative regression, imaging enhancement and report generation. The code
and pretrained model are publicly released, rendering EchoCare accessible for
fine-tuning and local adaptation, supporting extensibility to additional
applications. EchoCare provides a fully open and generalizable foundation model
to boost the development of AI technologies for diverse clinical ultrasound
applications.

</details>


### [96] [MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face Reconstruction From Unconstrained Images](https://arxiv.org/abs/2509.11763)
*Danling Cao*

Main category: cs.CV

TL;DR: A Multi-Scale Feature Fusion with Multi-Attribute (MSMA) framework improves 3D face reconstruction from a single unconstrained image by enhancing cross-scale feature extraction with large-kernel attention, achieving SOTA on several datasets.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing 3D face reconstruction methods under unconstrained conditions, particularly their reliance on projection-based training signals and their difficulty capturing detailed multi-scale features; aims to reduce dependence on large labeled 3D data while improving reconstruction quality.

Method: Integrates multi-scale feature fusion and multi-attribute learning, augmented by a large-kernel attention module to refine features across scales, enabling accurate estimation of 3D facial parameters from a single 2D image.

Result: Experimental evaluation on MICC Florence, Facewarehouse, and a custom dataset showing MSMA achieves results on par with the current state-of-the-art, and in some cases surpasses it under challenging conditions.

Conclusion: MSMA effectively facilitates high-quality 3D face reconstruction from unconstrained images by combining multi-scale fusion, multi-attribute learning, and large-kernel attention, demonstrating competitive or superior performance.

Abstract: Reconstructing 3D face from a single unconstrained image remains a
challenging problem due to diverse conditions in unconstrained environments.
Recently, learning-based methods have achieved notable results by effectively
capturing complex facial structures and details across varying conditions.
Consequently, many existing approaches employ projection-based losses between
generated and input images to constrain model training. However, learning-based
methods for 3D face reconstruction typically require substantial amounts of 3D
facial data, which is difficult and costly to obtain. Consequently, to reduce
reliance on labeled 3D face datasets, many existing approaches employ
projection-based losses between generated and input images to constrain model
training. Nonetheless, despite these advancements, existing approaches
frequently struggle to capture detailed and multi-scale features under diverse
facial attributes and conditions, leading to incomplete or less accurate
reconstructions. In this paper, we propose a Multi-Scale Feature Fusion with
Multi-Attribute (MSMA) framework for 3D face reconstruction from unconstrained
images. Our method integrates multi-scale feature fusion with a focus on
multi-attribute learning and leverages a large-kernel attention module to
enhance the precision of feature extraction across scales, enabling accurate 3D
facial parameter estimation from a single 2D image. Comprehensive experiments
on the MICC Florence, Facewarehouse and custom-collect datasets demonstrate
that our approach achieves results on par with current state-of-the-art
methods, and in some instances, surpasses SOTA performance across challenging
conditions.

</details>


### [97] [Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization](https://arxiv.org/abs/2509.11772)
*Diogo Mendonça,Tiago Barros,Cristiano Premebida,Urbano J. Nunes*

Main category: cs.CV

TL;DR: Seg2Track-SAM2 combines pre-trained detectors, SAM2, and a new Seg2Track module to enable detector-agnostic, zero-shot MOTS without fine-tuning, with strong accuracy and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: MOTS requires robust identity management and memory efficiency in dynamic scenes. While SAM2 excels at zero-shot video segmentation, it lacks strong identity tracking and scalable memory handling, hindering MOTS performance and deployment.

Method: Integrate pre-trained object detectors with SAM2 and introduce Seg2Track for track initialization, management, and reinforcement. Operate without fine-tuning and maintain detector-agnosticism. Employ a sliding-window memory strategy to reduce memory usage.

Result: Achieves state-of-the-art-like performance: fourth place in both car and pedestrian classes on KITTI MOTS and sets a new benchmark in association accuracy (AssA). Sliding-window memory reduces memory consumption by up to 75% with negligible performance loss.

Conclusion: Seg2Track-SAM2 advances MOTS by leveraging robust zero-shot tracking, improved identity preservation, and efficient memory usage, without requiring fine-tuning. Code is available at the project URL.

Abstract: Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to
operate reliably in dynamic environments. MOT ensures consistent object
identity assignment and precise spatial delineation. Recent advances in
foundation models, such as SAM2, have demonstrated strong zero-shot
generalization for video segmentation, but their direct application to MOTS
(MOT+Segmentation) remains limited by insufficient identity management and
memory efficiency. This work introduces Seg2Track-SAM2, a framework that
integrates pre-trained object detectors with SAM2 and a novel Seg2Track module
to address track initialization, track management, and reinforcement. The
proposed approach requires no fine-tuning and remains detector-agnostic.
Experimental results on KITTI MOT and KITTI MOTS benchmarks show that
Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth
overall in both car and pedestrian classes on KITTI MOTS, while establishing a
new benchmark in association accuracy (AssA). Furthermore, a sliding-window
memory strategy reduces memory usage by up to 75% with negligible performance
degradation, supporting deployment under resource constraints. These results
confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot
tracking, enhanced identity preservation, and efficient memory utilization. The
code is available at https://github.com/hcmr-lab/Seg2Track-SAM2

</details>


### [98] [SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation](https://arxiv.org/abs/2509.11774)
*Changlu Guo,Anders Nymark Christensen,Anders Bjorholm Dahl,Yugen Yi,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: A lightweight retinal vessel segmentation model (SA-UNetv2) uses cross-scale spatial attention in all skip connections and a class-imbalance robust loss (weighted BCE + MCC) to achieve state-of-the-art results on DRIVE and STARE with a tiny model and fast CPU inference.


<details>
  <summary>Details</summary>
Motivation: To address the underutilization of attention in skip connections and to mitigate severe foreground-background imbalance in retinal vessel segmentation, while ensuring the model is extremely lightweight and CPU-friendly for deployment.

Method: Introduce cross-scale spatial attention in all skip connections of the network (SA-UNetv2) and train with a weighted Binary Cross-Entropy combined with Matthews Correlation Coefficient (MCC) loss to improve robustness to class imbalance. Model is compact (0.26M parameters; ~1.2 MB) and designed for CPU-only inference (1s on 592x592x3 images).

Result: SA-UNetv2 achieves state-of-the-art performance on public DRIVE and STARE datasets, with significantly reduced model size and only CPU-based inference, indicating strong efficiency and deployability in resource-constrained settings.

Conclusion: Cross-scale attention in skip connections and an imbalance-robust loss enable a very lightweight model to achieve top performance in retinal vessel segmentation, making it suitable for real-time, CPU-only deployment in clinical workflows.

Abstract: Retinal vessel segmentation is essential for early diagnosis of diseases such
as diabetic retinopathy, hypertension, and neurodegenerative disorders.
Although SA-UNet introduces spatial attention in the bottleneck, it underuses
attention in skip connections and does not address the severe
foreground-background imbalance. We propose SA-UNetv2, a lightweight model that
injects cross-scale spatial attention into all skip connections to strengthen
multi-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE)
plus Matthews Correlation Coefficient (MCC) loss to improve robustness to class
imbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves
state-of-the-art performance with only 1.2MB memory and 0.26M parameters (less
than 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images,
demonstrating strong efficiency and deployability in resource-constrained,
CPU-only settings.

</details>


### [99] [FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning](https://arxiv.org/abs/2509.11796)
*Haodong Chen,Haojian Huang,XinXiang Yin,Dian Shao*

Main category: cs.CV

TL;DR: FineQuest is a training-free, dual-mode VideoQA framework for sports, using SSGraph and domain knowledge to achieve state-of-the-art results on new Gym-QA and Diving-QA benchmarks as well as SPORTU, while preserving general VideoQA ability.


<details>
  <summary>Details</summary>
Motivation: Sports videos are richly domain-specific and challenging for general-purpose LLMs; a knowledge-rich, efficient approach is needed to bridge the gap without training data.

Method: Dual-mode reasoning (Reactive for simple questions, Deliberative for complex ones) combined with SSGraph—a multimodal sports knowledge scene graph covering nine sports—to provide domain knowledge; introduces Gym-QA and Diving-QA benchmarks derived from FineGym and FineDiving; evaluation is on SPORTU and existing datasets.

Result: State-of-the-art performance on Gym-QA, Diving-QA, and SPORTU while maintaining strong general VideoQA capabilities.

Conclusion: Dual-mode cognitive-inspired reasoning with a sports knowledge graph is effective for sports VideoQA and the proposed benchmarks enable robust evaluation and advancement in domain-specific VQA.

Abstract: Video Question Answering (VideoQA) based on Large Language Models (LLMs) has
shown potential in general video understanding but faces significant challenges
when applied to the inherently complex domain of sports videos. In this work,
we propose FineQuest, the first training-free framework that leverages
dual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for
straightforward sports queries and ii) Deliberative Reasoning for more complex
ones. To bridge the knowledge gap between general-purpose models and
domain-specific sports understanding, FineQuest incorporates SSGraph, a
multimodal sports knowledge scene graph spanning nine sports, which encodes
both visual instances and domain-specific terminology to enhance reasoning
accuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA
and Diving-QA, derived from the FineGym and FineDiving datasets, enabling
diverse and comprehensive evaluation. FineQuest achieves state-of-the-art
performance on these benchmarks as well as the existing SPORTU dataset, while
maintains strong general VideoQA capabilities.

</details>


### [100] [Pseudo-D: Informing Multi-View Uncertainty Estimation with Calibrated Neural Training Dynamics](https://arxiv.org/abs/2509.11800)
*Ang Nan Gu,Michael Tsang,Hooman Vaseli,Purang Abolmaesumi,Teresa Tsang*

Main category: cs.CV

TL;DR: Uncertainty-aware label augmentation via neural network training dynamics (NNTD) improves calibration and robustness in medical image classification by generating pseudo-labels that reflect ambiguity.


<details>
  <summary>Details</summary>
Motivation: Standard one-hot labels hide inter-rater variability and lead to overconfident predictions on noisy/ambiguous medical images; there is a need to incorporate diagnostic uncertainty into training to improve reliability of AI-assisted diagnosis.

Method: Use neural network training dynamics to measure per-sample difficulty during training; aggregate calibrated predictions over training to produce uncertainty-aware pseudo-labels; apply label augmentation to any supervised pipeline; architecture-agnostic; demonstrated on echocardiography classification with multi-view fusion.

Result: The approach achieves superior calibration, selective classification, and multi-view fusion performance compared to specialized baselines on a challenging echocardiography benchmark.

Conclusion: Uncertainty-aware label augmentation guided by NNTD offers a general, effective strategy to inject uncertainty into labels, improving uncertainty estimation and robustness in medical image classification.

Abstract: Computer-aided diagnosis systems must make critical decisions from medical
images that are often noisy, ambiguous, or conflicting, yet today's models are
trained on overly simplistic labels that ignore diagnostic uncertainty. One-hot
labels erase inter-rater variability and force models to make overconfident
predictions, especially when faced with incomplete or artifact-laden inputs. We
address this gap by introducing a novel framework that brings uncertainty back
into the label space. Our method leverages neural network training dynamics
(NNTD) to assess the inherent difficulty of each training sample. By
aggregating and calibrating model predictions during training, we generate
uncertainty-aware pseudo-labels that reflect the ambiguity encountered during
learning. This label augmentation approach is architecture-agnostic and can be
applied to any supervised learning pipeline to enhance uncertainty estimation
and robustness. We validate our approach on a challenging echocardiography
classification benchmark, demonstrating superior performance over specialized
baselines in calibration, selective classification, and multi-view fusion.

</details>


### [101] [LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio](https://arxiv.org/abs/2509.11811)
*Mehwish Mehmood,Shahzaib Iqbal,Tariq Mahmood Khan,Ivor Spence,Muhammad Fahim*

Main category: cs.CV

TL;DR: LFRA-Net is a lightweight encoder-decoder network for retinal vessel segmentation that uses focal modulation attention at the bottleneck and region-aware attention in selective skip connections. It achieves competitive Dice/Jaccard scores on DRIVE, STARE, and CHASE_DB with only 0.17M parameters, 0.66 MB memory, and 10.50 GFLOPs, offering a strong accuracy-efficiency balance for resource-limited, real-time clinical applications; code is available on GitHub.


<details>
  <summary>Details</summary>
Motivation: The abstract addresses the need for accurate retinal vessel segmentation while operating under constrained computational resources. Tiny vessel detection is challenging and existing deep models are often costly, hindering real-time, low-resource deployment in clinical settings.

Method: LFRA-Net introduces (1) focal modulation attention at the encoder-decoder bottleneck to enhance global-local feature interactions, and (2) region-aware attention in selective skip connections to improve regional focus. The network is designed to be lightweight with a total of 0.17 million parameters, low memory footprint (0.66 MB), and 10.50 GFLOPs, validated on DRIVE, STARE, and CHASE_DB.

Result: On DRIVE, STARE, and CHASE_DB, LFRA-Net achieved Dice scores of 84.28%, 88.44%, and 85.50%, and Jaccard indices of 72.86%, 79.31%, and 74.70%, respectively, outperforming many state-of-the-art models while maintaining a very lightweight profile.

Conclusion: LFRA-Net provides a favorable accuracy-to-computation trade-off, making it suitable for real-time retinal vessel segmentation in resource-limited clinical environments. The authors release code for public use, enabling reproducibility and potential adoption in practical settings.

Abstract: Retinal vessel segmentation is critical for the early diagnosis of
vision-threatening and systemic diseases, especially in real-world clinical
settings with limited computational resources. Although significant
improvements have been made in deep learning-based segmentation methods,
current models still face challenges in extracting tiny vessels and suffer from
high computational costs. In this study, we present LFRA-Net by incorporating
focal modulation attention at the encoder-decoder bottleneck and region-aware
attention in the selective skip connections. LFRA-Net is a lightweight network
optimized for precise and effective retinal vascular segmentation. It enhances
feature representation and regional focus by efficiently capturing local and
global dependencies. LFRA-Net outperformed many state-of-the-art models while
maintaining lightweight characteristics with only 0.17 million parameters, 0.66
MB memory size, and 10.50 GFLOPs. We validated it on three publicly available
datasets: DRIVE, STARE, and CHASE\_DB. It performed better in terms of Dice
score (84.28\%, 88.44\%, and 85.50\%) and Jaccard index (72.86\%, 79.31\%, and
74.70\%) on the DRIVE, STARE, and CHASE\_DB datasets, respectively. LFRA-Net
provides an ideal ratio between segmentation accuracy and computational cost
compared to existing deep learning methods, which makes it suitable for
real-time clinical applications in areas with limited resources. The code can
be found at https://github.com/Mehwish4593/LFRA-Net.

</details>


### [102] [SpecVLM: Fast Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.11815)
*Haiduo Huang,Fuwei Yang,Zhenhua Liu,Xuanwu Yin,Dong Li,Pengju Ren,Emad Barsoum*

Main category: cs.CV

TL;DR: SpecVLM enables practical speculative decoding for vision-language models, delivering 2.5–2.9x end-to-end speedups (within 5 epochs) with online distillation and an elastic visual compressor, while preserving lossless decoding compared to full autoregressive inference.


<details>
  <summary>Details</summary>
Motivation: Autoregressive VLMs have heavy compute and memory costs, especially in the prefill stage where visual tokens scale with image resolution and video length. There is a need for a practical speculative decoding framework for VLMs that reduces compute/memory (including KV cache) without offline distillation data and without sacrificing the model's output distribution.

Method: Establish EagleVLM as a strong EAGLE-2–style speculative-decoding baseline for VLMs. Introduce an elastic visual compressor that adaptively selects among pruning, pooling, convolution, and resampler primitives to balance FLOPs/parameters and accuracy per input. Propose an online-logit distillation protocol that trains a draft model with on-the-fly teacher logits and penultimate features using a combined cross-entropy and Smooth L1 objective, removing the need for offline corpora. Demonstrate a training-time scaling effect: longer online training increases the draft model’s average accepted length and speculative efficiency.

Result: EagleVLM achieves 1.5–2.3x end-to-end speedups over full autoregressive inference. SpecVLM adds further acceleration, achieving 2.5–2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU, consistently across resolutions and task difficulties, while preserving the target model’s output distribution (lossless decoding).

Conclusion: SpecVLM provides a practical blueprint for fast speculative decoding in vision–language models by combining an adaptive visual compressor with online distillation, yielding substantial speedups without compromising output quality; code is released for reproduction.

Abstract: Speculative decoding is a powerful way to accelerate autoregressive large
language models (LLMs), but directly porting it to vision-language models
(VLMs) faces unique systems constraints: the prefill stage is dominated by
visual tokens whose count scales with image resolution and video length,
inflating both compute and memory, especially the key-value (KV) cache. We
study speculative decoding for VLMs and introduce SpecVLM, a practical system
that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering
1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)
further accelerates VLM inference with an elastic visual compressor that
adaptively selects among pruning, pooling, convolution, and resampler
primitives to balance FLOPs/parameters and accuracy per input. To avoid costly
offline distillation corpora, we propose an online-logit distillation protocol
that trains the draft model with on-the-fly teacher logits and penultimate
features using a combined cross-entropy and Smooth L1 objective, eliminating
storage and preprocessing while remaining compute-efficient. This protocol
reveals a training-time scaling effect: longer online training monotonically
increases the draft model's average accepted length, improving speculative
efficiency. Empirically, SpecVLM achieves additional acceleration, culminating
in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,
consistently over resolutions and task difficulties, while preserving the
target model's output distribution (lossless decoding). Our code is available
at https://github.com/haiduo/SpecVLM.

</details>


### [103] [MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic Segmentation](https://arxiv.org/abs/2509.11817)
*Liying Wang,Xiaoli Zhang,Chuanmin Jia,Siwei Ma*

Main category: cs.CV

TL;DR: MAFS proposes a unified, parallel network for infrared-visible image fusion and semantic segmentation, featuring a multi-stage Transformer decoder and a dynamic max-min fairness-based task weighting to enable mutual promotion between fusion and segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing semantic-driven fusion methods leverage semantic information for downstream tasks but neglect a macroscopic, reciprocal interaction between pixel-level fusion and cross-modal perception tasks. A unified framework could exploit mutual benefits at the task level to improve both fusion quality and segmentation performance.

Method: MAFS consists of parallel fusion and segmentation sub-networks. It uses a heterogeneous feature fusion strategy to enhance semantic awareness in fusion, and cascades the fusion sub-network with a segmentation backbone to transfer segmentation knowledge to fusion features. A novel multi-stage Transformer decoder aggregates multi-scale fused features efficiently. A dynamic factor based on max-min fairness allocates adaptive weights to the two tasks to stabilize multi-task training.

Result: Experiments show competitive performance against state-of-the-art methods on infrared-visible image fusion and segmentation benchmarks. The authors provide code at the linked GitHub repository.

Conclusion: The work demonstrates that joint fusion and segmentation with cross-task knowledge transfer and adaptive task balancing is effective, highlighting the potential of macroscopic task-level collaboration between pixel-level fusion and high-level perception tasks in multimodal imaging.

Abstract: Infrared-visible image fusion methods aim at generating fused images with
good visual quality and also facilitate the performance of high-level tasks.
Indeed, existing semantic-driven methods have considered semantic information
injection for downstream applications. However, none of them investigates the
potential for reciprocal promotion between pixel-wise image fusion and
cross-modal feature fusion perception tasks from a macroscopic task-level
perspective. To address this limitation, we propose a unified network for image
fusion and semantic segmentation. MAFS is a parallel structure, containing a
fusion sub-network and a segmentation sub-network. On the one hand, We devise a
heterogeneous feature fusion strategy to enhance semantic-aware capabilities
for image fusion. On the other hand, by cascading the fusion sub-network and a
segmentation backbone, segmentation-related knowledge is transferred to promote
feature-level fusion-based segmentation. Within the framework, we design a
novel multi-stage Transformer decoder to aggregate fine-grained multi-scale
fused features efficiently. Additionally, a dynamic factor based on the max-min
fairness allocation principle is introduced to generate adaptive weights of two
tasks and guarantee smooth training in a multi-task manner. Extensive
experiments demonstrate that our approach achieves competitive results compared
with state-of-the-art methods. The code is available at
https://github.com/Abraham-Einstein/MAFS/.

</details>


### [104] [Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network](https://arxiv.org/abs/2509.11838)
*Navid Hashemi,Samuel Sasaki,Diego Manzanas Lopez,Ipek Oguz,Meiyi Ma,Taylor T. Johnson*

Main category: cs.CV

TL;DR: A scalable, architecture-agnostic probabilistic verification framework for semantic segmentation that combines sampling-based reachability with conformal inference to provide provable but less conservative guarantees, validated on multiple large-scale datasets with a GitHub toolbox.


<details>
  <summary>Details</summary>
Motivation: Current probabilistic verification methods for semantic segmentation struggle with high dimensionality and models' complexity, yielding overly conservative guarantees that are impractical for real-world safety-critical applications like medical imaging and autonomous driving.

Method: The framework integrates sampling-based reachability analysis with conformal inference (CI) to obtain probabilistic guarantees for high-dimensional segmentation outputs. It introduces novel strategies to mitigate CI's conservatism in high dimensions and remains architecture-agnostic to scale across large models and datasets.

Result: Empirical evaluation on CamVid, OCTA-500, Lung Segmentation, and Cityscapes shows reliable safety guarantees with substantially tighter bounds than state-of-the-art methods, demonstrating scalability to high-dimensional outputs.

Conclusion: The proposed framework offers scalable, rigorous probabilistic guarantees for semantic segmentation without excessive conservatism and is accompanied by a GitHub toolbox, indicating practical applicability across diverse domains.

Abstract: Semantic segmentation networks (SSNs) play a critical role in domains such as
medical imaging, autonomous driving, and environmental monitoring, where safety
hinges on reliable model behavior under uncertainty. Yet, existing
probabilistic verification approaches struggle to scale with the complexity and
dimensionality of modern segmentation tasks, often yielding guarantees that are
too conservative to be practical. We introduce a probabilistic verification
framework that is both architecture-agnostic and scalable to high-dimensional
outputs. Our approach combines sampling-based reachability analysis with
conformal inference (CI) to deliver provable guarantees while avoiding the
excessive conservatism of prior methods. To counteract CI's limitations in
high-dimensional settings, we propose novel strategies that reduce conservatism
without compromising rigor. Empirical evaluation on large-scale segmentation
models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates
that our method provides reliable safety guarantees while substantially
tightening bounds compared to SOTA. We also provide a toolbox implementing this
technique, available on Github.

</details>


### [105] [Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation](https://arxiv.org/abs/2509.11840)
*Tim Lebailly,Vijay Veerabadran,Satwik Kottur,Karl Ridgeway,Michael Louis Iuzzolino*

Main category: cs.CV

TL;DR: Dense alignment of images with synthetic descriptions generated by vision-language models (VLMs) to enable scalable, high-level semantic supervision for dense tasks like segmentation, achieving strong zero-shot open-vocabulary performance and improved data efficiency.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between the strong high-level reasoning of generative VLMs and the need for dense vision–language alignment for tasks like segmentation, using inexpensive synthetic captions as supervision.

Method: Generate synthetic captions from VLMs for images and use these captions to train representation-learning models for dense alignment, enabling zero-shot open-vocabulary segmentation.

Result: Demonstrates improved performance over prior dense alignment/zero-shot methods on standard open-vocabulary segmentation benchmarks and shows better data efficiency.

Conclusion: Synthetic captions serve as a scalable, cost-effective source of high-level semantic supervision that enables effective dense vision–language alignment and closes the gap between generative VLMs and dense segmentation tasks.

Abstract: Generative vision-language models (VLMs) exhibit strong high-level image
understanding but lack spatially dense alignment between vision and language
modalities, as our findings indicate. Orthogonal to advancements in generative
VLMs, another line of research has focused on representation learning for
vision-language alignment, targeting zero-shot inference for dense tasks like
segmentation. In this work, we bridge these two directions by densely aligning
images with synthetic descriptions generated by VLMs. Synthetic captions are
inexpensive, scalable, and easy to generate, making them an excellent source of
high-level semantic understanding for dense alignment methods. Empirically, our
approach outperforms prior work on standard zero-shot open-vocabulary
segmentation benchmarks/datasets, while also being more data-efficient.

</details>


### [106] [Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2509.11853)
*Yi-Hsin Li,Thomas Sikora,Sebastian Knorr,Måarten Sjöström*

Main category: cs.CV

TL;DR: Segmentation-driven initialization reduces Gaussian count in 3D Gaussian Splatting for sparse-view synthesis, achieving similar/better quality with lower memory and faster training.


<details>
  <summary>Details</summary>
Motivation: 3DGS pipelines struggle in sparse-view settings due to SfM limitations and MVS-based approaches generating too many Gaussians, causing high memory usage. There's a need for efficient initialization and downsampling that preserves fidelity.

Method: Use region-based segmentation to identify structurally significant regions; selectively downsample dense point cloud; adapt Gaussian representation accordingly; integrate into 3DGS pipeline to maintain fidelity while reducing Gaussian count.

Result: Gaussian count reduced up to 50%; PSNR and SSIM comparable or superior; LPIPS slightly degraded; faster training and lower memory footprint; gains consistent across benchmarks in constrained-view scenarios.

Conclusion: Segmentation-driven initialization makes 3D Gaussian Splatting more practical in sparse-view contexts by reducing resource demands with minimal impact on rendering quality.

Abstract: Sparse-view synthesis remains a challenging problem due to the difficulty of
recovering accurate geometry and appearance from limited observations. While
recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time
rendering with competitive quality, existing pipelines often rely on
Structure-from-Motion (SfM) for camera pose estimation, an approach that
struggles in genuinely sparse-view settings. Moreover, several SfM-free methods
replace SfM with multi-view stereo (MVS) models, but generate massive numbers
of 3D Gaussians by back-projecting every pixel into 3D space, leading to high
memory costs. We propose Segmentation-Driven Initialization for Gaussian
Splatting (SDI-GS), a method that mitigates inefficiency by leveraging
region-based segmentation to identify and retain only structurally significant
regions. This enables selective downsampling of the dense point cloud,
preserving scene fidelity while substantially reducing Gaussian count.
Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count
by up to 50% and achieves comparable or superior rendering quality in PSNR and
SSIM, with only marginal degradation in LPIPS. It further enables faster
training and lower memory footprint, advancing the practicality of 3DGS for
constrained-view scenarios.

</details>


### [107] [Bridging Vision Language Models and Symbolic Grounding for Video Question Answering](https://arxiv.org/abs/2509.11862)
*Haodi Ma,Vyom Pathak,Daisy Zhe Wang*

Main category: cs.CV

TL;DR: SG-VLM fuses frozen vision-language models with symbolic scene-graphs for video QA, using prompting and visual localization. It improves causal and temporal reasoning across several benchmarks but gains over strong VLMs remain modest, highlighting promise and limitations of symbolic grounding.


<details>
  <summary>Details</summary>
Motivation: Vision-language models often rely on shallow correlations, leading to weak temporal grounding and limited interpretability in video QA. Symbolic scene graphs offer structured, interpretable object-relations that could complement holistic VLM reasoning.

Method: SG-VLM is a modular framework that couples frozen VLMs with scene-graph grounding. It uses prompting and visual localization to integrate SGs as intermediate grounding signals for VQA, across multiple VLM backbones (e.g., QwenVL, InternVL) and datasets.

Result: Across NExT-QA, iVQA, and ActivityNet-QA, SG-VLM improves causal and temporal reasoning and outperforms prior baselines. However, gains over strong VLMs are limited.

Conclusion: Symbolic grounding via scene graphs shows promise for improving video understanding and interpretability, but current methods have limitations. The work provides guidance for future hybrid VLM-symbolic approaches and emphasizes the need for stronger integration and reasoning capabilities.

Abstract: Video Question Answering (VQA) requires models to reason over spatial,
temporal, and causal cues in videos. Recent vision language models (VLMs)
achieve strong results but often rely on shallow correlations, leading to weak
temporal grounding and limited interpretability. We study symbolic scene graphs
(SGs) as intermediate grounding signals for VQA. SGs provide structured
object-relation representations that complement VLMs holistic reasoning. We
introduce SG-VLM, a modular framework that integrates frozen VLMs with scene
graph grounding via prompting and visual localization. Across three benchmarks
(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM
improves causal and temporal reasoning and outperforms prior baselines, though
gains over strong VLMs are limited. These findings highlight both the promise
and current limitations of symbolic grounding, and offer guidance for future
hybrid VLM-symbolic approaches in video understanding.

</details>


### [108] [Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding](https://arxiv.org/abs/2509.11866)
*Meng Luo,Shengqiong Wu,Liqiang Jing,Tianjie Ju,Li Zheng,Jinxiang Lai,Tianlong Wu,Xinya Du,Jian Li,Siyuan Yan,Jiebo Luo,William Yang Wang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: A hierarchical framework (Dr.V) with a benchmark dataset (Dr.V-Bench) and a satellite video agent (Dr.V-Agent) to diagnose hallucinations in large video models via fine-grained spatial-temporal grounding and cognitive reasoning, improving interpretability and reliability.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in large video models (LVMs) undermine trust and reliability. There is a need for interpretable, fine-grained grounding across perceptive, temporal, and cognitive levels to diagnose and mitigate hallucinations, supported by a dedicated dataset and tooling for robust video understanding.

Method: Introduce Dr.V, comprising Dr.V-Bench and Dr.V-Agent. Dr.V-Bench provides 10k annotated instances from 4,974 videos with detailed spatial-temporal labels. Dr.V-Agent applies fine-grained spatial-temporal grounding at perceptive and temporal levels, followed by cognitive-level reasoning, in a step-by-step pipeline to detect hallucinations in LVMs. Extensive experiments validate its diagnostic effectiveness and enhanced interpretability and reliability; data and code are released.

Result: Demonstrates effectiveness in diagnosing hallucinations and improving interpretability and reliability; offers a practical blueprint for robust video understanding; data and code released at GitHub.

Conclusion: Dr.V offers a practical, interpretable framework for diagnosing and addressing hallucinations in large video models, enabling robust video understanding in real-world scenarios.

Abstract: Recent advancements in large video models (LVMs) have significantly enhance
video understanding. However, these models continue to suffer from
hallucinations, producing content that conflicts with input videos. To address
this issue, we propose Dr.V, a hierarchical framework covering perceptive,
temporal, and cognitive levels to diagnose video hallucination by fine-grained
spatial-temporal grounding. Dr.V comprises of two key components: a benchmark
dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes
10k instances drawn from 4,974 videos spanning diverse tasks, each enriched
with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in
LVMs by systematically applying fine-grained spatial-temporal grounding at the
perceptive and temporal levels, followed by cognitive level reasoning. This
step-by-step pipeline mirrors human-like video comprehension and effectively
identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is
effective in diagnosing hallucination while enhancing interpretability and
reliability, offering a practical blueprint for robust video understanding in
real-world scenarios. All our data and code are available at
https://github.com/Eurekaleo/Dr.V.

</details>


### [109] [Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods](https://arxiv.org/abs/2509.11873)
*Anne Marthe Sophie Ngo Bibinbe,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: MOT-based methods outperform traditional MAT tools for long-term pig tracking, suggesting MOT advances can enhance automated livestock monitoring.


<details>
  <summary>Details</summary>
Motivation: Accurate long-term multi-animal tracking in livestock is needed for continuous behavior analysis and health monitoring, but domain-specific challenges (occlusion, similar appearances, erratic motion, diverse behaviors) hinder performance.

Method: Benchmark study comparing MAT tools (DeepLabCut, idTracker) with MOT-based methods (ByteTrack, DeepSORT, cross-input consistency, Track-Anything, PromptTrack) on a 10-minute pig tracking dataset to evaluate long-term MAT performance.

Result: Overall, MOT approaches outperform traditional MAT tools for long-term pig tracking, indicating that recent MOT techniques can improve the accuracy and reliability of automated livestock tracking.

Conclusion: Adopting contemporary MOT techniques holds promise for improving precision livestock tracking and downstream analysis; MAT tools remain user-friendly but underperform the MOT methods in this study.

Abstract: Precision livestock farming requires advanced monitoring tools to meet the
increasing management needs of the industry. Computer vision systems capable of
long-term multi-animal tracking (MAT) are essential for continuous behavioral
monitoring in livestock production. MAT, a specialized subset of multi-object
tracking (MOT), shares many challenges with MOT, but also faces domain-specific
issues including frequent animal occlusion, highly similar appearances among
animals, erratic motion patterns, and a wide range of behavior types.
  While some existing MAT tools are user-friendly and widely adopted, they
often underperform compared to state-of-the-art MOT methods, which can result
in inaccurate downstream tasks such as behavior analysis, health state
estimation, and related applications. In this study, we benchmarked both MAT
and MOT approaches for long-term tracking of pigs. We compared tools such as
DeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT,
cross-input consistency, and newer approaches like Track-Anything and
PromptTrack.
  All methods were evaluated on a 10-minute pig tracking dataset. Our results
demonstrate that, overall, MOT approaches outperform traditional MAT tools,
even for long-term tracking scenarios. These findings highlight the potential
of recent MOT techniques to enhance the accuracy and reliability of automated
livestock tracking.

</details>


### [110] [Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation](https://arxiv.org/abs/2509.11878)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,K J Joseph*

Main category: cs.CV

TL;DR: Proposes Weighted Prompt Manipulation (WPM) to control diffusion-based poem-to-image generation by reweighting tokens and embeddings, enabling zero-shot customization and more faithful poetry visuals.


<details>
  <summary>Details</summary>
Motivation: Poems invite multiple interpretations; readers bring diverse emotions and backgrounds. There is a need for controllable, semantically faithful image generation from poetic text in a zero-shot setting.

Method: Introduce Weighted Prompt Manipulation (WPM) that adjusts attention weights and text embeddings for selected words during diffusion sampling. Dynamically reweights word importance to amplify or suppress semantics in the generated image. Leverages diffusion models, LLMs (e.g., GPT), and poetry datasets in a zero-shot framework.

Result: The abstract claims that WPM yields semantically richer and more contextually accurate visualizations of poetry and represents the first attempt to integrate weighted prompt manipulation for poetic imagery.

Conclusion: WPM appears to be an effective approach for improving poetic imagery with user-driven customization and could generalize to other artistic domains; future work may explore broader evaluation, interfaces, and dataset expansion.

Abstract: Poetry is an expressive form of art that invites multiple interpretations, as
readers often bring their own emotions, experiences, and cultural backgrounds
into their understanding of a poem. Recognizing this, we aim to generate images
for poems and improve these images in a zero-shot setting, enabling audiences
to modify images as per their requirements. To achieve this, we introduce a
novel Weighted Prompt Manipulation (WPM) technique, which systematically
modifies attention weights and text embeddings within diffusion models. By
dynamically adjusting the importance of specific words, WPM enhances or
suppresses their influence in the final generated image, leading to
semantically richer and more contextually accurate visualizations. Our approach
exploits diffusion models and large language models (LLMs) such as GPT in
conjunction with existing poetry datasets, ensuring a comprehensive and
structured methodology for improved image generation in the literary domain. To
the best of our knowledge, this is the first attempt at integrating weighted
prompt manipulation for enhancing imagery in poetic language.

</details>


### [111] [SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection](https://arxiv.org/abs/2509.11884)
*Zhenni Yu,Li Zhao,Guobao Xiao,Xiaoqin Zhang*

Main category: cs.CV

TL;DR: A SAM-based COD framework, SAM-TTT, combines a Reverse SAM Parameter Configuration Module and a Test-Time Training (T-Visioner) Module to suppress adverse parameters and strengthen beneficial ones, achieving state-of-the-art results on camouflaged object detection.


<details>
  <summary>Details</summary>
Motivation: Existing SAM-based COD approaches focus on enhancing favorable features but neglect adverse parameters that degrade semantic understanding; a train-free mitigation plus test-time training is needed to improve SAM's downstream performance on COD.

Method: Proposes two modules: Reverse SAM Parameter Configuration Module to mitigate adverse parameters in a train-free way; T-Visioner Module to insert Test-Time Training layers into vision tasks, enabling two-layer sequence modeling with linear complexity and expressive hidden states; together, they suppress adverse parameters and reinforce advantageous ones during inference.

Result: Experiments on multiple COD benchmarks show state-of-the-art performance; code will be released.

Conclusion: SAM-TTT enhances SAM's semantic understanding for COD by addressing parameter-level limitations and introducing test-time adaptive layers, establishing a new benchmark and potential generalization to other tasks.

Abstract: This paper introduces a new Segment Anything Model (SAM) that leverages
reverse parameter configuration and test-time training to enhance its
performance on Camouflaged Object Detection (COD), named SAM-TTT. While most
existing SAM-based COD models primarily focus on enhancing SAM by extracting
favorable features and amplifying its advantageous parameters, a crucial gap is
identified: insufficient attention to adverse parameters that impair SAM's
semantic understanding in downstream tasks. To tackle this issue, the Reverse
SAM Parameter Configuration Module is proposed to effectively mitigate the
influence of adverse parameters in a train-free manner by configuring SAM's
parameters. Building on this foundation, the T-Visioner Module is unveiled to
strengthen advantageous parameters by integrating Test-Time Training layers,
originally developed for language tasks, into vision tasks. Test-Time Training
layers represent a new class of sequence modeling layers characterized by
linear complexity and an expressive hidden state. By integrating two modules,
SAM-TTT simultaneously suppresses adverse parameters while reinforcing
advantageous ones, significantly improving SAM's semantic understanding in COD
task. Our experimental results on various COD benchmarks demonstrate that the
proposed approach achieves state-of-the-art performance, setting a new
benchmark in the field. The code will be available at
https://github.com/guobaoxiao/SAM-TTT.

</details>


### [112] [BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation](https://arxiv.org/abs/2509.11885)
*Francis Xiatian Zhang,Emile Mackute,Mohammadreza Kasaei,Kevin Dhaliwal,Robert Thomson,Mohsen Khadem*

Main category: cs.CV

TL;DR: Brea-Depth introduces airway-specific geometric priors into foundation-model adaptation for monocular bronchoscopy depth estimation, using a depth-aware CycleGAN and an airway-structure loss, plus a new Airway Depth Structure Evaluation metric, achieving improved anatomical depth preservation and robust 3D airway reconstructions on ex vivo and open datasets.


<details>
  <summary>Details</summary>
Motivation: Monocular depth estimation in bronchoscopy is crucial for real-time navigation and safety, but current depth foundation models lack anatomical awareness, overfit textures, and struggle with ambiguous cues under poor lighting, leading to weak global airway structure capture.

Method: Integrate airway geometric priors into foundation-model adaptation. Develop a depth-aware CycleGAN to refine translation between real bronchoscopic images and airway geometries from anatomical data. Introduce an airway structure awareness loss to enforce depth consistency within the airway lumen while preserving smooth transitions and structural integrity. Propose Airway Depth Structure Evaluation to assess structural realism.

Result: On an ex vivo human lung dataset and an open bronchoscopic dataset, Brea-Depth outperforms existing methods in anatomical depth preservation and yields more robust, accurate 3D airway reconstructions.

Conclusion: Incorporating anatomical priors into foundation-model adaptation enhances generalization and robustness for monocular bronchoscopy depth estimation, with a new metric to evaluate structural consistency and improved anatomical realism.

Abstract: Monocular depth estimation in bronchoscopy can significantly improve
real-time navigation accuracy and enhance the safety of interventions in
complex, branching airways. Recent advances in depth foundation models have
shown promise for endoscopic scenarios, yet these models often lack anatomical
awareness in bronchoscopy, overfitting to local textures rather than capturing
the global airway structure, particularly under ambiguous depth cues and poor
lighting. To address this, we propose Brea-Depth, a novel framework that
integrates airway-specific geometric priors into foundation model adaptation
for bronchoscopic depth estimation. Our method introduces a depth-aware
CycleGAN, refining the translation between real bronchoscopic images and airway
geometries from anatomical data, effectively bridging the domain gap. In
addition, we introduce an airway structure awareness loss to enforce depth
consistency within the airway lumen while preserving smooth transitions and
structural integrity. By incorporating anatomical priors, Brea-Depth enhances
model generalization and yields more robust, accurate 3D airway
reconstructions. To assess anatomical realism, we introduce Airway Depth
Structure Evaluation, a new metric for structural consistency. We validate
BREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic
dataset, where it outperforms existing methods in anatomical depth
preservation.

</details>


### [113] [Logit Mixture Outlier Exposure for Fine-grained Out-of-Distribution Detection](https://arxiv.org/abs/2509.11892)
*Akito Shinohara,Kohei Fukuda,Hiroaki Aizawa*

Main category: cs.CV

TL;DR: Logit-space interpolation between in-distribution and out-of-distribution data to smooth logits and improve OOD detection, with a consistency regularization between logit-space and input-space mixing; gains especially for near-boundary OOD, including fine-grained tasks.


<details>
  <summary>Details</summary>
Motivation: Out-of-distribution (OOD) detection is critical for robustness and better generalization. Methods like Outlier Exposure (OE) and Mixture Outlier Exposure (MOE) help, but models still struggle to learn clear relationships between classes and to separate in-distribution (ID) from OOD data. The logit space exhibits clearer class-wise separation than input or feature spaces, motivating a logit-focused approach to enhance OOD robustness.

Method: Introduce a linear interpolation (mixing) mechanism in the logit space that blends ID and OOD data logits. Train with these logit-mixed targets to smooth inter-class logits. Additionally, enforce a consistency constraint so that logits from logit-space mixing align with logits produced by mixing in the input space.

Result: Empirical evaluations show that logit-space mixing reduces abrupt fluctuations of outputs near decision boundaries, yielding smoother, more reliable separation between ID and OOD data. The approach improves OOD detection performance, including on fine-grained OOD tasks.

Conclusion: A simple, effective augmentation in the logit space that complements existing OE/MOE methods. Logit-space mixing stabilizes boundary behavior and enhances OOD robustness and generalization, with validated improvements on challenging (fine-grained) OOD scenarios.

Abstract: The ability to detect out-of-distribution data is essential not only for
ensuring robustness against unknown or unexpected input data but also for
improving the generalization performance of the model. Among various
out-of-distribution detection methods, Outlier Exposure and Mixture Outlier
Exposure are promising approaches that enhance out-of-distribution detection
performance by exposing the outlier data during training. However, even with
these sophisticated techniques, it remains challenging for models to learn the
relationships between classes effectively and to distinguish data sampling from
in-distribution and out-of-distribution clearly. Therefore, we focus on the
logit space, where the properties between class-wise distributions are
distinctly separated from those in the input or feature spaces. Specifically,
we propose a linear interpolation technique in the logit space that mixes
in-distribution and out-of-distribution data to facilitate smoothing logits
between classes and improve the out-of-distribution detection performance,
particularly for out-of-distribution data that lie close to the in-distribution
data. Additionally, we enforce consistency between the logits obtained through
mixing in the logit space and those generated via mixing in the input space.
Our experiments demonstrate that our logit-space mixing technique reduces the
abrupt fluctuations in the model outputs near the decision boundaries,
resulting in smoother and more reliable separation between in-distribution and
out-of-distribution data. Furthermore, we evaluate the effectiveness of the
proposed method on a fine-grained out-of-distribution detection task.

</details>


### [114] [Integrating Prior Observations for Incremental 3D Scene Graph Prediction](https://arxiv.org/abs/2509.11895)
*Marian Renz,Felix Igelbrink,Martin Atzmueller*

Main category: cs.CV

TL;DR: Incremental 3D semantic scene graph prediction using a multi-modal, heterogeneous GNN that integrates priors and semantic embeddings without requiring full scene reconstructions.


<details>
  <summary>Details</summary>
Motivation: Current 3DSSG methods rely heavily on sensor data and complete scene reconstructions, hindering deployment in real-world incremental settings. There's a need to leverage rich, multi-modal information to improve robustness and scalability.

Method: A novel heterogeneous graph model for incremental 3DSSG prediction that incorporates multi-modal information (e.g., semantic embeddings like CLIP and prior observations) into multi-layer message passing. It does not rely on specialized modules or full scene reconstructions and supports global/local representations.

Result: Evaluated on the 3DSSG dataset; integrating multi-modal information with GNNs yields scalable, generalizable performance for complex environments; code will be released.

Conclusion: The proposed framework provides a flexible, incremental, multi-modal GNN approach for 3DSSG that can operate without full reconstructions and utilize priors and semantic cues to improve predictive capabilities in real-world settings.

Abstract: 3D semantic scene graphs (3DSSG) provide compact structured representations
of environments by explicitly modeling objects, attributes, and relationships.
While 3DSSGs have shown promise in robotics and embodied AI, many existing
methods rely mainly on sensor data, not integrating further information from
semantically rich environments. Additionally, most methods assume access to
complete scene reconstructions, limiting their applicability in real-world,
incremental settings. This paper introduces a novel heterogeneous graph model
for incremental 3DSSG prediction that integrates additional, multi-modal
information, such as prior observations, directly into the message-passing
process. Utilizing multiple layers, the model flexibly incorporates global and
local scene representations without requiring specialized modules or full scene
reconstructions. We evaluate our approach on the 3DSSG dataset, showing that
GNNs enriched with multi-modal information such as semantic embeddings (e.g.,
CLIP) and prior observations offer a scalable and generalizable solution for
complex, real-world environments. The full source code of the presented
architecture will be made available at
https://github.com/m4renz/incremental-scene-graph-prediction.

</details>


### [115] [NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition](https://arxiv.org/abs/2509.11916)
*Zilin Li,Weiwei Xu,Xuanqi Zhao,Yiran Zhu*

Main category: cs.CV

TL;DR: Cross-modal FER using EEG-informed priors: a frozen 5x5 valence/arousal prototype grid plus a depression-inspired geometric prior distill into a pixel-based FER model, improving cross-dataset robustness with lightweight regularizers.


<details>
  <summary>Details</summary>
Motivation: Pixel-only FER models overfit to facial appearance, hindering cross-dataset generalization; injecting brain-informed priors through teacher-student distillation can regularize representations.

Method: Train a EEG-based teacher on DREAMER (MAHNOB-HCI unlabeled support) to produce a frozen 5x5 V/A prototype grid; train a ResNet-18/50 FER student on FERPlus with cross-entropy and knowledge distillation, plus Proto-KD (cosine alignment to prototypes) and D-Geo (soft geometric shaping aligned with depression literature); evaluate within FERPlus and cross-dataset AffectNet-mini (and CK+); compare 5x5 vs denser grids; ablation studies.

Result: Consistent performance gains attributed to use of prototypes and D-Geo; 5x5 grid provides stability; improvements observed in both within-domain and cross-domain evaluations; simple, deployable, no additional architectural complexity.

Conclusion: NeuroGaze-Distill demonstrates that static EEG-informed priors can be effectively transferred into an image-only FER model, yielding improved generalization with lightweight regularizers; the 5x5 prototype grid is preferable for stability, and the approach avoids EEG-face pairing at deployment.

Abstract: Facial emotion recognition (FER) models trained only on pixels often fail to
generalize across datasets because facial appearance is an indirect and biased
proxy for underlying affect. We present NeuroGaze-Distill, a cross-modal
distillation framework that transfers brain-informed priors into an image-only
FER student via static Valence/Arousal (V/A) prototypes and a
depression-inspired geometric prior (D-Geo). A teacher trained on EEG
topographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a
consolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face
pairing and no non-visual signals at deployment are required. The student
(ResNet-18/50) is trained on FERPlus with conventional CE/KD and two
lightweight regularizers: (i) Proto-KD (cosine) aligns student features to the
static prototypes; (ii) D-Geo softly shapes the embedding geometry in line with
affective findings often reported in depression research (e.g., anhedonia-like
contraction in high-valence regions). We evaluate both within-domain (FERPlus
validation) and cross-dataset protocols (AffectNet-mini; optional CK+),
reporting standard 8-way scores alongside present-only Macro-F1 and balanced
accuracy to fairly handle label-set mismatch. Ablations attribute consistent
gains to prototypes and D-Geo, and favor 5x5 over denser grids for stability.
The method is simple, deployable, and improves robustness without architectural
complexity.

</details>


### [116] [Enriched text-guided variational multimodal knowledge distillation network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid artery MRI](https://arxiv.org/abs/2509.11924)
*Bo Cao,Fan Yu,Mengmeng Feng,SenHao Zhang,Xin Meng,Yue Zhang,Zhen Qian,Jie Lu*

Main category: cs.CV

TL;DR: Presents VMD, a multimodal framework that uses variation inference and multimodal knowledge distillation to diagnose carotid plaque vulnerability from 3D MRI by incorporating radiologists' domain knowledge and cross-modality priors, enabling better accuracy with limited annotations.


<details>
  <summary>Details</summary>
Motivation: Diagnosing plaque vulnerability from carotid 3D MRI is challenging; clinicians rely on multimodal information and expert knowledge. The work aims to mimic this by leveraging radiologists' domain knowledge and cross-modality priors to improve diagnostic performance with limited annotations.

Method: Variation inference and Multimodal knowledge Distillation (VMD). The approach distills radiologist expertise and cross-modality priors from imaging data and radiology reports into a diagnostic network, designed to work effectively with scarce image annotations, validated on an in-house dataset.

Result: Experiments on an in-house dataset show that VMD effectively harnesses cross-modality priors and limited annotations to improve diagnostic accuracy for unannotated 3D MRI carotid images, verifying the proposed strategy's effectiveness.

Conclusion: VMD demonstrates that incorporating radiologist-domain knowledge and cross-modality priors can enhance multimodal diagnosis of carotid plaque vulnerability from 3D MRI with limited labels, suggesting potential applicability to broader multimodal medical imaging tasks and pointing to future work such as external validation and inclusion of additional modalities.

Abstract: Multimodal learning has attracted much attention in recent years due to its
ability to effectively utilize data features from a variety of different
modalities. Diagnosing the vulnerability of atherosclerotic plaques directly
from carotid 3D MRI images is relatively challenging for both radiologists and
conventional 3D vision networks. In clinical practice, radiologists assess
patient conditions using a multimodal approach that incorporates various
imaging modalities and domain-specific expertise, paving the way for the
creation of multimodal diagnostic networks. In this paper, we have developed an
effective strategy to leverage radiologists' domain knowledge to automate the
diagnosis of carotid plaque vulnerability through Variation inference and
Multimodal knowledge Distillation (VMD). This method excels in harnessing
cross-modality prior knowledge from limited image annotations and radiology
reports within training data, thereby enhancing the diagnostic network's
accuracy for unannotated 3D MRI images. We conducted in-depth experiments on
the dataset collected in-house and verified the effectiveness of the VMD
strategy we proposed.

</details>


### [117] [Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization](https://arxiv.org/abs/2509.11926)
*Xue Zhang,Bingshuo Hu,Gene Cheung*

Main category: cs.CV

TL;DR: A graph-based, interpretable interpolation framework that initializes a graph from a known interpolator, learns perturbations to augment it, and uses unrolled Douglas-Rachford iterations for a lightweight NN achieving state-of-the-art image interpolation with far fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Overcome SGD-driven local minima in DNNs for image interpolation by integrating principled graph-based priors and efficient optimization into an interpretable model, improving performance and efficiency.

Method: 1) Map a pseudo-linear interpolator Theta to a directed graph filter solving a MAP problem with a graph shift variation prior. 2) Initialize graph adjacency A from Theta. 3) Learn perturbation matrices P and P(2) to augment A from data. 4) Solve restoration with Douglas-Rachford iterations. 5) Unroll DR iterations into a lightweight neural net.

Result: Achieves state-of-the-art image interpolation results while drastically reducing network parameters.

Conclusion: Combining graph-based priors with DR-based optimization and unrolled networks yields high-performance, parameter-efficient image interpolation and provides an interpretable architecture.

Abstract: Conventional deep neural nets (DNNs) initialize network parameters at random
and then optimize each one via stochastic gradient descent (SGD), resulting in
substantial risk of poor-performing local minima.Focusing on the image
interpolation problem and leveraging a recent theorem that maps a
(pseudo-)linear interpolator {\Theta} to a directed graph filter that is a
solution to a MAP problem regularized with a graph shift variation (GSV) prior,
we first initialize a directed graph adjacency matrix A based on a known
interpolator {\Theta}, establishing a baseline performance.Then, towards
further gain, we learn perturbation matrices P and P(2) from data to augment A,
whose restoration effects are implemented via Douglas-Rachford (DR) iterations,
which we unroll into a lightweight interpretable neural net.Experimental
results demonstrate state-of-the-art image interpolation results, while
drastically reducing network parameters.

</details>


### [118] [Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360° Videos](https://arxiv.org/abs/2509.11948)
*Mahmoud Z. A. Wahba,Sara Baldoni,Federica Battisti*

Main category: cs.CV

TL;DR: Sphere-GAN uses a GAN with spherical convolutions to predict saliency maps for 360° videos, outperforming existing 360° saliency models.


<details>
  <summary>Details</summary>
Motivation: Saliency estimation for 360° content is underexplored compared to 2D content, yet essential for efficient processing and transmission of immersive media.

Method: A Generative Adversarial Network (Sphere-GAN) that employs spherical convolutions to detect saliency in 360° videos; trained and evaluated on a public 360° video saliency dataset.

Result: Sphere-GAN outperforms state-of-the-art models in accurately predicting saliency maps on a public dataset.

Conclusion: Sphere-GAN is an effective approach for 360° saliency estimation and can facilitate improved processing and transmission of immersive content.

Abstract: The recent success of immersive applications is pushing the research
community to define new approaches to process 360{\deg} images and videos and
optimize their transmission. Among these, saliency estimation provides a
powerful tool that can be used to identify visually relevant areas and,
consequently, adapt processing algorithms. Although saliency estimation has
been widely investigated for 2D content, very few algorithms have been proposed
for 360{\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN,
a saliency detection model for 360{\deg} videos that leverages a Generative
Adversarial Network with spherical convolutions. Extensive experiments were
conducted using a public 360{\deg} video saliency dataset, and the results
demonstrate that Sphere-GAN outperforms state-of-the-art models in accurately
predicting saliency maps.

</details>


### [119] [CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and Optical Land Cover Segmentation](https://arxiv.org/abs/2509.11952)
*Debopom Sutradhar,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sheikh Izzal Azid,Sami Azam*

Main category: cs.CV

TL;DR: A dual-encoder, cross-modality attention network (CLAIRE) fuses optical and SAR features for land-cover segmentation, addressing class imbalance with a Rare-Instance Focal-Tversky loss (RIFT) and adding a sample-specific interpretability module; achieves strong results across multiple SAR-Optical benchmarks and maintains robustness under cloud coverage.


<details>
  <summary>Details</summary>
Motivation: Land-cover classification from multimodal satellite imagery faces complexity, spectral/textural similarity across classes, and severe data imbalance; there is a need for effective cross-modal fusion and interpretable, robust performance.

Method: Two encoders for optical and SAR features, cross-modality attention-fusion CLAIRE; RIFT loss combining Weighted Focal Loss and Tversky Loss; metric-driven reasoning module Phi-3 generates expert-level justifications; evaluation on WHU-OPT-SAR, OpenEarthMap-SAR, PIE-RGB-SAR datasets.

Result: mIoU and OA metrics: WHU-OPT-SAR: 56.02% mIoU, 84.56% OA; OpenEarthMap-SAR: 59.89% mIoU, 73.92% OA; PIE-RGB-SAR under clouds: 86.86% mIoU, 94.58% OA; demonstrates generalization and robustness; interpretable explanations via Phi-3.

Conclusion: CLAIRE with RIFT and Phi-3 improves multimodal land-cover segmentation accuracy and transparency, mitigates class imbalance and cloud obstruction effects, and generalizes across datasets; cross-modality fusion is beneficial for capturing diverse land patterns.

Abstract: Accurate land cover classification from satellite imagery is crucial in
environmental monitoring and sustainable resource management. However, it
remains challenging due to the complexity of natural landscapes, the visual
similarity between classes, and the significant class imbalance in the
available datasets. To address these issues, we propose a dual encoder
architecture that independently extracts modality-specific features from
optical and Synthetic Aperture Radar (SAR) imagery, which are then fused using
a cross-modality attention-fusion module named Cross-modality Land cover
segmentation with Attention and Imbalance-aware Reasoning-Enhanced Explanations
(CLAIRE). This fusion mechanism highlights complementary spatial and textural
features, enabling the network to better capture detailed and diverse land
cover patterns. We incorporate a hybrid loss function that utilizes Weighted
Focal Loss and Tversky Loss named RIFT (Rare-Instance Focal-Tversky) to address
class imbalance and improve segmentation performance across underrepresented
categories. Our model achieves competitive performance across multiple
benchmarks: a mean Intersection over Union (mIoU) of 56.02% and Overall
Accuracy (OA) of 84.56% on the WHU-OPT-SAR dataset; strong generalization with
a mIoU of 59.89% and OA of 73.92% on the OpenEarthMap-SAR dataset; and
remarkable robustness under cloud-obstructed conditions, achieving an mIoU of
86.86% and OA of 94.58% on the PIE-RGB-SAR dataset. Additionally, we introduce
a metric-driven reasoning module generated by a Small Language Model (Phi-3),
which generates expert-level, sample-specific justifications for model
predictions, thereby enhancing transparency and interpretability.

</details>


### [120] [Lost in Embeddings: Information Loss in Vision-Language Models](https://arxiv.org/abs/2509.11986)
*Wenyan Li,Raphael Tang,Chengzu Li,Caiqi Zhang,Ivan Vulić,Anders Søgaard*

Main category: cs.CV

TL;DR: Two complementary analyses quantify information loss in the vision-to-language projection: kNN-preservation and patch-level embedding reconstruction, revealing substantial distortion (40–60% kNN divergence) and that high-loss patches predict QA difficulties.


<details>
  <summary>Details</summary>
Motivation: To understand how the connector between the pretrained vision encoder and the language model embedding space may erode semantic/visual information, and how this loss impacts capabilities, especially retrieval and visually grounded QA, which has been understudied.

Method: 1) assess semantic preservation by comparing kNN structure of image representations before vs after projection; 2) reconstruct projected visual embeddings from the post-projection representations to localize loss at patch level; perform experiments and correlate findings with retrieval performance and visually grounded QA tasks.

Result: Connectors substantially distort the local geometry of visual representations; kNN divergence after projection ranges ~40–60%, correlating with degradation in retrieval performance. Patch-level embedding reconstruction yields interpretable insights for model behavior on visually grounded QA, with high-information-loss areas reliably predicting instances where models struggle.

Conclusion: Projection-induced information loss is measurable and predictive of downstream task performance. The proposed diagnostics quantify and localize this loss, offering guidance for understanding and potentially mitigating fusion bottlenecks in vision-language models.

Abstract: Vision--language models (VLMs) often process visual inputs through a
pretrained vision encoder, followed by a projection into the language model's
embedding space via a connector component. While crucial for modality fusion,
the potential information loss induced by this projection step and its direct
impact on model capabilities remain understudied. We introduce two
complementary approaches to examine and quantify this loss by analyzing the
latent representation space. First, we evaluate semantic information
preservation by analyzing changes in k-nearest neighbor relationships between
image representations, before and after projection. Second, we directly measure
information loss by reconstructing visual embeddings from the projected
representation, localizing loss at an image patch level. Experiments reveal
that connectors substantially distort the local geometry of visual
representations, with k-nearest neighbors diverging by 40--60\%
post-projection, correlating with degradation in retrieval performance. The
patch-level embedding reconstruction provides interpretable insights for model
behavior on visually grounded question-answering tasks, finding that areas of
high information loss reliably predict instances where models struggle.

</details>


### [121] [Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness](https://arxiv.org/abs/2509.12024)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wen,Le Ku,Daheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: SCORE offers a principled, provably-grounded framework for erasing concepts from diffusion models by treating erasure as an adversarial independence problem and minimizing mutual information between the erased concept and outputs, with convergence guarantees and strong empirical gains across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: There is rising demand to remove sensitive or harmful concepts (e.g., NSFW content, private individuals, artistic styles) from diffusion models without sacrificing generative quality. Prior heuristic methods lack formal guarantees, raising privacy, fairness, and safety concerns.

Method: Formulate concept erasure as an adversarial independence optimization that minimizes mutual information between the erased concept and generated outputs. Combines adversarial optimization, trajectory consistency, and saliency-driven fine-tuning. Provides formal proofs of convergence and upper bounds on residual leakage. Empirically evaluated on Stable Diffusion and FLUX across four benchmarks.

Result: SCORE consistently outperforms state-of-the-art methods (EraseAnything, ANT, MACE, ESD, UCE) with up to 12.5% higher erasure efficacy while maintaining comparable or superior image quality across object erasure, NSFW removal, celebrity face suppression, and artistic style unlearning.

Conclusion: SCORE sets a new standard for secure and robust concept erasure in diffusion models by delivering provable guarantees on erasure (mutual information minimization and convergence) and strong empirical performance across diverse tasks.

Abstract: Diffusion models have achieved unprecedented success in image generation but
pose increasing risks in terms of privacy, fairness, and security. A growing
demand exists to \emph{erase} sensitive or harmful concepts (e.g., NSFW
content, private individuals, artistic styles) from these models while
preserving their overall generative capabilities. We introduce \textbf{SCORE}
(Secure and Concept-Oriented Robust Erasure), a novel framework for robust
concept removal in diffusion models. SCORE formulates concept erasure as an
\emph{adversarial independence} problem, theoretically guaranteeing that the
model's outputs become statistically independent of the erased concept. Unlike
prior heuristic methods, SCORE minimizes the mutual information between a
target concept and generated outputs, yielding provable erasure guarantees. We
provide formal proofs establishing convergence properties and derive upper
bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable
Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW
removal, celebrity face suppression, and artistic style unlearning. SCORE
consistently outperforms state-of-the-art methods including EraseAnything, ANT,
MACE, ESD, and UCE, achieving up to \textbf{12.5\%} higher erasure efficacy
while maintaining comparable or superior image quality. By integrating
adversarial optimization, trajectory consistency, and saliency-driven
fine-tuning, SCORE sets a new standard for secure and robust concept erasure in
diffusion models.

</details>


### [122] [RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration](https://arxiv.org/abs/2509.12039)
*Zilong Zhang,Chujie Qin,Chunle Guo,Yong Zhang,Chao Xue,Ming-Ming Cheng,Chongyi Li*

Main category: cs.CV

TL;DR: RAM++ is a two-stage, content-oriented image restoration framework that fuses high-level semantic understanding with low-level texture generation to handle extreme degradations, with AdaSAM, MAC, and RFR to ensure robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Degradation-focused methods struggle under degradations strongly coupled with image structure; existing models often show unbalanced task performance, overfit to seen degradations, and weak generalization to unseen mixtures.

Method: A two-stage framework: (1) AdaSAM pretraining uses pixel-level masks on semantically rich/ textured regions to learn generative and image priors; (2) MAC selectively tunes layers with higher contributions to bridge masked pretraining and full-image fine-tuning while preserving priors; (3) RFR leverages DINOv2's semantically consistent and degradation-invariant representations and efficient feature fusion for faithful, semantically coherent restoration.

Result: RAM++ claims robust, well-balanced, and state-of-the-art performance across seen, unseen, extreme, and mixed degradations.

Conclusion: RAM++ offers a robust, generalizable all-in-one restoration approach and will release code and model at the provided GitHub link.

Abstract: This work presents Robust Representation Learning via Adaptive Mask (RAM++),
a two-stage framework for all-in-one image restoration. RAM++ integrates
high-level semantic understanding with low-level texture generation to achieve
content-oriented robust restoration. It addresses the limitations of existing
degradation-oriented methods in extreme scenarios (e.g., degradations strongly
coupled with image structures). RAM++ also mitigates common challenges such as
unbalanced performance across tasks, overfitting to seen degradations, and weak
generalization to unseen ones through three key designs: 1) Adaptive
Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level
masks to semantically rich and textured regions. This design enables the
network to learn both generative priors and image content priors from various
degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning
strategy that adjusts the layers with higher contributions to bridge the
integrity gap between masked pretraining and full-image fine-tuning while
retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy
that leverages DINOv2's semantically consistent and degradation-invariant
representations, together with efficient feature fusion, to achieve faithful
and semantically coherent restoration. With these designs, RAM++ achieves
robust, well-balanced, and state-of-the-art performance across seen, unseen,
extreme, and mixed degradations. Our code and model will be released at
https://github.com/DragonisCV/RAM

</details>


### [123] [Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing](https://arxiv.org/abs/2509.12040)
*Bingyu Li,Haocheng Dong,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: Proposes OVRSISBench for standard open-vocabulary RS segmentation and introduces RSKT-Seg, a three-component framework (RS-CMA, RS-Fusion, RS-Transfer) for improved accuracy and efficiency; achieves +3.8 mIoU and +5.9 mACC with 2x faster inference; code released.


<details>
  <summary>Details</summary>
Motivation: There is no unified benchmark for open-vocabulary RS segmentation and a domain gap between natural and RS images, hindering transfer of OVS methods to RS. A standardized benchmark and a tailored RS method are needed.

Method: 1) Build OVRSISBench on widely-used RS segmentation datasets to enable consistent evaluation. 2) Benchmark existing OVS/OVRSIS models to identify limitations on RS data. 3) Propose RSKT-Seg with (a) RS-CMA: multi-directional cosine similarities to capture rotation-invariant cues; (b) RS-Fusion: lightweight transformer with dimensionality-reduction for joint spatial-semantic modeling; (c) RS-Transfer: pre-training knowledge injection and enhanced upsampling for domain adaptation.

Result: On OVRSISBench, RSKT-Seg consistently outperforms strong OVS baselines by about 3.8 mIoU and 5.9 mACC, and achieves approximately 2x faster inference due to efficient aggregation.

Conclusion: The standardized OVRSIS benchmark enables fair evaluation of open-vocabulary RS methods, and RSKT-Seg provides an effective, efficient solution for RS open-vocabulary segmentation with strong empirical gains. Code is publicly available.

Abstract: Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task
that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)
domain, remains underexplored due to the absence of a unified evaluation
benchmark and the domain gap between natural and RS images. To bridge these
gaps, we first establish a standardized OVRSIS benchmark (\textbf{OVRSISBench})
based on widely-used RS segmentation datasets, enabling consistent evaluation
across methods. Using this benchmark, we comprehensively evaluate several
representative OVS/OVRSIS models and reveal their limitations when directly
applied to remote sensing scenarios. Building on these insights, we propose
\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for
remote sensing. RSKT-Seg integrates three key components: (1) a
Multi-Directional Cost Map Aggregation (RS-CMA) module that captures
rotation-invariant visual cues by computing vision-language cosine similarities
across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)
transformer, which jointly models spatial and semantic dependencies with a
lightweight dimensionality reduction strategy; and (3) a Remote Sensing
Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and
facilitates domain adaptation via enhanced upsampling. Extensive experiments on
the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines
by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through
efficient aggregation. Our code is
\href{https://github.com/LiBingyu01/RSKT-Seg}{\textcolor{blue}{here}}.

</details>


### [124] [Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking](https://arxiv.org/abs/2509.12046)
*Zirui Zheng,Takashi Isobe,Tong Shen,Xu Jia,Jianbin Zhao,Xiaomin Li,Mengmeng Ge,Baolu Li,Qinghe Wang,Dong Li,Dong Zhou,Yunzhi Zhuge,Huchuan Lu,Emad Barsoum*

Main category: cs.CV

TL;DR: Structured Masking for AR-based Layout-to-Image (SMARLI) integrates layout constraints into autoregressive generation and uses GRPO-based post-training to improve layout accuracy without hurting quality.


<details>
  <summary>Details</summary>
Motivation: AR models excel in image generation but struggle with sparse, entangled layout-conditioned generation; there is a need for reliable mechanisms to incorporate spatial layout constraints.

Method: Apply a structured masking strategy in attention to govern interactions among the global prompt, layout tokens, and image tokens; prevent mis-associations while injecting layout constraints. Augment this with a Group Relative Policy Optimization (GRPO) post-training scheme using specialized layout reward functions for next-set-based AR models.

Result: Empirical results show SMARLI can integrate layout tokens with text and image tokens without compromising generation quality and achieves superior layout-aware control while preserving model simplicity and efficiency.

Conclusion: SMARLI provides an effective, efficient approach for layout-conditioned AR-based image generation, achieving better alignment between layout and content with minimal disruption to AR generation strengths.

Abstract: While autoregressive (AR) models have demonstrated remarkable success in
image generation, extending them to layout-conditioned generation remains
challenging due to the sparse nature of layout conditions and the risk of
feature entanglement. We present Structured Masking for AR-based
Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that
effectively integrates spatial layout constraints into AR-based image
generation. To equip AR model with layout control, a specially designed
structured masking strategy is applied to attention computation to govern the
interaction among the global prompt, layout, and image tokens. This design
prevents mis-association between different regions and their descriptions while
enabling sufficient injection of layout constraints into the generation
process. To further enhance generation quality and layout accuracy, we
incorporate Group Relative Policy Optimization (GRPO) based post-training
scheme with specially designed layout reward functions for next-set-based AR
models. Experimental results demonstrate that SMARLI is able to seamlessly
integrate layout tokens with text and image tokens without compromising
generation quality. It achieves superior layoutaware control while maintaining
the structural simplicity and generation efficiency of AR models.

</details>


### [125] [A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset](https://arxiv.org/abs/2509.12047)
*Haiyu Yang,Enhong Liu,Jennifer Sun,Sumit Sharma,Meike van Leerdam,Sebastien Franceschini,Puchun Niu,Miel Hostens*

Main category: cs.CV

TL;DR: A modular, open-source CV pipeline automatises pig behavior analysis in group housing, achieving high accuracy and robust tracking; validated on the Edinburgh Pig Behavior Dataset with strong improvements over prior methods.


<details>
  <summary>Details</summary>
Motivation: Manual animal behavior observation is time-consuming, subjective, and hard to scale; there is a need for automated, objective, continuous welfare assessment in precision farming.

Method: A modular pipeline combining zero-shot object detection, motion-aware tracking and segmentation, and vision-transformer-based feature extraction; designed to handle occlusions and group housing in indoor pig monitoring; validated on the Edinburgh Pig Behavior Video Dataset.

Result: Temporal model accuracy of 94.2% (a 21.2-point improvement over existing methods); 93.3% identity preservation; 89.3% object detection precision; robust tracking; open-source implementation with potential adaptation to other contexts.

Conclusion: An open-source, scalable approach for automated behavior monitoring that supports welfare assessment and precision farming; further validation across species is recommended.

Abstract: Animal behavior analysis plays a crucial role in understanding animal
welfare, health status, and productivity in agricultural settings. However,
traditional manual observation methods are time-consuming, subjective, and
limited in scalability. We present a modular pipeline that leverages
open-sourced state-of-the-art computer vision techniques to automate animal
behavior analysis in a group housing environment. Our approach combines
state-of-the-art models for zero-shot object detection, motion-aware tracking
and segmentation, and advanced feature extraction using vision transformers for
robust behavior recognition. The pipeline addresses challenges including animal
occlusions and group housing scenarios as demonstrated in indoor pig
monitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset
for multiple behavioral tasks. Our temporal model achieved 94.2% overall
accuracy, representing a 21.2 percentage point improvement over existing
methods. The pipeline demonstrated robust tracking capabilities with 93.3%
identity preservation score and 89.3% object detection precision. The modular
design suggests potential for adaptation to other contexts, though further
validation across species would be required. The open-source implementation
provides a scalable solution for behavior monitoring, contributing to precision
pig farming and welfare assessment through automated, objective, and continuous
analysis.

</details>


### [126] [AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective](https://arxiv.org/abs/2509.12052)
*Yuchen Deng,Xiuyang Wu,Hai-Tao Zheng,Suiyang Zhang,Yi He,Yuxing Han*

Main category: cs.CV

TL;DR: AvatarSync is a two-stage autoregressive framework that converts single-image talking-heads into realistic, controllable animations driven by text or audio, reducing flicker, identity drift, and latency.


<details>
  <summary>Details</summary>
Motivation: To overcome common drawbacks of GAN- and diffusion-based talking-head methods—inter-frame flicker, identity drift, and slow inference—by decoupling semantic modeling from visual dynamics and leveraging phoneme-level representations.

Method: A two-stage pipeline: (1) Facial Keyframe Generation (FKG) that maps phonemes (via text/audio-to-phoneme) to phoneme-to-visual anchors using a Text-Frame Causal Attention Mask to produce keyframes; (2) inter-frame interpolation with a timestamp-aware adaptive strategy based on a selective state space model for efficient bidirectional context reasoning and temporal coherence. The framework operates autoregressively on phoneme representations and optimizes the inference pipeline for low latency from a single reference image and text/audio input.

Result: AvatarSync outperforms existing talking-head methods in visual fidelity, temporal consistency, and computational efficiency, offering a scalable and controllable solution for high-quality talking-head animation.

Conclusion: The divide-and-conquer design—semantic keyframe generation followed by efficient inter-frame interpolation—yields stable, expressive talking-head animations with reduced flicker and identity drift, suitable for real-time or scalable deployment.

Abstract: Existing talking-head animation approaches based on Generative Adversarial
Networks (GANs) or diffusion models often suffer from inter-frame flicker,
identity drift, and slow inference. These limitations inherent to their video
generation pipelines restrict their suitability for applications. To address
this, we introduce AvatarSync, an autoregressive framework on phoneme
representations that generates realistic and controllable talking-head
animations from a single reference image, driven directly text or audio input.
In addition, AvatarSync adopts a two-stage generation strategy, decoupling
semantic modeling from visual dynamics, which is a deliberate "Divide and
Conquer" design. The first stage, Facial Keyframe Generation (FKG), focuses on
phoneme-level semantic representation by leveraging the many-to-one mapping
from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to
anchor abstract phonemes to character-level units. Combined with a customized
Text-Frame Causal Attention Mask, the keyframes are generated. The second
stage, inter-frame interpolation, emphasizes temporal coherence and visual
smoothness. We introduce a timestamp-aware adaptive strategy based on a
selective state space model, enabling efficient bidirectional context
reasoning. To support deployment, we optimize the inference pipeline to reduce
latency without compromising visual fidelity. Extensive experiments show that
AvatarSync outperforms existing talking-head animation methods in visual
fidelity, temporal consistency, and computational efficiency, providing a
scalable and controllable solution.

</details>


### [127] [Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation](https://arxiv.org/abs/2509.12062)
*Sebastian Diaz,Benjamin Billot,Neel Dey,Molin Zhang,Esra Abaci Turk,P. Ellen Grant,Polina Golland,Elfar Adalsteinsson*

Main category: cs.CV

TL;DR: A cross-population data augmentation framework improves fetal pose estimation generalization from older to earlier gestational ages in 3D EPI imaging.


<details>
  <summary>Details</summary>
Motivation: Fetal motion quantification is valuable for neurological development and intrauterine health but is hard to quantify, especially at early gestational ages (GA). Existing landmark-prediction methods trained on older GA struggle to generalize to early GA due to anatomical changes and scarce early GA annotated data.

Method: A fetal-specific augmentation strategy that simulates early GA intrauterine environments and fetal positioning, enabling pose estimation models trained solely on annotated older-GA images to generalize to younger-GA cohorts.

Result: Cross-population augmentation reduces variability and yields significant improvements across both older-GA and challenging early-GA cases.

Conclusion: This approach enables more reliable pose estimation across gestation, potentially enabling earlier clinical detection/intervention in 4D fetal imaging; code is available at the provided GitHub link.

Abstract: Fetal motion is a critical indicator of neurological development and
intrauterine health, yet its quantification remains challenging, particularly
at earlier gestational ages (GA). Current methods track fetal motion by
predicting the location of annotated landmarks on 3D echo planar imaging (EPI)
time-series, primarily in third-trimester fetuses. The predicted landmarks
enable simplification of the fetal body for downstream analysis. While these
methods perform well within their training age distribution, they consistently
fail to generalize to early GAs due to significant anatomical changes in both
mother and fetus across gestation, as well as the difficulty of obtaining
annotated early GA EPI data. In this work, we develop a cross-population data
augmentation framework that enables pose estimation models to robustly
generalize to younger GA clinical cohorts using only annotated images from
older GA cohorts. Specifically, we introduce a fetal-specific augmentation
strategy that simulates the distinct intrauterine environment and fetal
positioning of early GAs. Our experiments find that cross-population
augmentation yields reduced variability and significant improvements across
both older GA and challenging early GA cases. By enabling more reliable pose
estimation across gestation, our work potentially facilitates early clinical
detection and intervention in challenging 4D fetal imaging settings. Code is
available at https://github.com/sebodiaz/cross-population-pose.

</details>


### [128] [End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical Imaging Data](https://arxiv.org/abs/2509.12068)
*Farahdiba Zarin,Nicolas Padoy,Jérémy Dana,Vinkle Srivastav*

Main category: cs.CV

TL;DR: ImplMORe presents an end-to-end implicit surface approach for multi-organ reconstruction from 3D medical images, using a 3D CNN encoder and multi-scale occupancy-function-based representations to achieve higher-resolution, fine-grained organ surfaces, outperforming explicit surface methods on the totalsegmentator dataset; code to be released.


<details>
  <summary>Details</summary>
Motivation: Implicit representations offer compact, differentiable, continuous 3D shapes, but applying them to medical imaging is challenging due to resolution limits and domain-specific data/architecture constraints. A learned, scalable implicit approach could enable high-fidelity, multi-organ surface reconstruction from volumetric medical images.

Method: End-to-end deep learning method (ImplMORe) that uses a 3D CNN encoder to extract local features, multi-scale interpolation to map features into a continuous domain, and occupancy functions to represent surfaces. Applied to single- and multi-organ reconstruction on the totalsegmentator dataset.

Result: The approach outperforms discrete explicit surface reconstruction methods, delivering fine-grained organ surface details at a resolution higher than the input image.

Conclusion: Implicit surface representations, combined with multi-scale feature learning, are effective for high-fidelity medical organ surface reconstruction; the method shows promise for diagnostic support and planning; source code will be publicly available.

Abstract: The fine-grained surface reconstruction of different organs from 3D medical
imaging can provide advanced diagnostic support and improved surgical planning.
However, the representation of the organs is often limited by the resolution,
with a detailed higher resolution requiring more memory and computing
footprint. Implicit representations of objects have been proposed to alleviate
this problem in general computer vision by providing compact and differentiable
functions to represent the 3D object shapes. However, architectural and
data-related differences prevent the direct application of these methods to
medical images. This work introduces ImplMORe, an end-to-end deep learning
method using implicit surface representations for multi-organ reconstruction
from 3D medical images. ImplMORe incorporates local features using a 3D CNN
encoder and performs multi-scale interpolation to learn the features in the
continuous domain using occupancy functions. We apply our method for single and
multiple organ reconstructions using the totalsegmentator dataset. By
leveraging the continuous nature of occupancy functions, our approach
outperforms the discrete explicit representation based surface reconstruction
approaches, providing fine-grained surface details of the organ at a resolution
higher than the given input image. The source code will be made publicly
available at: https://github.com/CAMMA-public/ImplMORe

</details>


### [129] [U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT](https://arxiv.org/abs/2509.12069)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: Introduces U-Mamba2 for multi-anatomy CBCT segmentation; combines Mamba2 state-space models with U-Net, adds interactive prompts, self-supervised pretraining, and domain knowledge; achieves top-3 on ToothFairy3 tasks with mean Dice ~0.79–0.85.


<details>
  <summary>Details</summary>
Motivation: Accurate, multi-anatomy CBCT segmentation is essential for dental diagnosis and surgical planning but remains time-consuming and challenging; existing methods struggle with enforcing structural consistency and incorporating domain knowledge.

Method: Embed Mamba2 state-space models into U-Net to enforce stronger structural constraints and efficiency; incorporate interactive click prompts with cross-attention blocks; pre-train with self-supervised learning; integrate dental domain knowledge; evaluate on ToothFairy3 challenge tasks; publish code at GitHub.

Result: Task 1: mean Dice 0.792, HD95 93.19; Task 2: mean Dice 0.852, HD95 7.39; held-out test data; inference time reported as XX (not disclosed); top-3 placements in both tasks; code released publicly.

Conclusion: U-Mamba2 demonstrates effective and efficient multi-anatomy CBCT segmentation with strong performance and reproducibility; further work could address missing runtime specifics, robustness, and broader validation across datasets.

Abstract: Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in
dentistry, providing volumetric information about the anatomical structures of
jaws and teeth. Accurate segmentation of these anatomies is critical for
clinical applications such as diagnosis and surgical planning, but remains
time-consuming and challenging. In this paper, we present U-Mamba2, a new
neural network architecture designed for multi-anatomy CBCT segmentation in the
context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state
space models into the U-Net architecture, enforcing stronger structural
constraints for higher efficiency without compromising performance. In
addition, we integrate interactive click prompts with cross-attention blocks,
pre-train U-Mamba2 using self-supervised learning, and incorporate dental
domain knowledge into the model design to address key challenges of dental
anatomy segmentation in CBCT. Extensive experiments, including independent
tests, demonstrate that U-Mamba2 is both effective and efficient, securing top
3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2
achieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with
an average inference time of XX (TBC during the ODIN workshop). In Task 2,
U-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out
test data. The code is publicly available at
https://github.com/zhiqin1998/UMamba2.

</details>


### [130] [Progressive Flow-inspired Unfolding for Spectral Compressive Imaging](https://arxiv.org/abs/2509.12079)
*Xiaodong Wang,Ping Wang,Zijun He,Mengjie Qin,Xin Yuan*

Main category: cs.CV

TL;DR: A trajectory-controllable unfolding framework for CASSI hyperspectral reconstruction that enforces smooth optimization paths using diffusion-inspired trajectory control, paired with a spatial-spectral Transformer and a frequency-domain fusion module to improve quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Unfolding-based CASSI reconstructions often exhibit uncontrollable trajectories with abrupt quality jumps, hindering gradual refinement and stability; there is a need for smoother optimization paths and more efficient architectures for HSIs.

Method: Introduces a trajectory-controllable unfolding framework inspired by diffusion trajectories and flow matching to ensure smooth, continuous progress from noisy initial estimates to high-quality reconstructions. Develops an efficient spatial-spectral Transformer tailored for hyperspectral reconstruction and a frequency-domain fusion module to maintain feature consistency across domains.

Result: Experiments on both simulated and real data show that the proposed method achieves higher reconstruction quality and greater efficiency compared with prior state-of-the-art methods.

Conclusion: The approach delivers smoother optimization trajectories and improved performance/efficiency in CASSI reconstruction, demonstrating the effectiveness of diffusion-inspired trajectory control and Transformer-based architectures for hyperspectral imaging.

Abstract: Coded aperture snapshot spectral imaging (CASSI) retrieves a 3D hyperspectral
image (HSI) from a single 2D compressed measurement, which is a highly
challenging reconstruction task. Recent deep unfolding networks (DUNs),
empowered by explicit data-fidelity updates and implicit deep denoisers, have
achieved the state of the art in CASSI reconstruction. However, existing
unfolding approaches suffer from uncontrollable reconstruction trajectories,
leading to abrupt quality jumps and non-gradual refinement across stages.
Inspired by diffusion trajectories and flow matching, we propose a novel
trajectory-controllable unfolding framework that enforces smooth, continuous
optimization paths from noisy initial estimates to high-quality
reconstructions. To achieve computational efficiency, we design an efficient
spatial-spectral Transformer tailored for hyperspectral reconstruction, along
with a frequency-domain fusion module to gurantee feature consistency.
Experiments on simulation and real data demonstrate that our method achieves
better reconstruction quality and efficiency than prior state-of-the-art
approaches.

</details>


### [131] [End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI](https://arxiv.org/abs/2509.12090)
*Yihong Chen,Jiancheng Yang,Deniz Sayin Mercadier,Hieu Le,Juerg Schwitter,Pascal Fua*

Main category: cs.CV

TL;DR: TetHeart is the first end-to-end framework that recovers 4D multi-structure heart meshes from offline full stacks and intra-procedural sparse slices using a hybrid explicit-implicit deformable tetrahedra representation.


<details>
  <summary>Details</summary>
Motivation: Current cardiac motion reconstruction methods depend on complete CMR stacks and perform poorly with sparse observations, limiting intra-procedural utility. There is a need for a unified approach that handles both offline full-stack data and sparse intra-procedural slices with strong generalization across structures.

Method: An end-to-end framework built on deep deformable tetrahedra with an explicit-implicit hybrid representation. It initializes patient-specific heart meshes from high-quality full stacks and updates them using available slices, from full stacks to single slices. Key innovations include (i) an attentive slice-adaptive 2D-3D feature assembly with a slice density distillation strategy to handle extreme sparsity; and (ii) a two-stage weakly supervised motion learning regime requiring only keyframe annotations (e.g., ED/ES). The approach unifies full 4D mesh recovery across multiple cardiac structures and supports intra-procedural updates.

Result: Empirically demonstrates state-of-the-art accuracy and strong generalization in both pre- and intra-procedural settings. Validated on three large public datasets and zero-shot externally evaluated on private interventional and public CMR datasets.

Conclusion: TetHeart provides a unified, robust framework for 4D multi-structure heart mesh recovery that gracefully handles varying data availability—from full offline stacks to sparse intra-procedural slices—potentially improving diagnosis, prediction, and intervention planning.

Abstract: Reconstructing cardiac motion from cine CMR sequences is critical for
diagnosis, prediction, and intervention. Existing methods rely on complete CMR
stacks to infer full heart motion, limiting their utility in intra-procedural
scenarios where only sparse observations are available. We present TetHeart,
the first end-to-end framework that unifies full 4D multi-structure heart mesh
recovery from both offline full-stack acquisitions and intra-procedural
sparse-slice observations. Our method leverages deep deformable tetrahedra, an
explicit-implicit hybrid representation, to capture shape and motion in a
coherent space shared across cardiac structures. It is initialized from
high-quality pre-procedural or offline-acquired full stacks to build detailed,
patient-specific heart meshes, which can then be updated using whatever slices
are available, from full stacks down to a single slice. We further incorporate
several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D
feature assembly that dynamically integrates information from arbitrary numbers
of slices at any position, combined with a distillation strategy from
full-slice to sparse-slice settings to ensure accurate reconstruction under
extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme
requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on
three large public datasets and externally evaluated zero-shot on additional
private interventional and public CMR datasets, TetHeart achieves
state-of-the-art accuracy and strong generalization in both pre- and
intra-procedural settings.

</details>


### [132] [FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation](https://arxiv.org/abs/2509.12105)
*Bernardo Forni,Gabriele Lombardi,Federico Pozzi,Mirco Planamente*

Main category: cs.CV

TL;DR: A few-shot segmentation method FS-SAM2 that adapts SAM2 using LoRA to efficiently segment unseen classes with few annotations, achieving strong results across standard datasets with low training cost.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of segmenting unseen classes with limited labeled data, leveraging powerful foundation model SAM2 while avoiding full retraining on large datasets.

Method: Adapt SAM2 for few-shot segmentation by repurposing its video capabilities; apply Low-Rank Adaptation (LoRA) to the core modules; meta-train only a small set of parameters; support any K-shot; evaluate on PASCAL-5^i, COCO-20^i, FSS-1000.

Result: Achieves remarkable performance on standard benchmarks and exhibits excellent inference efficiency; code available.

Conclusion: FS-SAM2 demonstrates that few-shot segmentation can benefit from foundation-model-based architectures via lightweight adapters, achieving strong accuracy with minimal additional parameters.

Abstract: Few-shot semantic segmentation has recently attracted great attention. The
goal is to develop a model capable of segmenting unseen classes using only a
few annotated samples. Most existing approaches adapt a pre-trained model by
training from scratch an additional module. Achieving optimal performance with
these approaches requires extensive training on large-scale datasets. The
Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and
video segmentation with a modular design. In this paper, we propose a Few-Shot
segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities
are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank
Adaptation (LoRA) to the original modules in order to handle the diverse images
typically found in standard datasets, unlike the temporally connected frames
used in SAM2's pre-training. With this approach, only a small number of
parameters is meta-trained, which effectively adapts SAM2 while benefiting from
its impressive segmentation performance. Our method supports any K-shot
configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and
FSS-1000 datasets, achieving remarkable results and demonstrating excellent
computational efficiency during inference. Code is available at
https://github.com/fornib/FS-SAM2

</details>


### [133] [RailSafeNet: Visual Scene Understanding for Tram Safety](https://arxiv.org/abs/2509.12125)
*Ing. Ondrej Valach,Ing. Ivan Gruber*

Main category: cs.CV

TL;DR: RailSafeNet is a real-time monocular-video system that fuses semantic segmentation, object detection, and a rule-based distance checker to detect track intrusions and warn tram drivers; evaluated on RailSem19 with SegFormer B3 achieving 65% IoU and YOLOv8 achieving 75.6% mAP at IoU 0.50; code available.


<details>
  <summary>Details</summary>
Motivation: Enhance safety in tram operations in dense urban areas by providing a lightweight, annotation-efficient perception system that can warn operators before dangerous situations escalate.

Method: A real-time framework combining semantic segmentation (SegFormer B3), object detection (YOLOv8), and a rule-based Distance Assessor. It uses monocular video to locate rails, localize nearby objects, and classify risk by projecting distances against the standard 1435 mm rail gauge. Evaluated on the RailSem19 dataset with a class-filtered SegFormer B3 (65% IoU) and a fine-tuned YOLOv8 (75.6% mAP @ IoU 0.50).

Result: RailSafeNet provides accurate, annotation-light scene understanding from monocular video, enabling warnings to drivers before dangerous situations develop; demonstrated competitive metrics on RailSem19.

Conclusion: RailSafeNet offers a practical, real-time safety solution for tram operations using monocular vision, with ready-to-use code for deployment and further improvements.

Abstract: Tram-human interaction safety is an important challenge, given that trams
frequently operate in densely populated areas, where collisions can range from
minor injuries to fatal outcomes. This paper addresses the issue from the
perspective of designing a solution leveraging digital image processing, deep
learning, and artificial intelligence to improve the safety of pedestrians,
drivers, cyclists, pets, and tram passengers. We present RailSafeNet, a
real-time framework that fuses semantic segmentation, object detection and a
rule-based Distance Assessor to highlight track intrusions. Using only
monocular video, the system identifies rails, localises nearby objects and
classifies their risk by comparing projected distances with the standard 1435mm
rail gauge. Experiments on the diverse RailSem19 dataset show that a
class-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),
while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated
at an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore
delivers accurate, annotation-light scene understanding that can warn drivers
before dangerous situations escalate. Code available at
https://github.com/oValach/RailSafeNet.

</details>


### [134] [Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models](https://arxiv.org/abs/2509.12132)
*Pu Jian,Junhong Wu,Wei Sun,Chen Wang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: Reflection-V enhances visual reflection in vision-language models by constructing vision-centered reasoning data via a VLM–reasoning LLM interaction and applying a visual-attention reward in reinforcement learning, leading to improved performance on visual reasoning benchmarks and steadier reliance on visual information.


<details>
  <summary>Details</summary>
Motivation: Current VRMs struggle to maintain attention to visual information as reasoning lengthens, indicating a lack of true slow-thinking grounded in vision. This hinders accurate visual reasoning and generalization.

Method: 1) Build vision-centered reasoning data by an agent that interfaces between VLMs and reasoning LLMs to bootstrap visual-reflection patterns (cold-start). 2) Use a visual-attention-based reward model during reinforcement learning to encourage reasoning grounded in visual information.

Result: Significant performance improvements across multiple visual reasoning benchmarks and a stronger, more consistent reliance on visual information during reasoning.

Conclusion: Reflection-V effectively enhances visual reflection in VRMs, suggesting that vision-centered data construction and attention-guided RL rewards are a promising direction for improving vision-language visual reasoning.

Abstract: Recent advances in text-only "slow-thinking" reasoning have prompted efforts
to transfer this capability to vision-language models (VLMs), for training
visual reasoning models (\textbf{VRMs}). owever, such transfer faces critical
challenges: Effective "slow thinking" in VRMs requires \textbf{visual
reflection}, the ability to check the reasoning process based on visual
information. Through quantitative analysis, we observe that current VRMs
exhibit limited visual reflection, as their attention to visual information
diminishes rapidly with longer generated responses. To address this challenge,
we propose a new VRM \textbf{Reflection-V}, which enhances visual reflection
based on reasoning data construction for cold-start and reward design for
reinforcement learning (RL). Firstly, we construct vision-centered reasoning
data by leveraging an agent that interacts between VLMs and reasoning LLMs,
enabling cold-start learning of visual reflection patterns. Secondly, a visual
attention based reward model is employed during RL to encourage reasoning based
on visual information. Therefore, \textbf{Reflection-V} demonstrates
significant improvements across multiple visual reasoning benchmarks.
Furthermore, \textbf{Reflection-V} maintains a stronger and more consistent
reliance on visual information during visual reasoning, indicating effective
enhancement in visual reflection capabilities.

</details>


### [135] [3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data](https://arxiv.org/abs/2509.12143)
*Nojod M. Alotaibi,Areej M. Alhothali,Manar S. Ali*

Main category: cs.CV

TL;DR: A unified Vision Transformer + Graph Neural Network pipeline for detecting major depressive disorder from structural MRI, comparing atlas-based vs. patch-based region definitions; atlas-based regions yield better performance (~79% accuracy).


<details>
  <summary>Details</summary>
Motivation: MDD is highly prevalent and current DL approaches using voxel-level features or predefined atlas regions may miss complex brain patterns. A unified framework that captures 3D regional embeddings and interregional relationships could improve diagnostic accuracy and enable early intervention.

Method: Extract 3D region embeddings from sMRI using Vision Transformers. Define regions via (1) atlas-based approach using predefined structural/functional brain atlases, and (2) cube-based method where ViTs learn to identify regions from uniformly extracted 3D patches. Build cosine similarity graphs to model interregional relationships and apply Graph Neural Networks for classification. Evaluate on REST-meta-MDD with stratified 10-fold cross-validation.

Result: Best model achieved 78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and 78.98% F1-score. Atlas-based models consistently outperformed the cube-based approach.

Conclusion: Incorporating anatomical priors via atlas-based region definitions enhances MDD detection from sMRI when using a ViT+GNN pipeline; the approach demonstrates strong potential for robust, surgery-free biomarkers and supports the value of domain-specific priors in DL-based neuroimaging diagnostics.

Abstract: Major depressive disorder (MDD) is a prevalent mental health condition that
negatively impacts both individual well-being and global public health.
Automated detection of MDD using structural magnetic resonance imaging (sMRI)
and deep learning (DL) methods holds increasing promise for improving
diagnostic accuracy and enabling early intervention. Most existing methods
employ either voxel-level features or handcrafted regional representations
built from predefined brain atlases, limiting their ability to capture complex
brain patterns. This paper develops a unified pipeline that utilizes Vision
Transformers (ViTs) for extracting 3D region embeddings from sMRI data and
Graph Neural Network (GNN) for classification. We explore two strategies for
defining regions: (1) an atlas-based approach using predefined structural and
functional brain atlases, and (2) an cube-based method by which ViTs are
trained directly to identify regions from uniformly extracted 3D patches.
Further, cosine similarity graphs are generated to model interregional
relationships, and guide GNN-based classification. Extensive experiments were
conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of
our model. With stratified 10-fold cross-validation, the best model obtained
78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and
78.98% F1-score. Further, atlas-based models consistently outperformed the
cube-based approach, highlighting the importance of using domain-specific
anatomical priors for MDD detection.

</details>


### [136] [Open-ended Hierarchical Streaming Video Understanding with Vision Language Models](https://arxiv.org/abs/2509.12145)
*Hyolim Kang,Yunsu Park,Youngbeom Yoo,Yeeun Choi,Seon Joo Kim*

Main category: cs.CV

TL;DR: Proposes OpenHOUSE for hierarchical, online video understanding by combining online action localization with free-form description generation, using LLMs to build hierarchical event structure and a streaming module for precise boundaries.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of datasets with hierarchical and fine-grained temporal annotations and the need to move beyond action classification to descriptive, online understanding; leverages LLMs and generative models for richer, scalable streaming perception.

Method: Uses LLMs to group atomic actions into higher-level events and introduces OpenHOUSE, a specialized streaming module that detects boundaries between closely adjacent actions, extending streaming action perception beyond simple classification.

Result: Shows LLMs can enrich datasets by grouping actions into higher-level events; the specialized streaming module nearly doubles boundary-detection performance compared to naive extensions of existing methods.

Conclusion: OpenHOUSE is a key step toward integrating powerful generative models into streaming action perception and online understanding of events.

Abstract: We introduce Hierarchical Streaming Video Understanding, a task that combines
online temporal action localization with free-form description generation.
Given the scarcity of datasets with hierarchical and fine-grained temporal
annotations, we demonstrate that LLMs can effectively group atomic actions into
higher-level events, enriching existing datasets. We then propose OpenHOUSE
(Open-ended Hierarchical Online Understanding System for Events), which extends
streaming action perception beyond action classification. OpenHOUSE features a
specialized streaming module that accurately detects boundaries between closely
adjacent actions, nearly doubling the performance of direct extensions of
existing methods. We envision the future of streaming action perception in the
integration of powerful generative models, with OpenHOUSE representing a key
step in that direction.

</details>


### [137] [Multi Anatomy X-Ray Foundation Model](https://arxiv.org/abs/2509.12146)
*Nishank Singla,Krisztian Koos,Farzin Haddadpour,Amin Honarmandi Shandiz,Lovish Chum,Xiaojian Xu,Qing Jin,Erhan Bas*

Main category: cs.CV

TL;DR: A multi-anatomy X-ray foundation model XR-0 trained with self-supervision on 1.15M images from diverse anatomical regions achieves state-of-the-art on most multi-anatomy tasks across 12 datasets and 20 downstream tasks, while remaining competitive on chest-only benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current AI radiology models are largely restricted to chest anatomy and do not generalize across a broader set of tasks or anatomical regions; there is a need for a robust, general-purpose medical vision model for radiology applications.

Method: Self-supervised pretraining on a large private dataset of 1.15 million X-ray images spanning diverse anatomical regions; evaluation across 12 datasets and 20 downstream tasks including classification, retrieval, segmentation, localization, visual grounding, and report generation.

Result: XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks.

Conclusion: Anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, enabling scalable and adaptable AI systems in radiology.

Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation
models are limited to chest anatomy and fail to generalize across broader
clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray
foundation model using self-supervised learning on a large, private dataset of
1.15 million images spanning diverse anatomical regions and evaluated across 12
datasets and 20 downstream tasks, including classification, retrieval,
segmentation, localization, visual grounding, and report generation. XR-0
achieves state-of-the-art performance on most multi-anatomy tasks and remains
competitive on chest-specific benchmarks. Our results demonstrate that
anatomical diversity and supervision are critical for building robust,
general-purpose medical vision models, paving the way for scalable and
adaptable AI systems in radiology.

</details>


### [138] [LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury](https://arxiv.org/abs/2509.12155)
*M. Bolhassani,B. Veasey,E. Daugherty,S. Keltner,N. Kumar,N. Dunlap,A. Amini*

Main category: cs.CV

TL;DR: LoRA enables efficient fine-tuning of large vision models (DinoV2, SwinV2) for RILI diagnosis from SBRT X-ray CT, matching or surpassing full fine-tuning while reducing training cost and parameter count.


<details>
  <summary>Details</summary>
Motivation: Need for robust, efficient adaptation of large vision models to 3D medical imaging tasks with limited data and compute; assess whether parameter-efficient fine-tuning can match full fine-tuning performance in diagnosing Radiation-Induced Lung Injury from SBRT-derived imaging; explore 2D-to-3D adaptation strategies and robustness to spatial context.

Method: Apply Low-Rank Adaptation (LoRA) to DinoV2 and SwinV2 for RILI diagnosis on X-ray CT after SBRT; compare against full fine-tuning and inference-only baselines; use cropped 2D patches of two sizes (50 mm^3 and 75 mm^3) centered at the treatment isocenter; test various 2D-to-3D adaptation techniques to handle 3D data; measure performance and training cost.

Result: LoRA achieves comparable or superior performance to traditional full fine-tuning while substantially reducing computational costs and training time due to fewer trainable parameters.

Conclusion: LoRA is a viable, efficient strategy for fine-tuning large vision models in medical imaging tasks like SBRT-related RILI diagnosis, balancing accuracy with reduced resource requirements.

Abstract: This study investigates the efficacy of Low-Rank Adaptation (LoRA) for
fine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose
Radiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic
Body Radiation Therapy (SBRT). To evaluate the robustness and efficiency of
this approach, we compare LoRA with traditional full fine-tuning and
inference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3
and 75 mm3), centered at the treatment isocenter, in addition to different
adaptation techniques for adapting the 2D LVMs for 3D data were used to
determine the sensitivity of the models to spatial context. Experimental
results show that LoRA achieves comparable or superior performance to
traditional fine-tuning while significantly reducing computational costs and
training times by requiring fewer trainable parameters.

</details>


### [139] [HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments](https://arxiv.org/abs/2509.12187)
*Johanna Karras,Yingwei Li,Yasamin Jafarian,Ira Kemelmacher-Shlizerman*

Main category: cs.CV

TL;DR: HoloGarment enables 360-degree novel view synthesis of in-the-wild garments from 1-3 images or video by learning a shared garment embedding space that bridges real and synthetic data; it builds a garment atlas per video for consistent, photorealistic NVS, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: NVS for real clothing is hindered by occlusions, complex poses, cloth deformations, and a domain gap between synthetic training data and real-world garments; robust, generalizable methods are needed.

Method: Train with a hybrid data regime: large-scale real video data plus small-scale synthetic 3D data to optimize a shared garment embedding space; at inference, finetune embedding on a target video to build a garment atlas capturing geometry and texture across viewpoints, enabling 360-degree NVS from 1-3 images or video.

Result: State-of-the-art performance on NVS of in-the-wild garments from images and videos; robust to wrinkling, pose variation, and occlusion; preserves photorealism, view consistency, texture details, and accurate geometry.

Conclusion: The implicit training paradigm plus atlas-based embedding effectively bridges real/synthetic data gaps and enables high-quality, pose-robust NVS for real garments; results validated on images and video, with a project page provided.

Abstract: Novel view synthesis (NVS) of in-the-wild garments is a challenging task due
significant occlusions, complex human poses, and cloth deformations. Prior
methods rely on synthetic 3D training data consisting of mostly unoccluded and
static objects, leading to poor generalization on real-world clothing. In this
paper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3
images or a continuous video of a person wearing a garment and generates
360{\deg} novel views of the garment in a canonical pose. Our key insight is to
bridge the domain gap between real and synthetic data with a novel implicit
training paradigm leveraging a combination of large-scale real video data and
small-scale synthetic 3D data to optimize a shared garment embedding space.
During inference, the shared embedding space further enables dynamic
video-to-360{\deg} NVS through the construction of a garment "atlas"
representation by finetuning a garment embedding on a specific real-world
video. The atlas captures garment-specific geometry and texture across all
viewpoints, independent of body pose or motion. Extensive experiments show that
HoloGarment achieves state-of-the-art performance on NVS of in-the-wild
garments from images and videos. Notably, our method robustly handles
challenging real-world artifacts -- such as wrinkling, pose variation, and
occlusion -- while maintaining photorealism, view consistency, fine texture
details, and accurate geometry. Visit our project page for additional results:
https://johannakarras.github.io/HoloGarment

</details>


### [140] [Domain-Adaptive Pretraining Improves Primate Behavior Recognition](https://arxiv.org/abs/2509.12193)
*Felix B. Mueller,Timo Lueddecke,Richard Vogg,Alexander S. Ecker*

Main category: cs.CV

TL;DR: Self-supervised, domain-adaptive pretraining with V-JEPA improves primate behavior action recognition; outperforms SOTA on PanAf and ChimpACT by ~6 percentage points; DAP is the main gain; code released.


<details>
  <summary>Details</summary>
Motivation: Data labeling for animal-behavior video is expensive; need data-efficient learning that can leverage unlabeled, in-domain data.

Method: Use pretrained V-JEPA model; perform domain-adaptive pretraining (continue pretraining on in-domain data); evaluate on PanAf and ChimpACT; compare against published SOTA.

Result: Accuracy on PanAf improves by 6.1 percentage points; mAP on ChimpACT improves by 6.3 percentage points; majority of gain attributed to DAP; code available at provided URL.

Conclusion: DAP is effective for improving animal-behavior recognition without labeled data; enhances scalability for ecology and conservation work; code release facilitates adoption.

Abstract: Computer vision for animal behavior offers promising tools to aid research in
ecology, cognition, and to support conservation efforts. Video camera traps
allow for large-scale data collection, but high labeling costs remain a
bottleneck to creating large-scale datasets. We thus need data-efficient
learning approaches. In this work, we show that we can utilize self-supervised
learning to considerably improve action recognition on primate behavior. On two
datasets of great ape behavior (PanAf and ChimpACT), we outperform published
state-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt.
mAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and
applying domain-adaptive pretraining (DAP), i.e. continuing the pretraining
with in-domain data. We show that most of the performance gain stems from the
DAP. Our method promises great potential for improving the recognition of
animal behavior, as DAP does not require labeled samples. Code is available at
https://github.com/ecker-lab/dap-behavior

</details>


### [141] [3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review](https://arxiv.org/abs/2509.12197)
*Salma Galaaoui,Eduardo Valle,David Picard,Nermin Samet*

Main category: cs.CV

TL;DR: A comprehensive review and benchmark framework for LiDAR-based 3D human pose estimation and mesh recovery, featuring a taxonomy, dataset analysis, unified metrics, benchmark tables, and identified open challenges.


<details>
  <summary>Details</summary>
Motivation: Lack of standardization in LiDAR-based 3D human understanding; need a clear taxonomy, fair benchmarks, and unified evaluation to drive progress.

Method: Propose a structured taxonomy to classify methods; analyze strengths/limitations and design choices; perform quantitative comparison of three major datasets; compile unified definitions of evaluation metrics; establish benchmark tables for both tasks; provide an accompanying public web resource.

Result: A structured taxonomy; unified metric definitions; benchmark tables for pose and mesh tasks; dataset analyses; open challenges and directions; public webpage to organize papers.

Conclusion: Establishing a standardized evaluation and benchmark framework will enable fair comparisons, accelerate progress, and reveal remaining gaps in LiDAR-based 3D human pose/mesh understanding.

Abstract: In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds. We compare
existing approaches across several key dimensions, and propose a structured
taxonomy to classify these methods. Following this taxonomy, we analyze each
method's strengths, limitations, and design choices. In addition, (i) we
perform a quantitative comparison of the three most widely used datasets,
detailing their characteristics; (ii) we compile unified definitions of all
evaluation metrics; and (iii) we establish benchmark tables for both tasks on
these datasets to enable fair comparisons and promote progress in the field. We
also outline open challenges and research directions critical for advancing
LiDAR-based 3D human understanding. Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR

</details>


### [142] [OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling](https://arxiv.org/abs/2509.12201)
*Yang Zhou,Yifan Wang,Jianjun Zhou,Wenzheng Chang,Haoyu Guo,Zizun Li,Kaijing Ma,Xinyue Li,Yating Wang,Haoyi Zhu,Mingyu Liu,Dingning Liu,Jiange Yang,Zhoujie Fu,Junyi Chen,Chunhua Shen,Jiangmiao Pang,Kaipeng Zhang,Tong He*

Main category: cs.CV

TL;DR: OmniWorld introduces a large-scale, multi-domain, multi-modal 4D world modeling dataset (including OmniWorld-Game) and a benchmark; fine-tuning SOTA models on it yields notable gains in 4D reconstruction and video generation, validating its potential to advance general 4D world models.


<details>
  <summary>Details</summary>
Motivation: Current 4D modeling data suffers from limited dynamic complexity, domain diversity, and insufficient spatial-temporal annotations, hindering progress on 4D reconstruction, future prediction, and camera-controlled video generation.

Method: Create OmniWorld: a large, multi-domain, multi-modal dataset consisting of OmniWorld-Game plus curated public datasets; establish a challenging benchmark; demonstrate utility by fine-tuning existing SOTA methods on OmniWorld for improved 4D reconstruction and video generation.

Result: OmniWorld offers richer modalities, greater scale, and more realistic dynamic interactions than prior synthetic datasets; fine-tuning SOTA models on OmniWorld yields significant performance gains on 4D reconstruction and video generation tasks; validates OmniWorld as a valuable resource for training and evaluation.

Conclusion: OmniWorld is positioned to accelerate development of general-purpose 4D world models and enhance machines' holistic understanding of the physical world.

Abstract: The field of 4D world modeling - aiming to jointly capture spatial geometry
and temporal dynamics - has witnessed remarkable progress in recent years,
driven by advances in large-scale generative models and multimodal learning.
However, the development of truly general 4D world models remains fundamentally
constrained by the availability of high-quality data. Existing datasets and
benchmarks often lack the dynamic complexity, multi-domain diversity, and
spatial-temporal annotations required to support key tasks such as 4D geometric
reconstruction, future prediction, and camera-control video generation. To
address this gap, we introduce OmniWorld, a large-scale, multi-domain,
multi-modal dataset specifically designed for 4D world modeling. OmniWorld
consists of a newly collected OmniWorld-Game dataset and several curated public
datasets spanning diverse domains. Compared with existing synthetic datasets,
OmniWorld-Game provides richer modality coverage, larger scale, and more
realistic dynamic interactions. Based on this dataset, we establish a
challenging benchmark that exposes the limitations of current state-of-the-art
(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning
existing SOTA methods on OmniWorld leads to significant performance gains
across 4D reconstruction and video generation tasks, strongly validating
OmniWorld as a powerful resource for training and evaluation. We envision
OmniWorld as a catalyst for accelerating the development of general-purpose 4D
world models, ultimately advancing machines' holistic understanding of the
physical world.

</details>


### [143] [LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence](https://arxiv.org/abs/2509.12203)
*Zixin Yin,Xili Dai,Duomin Wang,Xianfang Zeng,Lionel M. Ni,Gang Yu,Heung-Yeung Shum*

Main category: cs.CV

TL;DR: LazyDrag eliminates implicit point matching in drag-based editing by producing an explicit correspondence map from drags, enabling stable full-strength inversion without test-time optimization and unlocking high-fidelity, text-guided edits with diffusion transformers.


<details>
  <summary>Details</summary>
Motivation: Implicit attention-based point matching bottlenecks drag-based editing, causing weakened inversion strength and costly test-time optimization, which limits the generative capabilities of diffusion models.

Method: Introduce an explicit correspondence map derived from user drags as a reliable reference to guide attention control in Multi-Modal Diffusion Transformers, enabling stable full-strength inversion, removing the need for test-time optimization, supporting multi-round move/scale edits, and achieving high-quality edits evaluated on DragBench.

Result: Outperforms baselines in drag accuracy and perceptual quality (VIEScore and human evaluation), achieving state-of-the-art results; enables complex edits such as opening a dog’s mouth with interior inpainting, adding objects (e.g., a tennis ball), and context-aware edits for ambiguous drags; supports multi-round workflows.

Conclusion: LazyDrag establishes a new editing paradigm by removing reliance on implicit matching and test-time optimization, uniting precise geometric control with text guidance and unlocking the full generative potential of diffusion-based editors.

Abstract: The reliance on implicit point matching via attention has become a core
bottleneck in drag-based editing, resulting in a fundamental compromise on
weakened inversion strength and costly test-time optimization (TTO). This
compromise severely limits the generative capabilities of diffusion models,
suppressing high-fidelity inpainting and text-guided creation. In this paper,
we introduce LazyDrag, the first drag-based image editing method for
Multi-Modal Diffusion Transformers, which directly eliminates the reliance on
implicit point matching. In concrete terms, our method generates an explicit
correspondence map from user drag inputs as a reliable reference to boost the
attention control. This reliable reference opens the potential for a stable
full-strength inversion process, which is the first in the drag-based editing
task. It obviates the necessity for TTO and unlocks the generative capability
of models. Therefore, LazyDrag naturally unifies precise geometric control with
text guidance, enabling complex edits that were previously out of reach:
opening the mouth of a dog and inpainting its interior, generating new objects
like a ``tennis ball'', or for ambiguous drags, making context-aware changes
like moving a hand into a pocket. Additionally, LazyDrag supports multi-round
workflows with simultaneous move and scale operations. Evaluated on the
DragBench, our method outperforms baselines in drag accuracy and perceptual
quality, as validated by VIEScore and human evaluation. LazyDrag not only
establishes new state-of-the-art performance, but also paves a new way to
editing paradigms.

</details>


### [144] [Character-Centric Understanding of Animated Movies](https://arxiv.org/abs/2509.12204)
*Zhongrui Gui,Junyu Xie,Tengda Han,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: An audio-visual pipeline for automatic animated character recognition is proposed, built by automatically constructing a cross-modal character bank from online sources. It enables two downstream tasks (Audio Description generation and character-aware subtitling) and introduces CMD-AM (75 animated movies) to advance accessibility and narrative comprehension, outperforming face-detection baselines.


<details>
  <summary>Details</summary>
Motivation: Animated characters vary wildly in appearance, motion, and deformation, making traditional face recognition ineffective. A robust, multi-modal approach is needed to support character-centric understanding and accessibility in animated content.

Method: Automatically build an audio-visual character bank from online sources containing visual exemplars and voice samples per character; use this bank for multi-modal recognition of characters in animation; apply the recognition to two downstream tasks (Audio Description generation and character-aware subtitling); release CMD-AM dataset with 75 animated movies and annotations.

Result: The proposed pipeline yields significant improvements over prior face-detection-based approaches in accessibility (Audio Description) and narrative comprehension (subtitling).

Conclusion: A character-centric pipeline enables robust recognition of animated characters and enhances accessibility; CMD-AM dataset and associated code are provided to support future research in this area.

Abstract: Animated movies are captivating for their unique character designs and
imaginative storytelling, yet they pose significant challenges for existing
recognition systems. Unlike the consistent visual patterns detected by
conventional face recognition methods, animated characters exhibit extreme
diversity in their appearance, motion, and deformation. In this work, we
propose an audio-visual pipeline to enable automatic and robust animated
character recognition, and thereby enhance character-centric understanding of
animated movies. Central to our approach is the automatic construction of an
audio-visual character bank from online sources. This bank contains both visual
exemplars and voice (audio) samples for each character, enabling subsequent
multi-modal character recognition despite long-tailed appearance distributions.
Building on accurate character recognition, we explore two downstream
applications: Audio Description (AD) generation for visually impaired
audiences, and character-aware subtitling for the hearing impaired. To support
research in this domain, we introduce CMD-AM, a new dataset of 75 animated
movies with comprehensive annotations. Our character-centric pipeline
demonstrates significant improvements in both accessibility and narrative
comprehension for animated content over prior face-detection-based approaches.
For the code and dataset, visit
https://www.robots.ox.ac.uk/~vgg/research/animated_ad/.

</details>


### [145] [Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling](https://arxiv.org/abs/2310.06389)
*Huangjie Zheng,Zhendong Wang,Jianbo Yuan,Guanghan Ning,Pengcheng He,Quanzeng You,Hongxia Yang,Mingyuan Zhou*

Main category: cs.CV

TL;DR: LEGO bricks introduce a test-time reconfigurable diffusion backbone by stacking local-feature bricks that enrich local regions with MLPs and transform them with Transformer blocks, enabling selective skipping, higher-resolution generation, faster sampling, and improved training efficiency without sacrificing generative performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are powerful but computationally heavy; existing backbones (U-Net, ViT) struggle to scale to variable resolutions and smaller networks; there's a need for a flexible backbone that can adapt at test time to reduce cost while maintaining quality.

Method: Propose LEGO bricks that consist of local-feature enrichment via MLPs and global content orchestration via Transformer blocks, stacked in a full-resolution backbone. Bricks can be skipped at sampling to reduce cost; designed to allow higher-res outputs than training; maintain full-resolution image across bricks; train with brick configuration; evaluate efficiency and quality.

Result: Shows faster convergence and training efficiency; enables variable-resolution generation; reduces sampling time compared to prior methods; maintains strong generative performance; code and project page provided.

Conclusion: LEGO bricks offer a versatile, reconfigurable diffusion backbone that balances efficiency and quality, enabling lower-cost training and faster sampling, and supporting generation at resolutions beyond training data.

Abstract: Diffusion models excel at generating photo-realistic images but come with
significant computational costs in both training and sampling. While various
techniques address these computational challenges, a less-explored issue is
designing an efficient and adaptable network backbone for iterative refinement.
Current options like U-Net and Vision Transformer often rely on
resource-intensive deep networks and lack the flexibility needed for generating
images at variable resolutions or with a smaller network than used in training.
This study introduces LEGO bricks, which seamlessly integrate Local-feature
Enrichment and Global-content Orchestration. These bricks can be stacked to
create a test-time reconfigurable diffusion backbone, allowing selective
skipping of bricks to reduce sampling costs and generate higher-resolution
images than the training data. LEGO bricks enrich local regions with an MLP and
transform them using a Transformer block while maintaining a consistent
full-resolution image across all bricks. Experimental results demonstrate that
LEGO bricks enhance training efficiency, expedite convergence, and facilitate
variable-resolution image generation while maintaining strong generative
performance. Moreover, LEGO significantly reduces sampling time compared to
other methods, establishing it as a valuable enhancement for diffusion models.
Our code and project page are available at
https://jegzheng.github.io/LEGODiffusion.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [146] [Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2509.10570)
*Wei Dai,Shengen Wu,Wei Wu,Zhenhao Wang,Sisuo Lyu,Haicheng Liao,Limin Yu,Weiping Ding,Runwei Guan,Yutao Yue*

Main category: cs.RO

TL;DR: A survey of large foundation models (LLMs and MLLMs) for trajectory prediction in autonomous driving, outlining core methods, tasks, metrics, datasets, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Addresses core limitations of traditional trajectory prediction methods—low interpretability, heavy data needs, and weak long-tail generalization—by leveraging language and multimodal models to enable interpretable, context-aware reasoning and safer, more robust predictions.

Method: Systematic review focusing on three core methodologies: trajectory-language mapping, multimodal fusion, and constraint-based reasoning within LFM frameworks. It covers prediction tasks for vehicles and pedestrians, evaluation metrics, dataset analyses, and synthesizes findings across datasets and tasks.

Result: Synthesizes recent advances in LFMs for trajectory prediction, identifies three key methodological strands, and evaluates their impact on safety and generalization; provides a structured overview of prediction tasks, metrics, and datasets.

Conclusion: LFMs offer a promising shift for trajectory prediction by enabling interpretable, context-rich reasoning; future work should target low-latency inference, causality-aware modeling, and development of motion foundation models to improve robustness and real-world deployment.

Abstract: Trajectory prediction serves as a critical functionality in autonomous
driving, enabling the anticipation of future motion paths for traffic
participants such as vehicles and pedestrians, which is essential for driving
safety. Although conventional deep learning methods have improved accuracy,
they remain hindered by inherent limitations, including lack of
interpretability, heavy reliance on large-scale annotated data, and weak
generalization in long-tail scenarios. The rise of Large Foundation Models
(LFMs) is transforming the research paradigm of trajectory prediction. This
survey offers a systematic review of recent advances in LFMs, particularly
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for
trajectory prediction. By integrating linguistic and scene semantics, LFMs
facilitate interpretable contextual reasoning, significantly enhancing
prediction safety and generalization in complex environments. The article
highlights three core methodologies: trajectory-language mapping, multimodal
fusion, and constraint-based reasoning. It covers prediction tasks for both
vehicles and pedestrians, evaluation metrics, and dataset analyses. Key
challenges such as computational latency, data scarcity, and real-world
robustness are discussed, along with future research directions including
low-latency inference, causality-aware modeling, and motion foundation models.

</details>


### [147] [STL-Based Motion Planning and Uncertainty-Aware Risk Analysis for Human-Robot Collaboration with a Multi-Rotor Aerial Vehicle](https://arxiv.org/abs/2509.10692)
*Giuseppe Silano,Amr Afifi,Martin Saska,Antonio Franchi*

Main category: cs.RO

TL;DR: STL-based optimization framework for MRAV motion planning and risk analysis in human-robot collaboration, with event-triggered replanning and uncertainty-aware risk assessment, validated in simulation.


<details>
  <summary>Details</summary>
Motivation: To enable safe, ergonomic, and efficient MRAV-assisted human-robot collaboration under uncertainty in tasks like handover and maintenance, addressing nonlinear, non-convex optimization and disturbances.

Method: Encode mission objectives in Signal Temporal Logic; solve a dynamically feasible trajectory optimization with smooth approximations and gradient-based methods; incorporate uncertainty-aware risk analysis; implement event-triggered replanning; validate in MATLAB and Gazebo on a handover task in a mock power-line maintenance environment.

Result: Demonstrates safe, efficient, and resilient collaboration under disturbances, with effective planning and replanning capabilities in simulation.

Conclusion: An STL-guided optimization and risk-analysis framework provides a practical path to robust MRAV-human collaboration, combining formal objectives, dynamic re-planning, and uncertainty handling for real-world scenarios.

Abstract: This paper presents a novel approach to motion planning and risk analysis for
enhancing human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV).
The proposed method uses Signal Temporal Logic (STL) to encode key mission
objectives, such as safety, timing, and human preferences, with a strong focus
on ergonomics and comfort. An optimization framework generates dynamically
feasible trajectories while considering the MRAV's physical constraints. Given
the nonlinear and non-convex nature of the problem, smooth approximations and
gradient-based techniques assist in handling the problem's computational
complexity. Additionally, an uncertainty-aware risk analysis is incorporated to
assess potential deviations from the mission specifications, providing insights
into the likelihood of mission success under uncertain conditions. Further, an
event-triggered replanning strategy is implemented to respond to unforeseen
events and external disturbances. The approach is validated through MATLAB and
Gazebo simulations, using an object handover task in a mock-up environment
inspired by power line maintenance scenarios. The results highlight the
method's effectiveness in achieving safe, efficient, and resilient human-robot
collaboration.

</details>


### [148] [A Survey on LiDAR-based Autonomous Aerial Vehicles](https://arxiv.org/abs/2509.10730)
*Yunfan Ren,Yixi Cai,Haotian Li,Nan Chen,Fangcheng Zhu,Longji Yin,Fanze Kong,Rundong Li,Fu Zhang*

Main category: cs.RO

TL;DR: A survey of LiDAR-based UAVs covering design, perception, planning, and control, highlighting sensor advances, integration benefits, applications, challenges, and future directions for single and multi-UAV systems.


<details>
  <summary>Details</summary>
Motivation: To synthesize recent progress in LiDAR-enabled UAV autonomy and provide guidance for researchers and practitioners in GPS-denied environments.

Method: Systematic literature survey and qualitative synthesis of LiDAR sensor evolution, integration with UAVs, perception, planning and control methods, applications, and challenges; includes discussion of multi-UAV collaboration.

Result: Provides a comprehensive overview of state-of-the-art LiDAR-based UAV systems, clarifying components, adoption, and practical applications, and identifying challenges and future research directions.

Conclusion: LiDAR-based UAVs enable high-speed, reliable autonomous flight in challenging environments; ongoing work is needed to address remaining challenges and advance multi-UAV collaboration.

Abstract: This survey offers a comprehensive overview of recent advancements in
LiDAR-based autonomous Unmanned Aerial Vehicles (UAVs), covering their design,
perception, planning, and control strategies. Over the past decade, LiDAR
technology has become a crucial enabler for high-speed, agile, and reliable UAV
navigation, especially in GPS-denied environments. The paper begins by
examining the evolution of LiDAR sensors, emphasizing their unique advantages
such as high accuracy, long-range depth measurements, and robust performance
under various lighting conditions, making them particularly well-suited for UAV
applications. The integration of LiDAR with UAVs has significantly enhanced
their autonomy, enabling complex missions in diverse and challenging
environments. Subsequently, we explore essential software components, including
perception technologies for state estimation and mapping, as well as trajectory
planning and control methodologies, and discuss their adoption in LiDAR-based
UAVs. Additionally, we analyze various practical applications of the
LiDAR-based UAVs, ranging from industrial operations to supporting different
aerial platforms and UAV swarm deployments. The survey concludes by discussing
existing challenges and proposing future research directions to advance
LiDAR-based UAVs and enhance multi-UAV collaboration. By synthesizing recent
developments, this paper aims to provide a valuable resource for researchers
and practitioners working to push the boundaries of LiDAR-based UAV systems.

</details>


### [149] [Analytical Design and Development of a Modular and Intuitive Framework for Robotizing and Enhancing the Existing Endoscopic Procedures](https://arxiv.org/abs/2509.10735)
*Mohammad Rafiee Javazm,Yash Kulkarni,Jiaqi Xue,Naruhiko Ikoma,Farshid Alambeigi*

Main category: cs.RO

TL;DR: A modular mechatronic framework for endoscopy is proposed, featuring a nested collet-chuck gripping mechanism, a feeder for insertion/retraction, and an intuitive UI to enable simultaneous control of all DoFs. It is paired with mathematical modeling and a design space to guide parameter choices, and validated through simulations and experiments.


<details>
  <summary>Details</summary>
Motivation: Manual control of endoscopic devices is challenging and leads to increased workload, fatigue, and distraction. There is a need for an intuitive, installable, modular robotic framework to assist clinicians.

Method: Design and develop a modular framework comprising (i) a novel nested collet-chuck gripping mechanism to interface with endoscopic devices and control bending DoFs; (ii) a feeder mechanism to actuate insertion/retraction of a colonoscope; (iii) a user interface enabling simultaneous control of all DoFs. The work includes a mathematical modeling approach and a design-space analysis for optimal parameter selection, and uses simulations and experimental studies to evaluate performance.

Result: Simulation and experimental studies thoroughly demonstrate the performance of the proposed mathematical model and robotic framework.

Conclusion: The framework provides an intuitive, modular, and easily installable solution to reduce manual workload/ fatigue during endoscopic procedures, supports simultaneous multi-DoF control, and offers a design-space-guided approach for selecting gripping/feeder parameters.

Abstract: Despite the widespread adoption of endoscopic devices for several cancer
screening procedures, manual control of these devices still remains challenging
for clinicians, leading to several critical issues such as increased workload,
fatigue, and distractions. To address these issues, in this paper, we introduce
the design and development of an intuitive, modular, and easily installable
mechatronic framework. This framework includes (i) a novel nested collet-chuck
gripping mechanism that can readily be integrated and assembled with the
existing endoscopic devices and control their bending degrees-of-freedom
(DoFs); (ii) a feeder mechanism that can control the insertion/retraction DoF
of a colonoscope, and (iii) a complementary and intuitive user interface that
enables simultaneous control of all DoFs during the procedure. To analyze the
design of the proposed mechanisms, we also introduce a mathematical modeling
approach and a design space for optimal selection of the parameters involved in
the design of gripping and feeder mechanisms. Our simulation and experimental
studies thoroughly demonstrate the performance of the proposed mathematical
modeling and robotic framework.

</details>


### [150] [FastTrack: GPU-Accelerated Tracking for Visual SLAM](https://arxiv.org/abs/2509.10757)
*Kimia Khabiri,Parsa Hosseininejad,Shishir Gopinath,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: GPU-accelerated tracking enhances ORB-SLAM3 performance via CUDA, speeding stereo feature matching and local map tracking, achieving up to 2.8x speedups on EuRoC/TUM-VI.


<details>
  <summary>Details</summary>
Motivation: To ensure timely tracking and reduce localization loss by speeding up time-consuming components (stereo feature matching and local map tracking) in a visual-inertial SLAM system.

Method: Integrates CUDA-based GPU acceleration into the ORB-SLAM3 tracking pipeline to accelerate stereo feature matching and local map tracking.

Result: Demonstrates up to 2.8x improvement in tracking performance on desktop and Jetson Xavier NX in stereo-inertial mode; evaluated on EuRoC and TUM-VI datasets.

Conclusion: GPU acceleration is effective for real-time visual-inertial SLAM tracking, enabling faster and potentially more robust localization.

Abstract: The tracking module of a visual-inertial SLAM system processes incoming image
frames and IMU data to estimate the position of the frame in relation to the
map. It is important for the tracking to complete in a timely manner for each
frame to avoid poor localization or tracking loss. We therefore present a new
approach which leverages GPU computing power to accelerate time-consuming
components of tracking in order to improve its performance. These components
include stereo feature matching and local map tracking. We implement our design
inside the ORB-SLAM3 tracking process using CUDA. Our evaluation demonstrates
an overall improvement in tracking performance of up to 2.8x on a desktop and
Jetson Xavier NX board in stereo-inertial mode, using the well-known SLAM
datasets EuRoC and TUM-VI.

</details>


### [151] [RSL-RL: A Learning Library for Robotics Research](https://arxiv.org/abs/2509.10771)
*Clemens Schwarke,Mayank Mittal,Nikita Rudin,David Hoeller,Marco Hutter*

Main category: cs.RO

TL;DR: RSL-RL is an open-source, GPU-optimized, compact RL library for robotics, focused on widely used algorithms and robotics-specific techniques, validated in simulation and real robots.


<details>
  <summary>Details</summary>
Motivation: There is a need for a lightweight, easily modifiable RL toolkit tailored to robotics, rather than broad general-purpose frameworks, to accelerate research and deployment.

Method: Develop a compact codebase that is easy to modify; include robotics-focused algorithms and auxiliary techniques; optimize for GPU-only training and high-throughput in large-scale simulations.

Result: Demonstrated effectiveness in both simulation benchmarks and real-world robotic experiments, proving utility as a practical, extensible framework for learning-based robotic controllers.

Conclusion: RSL-RL provides a practical, extensible, lightweight RL framework for robotics, and its open-source release facilitates research and deployment.

Abstract: RSL-RL is an open-source Reinforcement Learning library tailored to the
specific needs of the robotics community. Unlike broad general-purpose
frameworks, its design philosophy prioritizes a compact and easily modifiable
codebase, allowing researchers to adapt and extend algorithms with minimal
overhead. The library focuses on algorithms most widely adopted in robotics,
together with auxiliary techniques that address robotics-specific challenges.
Optimized for GPU-only training, RSL-RL achieves high-throughput performance in
large-scale simulation environments. Its effectiveness has been validated in
both simulation benchmarks and in real-world robotic experiments, demonstrating
its utility as a lightweight, extensible, and practical framework to develop
learning-based robotic controllers. The library is open-sourced at:
https://github.com/leggedrobotics/rsl_rl.

</details>


### [152] [Follow-Bench: A Unified Motion Planning Benchmark for Socially-Aware Robot Person Following](https://arxiv.org/abs/2509.10796)
*Hanjing Ye,Weixi Situ,Jianwei Peng,Yu Zhan,Bingyi Xia,Kuanqi Cai,Hong Zhang*

Main category: cs.RO

TL;DR: First end-to-end study of Robot Person Following (RPF): surveys scenarios and methods, introduces Follow-Bench benchmark, re-implements six planners, and validates in simulation and on a real robot; analyzes safety and comfort trade-offs.


<details>
  <summary>Details</summary>
Motivation: RPF has great potential but lacks a unified end-to-end evaluation framework that explicitly balances safety and comfort; existing work is fragmented across scenarios, planners, and metrics.

Method: Survey representative RPF scenarios, motion-planning methods, and evaluation metrics; develop Follow-Bench—an integrated benchmark simulating diverse target trajectories, dynamic crowds, and layouts; re-implement six popular RPF planners; evaluate top performers in simulation and on a differential-drive robot.

Result: Quantitative insights into safety-comfort trade-offs of existing planners; identification of open challenges; demonstration of real-world deployment feasibility; Follow-Bench provides a unified evaluation platform for RPF research.

Conclusion: Establishes a baseline for safe and comfortable RPF, introduces Follow-Bench as a standard benchmark, and highlights key open challenges and future research directions to improve safety and comfort in real-world deployments.

Abstract: Robot person following (RPF) -- mobile robots that follow and assist a
specific person -- has emerging applications in personal assistance, security
patrols, eldercare, and logistics. To be effective, such robots must follow the
target while ensuring safety and comfort for both the target and surrounding
people. In this work, we present the first end-to-end study of RPF, which (i)
surveys representative scenarios, motion-planning methods, and evaluation
metrics with a focus on safety and comfort; (ii) introduces Follow-Bench, a
unified benchmark simulating diverse scenarios, including various target
trajectory patterns, dynamic-crowd flows, and environmental layouts; and (iii)
re-implements six popular RPF planners, ensuring that both safety and comfort
are systematically considered. Moreover, we evaluate the two highest-performing
planners from our benchmark on a differential-drive robot to provide insights
into real-world deployment. Extensive simulation and real-world experiments
provide quantitative insights into the safety-comfort trade-offs of existing
planners, while revealing open challenges and future research directions.

</details>


### [153] [A Universal Wire Testing Machine for Enhancing the Performance of Wire-Driven Robots](https://arxiv.org/abs/2509.10862)
*Temma Suzuki,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: Introduces a Universal Wire Testing Machine to measure and tune wire characteristics, enabling removal of initial stretch, evaluating tension transmission across eight passive pulley diameters, and characterizing dynamic wire behavior; applying these data improves force control in a wire-driven robot and reduces end-effector force error.


<details>
  <summary>Details</summary>
Motivation: Wires offer lightweight, low-friction transmission but their compliance introduces modeling errors, limiting adoption in industrial/research robots; there is a need for systematic measurement and calibration to enable reliable wire-driven actuation.

Method: Design and use a Universal Wire Testing Machine to (1) remove initial wire stretch, (2) measure tension transmission efficiency across eight passive pulley diameters, (3) measure dynamic behavior of variable-length wires, and (4) apply the resulting data to force control of a real wire-driven robot.

Result: The testing data were used to improve force control, resulting in reduced end-effector force error in the actual wire-driven robot.

Conclusion: A practical characterization and calibration platform for wire-driven systems; can enable more accurate models and broader adoption by providing actionable data for control and design.

Abstract: Compared with gears and linkages, wires constitute a lightweight,
low-friction transmission mechanism. However, because wires are flexible
materials, they tend to introduce large modeling errors, and their adoption in
industrial and research robots remains limited.In this study, we built a
Universal Wire Testing Machine that enables measurement and adjustment of wire
characteristics to improve the performance of wire-driven mechanisms. Using
this testing machine, we carried out removal of initial wire stretch,
measurement of tension transmission efficiency for eight different diameters of
passive pulleys, and measurement of the dynamic behavior of variable-length
wires. Finally, we applied the data obtained from this testing machine to the
force control of an actual wire-driven robot, reducing the end-effector force
error.

</details>


### [154] [Nav-R1: Reasoning and Navigation in Embodied Scenes](https://arxiv.org/abs/2509.10884)
*Qingxiang Liu,Ting Huang,Zeyu Zhang,Hao Tang*

Main category: cs.RO

TL;DR: Nav-R1 is an embodied foundation model that unifies reasoning for navigation via a CoT dataset (Nav-CoT-110K), a GRPO RL framework with format/understanding/navigation rewards, and a Fast-in-Slow reasoning paradigm, achieving ~8% gains and real-world robot validation.


<details>
  <summary>Details</summary>
Motivation: Embodied navigation currently suffers from incoherent reasoning traces, instability, and the clash between long-horizon semantic reasoning and low-latency control. A unified, scalable CoT-based approach is needed to improve generalization and real-time performance.

Method: Construct Nav-CoT-110K dataset of step-by-step CoTs for embodied tasks; implement Nav-R1 with GRPO-based RL and three rewards (format, understanding, navigation); introduce Fast-in-Slow reasoning to decouple deliberative reasoning from reactive control.

Result: Extensive evaluations on embodied AI benchmarks show Nav-R1 consistently outperforms baselines, with over 8% average improvement in reasoning and navigation; real-world mobile robot deployment demonstrates robustness under limited onboard resources.

Conclusion: Nav-R1 demonstrates that unifying structured CoT reasoning with decoupled fast-slow processing yields improved generalization and performance in embodied navigation, with strong potential for broader real-world deployment.

Abstract: Embodied navigation requires agents to integrate perception, reasoning, and
action for robust interaction in complex 3D environments. Existing approaches
often suffer from incoherent and unstable reasoning traces that hinder
generalization across diverse environments, and difficulty balancing
long-horizon semantic reasoning with low-latency control for real-time
navigation. To address these challenges, we propose Nav-R1, an embodied
foundation model that unifies reasoning in embodied environments. We first
construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought
(CoT) for embodied tasks, which enables cold-start initialization with
structured reasoning. Building on this foundation, we design a GRPO-based
reinforcement learning framework with three complementary rewards: format,
understanding, and navigation, to improve structural adherence, semantic
grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow
reasoning paradigm, decoupling deliberate semantic reasoning from low-latency
reactive control for efficient yet coherent navigation. Extensive evaluations
on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms
strong baselines, with over 8% average improvement in reasoning and navigation
performance. Real-world deployment on a mobile robot further validates its
robustness under limited onboard resources. Code:
https://github.com/AIGeeksGroup/Nav-R1. Website:
https://aigeeksgroup.github.io/Nav-R1.

</details>


### [155] [Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics](https://arxiv.org/abs/2509.10888)
*Weijie Liu,Ziyi Qiu,Shihang Wang,Deqing Mei,Yancheng Wang*

Main category: cs.RO

TL;DR: A CDMA-inspired, orthogonal digital encoding scheme for soft tactile sensors enables parallel sensing with drastically reduced wiring and scalable, low-latency perception; demonstrated on a 16-node array with 12.8 ms latency and sub-20 ms latency for thousands of nodes.


<details>
  <summary>Details</summary>
Motivation: To overcome encoding efficiency and wiring bottlenecks in large-area, soft-bodied tactile sensing to achieve human-skin-like perception and real-time performance.

Method: Decentralized encoding using orthogonal base codes, inspired by code-division multiple access (CDMA); enables parallel superposition of signals from distributed nodes; single transmission wire; validated with a 16-node array.

Result: Reconstructed pressure distribution; temporal resolution of 12.8 ms with only a single transmission wire; architecture can maintain sub-20 ms latency across orders-of-magnitude variations in node number (to thousands).

Conclusion: Redefines signal encoding paradigms in soft electronics, enabling scalable embodied intelligent systems with human-like sensory capabilities.

Abstract: Human-like embodied tactile perception is crucial for the next-generation
intelligent robotics. Achieving large-area, full-body soft coverage with high
sensitivity and rapid response, akin to human skin, remains a formidable
challenge due to critical bottlenecks in encoding efficiency and wiring
complexity in existing flexible tactile sensors, thus significantly hinder the
scalability and real-time performance required for human skin-level tactile
perception. Herein, we present a new architecture employing code division
multiple access-inspired orthogonal digital encoding to overcome these
challenges. Our decentralized encoding strategy transforms conventional serial
signal transmission by enabling parallel superposition of energy-orthogonal
base codes from distributed sensing nodes, drastically reducing wiring
requirements and increasing data throughput. We implemented and validated this
strategy with off-the-shelf 16-node sensing array to reconstruct the pressure
distribution, achieving a temporal resolution of 12.8 ms using only a single
transmission wire. Crucially, the architecture can maintain sub-20ms latency
across orders-of-magnitude variations in node number (to thousands of nodes).
By fundamentally redefining signal encoding paradigms in soft electronics, this
work opens new frontiers in developing scalable embodied intelligent systems
with human-like sensory capabilities.

</details>


### [156] [ViSTR-GP: Online Cyberattack Detection via Vision-to-State Tensor Regression and Gaussian Processes in Automated Robotic Operations](https://arxiv.org/abs/2509.10948)
*Navid Aftabi,Philip Samaha,Jin Ma,Long Cheng,Ramy Harik,Dan Li*

Main category: cs.RO

TL;DR: ViSTR-GP provides online cross-modal verification for data-integrity attacks in robotic manufacturing using vision-based estimates to monitor encoder data, enabling earlier detection without extra instrumentation.


<details>
  <summary>Details</summary>
Motivation: Data-integrity attacks in cyber-physical systems are difficult to detect with traditional intrusion detection systems; robotic manufacturing requires timely detection; leveraging an independent vision channel offers a robust cross-check against controller-reported measurements.

Method: One-time interactive segmentation initializes SAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate maps each mask to measurements, while a matrix-variate Gaussian process models nominal residuals to capture temporal structure and cross-joint correlations. A frame-wise test statistic derived from the predictive distribution enables online detection with interpretable thresholds. Validation on a real-world robotic testbed with synchronized video and encoder data, including replay attack scenarios with graded end-effector deviations.

Result: The framework accurately recovers joint angles and detects data-integrity attacks earlier with more frequent alarms than baselines, particularly for subtle attacks.

Conclusion: An independent physical channel outside the controller’s authority can detect data-integrity attacks without complex instrumentation, improving resilience of robotic manufacturing systems.

Abstract: Industrial robotic systems are central to automating smart manufacturing
operations. Connected and automated factories face growing cybersecurity risks
that can potentially cause interruptions and damages to physical operations.
Among these attacks, data-integrity attacks often involve sophisticated
exploitation of vulnerabilities that enable an attacker to access and
manipulate the operational data and are hence difficult to detect with only
existing intrusion detection or model-based detection. This paper addresses the
challenges in utilizing existing side-channels to detect data-integrity attacks
in robotic manufacturing processes by developing an online detection framework,
ViSTR-GP, that cross-checks encoder-reported measurements against a
vision-based estimate from an overhead camera outside the controller's
authority. In this framework, a one-time interactive segmentation initializes
SAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate
maps each mask to measurements, while a matrix-variate Gaussian process models
nominal residuals, capturing temporal structure and cross-joint correlations. A
frame-wise test statistic derived from the predictive distribution provides an
online detector with interpretable thresholds. We validate the framework on a
real-world robotic testbed with synchronized video frame and encoder data,
collecting multiple nominal cycles and constructing replay attack scenarios
with graded end-effector deviations. Results on the testbed indicate that the
proposed framework recovers joint angles accurately and detects data-integrity
attacks earlier with more frequent alarms than all baselines. These
improvements are most evident in the most subtle attacks. These results show
that plants can detect data-integrity attacks by adding an independent physical
channel, bypassing the controller's authority, without needing complex
instrumentation.

</details>


### [157] [ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation](https://arxiv.org/abs/2509.10952)
*Yangcen Liu,Woo Chul Shin,Yunhai Han,Zhenyang Chen,Harish Ravichandar,Danfei Xu*

Main category: cs.RO

TL;DR: ImMimic proposes an embodiment-agnostic co-training framework that uses DTW-based mapping from human videos to robot joints and MixUp interpolation to bridge visual, morphological, and physical domain gaps, improving manipulation performance across multiple tasks and robot embodiments.


<details>
  <summary>Details</summary>
Motivation: To scale robot manipulation by leveraging abundant human videos while addressing cross-domain gaps in appearance, morphology, and physics that hinder direct imitation from human demonstrations.

Method: ImMimic combines (1) retargeted human hand trajectories mapped to robot joints via DTW with either action- or visual-based mapping, and (2) MixUp interpolation between paired human and robot trajectories to create intermediate domains. The framework uses co-training with a small set of teleoperated robot demonstrations to align the domains.

Result: Evaluations on four real-world tasks (Pick and Place, Push, Hammer, Flip) across four robots (Robotiq, Fin Ray, Allegro, Ability) show increased task success rates and smoother execution when using ImMimic, indicating effective domain bridging for robust manipulation.

Conclusion: Embodiment-agnostic co-training with DTW-based retargeting and MixUp interpolation effectively bridges visual, morphological, and physical domain gaps, enabling robust manipulation across diverse embodiments and tasks.

Abstract: Learning robot manipulation from abundant human videos offers a scalable
alternative to costly robot-specific data collection. However, domain gaps
across visual, morphological, and physical aspects hinder direct imitation. To
effectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic
co-training framework that leverages both human videos and a small amount of
teleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with
either action- or visual-based mapping to map retargeted human hand poses to
robot joints, followed by MixUp interpolation between paired human and robot
trajectories. Our key insights are (1) retargeted human hand trajectories
provide informative action labels, and (2) interpolation over the mapped data
creates intermediate domains that facilitate smooth domain adaptation during
co-training. Evaluations on four real-world manipulation tasks (Pick and Place,
Push, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro,
Ability) show that ImMimic improves task success rates and execution
smoothness, highlighting its efficacy to bridge the domain gap for robust robot
manipulation. The project website can be found at
https://sites.google.com/view/immimic.

</details>


### [158] [Pogosim -- a Simulator for Pogobot robots](https://arxiv.org/abs/2509.10968)
*Leo Cazenille,Loona Macabre,Nicolas Bredeche*

Main category: cs.RO

TL;DR: Pogosim is a fast, scalable simulator for Pogobots that enables code reuse between simulation and real robots, supports large-scale parallel experiments and parameter optimization, and aims to reduce swarm robotics development costs.


<details>
  <summary>Details</summary>
Motivation: To reduce the labor-intensive testing of distributed algorithms on real Pogobots, and to scale experiments and calibrate parameters within limited resources.

Method: Describe Pogosim’s software architecture, how to write configuration files and user programs, how simulations approximate or differ from real experiments, how to launch large sets of simulations in parallel, how results are retrieved and analyzed, and how optimization algorithms can tune user code parameters.

Result: Pogosim enables large-scale, parallel simulations with consistent code paths to real robots, facilitating result retrieval, analysis, and parameter optimization, thereby reducing development costs and accelerating algorithm development.

Conclusion: Pogosim bridges simulation and hardware for Pogobots, providing a cost-effective, scalable environment that accelerates swarm robotics research by enabling rapid testing and parameter tuning.

Abstract: Pogobots are a new type of open-source/open-hardware robots specifically
designed for swarm robotics research. Their cost-effective and modular design,
complemented by vibration-based and wheel-based locomotion, fast infrared
communication and extensive software architecture facilitate the implementation
of swarm intelligence algorithms. However, testing even simple distributed
algorithms directly on robots is particularly labor-intensive. Scaling to more
complex problems or calibrate user code parameters will have a prohibitively
high strain on available resources. In this article we present Pogosim, a fast
and scalable simulator for Pogobots, designed to reduce as much as possible
algorithm development costs. The exact same code will be used in both
simulation and to experimentally drive real robots. This article details the
software architecture of Pogosim, explain how to write configuration files and
user programs and how simulations approximate or differ from experiments. We
describe how a large set of simulations can be launched in parallel, how to
retrieve and analyze the simulation results, and how to optimize user code
parameters using optimization algorithms.

</details>


### [159] [Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter](https://arxiv.org/abs/2509.10979)
*Dimitri Jacquemont,Carlo Bosio,Teaya Yang,Ruiqi Zhang,Ozgur Orun,Shuai Li,Reza Alam,Thomas M. Schutzius,Simo A. Makiharju,Mark W. Mueller*

Main category: cs.RO

TL;DR: A quadcopter UAV autonomously applies PV protective coatings using onboard sensing and a model-based controller, validated indoors and outdoors.


<details>
  <summary>Details</summary>
Motivation: Maintaining anti-reflective/self-cleaning coatings on photovoltaic panels is essential for efficiency but coatings degrade over time and require costly, labor-intensive reapplication. An autonomous UAV approach could reduce cost, increase frequency of maintenance, and improve coverage.

Method: Design of a quadcopter with a liquid dispersion mechanism, a purely onboard localization stack (visual-inertial odometry and relative pose to the PV panel), and a model-based controller that accounts for ground effect and the changing mass during dispersion. Extensive indoor and outdoor experiments validate autonomy.

Result: The system demonstrated autonomous operation capabilities for coating dispersion, with successful validation in both indoor and outdoor environments.

Conclusion: A UAV-based autonomous coating deployment system is viable for PV panels, offering potential reductions in maintenance cost and enabling more frequent reapplication of protective coatings.

Abstract: Photovoltaic (PV) panels are becoming increasingly widespread in the domain
of renewable energy, and thus, small efficiency gains can have massive effects.
Anti-reflective and self-cleaning coatings enhance panel performance but
degrade over time, requiring periodic reapplication. Uncrewed Aerial Vehicles
(UAVs) offer a flexible and autonomous way to apply protective coatings more
often and at lower cost compared to traditional manual coating methods. In this
letter, we propose a quadcopter-based system, equipped with a liquid dispersion
mechanism, designed to automate such tasks. The localization stack only uses
onboard sensors, relying on visual-inertial odometry and the relative position
of the PV panel detected with respect to the quadcopter. The control relies on
a model-based controller that accounts for the ground effect and the mass
decrease of the quadcopter during liquid dispersion. We validate the autonomy
capabilities of our system through extensive indoor and outdoor experiments.

</details>


### [160] [Multi-objective task allocation for electric harvesting robots: a hierarchical route reconstruction approach](https://arxiv.org/abs/2509.11025)
*Peng Chen,Jing Liang,Hui Song,Kang-Jia Qiao,Cai-Tong Yue,Kun-Jie Yu,Ponnuthurai Nagaratnam Suganthan,Witold Pedrycz*

Main category: cs.RO

TL;DR: Hybrid hierarchical route reconstruction for multi-robot agricultural task allocation (AMERTA) improves trade-offs between makespan and energy under load-dependent speed and battery constraints; validated on 45 instances showing superiority over seven baselines with statistical support.


<details>
  <summary>Details</summary>
Motivation: Address rising labor costs and the need for efficient, real-world coordinated control of multiple robots in orchards, acknowledging practical constraints like load-dependent speeds and limited batteries.

Method: Propose AMERTA problem formulation and a hybrid hierarchical route reconstruction algorithm (HRRA) featuring a hierarchical encoding, a dual-phase initialization, task sequence optimizers, and specialized route reconstruction operators; extensive experiments on 45 test instances with statistical analysis (Wilcoxon signed-rank and Friedman tests).

Result: HRRA outperforms seven state-of-the-art algorithms across the tested instances, and empirical analysis indicates it can explore previously inaccessible regions of the solution space.

Conclusion: The work advances multi-robot coordination theory by presenting a novel problem formulation (AMERTA) and an effective solution (HRRA), offering both theoretical insights and practical guidance for agricultural automation.

Abstract: The increasing labor costs in agriculture have accelerated the adoption of
multi-robot systems for orchard harvesting. However, efficiently coordinating
these systems is challenging due to the complex interplay between makespan and
energy consumption, particularly under practical constraints like
load-dependent speed variations and battery limitations. This paper defines the
multi-objective agricultural multi-electrical-robot task allocation (AMERTA)
problem, which systematically incorporates these often-overlooked real-world
constraints. To address this problem, we propose a hybrid hierarchical route
reconstruction algorithm (HRRA) that integrates several innovative mechanisms,
including a hierarchical encoding structure, a dual-phase initialization
method, task sequence optimizers, and specialized route reconstruction
operators. Extensive experiments on 45 test instances demonstrate HRRA's
superior performance against seven state-of-the-art algorithms. Statistical
analysis, including the Wilcoxon signed-rank and Friedman tests, empirically
validates HRRA's competitiveness and its unique ability to explore previously
inaccessible regions of the solution space. In general, this research
contributes to the theoretical understanding of multi-robot coordination by
offering a novel problem formulation and an effective algorithm, thereby also
providing practical insights for agricultural automation.

</details>


### [161] [FEWT: Improving Humanoid Robot Perception with Frequency-Enhanced Wavelet-based Transformers](https://arxiv.org/abs/2509.11109)
*Jiaxin Huang,Hanyu Liu,Yunsheng Ma,Jian Shen,Yilin Zheng,Jiayi Wen,Baishu Wan,Pan Li,Zhigong Song*

Main category: cs.RO

TL;DR: Hardware-enabled humanoid robot platform paired with a Frequency-Enhanced Wavelet-based Transformer (FEWT) for imitation learning, achieving significant performance gains over the ACT baseline (up to 30% in simulation, 6–12% in real-world).


<details>
  <summary>Details</summary>
Motivation: Bridge physical embodiment and information space for humanoid robots; enable intuitive remote manipulation and efficient collection of anthropomorphic action data; improve perception representations to enhance imitation learning.

Method: Introduce a hardware platform with a humanoid robot and an exoskeleton teleoperation cabin. Propose FEWT, an imitation learning framework with two modules: FE-EMA (Frequency-Enhanced Efficient Multi-Scale Attention) and TS-DWT (Time-Series Discrete Wavelet Transform). FE-EMA fuses time-domain and frequency-domain features across multiple scales using wavelet decomposition and residual networks, improving feature robustness for imitation learning.

Result: FEWT improves the state-of-the-art ACT baseline’s success rate by up to 30% in simulation and by 6–12% in real-world experiments.

Conclusion: FEWT’s frequency-aware, multi-scale feature fusion enhances imitation learning robustness and performance for humanoid robotics, validating the effectiveness of combining wavelet-based time-frequency representations with transformer-style attention in a hardware-enabled setting.

Abstract: The embodied intelligence bridges the physical world and information space.
As its typical physical embodiment, humanoid robots have shown great promise
through robot learning algorithms in recent years. In this study, a hardware
platform, including humanoid robot and exoskeleton-style teleoperation cabin,
was developed to realize intuitive remote manipulation and efficient collection
of anthropomorphic action data. To improve the perception representation of
humanoid robot, an imitation learning framework, termed Frequency-Enhanced
Wavelet-based Transformer (FEWT), was proposed, which consists of two primary
modules: Frequency-Enhanced Efficient Multi-Scale Attention (FE-EMA) and
Time-Series Discrete Wavelet Transform (TS-DWT). By combining multi-scale
wavelet decomposition with the residual network, FE-EMA can dynamically fuse
features from both time-domain and frequency-domain. This fusion is able to
capture feature information across various scales effectively, thereby
enhancing model robustness. Experimental performance demonstrates that FEWT
improves the success rate of the state-of-the-art algorithm (Action Chunking
with Transformers, ACT baseline) by up to 30% in simulation and by 6-12% in
real-world.

</details>


### [162] [ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations](https://arxiv.org/abs/2509.11125)
*Zheng Li,Pei Qu,Yufei Jia,Shihui Zhou,Haizhou Ge,Jiahang Cao,Jinni Zhou,Guyue Zhou,Jun Ma*

Main category: cs.RO

TL;DR: A view-invariant 3D RL framework for robotic manipulation (ManiVID-3D) using ViewNet to align arbitrary viewpoints and a fast GPU renderer, enabling scalable training and improved robustness to viewpoint changes.


<details>
  <summary>Details</summary>
Motivation: Policy performance degrades when the camera viewpoint changes; there is a need for calibration-free, view-invariant representations to operate in unstructured real-world settings.

Method: ManiVID-3D introduces ViewNet to geometrically align point clouds from different viewpoints into a common coordinate system without extrinsic calibration, plus an efficient GPU-accelerated batch renderer for thousands of frames per second; employs self-supervised disentangled feature learning to obtain geometrically consistent representations for 3D visual RL.

Result: Outperforms state-of-the-art by 44.7% higher success rate under viewpoint variation and uses 80% fewer parameters; demonstrates strong sim-to-real transfer and robustness to severe perspective changes across 10 simulated and 5 real tasks.

Conclusion: Learning geometrically consistent 3D representations with view alignment and fast rendering enables scalable, robust 3D visual RL for robotic manipulation in unstructured environments.

Abstract: Deploying visual reinforcement learning (RL) policies in real-world
manipulation is often hindered by camera viewpoint changes. A policy trained
from a fixed front-facing camera may fail when the camera is shifted--an
unavoidable situation in real-world settings where sensor placement is hard to
manage appropriately. Existing methods often rely on precise camera calibration
or struggle with large perspective changes. To address these limitations, we
propose ManiVID-3D, a novel 3D RL architecture designed for robotic
manipulation, which learns view-invariant representations through
self-supervised disentangled feature learning. The framework incorporates
ViewNet, a lightweight yet effective module that automatically aligns point
cloud observations from arbitrary viewpoints into a unified spatial coordinate
system without the need for extrinsic calibration. Additionally, we develop an
efficient GPU-accelerated batch rendering module capable of processing over
5000 frames per second, enabling large-scale training for 3D visual RL at
unprecedented speeds. Extensive evaluation across 10 simulated and 5 real-world
tasks demonstrates that our approach achieves a 44.7% higher success rate than
state-of-the-art methods under viewpoint variations while using 80% fewer
parameters. The system's robustness to severe perspective changes and strong
sim-to-real performance highlight the effectiveness of learning geometrically
consistent representations for scalable robotic manipulation in unstructured
environments. Our project website can be found in
https://zheng-joe-lee.github.io/manivid3d/.

</details>


### [163] [RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations](https://arxiv.org/abs/2509.11149)
*Mintae Kim,Jiaze Cai,Koushil Sreenath*

Main category: cs.RO

TL;DR: RoVerFly is a reinforcement-learning–based, unified tracking controller for quadrotors with and without cable-suspended payloads. It achieves zero-shot generalization across payload variations (no payload, different masses, different cable lengths) without switching or retuning, by training with task and domain randomization, while preserving a feedback-tracking controller's interpretability.


<details>
  <summary>Details</summary>
Motivation: Precise trajectory tracking for quadrotors is hampered by nonlinear dynamics and underactuation; adding cable-suspended payloads increases degrees of freedom and introduces hybrid dynamics. Classical model-based methods offer guarantees but require extensive tuning and struggle to adapt to configuration changes (payload added/removed, mass or length changes).

Method: A unified RL-based control framework (RoVerFly) where an RL policy acts as a robust tracking controller for standard quadrotors and cable-suspended payloads. Trained with task and domain randomization to promote robustness and generalization, preserving the interpretability and structure of a feedback controller.

Result: The approach shows strong zero-shot generalization across payload settings (including no payload, varying mass, and varying cable length) without controller switching or re-tuning, and is resilient to disturbances and dynamic changes. The solution retains the structure of a feedback tracking controller. Code and supplementary materials are provided.

Conclusion: RL-based unified control can generalize across diverse payload configurations without re-tuning, offering robust, adaptable trajectory tracking for quadrotors with payloads while maintaining interpretability; code is publicly available for reproduction.

Abstract: Designing robust controllers for precise, arbitrary trajectory tracking with
quadrotors is challenging due to nonlinear dynamics and underactuation, and
becomes harder with flexible cable-suspended payloads that introduce extra
degrees of freedom and hybridness. Classical model-based methods offer
stability guarantees but require extensive tuning and often do not adapt when
the configuration changes, such as when a payload is added or removed, or when
the payload mass or cable length varies. We present RoVerFly, a unified
learning-based control framework in which a reinforcement learning (RL) policy
serves as a robust and versatile tracking controller for standard quadrotors
and for cable-suspended payload systems across a range of configurations.
Trained with task and domain randomization, the controller is resilient to
disturbances and varying dynamics. It achieves strong zero-shot generalization
across payload settings, including no payload as well as varying mass and cable
length, without controller switching or re-tuning, while retaining the
interpretability and structure of a feedback tracking controller. Code and
supplementary materials are available at
https://github.com/mintaeshkim/roverfly

</details>


### [164] [SAMP: Spatial Anchor-based Motion Policy for Collision-Aware Robotic Manipulators](https://arxiv.org/abs/2509.11185)
*Kai Chen,Zhihai Bi,Guoyang Zhao,Chunxin Zheng,Yulin Li,Hang Zhao,Jun Ma*

Main category: cs.RO

TL;DR: A unified SDF-based anchor framework (SAMP) jointly encodes robot geometry and environment on a shared grid to train a neural motion policy for collision-free trajectories, improving success and reducing collisions in cluttered scenes.


<details>
  <summary>Details</summary>
Motivation: Existing motion planning often relies on simplified robot models or incomplete obstacle representations, leading to unsafe or suboptimal plans in cluttered environments. A precise, unified geometry model can improve collision detection and trajectory quality.

Method: Introduce spatial anchor-based motion policy (SAMP) with a dedicated robot SDF network to capture manipulator geometry; fuse robot and environment SDFs on spatial anchors; train a neural motion policy with an efficient feature alignment strategy to generate smooth, collision-free trajectories.

Result: Empirical evaluation in simulation and real-world tasks shows 11% higher success rate and 7% lower collision rate compared to baselines.

Conclusion: Joint geometry-aware modeling of robot and environment enhances safety and performance in motion planning, offering practical benefits for real-world manipulation in cluttered settings.

Abstract: Neural-based motion planning methods have achieved remarkable progress for
robotic manipulators, yet a fundamental challenge lies in simultaneously
accounting for both the robot's physical shape and the surrounding environment
when generating safe and feasible motions. Moreover, existing approaches often
rely on simplified robot models or focus primarily on obstacle representation,
which can lead to incomplete collision detection and degraded performance in
cluttered scenes. To address these limitations, we propose spatial anchor-based
motion policy (SAMP), a unified framework that simultaneously encodes the
environment and the manipulator using signed distance field (SDF) anchored on a
shared spatial grid. SAMP incorporates a dedicated robot SDF network that
captures the manipulator's precise geometry, enabling collision-aware reasoning
beyond coarse link approximations. These representations are fused on spatial
anchors and used to train a neural motion policy that generates smooth,
collision-free trajectories in the proposed efficient feature alignment
strategy. Experiments conducted in both simulated and real-world environments
consistently show that SAMP outperforms existing methods, delivering an 11%
increase in success rate and a 7% reduction in collision rate. These results
highlight the benefits of jointly modelling robot and environment geometry,
demonstrating its practical value in challenging real-world environments.

</details>


### [165] [DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2509.11197)
*Yunheng Wang,Yuetong Fang,Taowen Wang,Yixiao Feng,Yawen Tan,Shuning Zhang,Peiran Liu,Yiding Ji,Renjing Xu*

Main category: cs.RO

TL;DR: DreamNav advances zero-shot Vision-and-Language Navigation in continuous environments by combining egocentric perception alignment, global trajectory planning, and active imagination, achieving state-of-the-art results with no task-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot VLN methods rely on costly perception and point-level actions, leading to high sensory costs, misaligned action semantics, and limited long-horizon planning. There is a need for cheaper perception, semantics-aligned planning, and anticipatory reasoning in such tasks.

Method: Introduce EgoView Corrector to align and stabilize egocentric viewpoints; Trajectory Predictor for global trajectory-level planning; Imagination Predictor for proactive, long-horizon thinking. Evaluated on VLN-CE and real-world tests.

Result: DreamNav achieves a new zero-shot SOTA, outperforming the strongest egocentric baseline with extra information by up to 7.49% SR and 18.15% SPL.

Conclusion: First zero-shot VLN method to unify trajectory-level planning and active imagination using only egocentric inputs; reduces sensory costs and improves planning semantics; demonstrates strong generalization to real-world settings.

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
links language instructions to perception and control in the real world, is a
core capability of embodied robots. Recently, large-scale pretrained foundation
models have been leveraged as shared priors for perception, reasoning, and
action, enabling zero-shot VLN without task-specific training. However,
existing zero-shot VLN methods depend on costly perception and passive scene
understanding, collapsing control to point-level choices. As a result, they are
expensive to deploy, misaligned in action semantics, and short-sighted in
planning. To address these issues, we present DreamNav that focuses on the
following three aspects: (1) for reducing sensory cost, our EgoView Corrector
aligns viewpoints and stabilizes egocentric perception; (2) instead of
point-level actions, our Trajectory Predictor favors global trajectory-level
planning to better align with instruction semantics; and (3) to enable
anticipatory and long-horizon planning, we propose an Imagination Predictor to
endow the agent with proactive thinking capability. On VLN-CE and real-world
tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the
strongest egocentric baseline with extra information by up to 7.49\% and
18.15\% in terms of SR and SPL metrics. To our knowledge, this is the first
zero-shot VLN method to unify trajectory-level planning and active imagination
while using only egocentric inputs.

</details>


### [166] [MEMBOT: Memory-Based Robot in Intermittent POMDP](https://arxiv.org/abs/2509.11225)
*Youzhi Liang,Eyan Noronha*

Main category: cs.RO

TL;DR: MEMBOT is a memory-based modular architecture that learns a task-agnostic latent belief encoder via offline pretraining and then fine-tunes task policies with behavior cloning to tackle intermittent partial observability in robotic control, achieving robustness across dropout.


<details>
  <summary>Details</summary>
Motivation: Robotic systems often operate under partial and intermittent observability due to noisy/occluded sensors or outages. Standard RL assumes full observability and struggles in such settings; there is a need for persistent latent belief representations to drive robust policies.

Method: A two-phase training process: (1) offline multi-task pretraining to learn a robust task-agnostic latent belief encoder using reconstruction losses; (2) fine-tuning of task-specific policies via behavior cloning. The belief encoder uses a state-space model (SSM) and an LSTM to fuse sequences of observations and actions into latent states that persist when observations drop. This decouples belief inference from policy learning.

Result: MEMBOT outperforms both memoryless and naive recurrent baselines across 10 robotic manipulation tasks from MetaWorld and Robomimic, especially under observation dropout, maintaining up to 80% of peak performance when only 50% of observations are available.

Conclusion: Explicit belief modeling via MEMBOT yields robust, transferable, and data-efficient policies for real-world partially observable robotic systems.

Abstract: Robotic systems deployed in real-world environments often operate under
conditions of partial and often intermittent observability, where sensor inputs
may be noisy, occluded, or entirely unavailable due to failures or
environmental constraints. Traditional reinforcement learning (RL) approaches
that assume full state observability are ill-equipped for such challenges. In
this work, we introduce MEMBOT, a modular memory-based architecture designed to
address intermittent partial observability in robotic control tasks. MEMBOT
decouples belief inference from policy learning through a two-phase training
process: an offline multi-task learning pretraining stage that learns a robust
task-agnostic latent belief encoder using a reconstruction losses, followed by
fine-tuning of task-specific policies using behavior cloning. The belief
encoder, implemented as a state-space model (SSM) and a LSTM, integrates
temporal sequences of observations and actions to infer latent state
representations that persist even when observations are dropped. We train and
evaluate MEMBOT on 10 robotic manipulation benchmark tasks from MetaWorld and
Robomimic under varying rates of observation dropout. Results show that MEMBOT
consistently outperforms both memoryless and naively recurrent baselines,
maintaining up to 80% of peak performance under 50% observation availability.
These findings highlight the effectiveness of explicit belief modeling in
achieving robust, transferable, and data-efficient policies for real-world
partially observable robotic systems.

</details>


### [167] [CORB-Planner: Corridor as Observations for RL Planning in High-Speed Flight](https://arxiv.org/abs/2509.11240)
*Yechen Zhang,Bin Gao,Gang Wang,Jian Sun,Zhuo Li*

Main category: cs.RO

TL;DR: A real-time RL-based UAV trajectory planner combining B-spline trajectories with a Safe Flight Corridor (SFC) for cross-platform transfer, achieving 8.2 m/s in dense environments, trained via a progressive easy-to-hard simulation pipeline using a Soft Decomposed-Critic Q (SDCQ) algorithm.


<details>
  <summary>Details</summary>
Motivation: To enable cross-platform, robust UAV planning by reducing dependence on accurate dynamics and platform-specific sensing, and to close the sim-to-real gap for RL-based planners in aerial robotics.

Method: Generate B-spline trajectories with an RL policy that selects successive control points; derive a compact SFC representation via heuristic search to abstract obstacle information; train with an easy-to-hard progressive pipeline in simulation using a value-based SDCQ algorithm, achieving about ten minutes of training.

Result: Demonstrates real-time planning on lightweight onboard hardware, enabling up to 8.2 m/s speeds in cluttered environments without external positioning; works across quadrotors and hexarotors on modest onboard compute, showing generality and robustness.

Conclusion: CORB-Planner enables practical, platform-agnostic RL-based UAV trajectory planning with fast training and real-time onboard performance, mitigating model inaccuracies and enhancing cross-platform deployment.

Abstract: Reinforcement learning (RL) has shown promise in a large number of robotic
control tasks. Nevertheless, its deployment on unmanned aerial vehicles (UAVs)
remains challenging, mainly because of reliance on accurate dynamic models and
platform-specific sensing, which hinders cross-platform transfer. This paper
presents the CORB-Planner (Corridor-as-Observations for RL B-spline planner), a
real-time, RL-based trajectory planning framework for high-speed autonomous UAV
flight across heterogeneous platforms. The key idea is to combine B-spline
trajectory generation with the RL policy producing successive control points
with a compact safe flight corridor (SFC) representation obtained via heuristic
search. The SFC abstracts obstacle information in a low-dimensional form,
mitigating overfitting to platform-specific details and reducing sensitivity to
model inaccuracies. To narrow the sim-to-real gap, we adopt an easy-to-hard
progressive training pipeline in simulation. A value-based soft
decomposed-critic Q (SDCQ) algorithm is used to learn effective policies within
approximately ten minutes of training. Benchmarks in simulation and real-world
tests demonstrate real-time planning on lightweight onboard hardware and
support maximum flight speeds up to 8.2m/s in dense, cluttered environments
without external positioning. Compatibility with various UAV configurations
(quadrotors, hexarotors) and modest onboard compute underlines the generality
and robustness of CORB-Planner for practical deployment.

</details>


### [168] [Embodied Intelligence in Disassembly: Multimodal Perception Cross-validation and Continual Learning in Neuro-Symbolic TAMP](https://arxiv.org/abs/2509.11270)
*Ziwen He,Zhigang Wang,Yanlong Peng,Pengxu Chang,Hong Yang,Ming Chen*

Main category: cs.RO

TL;DR: A Neuro-Symbolic task and motion planning framework with continual learning improves robustness of robotic disassembly in dynamic environments, achieving 100% task success and fewer perception errors.


<details>
  <summary>Details</summary>
Motivation: Address robustness and adaptability of robotic perception in dynamic unstructured disassembly environments for electric vehicle battery recycling.

Method: Integrates a Neuro-Symbolic TAMP framework with a multimodal perception cross-validation mechanism and a bidirectional reasoning flow. The forward flow dynamically refines action strategies, while the backward flow autonomously collects effective data from historical task executions to enable continual learning and self-optimization in changing environments.

Result: In dynamic disassembly scenarios, task success rate improved from 81.68% to 100%, and the average number of perception misjudgments decreased from 3.389 to 1.128.

Conclusion: Proposes a new paradigm for enhancing robustness and adaptability of embodied intelligence in complex industrial environments.

Abstract: With the rapid development of the new energy vehicle industry, the efficient
disassembly and recycling of power batteries have become a critical challenge
for the circular economy. In current unstructured disassembly scenarios, the
dynamic nature of the environment severely limits the robustness of robotic
perception, posing a significant barrier to autonomous disassembly in
industrial applications. This paper proposes a continual learning framework
based on Neuro-Symbolic task and motion planning (TAMP) to enhance the
adaptability of embodied intelligence systems in dynamic environments. Our
approach integrates a multimodal perception cross-validation mechanism into a
bidirectional reasoning flow: the forward working flow dynamically refines and
optimizes action strategies, while the backward learning flow autonomously
collects effective data from historical task executions to facilitate continual
system learning, enabling self-optimization. Experimental results show that the
proposed framework improves the task success rate in dynamic disassembly
scenarios from 81.68% to 100%, while reducing the average number of perception
misjudgments from 3.389 to 1.128. This research provides a new paradigm for
enhancing the robustness and adaptability of embodied intelligence in complex
industrial environments.

</details>


### [169] [Policy Learning for Social Robot-Led Physiotherapy](https://arxiv.org/abs/2509.11297)
*Carl Bettosi,Lynne Ballie,Susan Shenkin,Marta Romeo*

Main category: cs.RO

TL;DR: A reinforcement learning-based policy for a social robot guiding physiotherapy, trained in simulation using expert-practitioner proxies to model patient behavior, enabling personalized exercise instructions across different exertion tolerances and recovery stages despite limited real patient data.


<details>
  <summary>Details</summary>
Motivation: To overcome the scarcity of real patient behavior data which hampers robust policy development for autonomous physiotherapy robots; leveraging expert proxies can inform patient behavior models and enable adaptable guidance.

Method: Engaged 33 healthcare practitioners as patient proxies and collected their interactions with the robot. Built a patient behavior model that generates exercise performance metrics and subjective exertion scores, then trained a reinforcement learning policy in simulation that can tailor instructions to individual exertion tolerances and fluctuating performance, generalizing to different recovery stages and exercise plans.

Result: The RL policy demonstrated adaptation of exercise instructions to varying exertion tolerances and performance fluctuations in simulation, with applicability across patients at different recovery stages and with diverse exercise plans.

Conclusion: Using expert proxies to build a patient behavior model and training policies in simulation can address data scarcity and yield adaptive, individualized robot-guided physiotherapy across a range of patient profiles.

Abstract: Social robots offer a promising solution for autonomously guiding patients
through physiotherapy exercise sessions, but effective deployment requires
advanced decision-making to adapt to patient needs. A key challenge is the
scarcity of patient behavior data for developing robust policies. To address
this, we engaged 33 expert healthcare practitioners as patient proxies, using
their interactions with our robot to inform a patient behavior model capable of
generating exercise performance metrics and subjective scores on perceived
exertion. We trained a reinforcement learning-based policy in simulation,
demonstrating that it can adapt exercise instructions to individual exertion
tolerances and fluctuating performance, while also being applicable to patients
at different recovery stages with varying exercise plans.

</details>


### [170] [Brain-Robot Interface for Exercise Mimicry](https://arxiv.org/abs/2509.11306)
*Carl Bettosi,Emilyann Nault,Lynne Baillie,Markus Garschall,Marta Romeo,Beatrix Wais-Zechmann,Nicole Binderlehner,Theodoros Georgio*

Main category: cs.RO

TL;DR: BRI enables real-time robot imitation of patient movements via mental commands; exploratory study shows feasibility and positive acceptance, with variable accuracy.


<details>
  <summary>Details</summary>
Motivation: To sustain long-term engagement in robot-provided exercise instruction by using motor mimicry to build rapport, enabled by a Brain-Robot Interface that decodes patients' intentions for real-time imitation.

Method: Developed a Brain-Robot Interface enabling a social robot instructor to mimic a patient’s movements in real time based on mental commands; evaluated in an exploratory study with 14 participants (3 physiotherapists, 11 hemiparetic patients) measuring mimicry success (12 sessions) and user perceptions (trust/acceptance).

Result: The system achieved exercise mimicry in 12 sessions; accuracy varied across sessions. Participants reported high trust and acceptance of the robot instructor, and these perceptions were not reduced by introducing BRI technology.

Conclusion: BRI-enabled motor mimicry is feasible and acceptable for social robot exercise instructors, potentially supporting engagement and rapport in robot-assisted rehabilitation, though reliability/accuracy varies and warrants further improvement.

Abstract: For social robots to maintain long-term engagement as exercise instructors,
rapport-building is essential. Motor mimicry--imitating one's physical
actions--during social interaction has long been recognized as a powerful tool
for fostering rapport, and it is widely used in rehabilitation exercises where
patients mirror a physiotherapist or video demonstration. We developed a novel
Brain-Robot Interface (BRI) that allows a social robot instructor to mimic a
patient's exercise movements in real-time, using mental commands derived from
the patient's intention. The system was evaluated in an exploratory study with
14 participants (3 physiotherapists and 11 hemiparetic patients recovering from
stroke or other injuries). We found our system successfully demonstrated
exercise mimicry in 12 sessions; however, accuracy varied. Participants had
positive perceptions of the robot instructor, with high trust and acceptance
levels, which were not affected by the introduction of BRI technology.

</details>


### [171] [ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation](https://arxiv.org/abs/2509.11364)
*Sheng Liu,Zhe Li,Weiheng Wang,Han Sun,Heng Zhang,Hongpeng Chen,Yusen Qin,Arash Ajoudani,Yizhao Wang*

Main category: cs.RO

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate 6-DoF object pose estimation and tracking are critical for reliable
robotic manipulation. However, zero-shot methods often fail under
viewpoint-induced ambiguities and fixed-camera setups struggle when objects
move or become self-occluded. To address these challenges, we propose an active
pose estimation pipeline that combines a Vision-Language Model (VLM) with
"robotic imagination" to dynamically detect and resolve ambiguities in real
time. In an offline stage, we render a dense set of views of the CAD model,
compute the FoundationPose entropy for each view, and construct a
geometric-aware prompt that includes low-entropy (unambiguous) and high-entropy
(ambiguous) examples. At runtime, the system: (1) queries the VLM on the live
image for an ambiguity score; (2) if ambiguity is detected, imagines a discrete
set of candidate camera poses by rendering virtual views, scores each based on
a weighted combination of VLM ambiguity probability and FoundationPose entropy,
and then moves the camera to the Next-Best-View (NBV) to obtain a disambiguated
pose estimation. Furthermore, since moving objects may leave the camera's field
of view, we introduce an active pose tracking module: a diffusion-policy
trained via imitation learning, which generates camera trajectories that
preserve object visibility and minimize pose ambiguity. Experiments in
simulation and real-world show that our approach significantly outperforms
classical baselines.

</details>


### [172] [Quantum deep reinforcement learning for humanoid robot navigation task](https://arxiv.org/abs/2509.11388)
*Romerik Lokossou,Birhanu Shimelis Girma,Ozan K. Tonguz,Ahmed Biyabani*

Main category: cs.RO

TL;DR: Quantum deep reinforcement learning (QDRL) is applied to humanoid robots using parameterized quantum circuits; quantum SAC outperforms classical SAC in MuJoCo Humanoid-v4 and Walker2d-v4, with higher returns and much faster learning.


<details>
  <summary>Details</summary>
Motivation: Overcome the inefficiencies of classical RL in high-dimensional, stochastic control tasks and leverage quantum speedups to improve sample efficiency in humanoid robotics.

Method: Hybrid quantum-classical architecture employing parameterized quantum circuits; apply Soft Actor-Critic (SAC) in both classical and quantum settings; evaluate on high-dimensional MuJoCo Humanoid-v4 and Walker2d-v4; compare performance and learning speed.

Result: Quantum SAC achieves ~8% higher average return (246.40 vs 228.36) after 92% fewer steps, indicating accelerated learning and potential advantages of QDRL in complex tasks.

Conclusion: QDRL shows promise for high-dimensional humanoid robotics, motivating further exploration into quantum-enhanced RL and scalability to real-world tasks.

Abstract: Classical reinforcement learning (RL) methods often struggle in complex,
high-dimensional environments because of their extensive parameter requirements
and challenges posed by stochastic, non-deterministic settings. This study
introduces quantum deep reinforcement learning (QDRL) to train humanoid agents
efficiently. While previous quantum RL models focused on smaller environments,
such as wheeled robots and robotic arms, our work pioneers the application of
QDRL to humanoid robotics, specifically in environments with substantial
observation and action spaces, such as MuJoCo's Humanoid-v4 and Walker2d-v4.
Using parameterized quantum circuits, we explored a hybrid quantum-classical
setup to directly navigate high-dimensional state spaces, bypassing traditional
mapping and planning. By integrating quantum computing with deep RL, we aim to
develop models that can efficiently learn complex navigation tasks in humanoid
robots. We evaluated the performance of the Soft Actor-Critic (SAC) in
classical RL against its quantum implementation. The results show that the
quantum SAC achieves an 8% higher average return (246.40) than the classical
SAC (228.36) after 92% fewer steps, highlighting the accelerated learning
potential of quantum computing in RL tasks.

</details>


### [173] [TRUST 2025: SCRITA and RTSS @ RO-MAN 2025](https://arxiv.org/abs/2509.11402)
*Alessandra Rossi,Patrick Holthaus,Gabriella Lakatos,Sílvia Moros,Ali Fallahi,Murat Kirtay,Marie Postma,Erhan Oztop*

Main category: cs.RO

TL;DR: The TRUST workshop results from merging SCRITA and RTSS to jointly advance trust research in Human-Robot Interaction from both human and robot perspectives.


<details>
  <summary>Details</summary>
Motivation: To accelerate progress in trust in HRI by integrating the perspectives and goals of two established workshops, enabling cross-disciplinary collaboration.

Method: Forming the TRUST workshop through collaboration between SCRITA and RTSS, uniting their aims and communities under a single event.

Result: A unified workshop that combines complementary objectives to push forward research on trust from human and robot viewpoints.

Conclusion: The collaboration creates a new, integrated platform (TRUST) for advancing trust research in HRI, fostering interdisciplinary exchange.

Abstract: The TRUST workshop is the result of a collaboration between two established
workshops in the field of Human-Robot Interaction: SCRITA (Trust, Acceptance
and Social Cues in Human-Robot Interaction) and RTSS (Robot Trust for Symbiotic
Societies). This joint initiative brings together the complementary goals of
these workshops to advance research on trust from both the human and robot
perspectives.
  Website: https://scrita.herts.ac.uk/2025/

</details>


### [174] [Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations](https://arxiv.org/abs/2509.11417)
*Shresth Grover,Akshay Gopalkrishnan,Bo Ai,Henrik I. Christensen,Hao Su,Xuanlin Li*

Main category: cs.RO

TL;DR: A dual-encoder VLA framework that preserves pretrained vision-language representations while adapting to robot manipulation, using a frozen vision encoder, a token-based action representation, and a co-training scheme; improves robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning on robot data can erode pretrained representations and hamper generalization. There is a need to leverage rich pretrained VLM representations to build generalist robots across tasks/environments without catastrophic forgetting.

Method: - Dual-encoder design with one frozen vision encoder to retain pretrained features and a trainable encoder for task adaptation. - A string-based action tokenizer converts continuous actions into character sequences aligned with the model's pretraining domain. - A co-training strategy that merges robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances.

Result: In both simulation and real-robot experiments, the method shows improved robustness to visual perturbations, better generalization to novel instructions and environments, and higher overall task success compared with baselines.

Conclusion: The framework successfully preserves pretrained features while enabling effective adaptation for robot manipulation, supporting robust generalist robotics and highlighting a viable path for scaling VLA models to real-world tasks.

Abstract: Vision-language-action (VLA) models finetuned from vision-language models
(VLMs) hold the promise of leveraging rich pretrained representations to build
generalist robots across diverse tasks and environments. However, direct
fine-tuning on robot data often disrupts these representations and limits
generalization. We present a framework that better preserves pretrained
features while adapting them for robot manipulation. Our approach introduces
three components: (i) a dual-encoder design with one frozen vision encoder to
retain pretrained features and another trainable for task adaptation, (ii) a
string-based action tokenizer that casts continuous actions into character
sequences aligned with the model's pretraining domain, and (iii) a co-training
strategy that combines robot demonstrations with vision-language datasets
emphasizing spatial reasoning and affordances. Evaluations in simulation and on
real robots show that our method improves robustness to visual perturbations,
generalization to novel instructions and environments, and overall task success
compared to baselines.

</details>


### [175] [A Software-Only Post-Processor for Indexed Rotary Machining on GRBL-Based CNCs](https://arxiv.org/abs/2509.11433)
*Pedro Portugal,Damian D. Venghaus,Diego Lopez*

Main category: cs.RO

TL;DR: A software-only workflow converts planar toolpaths into indexed rotary steps for GRBL CNCs, enabling rotary-axis fabrication without hardware changes or firmware mods; suitable for education and prototyping but not full 4-axis continuous machining.


<details>
  <summary>Details</summary>
Motivation: Affordable desktop CNCs often lack rotary axes; retrofits or alternative controllers and CAM software are costly and complex, limiting access in education, prototyping, and makerspaces.

Method: A custom post-processor translates 2D toolpaths into discrete rotary steps; a browser-based interface executes the steps; uses standard off-the-shelf mechanics and requires no firmware modification.

Result: Demonstrates practical rotary-axis fabrication on GRBL-based machines; while not equivalent to continuous 4-axis machining, the approach expands capabilities within existing hardware.

Conclusion: Reduces technical and financial barriers, broadening access to multi-axis machining in classrooms, makerspaces, and small workshops, and supports hands-on learning and rapid prototyping.

Abstract: Affordable desktop CNC routers are common in education, prototyping, and
makerspaces, but most lack a rotary axis, limiting fabrication of rotationally
symmetric or multi-sided parts. Existing solutions often require hardware
retrofits, alternative controllers, or commercial CAM software, raising cost
and complexity. This work presents a software-only framework for indexed rotary
machining on GRBL-based CNCs. A custom post-processor converts planar toolpaths
into discrete rotary steps, executed through a browser-based interface. While
not equivalent to continuous 4-axis machining, the method enables practical
rotary-axis fabrication using only standard, off-the-shelf mechanics, without
firmware modification. By reducing technical and financial barriers, the
framework expands access to multi-axis machining in classrooms, makerspaces,
and small workshops, supporting hands-on learning and rapid prototyping.

</details>


### [176] [RAPTOR: A Foundation Policy for Quadrotor Control](https://arxiv.org/abs/2509.11481)
*Jonas Eschmann,Dario Albani,Giuseppe Loianno*

Main category: cs.RO

TL;DR: A compact adaptive quadrotor policy enables zero-shot adaptation across diverse platforms via Meta-Imitation Learning; a 3-layer, 2084-parameter network with recurrence adapts in milliseconds after distilling 1000 RL-trained teachers into a single student.


<details>
  <summary>Details</summary>
Motivation: RL policies are highly specialized and suffer from Sim2Real gaps; there's a need for a single foundation policy that generalizes to many quadrotor variants with minimal retraining.

Method: Train 1000 teacher policies for diverse quadrotor configurations using RL; distill into a single adaptive student with a recurrent hidden layer. The student is a tiny 3-layer network (2084 params) trained end-to-end to control quadrotors; adaptation occurs via In-Context Learning.

Result: The foundation policy achieves zero-shot adaptation to unseen quadrotors within milliseconds; tested on 10 real quadrotors with varying hardware (motors, frames, props, flight controllers) and conditions (trajectory tracking, indoor/outdoor, wind, poking, different propellers).

Conclusion: A small, adaptive foundation policy learned via Meta-Imitation Learning can generalize across a broad spectrum of quadrotor platforms, enabling rapid, robust adaptation without per-platform training.

Abstract: Humans are remarkably data-efficient when adapting to new unseen conditions,
like driving a new car. In contrast, modern robotic control systems, like
neural network policies trained using Reinforcement Learning (RL), are highly
specialized for single environments. Because of this overfitting, they are
known to break down even under small differences like the Simulation-to-Reality
(Sim2Real) gap and require system identification and retraining for even
minimal changes to the system. In this work, we present RAPTOR, a method for
training a highly adaptive foundation policy for quadrotor control. Our method
enables training a single, end-to-end neural-network policy to control a wide
variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg
that also differ in motor type (brushed vs. brushless), frame type (soft vs.
rigid), propeller type (2/3/4-blade), and flight controller
(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy
with only 2084 parameters is sufficient for zero-shot adaptation to a wide
variety of platforms. The adaptation through In-Context Learning is made
possible by using a recurrence in the hidden layer. The policy is trained
through a novel Meta-Imitation Learning algorithm, where we sample 1000
quadrotors and train a teacher policy for each of them using Reinforcement
Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive
student policy. We find that within milliseconds, the resulting foundation
policy adapts zero-shot to unseen quadrotors. We extensively test the
capabilities of the foundation policy under numerous conditions (trajectory
tracking, indoor/outdoor, wind disturbance, poking, different propellers).

</details>


### [177] [FR-Net: Learning Robust Quadrupedal Fall Recovery on Challenging Terrains through Mass-Contact Prediction](https://arxiv.org/abs/2509.11504)
*Yidan Lu,Yinzhao Dong,Jiahui Zhang,Ji Ma,Peng Lu*

Main category: cs.RO

TL;DR: FR-Net enables robust quadruped fall recovery via a Mass-Contact Predictor trained in simulation; demonstrates cross-platform generalization and real-world validation on Go2 across 10 scenarios.


<details>
  <summary>Details</summary>
Motivation: Fall recovery on complex terrains is challenging due to incomplete perception and uncertain contacts; existing methods struggle with unsafe motions; a generalizable, perception-light solution is needed.

Method: Introduce FR-Net with a Mass-Contact Predictor that estimates the robot’s mass distribution and contact states from limited sensory inputs; use carefully crafted reward functions for safe recovery; train entirely in simulation with privileged learning; deploy without terrain data during deployment.

Result: The framework generalizes across different quadrupedal platforms in simulation and shows robust real-world performance on the Go2 robot in 10 challenging scenarios; explicit mass-contact prediction is highlighted as key to robust recovery.

Conclusion: Explicit mass-contact prediction is a critical enabler for robust, generalizable fall recovery in quadrupeds, pointing to a promising direction for broad quadrupedal skills.

Abstract: Fall recovery for legged robots remains challenging, particularly on complex
terrains where traditional controllers fail due to incomplete terrain
perception and uncertain interactions. We present \textbf{FR-Net}, a
learning-based framework that enables quadrupedal robots to recover from
arbitrary fall poses across diverse environments. Central to our approach is a
Mass-Contact Predictor network that estimates the robot's mass distribution and
contact states from limited sensory inputs, facilitating effective recovery
strategies. Our carefully designed reward functions ensure safe recovery even
on steep stairs without dangerous rolling motions common to existing methods.
Trained entirely in simulation using privileged learning, our framework guides
policy learning without requiring explicit terrain data during deployment. We
demonstrate the generalization capabilities of \textbf{FR-Net} across different
quadrupedal platforms in simulation and validate its performance through
extensive real-world experiments on the Go2 robot in 10 challenging scenarios.
Our results indicate that explicit mass-contact prediction is key to robust
fall recovery, offering a promising direction for generalizable quadrupedal
skills.

</details>


### [178] [Design and Development of a Remotely Wire-Driven Walking Robot](https://arxiv.org/abs/2509.11506)
*Takahiro Hattori,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: Remote Wire Drive enables remote actuation of mobile robots through a wire-based power transmission that connects decoupled joints, demonstrated on a wire-driven quadruped robot.


<details>
  <summary>Details</summary>
Motivation: Operating in harsh or inaccessible environments requires robots that can function with minimal electronics, but such environments also risk electronics. Electronics-free robots lack complex decision-making; hydraulically actuated robots and wire-driven robot arms are used in harsh settings, but remote actuation of mobile robots via wires has not been demonstrated. This work aims to enable mobile robots to be actuated remotely using wires to extend reach and environmental applicability.

Method: Introduce Remote Wire Drive (RWD), a series connection of decoupled joints adapted from the power transmission approach of wire-driven robot arms. Implement and validate the mechanism by actuating a wire-driven quadruped robot developed for the study.

Result: Experimental validation showing the feasibility of remotely actuating a wire-driven mobile robot through Remote Wire Drive.

Conclusion: Remote Wire Drive makes remote actuation of mobile robots via wires feasible and expands the use of wire-driven mechanisms to harsh environments, enabling greater reach and environmental applicability for mobile robots.

Abstract: Operating in environments too harsh or inaccessible for humans is one of the
critical roles expected of robots. However, such environments often pose risks
to electronic components as well. To overcome this, various approaches have
been developed, including autonomous mobile robots without electronics,
hydraulic remotely actuated mobile robots, and long-reach robot arms driven by
wires. Among these, electronics-free autonomous robots cannot make complex
decisions, while hydraulically actuated mobile robots and wire-driven robot
arms are used in harsh environments such as nuclear power plants. Mobile robots
offer greater reach and obstacle avoidance than robot arms, and wire mechanisms
offer broader environmental applicability than hydraulics. However, wire-driven
systems have not been used for remote actuation of mobile robots. In this
study, we propose a novel mechanism called Remote Wire Drive that enables
remote actuation of mobile robots via wires. This mechanism is a series
connection of decoupled joints, a mechanism used in wire-driven robot arms,
adapted for power transmission. We experimentally validated its feasibility by
actuating a wire-driven quadruped robot, which we also developed in this study,
through Remote Wire Drive.

</details>


### [179] [PaiP: An Operational Aware Interactive Planner for Unknown Cabinet Environments](https://arxiv.org/abs/2509.11516)
*Chengjin Wang,Zheng Yan,Yanmin Zhou,Runjie Shen,Zhipeng Wang,Bin Cheng,Bin He*

Main category: cs.RO

TL;DR: PaiP is an operationally aware interactive motion planner that uses multimodal tactile perception to infer interaction features at interfaces, embedding them as operational cost maps in grid representations and extending sampling-based planning to jointly optimize path and operational costs, yielding robust motion in narrow, occluded box/cabinet scenarios.


<details>
  <summary>Details</summary>
Motivation: Box/cabinet manipulation with stacked objects suffers from visual occlusions and tight free space; traditional collision-free planners can fail or cause catastrophic failures due to unseen objects, highlighting the need for real-time, tactile-informed planning.

Method: Develop PaiP, a real-time closed-loop planner that uses multimodal tactile perception to autonomously infer object interaction features at interaction interfaces. These features are encoded into grid-based operational cost maps, and sampling-based planning is extended to optimize both path cost and operational cost.

Result: Experimental results show that PaiP achieves robust motion in narrow spaces, demonstrating improved performance in occluded, cluttered environments where purely geometric planning struggles.

Conclusion: Operationally aware tactile-informed planning can effectively handle occlusions and constrained spaces by integrating interaction-feature-derived costs into planning, enabling safer and more robust robotic manipulation in cluttered boxes/cabinets.

Abstract: Box/cabinet scenarios with stacked objects pose significant challenges for
robotic motion due to visual occlusions and constrained free space. Traditional
collision-free trajectory planning methods often fail when no collision-free
paths exist, and may even lead to catastrophic collisions caused by invisible
objects. To overcome these challenges, we propose an operational aware
interactive motion planner (PaiP) a real-time closed-loop planning framework
utilizing multimodal tactile perception. This framework autonomously infers
object interaction features by perceiving motion effects at interaction
interfaces. These interaction features are incorporated into grid maps to
generate operational cost maps. Building upon this representation, we extend
sampling-based planning methods to interactive planning by optimizing both path
cost and operational cost. Experimental results demonstrate that PaiP achieves
robust motion in narrow spaces.

</details>


### [180] [Shape control of simulated multi-segment continuum robots via Koopman operators with per-segment projection](https://arxiv.org/abs/2509.11567)
*Eron Ristich,Jiahe Wang,Lei Zhang,Sultan Haidar Ali,Wanxin Jin,Yi Ren,Jiefeng Sun*

Main category: cs.RO

TL;DR: A data-driven Koopman-based method with per-segment projection enables accurate, real-time shape control for simulated soft continuum robots using linear MPC.


<details>
  <summary>Details</summary>
Motivation: Soft continuum robots offer biocompatible, compliant motion but current control methods handle only task-space (tip) control due to infinite degrees of freedom. Whole-shape (shape) control remains computationally expensive. A data-driven approach could enable real-time shape control.

Method: Simulate multi-segment tendon-driven soft continuum robots using the Kirchhoff rod model. Collect state data and apply a per-segment projection to identify control-affine Koopman models. Use these linear models in a model predictive control (MPC) framework to drive the robot to target shapes of varying complexity.

Result: The per-segment projection yields Koopman models that are an order of magnitude more accurate than without projection. The learned models enable computationally efficient closed-loop control, demonstrating feasibility of real-time shape control for soft robots.

Conclusion: The approach paves the way for practical, real-time shape control of soft continuum robots, bringing shape-level manipulation closer to real-world applications.

Abstract: Soft continuum robots can allow for biocompatible yet compliant motions, such
as the ability of octopus arms to swim, crawl, and manipulate objects. However,
current state-of-the-art continuum robots can only achieve real-time task-space
control (i.e., tip control) but not whole-shape control, mainly due to the high
computational cost from its infinite degrees of freedom. In this paper, we
present a data-driven Koopman operator-based approach for the shape control of
simulated multi-segment tendon-driven soft continuum robots with the Kirchhoff
rod model. Using data collected from these simulated soft robots, we conduct a
per-segment projection scheme on the state of the robots allowing for the
identification of control-affine Koopman models that are an order of magnitude
more accurate than without the projection scheme. Using these learned Koopman
models, we use a linear model predictive control (MPC) to control the robots to
a collection of target shapes of varying complexity. Our method realizes
computationally efficient closed-loop control, and demonstrates the feasibility
of real-time shape control for soft robots. We envision this work can pave the
way for practical shape control of soft continuum robots.

</details>


### [181] [GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning](https://arxiv.org/abs/2509.11594)
*Jizhuo Chen,Diwen Liu,Jiaming Wang,Harold Soh*

Main category: cs.RO

TL;DR: A data-efficient two-stage learning approach using a PointNet++ encoder to score dense candidate base poses for grasping from a single RGB-D snapshot, enabling fast, geometry-aware base placement.


<details>
  <summary>Details</summary>
Motivation: Enable safe, reachable robot base placements for grasping with minimal labeled data and computation by combining fast heuristics with targeted simulation calibration.

Method: Two-stage curriculum: (1) cheap auto-labeling of a large dataset via a distance-visibility rule; (2) a smaller set of high-fidelity simulation trials to refine the model. A PointNet++-style point-cloud encoder with an MLP scores dense grids of candidate base poses for rapid online selection.

Result: In simulation and on a real mobile manipulator, GBPP outperforms proximity and geometry-only baselines, yielding safer and more reachable stances and degrading gracefully when predictions are wrong.

Conclusion: A practical recipe for data-efficient, geometry-aware base placement: use inexpensive heuristics to cover pose space, then calibrate with targeted simulation.

Abstract: GBPP is a fast learning based scorer that selects a robot base pose for
grasping from a single RGB-D snapshot. The method uses a two stage curriculum:
(1) a simple distance-visibility rule auto-labels a large dataset at low cost;
and (2) a smaller set of high fidelity simulation trials refines the model to
match true grasp outcomes. A PointNet++ style point cloud encoder with an MLP
scores dense grids of candidate poses, enabling rapid online selection without
full task-and-motion optimization. In simulation and on a real mobile
manipulator, GBPP outperforms proximity and geometry only baselines, choosing
safer and more reachable stances and degrading gracefully when wrong. The
results offer a practical recipe for data efficient, geometry aware base
placement: use inexpensive heuristics for coverage, then calibrate with
targeted simulation.

</details>


### [182] [AssemMate: Graph-Based LLM for Robotic Assembly Assistance](https://arxiv.org/abs/2509.11617)
*Qi Zheng,Chaoran Zhang,Zijian Liang,EnTe Lin,Shubo Cui,Qinghongbing Xie,Zhaobo Xu,Long Zeng*

Main category: cs.RO

TL;DR: AssemMate introduces a graph-based input for LLMs to support knowledge graph question answering, sensing, and grasping in robotic assembly, achieving higher accuracy, faster inference, and much shorter context length compared to natural-language-based Knowledge representations.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-guided robotic assembly relies on long, redundant natural-language knowledge texts that hinder real-time and precise reasoning. A concise knowledge-graph representation can better capture domain knowledge and align with LLMs to enable efficient interaction and planning.

Method: A self-supervised Graph Convolutional Network (GCN) encodes knowledge graph entities and relations into a latent space and aligns them with the LLM’s representations. The system uses graph-based knowledge graph QA (KGQA) for interactive human-robot collaboration and assembly planning, plus a vision-enhanced approach to handle stacked scenes for grasping. Training/evaluation on random graphs demonstrates generalization; robotic grasping experiments validate real-world applicability.

Result: AssemMate reports 6.4% higher accuracy, 3x faster inference, and 28x shorter context length than baselines, with strong generalization to random graphs and successful grasping experiments in simulation and real-world settings.

Conclusion: Graph-based knowledge inputs can significantly improve LLM-driven robotic assembly by enabling compact, accurate knowledge representation, efficient reasoning, and robust execution across interactive QA, planning, and grasping tasks.

Abstract: Large Language Model (LLM)-based robotic assembly assistance has gained
significant research attention. It requires the injection of domain-specific
knowledge to guide the assembly process through natural language interaction
with humans. Despite some progress, existing methods represent knowledge in the
form of natural language text. Due to the long context and redundant content,
they struggle to meet the robots' requirements for real-time and precise
reasoning. In order to bridge this gap, we present AssemMate, which utilizes
the graph\textemdash a concise and accurate form of knowledge
representation\textemdash as input. This graph-based LLM enables knowledge
graph question answering (KGQA), supporting human-robot interaction and
assembly task planning for specific products. Beyond interactive QA, AssemMate
also supports sensing stacked scenes and executing grasping to assist with
assembly. Specifically, a self-supervised Graph Convolutional Network (GCN)
encodes knowledge graph entities and relations into a latent space and aligns
them with LLM's representation, enabling the LLM to understand graph
information. In addition, a vision-enhanced strategy is employed to address
stacked scenes in grasping. Through training and evaluation, AssemMate
outperforms existing methods, achieving 6.4\% higher accuracy, 3 times faster
inference, and 28 times shorter context length, while demonstrating strong
generalization ability on random graphs. And our approach further demonstrates
superiority through robotic grasping experiments in both simulated and
real-world settings. More details can be found on the project page:
https://github.com/cristina304/AssemMate.git

</details>


### [183] [Inference-stage Adaptation-projection Strategy Adapts Diffusion Policy to Cross-manipulators Scenarios](https://arxiv.org/abs/2509.11621)
*Xiangtong Yao,Yirui Zhou,Yuan Meng,Yanwen Liu,Liangyu Dong,Zitao Zhang,Zhenshan Bing,Kai Huang,Fuchun Sun,Alois Knoll*

Main category: cs.RO

TL;DR: Proposes an adaptation-projection strategy that enables diffusion policies to zero-shot adapt to unseen manipulators and task settings during inference, by projecting generated trajectories to satisfy kinematic and task constraints without retraining.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based visuomotor policies often fail to generalize to new hardware or task requirements, necessitating costly data recollection and policy retraining for each change.

Method: Train a diffusion policy in SE(3) space using demonstrations from a base manipulator. At deployment, project the policy’s generated trajectories to satisfy constraints of the new hardware and tasks, dynamically adapting to physical differences and task needs, without retraining.

Result: Real-world validation across pick-and-place, pushing, and pouring with manipulators like Franka Panda and Kuka iiwa 14 and diverse end-effectors; high success rates and robustness to tool offsets and obstacle variations.

Conclusion: The adaptation-projection approach enables zero-shot adaptation of diffusion policies to new manipulators and task settings at inference time, offering a practical, retrain-free path for cross-hardware robotic manipulation. Code release planned after peer review.

Abstract: Diffusion policies are powerful visuomotor models for robotic manipulation,
yet they often fail to generalize to manipulators or end-effectors unseen
during training and struggle to accommodate new task requirements at inference
time. Addressing this typically requires costly data recollection and policy
retraining for each new hardware or task configuration. To overcome this, we
introduce an adaptation-projection strategy that enables a diffusion policy to
perform zero-shot adaptation to novel manipulators and dynamic task settings,
entirely at inference time and without any retraining. Our method first trains
a diffusion policy in SE(3) space using demonstrations from a base manipulator.
During online deployment, it projects the policy's generated trajectories to
satisfy the kinematic and task-specific constraints imposed by the new hardware
and objectives. Moreover, this projection dynamically adapts to physical
differences (e.g., tool-center-point offsets, jaw widths) and task requirements
(e.g., obstacle heights), ensuring robust and successful execution. We validate
our approach on real-world pick-and-place, pushing, and pouring tasks across
multiple manipulators, including the Franka Panda and Kuka iiwa 14, equipped
with a diverse array of end-effectors like flexible grippers, Robotiq 2F/3F
grippers, and various 3D-printed designs. Our results demonstrate consistently
high success rates in these cross-manipulator scenarios, proving the
effectiveness and practicality of our adaptation-projection strategy. The code
will be released after peer review.

</details>


### [184] [ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering](https://arxiv.org/abs/2509.11663)
*Haisheng Wang,Weiming Zhi*

Main category: cs.RO

TL;DR: Formalizes Embodied Questions Answering (EQsA) and introduces ParaEQsA for parallel, urgency-aware scheduling; presents PAEQs benchmark; demonstrates improvements over sequential baselines and analyzes factors affecting performance.


<details>
  <summary>Details</summary>
Motivation: Real-world deployments require handling asynchronous, multi-question workloads with varying urgencies. Traditional single-question EQA struggles with efficiency and responsiveness; thus parallel, urgency-aware strategies and memory sharing are needed to reduce redundant exploration.

Method: Proposes ParaEQsA framework with a shared group memory across questions and a priority-planning module for dynamic, urgency-aware scheduling. Evaluates on the Parallel Asynchronous Embodied Questions (PAEQs) benchmark (40 indoor scenes, 5 questions per scene) and introduces metrics Direct Answer Rate (DAR) and Normalized Urgency-Weighted Latency (NUWL).

Result: ParaEQsA consistently outperforms strong sequential baselines adapted from recent EQA systems, reducing exploration and delay. Empirical analyses probe the impact of priority, urgency modeling, spatial scope, reward estimation, and dependency reasoning within the framework.

Conclusion: Urgency-aware, parallel scheduling is key to making embodied agents responsive and efficient under realistic, multi-question workloads.

Abstract: This paper formulates the Embodied Questions Answering (EQsA) problem,
introduces a corresponding benchmark, and proposes a system to tackle the
problem. Classical Embodied Question Answering (EQA) is typically formulated as
answering one single question by actively exploring a 3D environment. Real
deployments, however, often demand handling multiple questions that may arrive
asynchronously and carry different urgencies. We formalize this setting as
Embodied Questions Answering (EQsA) and present ParaEQsA, a framework for
parallel, urgency-aware scheduling and answering. ParaEQsA leverages a group
memory module shared among questions to reduce redundant exploration, and a
priority-planning module to dynamically schedule questions. To evaluate this
setting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)
benchmark containing 40 indoor scenes and five questions per scene (200 in
total), featuring asynchronous follow-up questions and urgency labels. We
further propose metrics for EQsA performance: Direct Answer Rate (DAR), and
Normalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency
and responsiveness of this system. ParaEQsA consistently outperforms strong
sequential baselines adapted from recent EQA systems, while reducing
exploration and delay. Empirical evaluations investigate the relative
contributions of priority, urgency modeling, spatial scope, reward estimation,
and dependency reasoning within our framework. Together, these results
demonstrate that urgency-aware, parallel scheduling is key to making embodied
agents responsive and efficient under realistic, multi-question workloads.

</details>


### [185] [Tensor Invariant Data-Assisted Control and Dynamic Decomposition of Multibody Systems](https://arxiv.org/abs/2509.11688)
*Mostafa Eslami,Maryam Babazadeh*

Main category: cs.RO

TL;DR: Coordinate-free tensor-based data-assisted control for robotic collaboration, enabling frame-invariant learning and stability via a closed-form augmented Newton-Euler model.


<details>
  <summary>Details</summary>
Motivation: To overcome data inefficiency and poor generalization caused by coordinate-dependent models when learning control from data in shared workspaces.

Method: Develop a non-recursive, closed-form Newton-Euler model in augmented matrix form; decompose into a structurally certain, physically grounded part and an uncertain, empirical part via a virtual port; build an end-to-end tensor-invariant pipeline; provide coordinate-free control laws; Lyapunov stability analysis.

Result: Validated in simulations; offers a data-efficient path for frame-invariant learning (e.g., equivariant learning) and improved explainability.

Conclusion: This framework reduces data requirements, improves robustness and generalization for robotic control in interactive environments, and enables principled, tensor-based learning pipelines.

Abstract: The control of robotic systems in complex, shared collaborative workspaces
presents significant challenges in achieving robust performance and safety when
learning from experienced or simulated data is employed in the pipeline. A
primary bottleneck is the reliance on coordinate-dependent models, which leads
to profound data inefficiency by failing to generalize physical interactions
across different frames of reference. This forces learning algorithms to
rediscover fundamental physical principles in every new orientation,
artificially inflating the complexity of the learning task. This paper
introduces a novel framework that synergizes a coordinate-free, unreduced
multibody dynamics and kinematics model based on tensor mechanics with a
Data-Assisted Control (DAC) architecture. A non-recursive, closed-form
Newton-Euler model in an augmented matrix form is derived that is optimized for
tensor-based control design. This structure enables a principled decomposition
of the system into a structurally certain, physically grounded part and an
uncertain, empirical, and interaction-focused part, mediated by a virtual port
variable. Then, a complete, end-to-end tensor-invariant pipeline for modeling,
control, and learning is proposed. The coordinate-free control laws for the
structurally certain part provide a stable and abstract command interface,
proven via Lyapunov analysis. Eventually, the model and closed-loop system are
validated through simulations. This work provides a naturally ideal input for
data-efficient, frame-invariant learning algorithms, such as equivariant
learning, designed to learn the uncertain interaction. The synergy directly
addresses the data-inefficiency problem, increases explainability and
interpretability, and paves the way for more robust and generalizable robotic
control in interactive environments.

</details>


### [186] [From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile Manipulator for Supermarket Stocking and Fronting](https://arxiv.org/abs/2509.11740)
*Davide Peron,Victor Nan Fernandez-Ayala,Lukas Segelmark*

Main category: cs.RO

TL;DR: An end-to-end ROS2-based autonomous shelf stocking system using off-the-shelf hardware, BT planning, vision models, and MPC; achieves 98% success over 700 stocking events; but current systems lag human workers in performance and cost-effectiveness; highlights areas for improvement before deployment.


<details>
  <summary>Details</summary>
Motivation: Autonomous stocking in supermarkets faces dynamic human interaction, tight spaces, and varied product geometry; there is a need for a scalable, deployable platform that integrates perception, planning, and control.

Method: Integrates off-the-shelf hardware with ROS2-based perception, planning, and control; uses Behavior Trees for task planning; applies fine-tuned object-detection vision models; employs a two-step Model Predictive Control framework for accurate shelf navigation using ArUco markers; validates in laboratory experiments simulating realistic supermarket conditions.

Result: Robust pick-and-place with >98% success across 700 stocking events; demonstrates viability of end-to-end platform; but benchmarks show autonomy is still inferior to human workers in performance and cost; identifies improvement areas for commercial deployment.

Conclusion: The proposed platform represents a scalable, deployable solution for autonomous shelf stocking; while promising, further advances in efficiency and cost-effectiveness are needed to reach widespread commercial deployment; outlines pathways for improvement.

Abstract: Autonomous stocking in retail environments, particularly supermarkets,
presents challenges due to dynamic human interactions, constrained spaces, and
diverse product geometries. This paper introduces an efficient end-to-end
robotic system for autonomous shelf stocking and fronting, integrating
commercially available hardware with a scalable algorithmic architecture. A
major contribution of this work is the system integration of off-the-shelf
hardware and ROS2-based perception, planning, and control into a single
deployable platform for retail environments. Our solution leverages Behavior
Trees (BTs) for task planning, fine-tuned vision models for object detection,
and a two-step Model Predictive Control (MPC) framework for precise shelf
navigation using ArUco markers. Laboratory experiments replicating realistic
supermarket conditions demonstrate reliable performance, achieving over 98%
success in pick-and-place operations across a total of more than 700 stocking
events. However, our comparative benchmarks indicate that the performance and
cost-effectiveness of current autonomous systems remain inferior to that of
human workers, which we use to highlight key improvement areas and quantify the
progress still required before widespread commercial deployment can
realistically be achieved.

</details>


### [187] [Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap](https://arxiv.org/abs/2509.11742)
*Jianping Li,Kaisong Zhu,Zhongyuan Liu,Rui Jin,Xinhang Xu,Pengfei Wan,Lihua Xie*

Main category: cs.RO

TL;DR: Adaptive LiDAR scanning guided by OpenStreetMap priors and observability prediction improves localization robustness and efficiency, reducing trajectory error while preserving scan coverage.


<details>
  <summary>Details</summary>
Motivation: OSM priors offer lightweight global constraints but are often incomplete or outdated. LiDAR has a limited FoV and traditional motorized scanning uses constant speed, wasting effort in sparse regions and harming accuracy. The work aims to combine priors with adaptive sensing to achieve robust, efficient localization.

Method: Proposes Adaptive LiDAR Scanning with OSM guidance. It augments an uncertainty-aware model predictive control with an OSM-aware term to allocate scanning effort based on scene observability and the spatial distribution of OSM features. Implemented in ROS with a motorized LiDAR odometry backend and evaluated in simulation and real-world environments (campus roads, indoor corridors, urban areas).

Result: Significant reductions in trajectory error compared with constant-speed baselines, while maintaining scan completeness.

Conclusion: Coupling open-source maps with adaptive LiDAR scanning enables robust and efficient LiDAR-OSM localization in complex environments.

Abstract: LiDAR-to-OpenStreetMap (OSM) localization has gained increasing attention, as
OSM provides lightweight global priors such as building footprints. These
priors enhance global consistency for robot navigation, but OSM is often
incomplete or outdated, limiting its reliability in real-world deployment.
Meanwhile, LiDAR itself suffers from a limited field of view (FoV), where
motorized rotation is commonly used to achieve panoramic coverage. Existing
motorized LiDAR systems, however, typically employ constant-speed scanning that
disregards both scene structure and map priors, leading to wasted effort in
feature-sparse regions and degraded localization accuracy. To address these
challenges, we propose Adaptive LiDAR Scanning with OSM guidance, a framework
that integrates global priors with local observability prediction to improve
localization robustness. Specifically, we augment uncertainty-aware model
predictive control with an OSM-aware term that adaptively allocates scanning
effort according to both scene-dependent observability and the spatial
distribution of OSM features. The method is implemented in ROS with a motorized
LiDAR odometry backend and evaluated in both simulation and real-world
experiments. Results on campus roads, indoor corridors, and urban environments
demonstrate significant reductions in trajectory error compared to
constant-speed baselines, while maintaining scan completeness. These findings
highlight the potential of coupling open-source maps with adaptive LiDAR
scanning to achieve robust and efficient localization in complex environments.

</details>


### [188] [Igniting VLMs toward the Embodied Space](https://arxiv.org/abs/2509.11766)
*Andy Zhai,Brae Liu,Bruno Fang,Chalse Cai,Ellie Ma,Ethan Yin,Hao Wang,Hugo Zhou,James Wang,Lights Shi,Lucy Liang,Make Wang,Qian Wang,Roy Gan,Ryan Yu,Shalfun Li,Starrick Liu,Sylas Chen,Vincent Chen,Zach Xu*

Main category: cs.RO

TL;DR: WALL-OSS is an end-to-end embodied foundation model that tightly integrates vision-language understanding with action generation via a Unified Cross-Level CoT, enabling strong long-horizon manipulation, instruction-following, and reasoning, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs show strong language/vision capabilities but lack robust spatial embodiment understanding; transferring to embodied domains reveals mismatches in modalities, pretraining distributions, and objectives, making action comprehension/generation the bottleneck toward AGI.

Method: A tightly coupled architecture with a multi-strategy training curriculum that unifies instruction reasoning, subgoal decomposition, and fine-grained action synthesis within a single differentiable framework, via Unified Cross-Level CoT.

Result: WALL-OSS achieves high success on complex long-horizon manipulations, demonstrates strong instruction-following and reasoning, and outperforms strong baselines.

Conclusion: The work provides a reliable and scalable path from vision-language models to embodied foundation models, enabling embodiment-aware understanding and manipulation that advances toward AGI.

Abstract: While foundation models show remarkable progress in language and vision,
existing vision-language models (VLMs) still have limited spatial and
embodiment understanding. Transferring VLMs to embodied domains reveals
fundamental mismatches between modalities, pretraining distributions, and
training objectives, leaving action comprehension and generation as a central
bottleneck on the path to AGI.
  We introduce WALL-OSS, an end-to-end embodied foundation model that leverages
large-scale multimodal pretraining to achieve (1) embodiment-aware
vision-language understanding, (2) strong language-action association, and (3)
robust manipulation capability.
  Our approach employs a tightly coupled architecture and multi-strategies
training curriculum that enables Unified Cross-Level CoT-seamlessly unifying
instruction reasoning, subgoal decomposition, and fine-grained action synthesis
within a single differentiable framework.
  Our results show that WALL-OSS attains high success on complex long-horizon
manipulations, demonstrates strong instruction-following capabilities, complex
understanding and reasoning, and outperforms strong baselines, thereby
providing a reliable and scalable path from VLMs to embodied foundation models.

</details>


### [189] [Augmented Reality-Enhanced Robot Teleoperation for Collecting User Demonstrations](https://arxiv.org/abs/2509.11783)
*Shiqi Gong,Sebastian Zudaire,Chi Zhang,Zhen Li*

Main category: cs.RO

TL;DR: AR-enhanced robot teleoperation system enabling contact-free demonstrations via AR and point-cloud rendering; validated on ABB IRB 1200 and GoFa; reported 28% task-performance boost and 12% SUS uplift, with contributions to intuitive teleoperation, AR UI, environmental perception, and safety.


<details>
  <summary>Details</summary>
Motivation: Reduce the complexity and time required for traditional industrial robot programming by making demonstration collection and control more intuitive, using AR to enable contact-free, remote teleoperation.

Method: Integrates AR-based control with spatial point cloud rendering to enable intuitive demonstrations without teach pendants; deployed on ABB robots (IRB 1200, GoFa); conducted a user study comparing with/without point cloud rendering to assess impact on task accuracy, efficiency, and user confidence.

Result: Enhanced environmental perception via point cloud rendering improves task performance by 28% and SUS score by 12%; demonstrates general applicability and potential ML training data from collected demonstrations.

Conclusion: AR-enhanced teleoperation with environmental perception improves performance and usability in industrial robot demonstration tasks; contributes to intuitive AR interfaces, perception, and teleoperation safety, with demonstrations usable for ML training.

Abstract: Traditional industrial robot programming is often complex and time-consuming,
typically requiring weeks or even months of effort from expert programmers.
Although Programming by Demonstration (PbD) offers a more accessible
alternative, intuitive interfaces for robot control and demonstration
collection remain challenging. To address this, we propose an Augmented Reality
(AR)-enhanced robot teleoperation system that integrates AR-based control with
spatial point cloud rendering, enabling intuitive, contact-free demonstrations.
This approach allows operators to control robots remotely without entering the
workspace or using conventional tools like the teach pendant. The proposed
system is generally applicable and has been demonstrated on ABB robot
platforms, specifically validated with the IRB 1200 industrial robot and the
GoFa 5 collaborative robot. A user study evaluates the impact of real-time
environmental perception, specifically with and without point cloud rendering,
on task completion accuracy, efficiency, and user confidence. Results indicate
that enhanced perception significantly improves task performance by 28% and
enhances user experience, as reflected by a 12% increase in the System
Usability Scale (SUS) score. This work contributes to the advancement of
intuitive robot teleoperation, AR interface design, environmental perception,
and teleoperation safety mechanisms in industrial settings for demonstration
collection. The collected demonstrations may serve as valuable training data
for machine learning applications.

</details>


### [190] [Synthetic vs. Real Training Data for Visual Navigation](https://arxiv.org/abs/2509.11791)
*Lauri Suomela,Sasanka Kuruppu Arachchige,German F. Torres,Harry Edelman,Joni-Kristian Kämäräinen*

Main category: cs.RO

TL;DR: Simulator-trained visual navigation policies can match or exceed real-world-trained policies when using a robust on-device architecture with pretrained visual encoders, enabling strong sim-to-real transfer and generalization to aerial platforms.


<details>
  <summary>Details</summary>
Motivation: Address the persistent sim-to-real performance gap in visual navigation and determine whether simulation can yield competitive policies, while understanding factors that enable cross-platform generalization and practical onboard deployment.

Method: Propose a navigation policy architecture that reduces sim-to-real appearance gap by leveraging pretrained visual representations and running in real time on robot hardware. Train policies in simulation, evaluate on a wheeled mobile robot against a real-world-trained baseline and prior SOTA, and test generalization by deploying the same model onboard a drone. Investigate the impact of diverse image encoder pretraining and on-policy learning on sim-to-real transfer.

Result: On a wheeled robot, the simulation-trained policy outperforms the real-world-trained version by 31% and the prior SOTA by 50% in navigation success rate. The same model generalizes to a drone when deployed onboard. The results highlight the importance of diverse image encoder pretraining for sim-to-real generalization and identify on-policy learning as a key advantage of simulated training.

Conclusion: Diverse image encoder pretraining and on-policy learning substantially improve sim-to-real generalization for visual navigation. Simulator-trained policies can match or exceed real-world-trained policies when paired with an architecture that bridges the appearance gap and supports onboard, on-policy learning, with demonstrated cross-platform generalization.

Abstract: This paper investigates how the performance of visual navigation policies
trained in simulation compares to policies trained with real-world data.
Performance degradation of simulator-trained policies is often significant when
they are evaluated in the real world. However, despite this well-known
sim-to-real gap, we demonstrate that simulator-trained policies can match the
performance of their real-world-trained counterparts.
  Central to our approach is a navigation policy architecture that bridges the
sim-to-real appearance gap by leveraging pretrained visual representations and
runs real-time on robot hardware. Evaluations on a wheeled mobile robot show
that the proposed policy, when trained in simulation, outperforms its
real-world-trained version by 31% and the prior state-of-the-art methods by 50%
in navigation success rate. Policy generalization is verified by deploying the
same model onboard a drone.
  Our results highlight the importance of diverse image encoder pretraining for
sim-to-real generalization, and identify on-policy learning as a key advantage
of simulated training over training with real data.

</details>


### [191] [UniPilot: Enabling GPS-Denied Autonomy Across Embodiments](https://arxiv.org/abs/2509.11793)
*Mihir Kulkarni,Mihir Dharmadhikari,Nikhil Khedekar,Morten Nissov,Mohit Singh,Philipp Weiss,Kostas Alexis*

Main category: cs.RO

TL;DR: UniPilot is a portable hardware-software autonomy payload that enables cross-platform autonomous operation in GPS-denied environments using multi-modal sensing and an integrated autonomy stack.


<details>
  <summary>Details</summary>
Motivation: There is a need for a compact, platform-agnostic autonomy solution that can operate without GPS and across diverse robots, reducing integration effort and increasing reliability in challenging environments.

Method: UniPilot combines LiDAR, radar, vision, and inertial sensing with a full autonomy stack (multi-modal perception, exploration/inspection path planning, and learning-based navigation). It offers robust localization, mapping, planning, and safety/control within a single deployable unit suitable for many platforms, validated by extensive experiments across different environments and robot types.

Result: The approach yields robust mapping, planning, and safe navigation across varied environments and robot platforms, demonstrating cross-platform operability and resilience of the multi-modal payload.

Conclusion: UniPilot provides a compact, versatile autonomy payload enabling GPS-denied autonomous operation across a broad range of platforms, with potential for rapid deployment and broad applicability.

Abstract: This paper presents UniPilot, a compact hardware-software autonomy payload
that can be integrated across diverse robot embodiments to enable autonomous
operation in GPS-denied environments. The system integrates a multi-modal
sensing suite including LiDAR, radar, vision, and inertial sensing for robust
operation in conditions where uni-modal approaches may fail. UniPilot runs a
complete autonomy software comprising multi-modal perception, exploration and
inspection path planning, and learning-based navigation policies. The payload
provides robust localization, mapping, planning, and safety and control
capabilities in a single unit that can be deployed across a wide range of
platforms. A large number of experiments are conducted across diverse
environments and on a variety of robot platforms to validate the mapping,
planning, and safe navigation capabilities enabled by the payload.

</details>


### [192] [TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning](https://arxiv.org/abs/2509.11839)
*Jiacheng Liu,Pengxiang Ding,Qihang Zhou,Yuxuan Wu,Da Huang,Zimian Peng,Wei Xiao,Weinan Zhang,Lixin Yang,Cewu Lu,Donglin Wang*

Main category: cs.RO

TL;DR: A Koopman-guided online residual refinement (KORR) framework for imitation learning that imposes linear latent dynamics to improve long-horizon robustness by conditioning residuals on Koopman-predicted states.


<details>
  <summary>Details</summary>
Motivation: Imitation learning suffers from compounding errors in long-horizon tasks and poor generalization; existing residual refinements are locally focused and lack global state-transition understanding. Global dynamics modeling can improve extrapolation and robustness.

Method: Train a latent representation with Koopman operator theory to enforce linear time-invariant (LTI) dynamics in latent space; develop KORR that conditions residual policy corrections on the Koopman-predicted latent states; perform online residual refinement guided by global dynamics; evaluate on long-horizon, fine-grained robotic furniture assembly under perturbations.

Result: Empirical results show consistent improvements in performance, robustness, and generalization across baselines on ensemble of long-horizon tasks with perturbations; demonstrates the efficacy of Koopman-based global dynamics in guiding residual refinements.

Conclusion: Koopman-based global dynamics modeling effectively bridges modern learning with classical control notions, enabling more stable, extrapolative residual refinement and broader applicability to long-horizon control problems.

Abstract: Imitation learning (IL) enables efficient skill acquisition from
demonstrations but often struggles with long-horizon tasks and high-precision
control due to compounding errors. Residual policy learning offers a promising,
model-agnostic solution by refining a base policy through closed-loop
corrections. However, existing approaches primarily focus on local corrections
to the base policy, lacking a global understanding of state evolution, which
limits robustness and generalization to unseen scenarios. To address this, we
propose incorporating global dynamics modeling to guide residual policy
updates. Specifically, we leverage Koopman operator theory to impose linear
time-invariant structure in a learned latent space, enabling reliable state
transitions and improved extrapolation for long-horizon prediction and unseen
environments. We introduce KORR (Koopman-guided Online Residual Refinement), a
simple yet effective framework that conditions residual corrections on
Koopman-predicted latent states, enabling globally informed and stable action
refinement. We evaluate KORR on long-horizon, fine-grained robotic furniture
assembly tasks under various perturbations. Results demonstrate consistent
gains in performance, robustness, and generalization over strong baselines. Our
findings further highlight the potential of Koopman-based modeling to bridge
modern learning methods with classical control theory. For more details, please
refer to https://jiachengliu3.github.io/TrajBooster.

</details>


### [193] [Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer](https://arxiv.org/abs/2509.11865)
*Travis Davies,Yiqi Huang,Yunxin Liu,Xiang Chen,Huxian Liu,Luhui Hu*

Main category: cs.RO

TL;DR: Tenma is a lightweight diffusion-transformer that fuses multimodal signals (multiview RGB, proprioception, language) for cross-embodiment manipulation, using cross-embodiment normalization and a Joint State-Time encoder, achieving strong in-distribution performance and robustness with moderate data.


<details>
  <summary>Details</summary>
Motivation: Address instability and poor generalization when scaling diffusion-transformer policies to heterogeneous multimodal data and cross-embodiment robot control, aiming for lightweight, robust learning.

Method: Introduce Tenma: cross-embodiment normalizer to map disparate state/action spaces to a shared latent; Joint State-Time encoder for temporally aligned observations; diffusion action decoder optimized for stability; integrates multiview RGB, proprioception, language for bi-manual arm control; evaluated under matched compute.

Result: Tenma achieves 88.95% average in-distribution success; maintains strong performance under object/scene shifts; baseline best in-distribution average is 18.12%; robust with moderate data scale.

Conclusion: Multimodal and cross-embodiment learning can substantially boost transformer-based imitation learning; Tenma demonstrates strong generalization and stability, highlighting potential for cross-embodiment diffusion-transformer policies in manipulation.

Abstract: Scaling Transformer policies and diffusion models has advanced robotic
manipulation, yet combining these techniques in lightweight, cross-embodiment
learning settings remains challenging. We study design choices that most affect
stability and performance for diffusion-transformer policies trained on
heterogeneous, multimodal robot data, and introduce Tenma, a lightweight
diffusion-transformer for bi-manual arm control. Tenma integrates multiview
RGB, proprioception, and language via a cross-embodiment normalizer that maps
disparate state/action spaces into a shared latent space; a Joint State-Time
encoder for temporally aligned observation learning with inference speed
boosts; and a diffusion action decoder optimized for training stability and
learning capacity. Across benchmarks and under matched compute, Tenma achieves
an average success rate of 88.95% in-distribution and maintains strong
performance under object and scene shifts, substantially exceeding baseline
policies whose best in-distribution average is 18.12%. Despite using moderate
data scale, Tenma delivers robust manipulation and generalization, indicating
the great potential for multimodal and cross-embodiment learning strategies for
further augmenting the capacity of transformer-based imitation learning
policies.

</details>


### [194] [VH-Diffuser: Variable Horizon Diffusion Planner for Time-Aware Goal-Conditioned Trajectory Planning](https://arxiv.org/abs/2509.11930)
*Ruijia Liu,Ancheng Hou,Shaoyuan Li,Xiang Yin*

Main category: cs.RO

TL;DR: A diffusion-based planner with a learned, instance-specific horizon (VHD) that adapts trajectory length without architectural changes, improving robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Fixed horizons in diffusion planners lead to length mismatches and brittle performance across tasks with varying geometric/dynamical difficulty; adaptivity is needed.

Method: Introduce a Length Predictor to forecast an instance-specific horizon given a start-goal pair; steer diffusion planning via initial noise shaping and training on randomly cropped sub-trajectories, preserving compatibility with existing planners without changing architecture.

Result: Empirically improves success rates and path efficiency in maze-navigation and robot-arm benchmarks; shows robustness to horizon mismatch and unseen lengths; training remains offline-only and simple.

Conclusion: Treating horizon as a learnable variable via VHD yields better robustness and performance across varying lengths while staying compatible with existing diffusion planners and simple to train.

Abstract: Diffusion-based planners have gained significant recent attention for their
robustness and performance in long-horizon tasks. However, most existing
planners rely on a fixed, pre-specified horizon during both training and
inference. This rigidity often produces length-mismatch (trajectories that are
too short or too long) and brittle performance across instances with varying
geometric or dynamical difficulty. In this paper, we introduce the Variable
Horizon Diffuser (VHD) framework, which treats the horizon as a learned
variable rather than a fixed hyperparameter. Given a start-goal pair, we first
predict an instance-specific horizon using a learned Length Predictor model,
which guides a Diffusion Planner to generate a trajectory of the desired
length. Our design maintains compatibility with existing diffusion planners by
controlling trajectory length through initial noise shaping and training on
randomly cropped sub-trajectories, without requiring architectural changes.
Empirically, VHD improves success rates and path efficiency in maze-navigation
and robot-arm control benchmarks, showing greater robustness to horizon
mismatch and unseen lengths, while keeping training simple and offline-only.

</details>


### [195] [E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for Uncertainty-aware Gaussian Semantic Mapping](https://arxiv.org/abs/2509.11964)
*Junyoung Kim,Minsik Jeon,Jihong Min,Kiho Kwak,Junwon Seo*

Main category: cs.RO

TL;DR: An uncertainty-aware semantic mapping framework integrates Evidential Deep Learning with Bayesian Kernel Inference and Gaussian primitives to robustly map outdoor environments in real time.


<details>
  <summary>Details</summary>
Motivation: Outdoor semantic mapping is hampered by multiple uncertainties from sparse/noisy data and complex scene geometries; robust uncertainty modeling is needed for reliable maps.

Method: Estimate semantic uncertainty with Evidential Deep Learning and propagate it into Bayesian Kernel Inference. Aggregate noisy observations into coherent Gaussian primitives and apply geometry-aligned kernels to fuse local geometric and semantic information.

Result: Demonstrates consistent improvements in mapping quality, uncertainty calibration, representational flexibility, and robustness across diverse off-road and urban environments while maintaining real-time efficiency.

Conclusion: The proposed framework enables robust, uncertainty-aware 3D semantic mapping in challenging outdoor settings and is suitable for real-world deployment.

Abstract: Semantic mapping aims to construct a 3D semantic representation of the
environment, providing essential knowledge for robots operating in complex
outdoor settings. While Bayesian Kernel Inference (BKI) addresses
discontinuities of map inference from sparse sensor data, existing semantic
mapping methods suffer from various sources of uncertainties in challenging
outdoor environments. To address these issues, we propose an uncertainty-aware
semantic mapping framework that handles multiple sources of uncertainties,
which significantly degrade mapping performance. Our method estimates
uncertainties in semantic predictions using Evidential Deep Learning and
incorporates them into BKI for robust semantic inference. It further aggregates
noisy observations into coherent Gaussian representations to mitigate the
impact of unreliable points, while employing geometry-aligned kernels that
adapt to complex scene structures. These Gaussian primitives effectively fuse
local geometric and semantic information, enabling robust, uncertainty-aware
mapping in complex outdoor scenarios. Comprehensive evaluation across diverse
off-road and urban outdoor environments demonstrates consistent improvements in
mapping quality, uncertainty calibration, representational flexibility, and
robustness, while maintaining real-time efficiency.

</details>


### [196] [Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study](https://arxiv.org/abs/2509.11971)
*James C. Ward,Alex Bott,Connor York,Edmund R. Hunt*

Main category: cs.RO

TL;DR: A machine-learning adversary model observes patrol behavior to attempt undetected access within a time limit, providing a stronger test of multi-robot patrol robustness than existing baselines.


<details>
  <summary>Details</summary>
Motivation: To evaluate robustness of autonomous multi-robot patrol systems against realistic, learning-based attacks and inform vulnerability-aware defense design.

Method: Train/deploy an ML-based adversary that watches patrol patterns to plan intrusion within a bounded timeframe; compare its performance to baselines across several decentralized patrol strategies.

Result: The adversary model outperforms baselines, delivering a more stringent evaluation and revealing weaknesses across multiple leading decentralized patrol approaches.

Conclusion: Adversary-informed testing with ML models is a valuable tool for designing more robust, vulnerability-aware multi-robot patrol systems.

Abstract: Simulating hostile attacks of physical autonomous systems can be a useful
tool to examine their robustness to attack and inform vulnerability-aware
design. In this work, we examine this through the lens of multi-robot patrol,
by presenting a machine learning-based adversary model that observes robot
patrol behavior in order to attempt to gain undetected access to a secure
environment within a limited time duration. Such a model allows for evaluation
of a patrol system against a realistic potential adversary, offering insight
into future patrol strategy design. We show that our new model outperforms
existing baselines, thus providing a more stringent test, and examine its
performance against multiple leading decentralized multi-robot patrol
strategies.

</details>


### [197] [Gesture-Based Robot Control Integrating Mm-wave Radar and Behavior Trees](https://arxiv.org/abs/2509.12008)
*Yuqing Song,Cesare Tonola,Stefano Savazzi,Sanaz Kianoush,Nicola Pedrocchi,Stephan Sigg*

Main category: cs.RO

TL;DR: A mm-wave radar-based system recognizes nine gestures to control a robotic arm in real time, providing contactless, privacy-preserving interaction that is robust to occlusion and lighting, and unifies gesture recognition with robotic control in a single pipeline. Case studies validate practicality and reliability.


<details>
  <summary>Details</summary>
Motivation: As robots proliferate in homes and industry, there is a need for intuitive, non-contact human–machine interaction. Camera-based systems raise privacy concerns and underperform in poor lighting or occlusion; radar offers privacy, robustness, and rich spatial data.

Method: Use mm-wave radar to detect and classify nine hand gestures and map them to real-time robotic arm commands, integrating gesture recognition and robot control into a single, end-to-end pipeline. Case studies evaluate practicality, performance, and reliability.

Result: Achieves recognition of nine gestures with precision and maps them to real-time commands, demonstrating a unified, contactless HMI pipeline. Case studies corroborate practicality and reliability in gesture-based robotic manipulation.

Conclusion: The work demonstrates that radar-based gesture recognition can effectively enable integrated, real-time, contactless human–robot interaction, suitable for practical robotic manipulation in privacy-conscious or challenging environments.

Abstract: As robots become increasingly prevalent in both homes and industrial
settings, the demand for intuitive and efficient human-machine interaction
continues to rise. Gesture recognition offers an intuitive control method that
does not require physical contact with devices and can be implemented using
various sensing technologies. Wireless solutions are particularly flexible and
minimally invasive. While camera-based vision systems are commonly used, they
often raise privacy concerns and can struggle in complex or poorly lit
environments. In contrast, radar sensing preserves privacy, is robust to
occlusions and lighting, and provides rich spatial data such as distance,
relative velocity, and angle. We present a gesture-controlled robotic arm using
mm-wave radar for reliable, contactless motion recognition. Nine gestures are
recognized and mapped to real-time commands with precision. Case studies are
conducted to demonstrate the system practicality, performance and reliability
for gesture-based robotic manipulation. Unlike prior work that treats gesture
recognition and robotic control separately, our system unifies both into a
real-time pipeline for seamless, contactless human-robot interaction.

</details>


### [198] [Embodied Navigation Foundation Model](https://arxiv.org/abs/2509.12129)
*Jiazhao Zhang,Anqi Li,Yunpeng Qi,Minghan Li,Jiahang Liu,Shaoan Wang,Haoran Liu,Gengze Zhou,Yuze Wu,Xingxing Li,Yuxin Fan,Wenjun Li,Zhibo Chen,Fei Gao,Qi Wu,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: NavFoM is a cross-embodiment, cross-task Navigation Foundation Model trained on 8M samples from diverse robots and tasks; using camera-view and temporal tokens with dynamic token sampling, it achieves state-of-the-art or competitive results without task-specific fine-tuning and generalizes in real-world tests.


<details>
  <summary>Details</summary>
Motivation: To overcome the generalization gap of vision-language models in embodied navigation, where current systems are limited to narrow tasks and embodiment-specific architectures, and to enable a single model that works across multiple embodiments, camera setups, and navigation horizons.

Method: A unified architecture that ingests multimodal navigation inputs from varying camera configurations and horizons. It uses identifier tokens to encode embodiment camera views and temporal task context. It dynamically samples observation tokens under a limited token-length budget. Trained on 8 million navigation samples spanning quadrupeds, drones, wheeled robots, and vehicles, covering tasks like vision-and-language navigation, object searching, target tracking, and autonomous driving.

Result: NavFoM achieves state-of-the-art or highly competitive performance across multiple navigation tasks and embodiments without task-specific fine-tuning. Real-world experiments validate strong generalization and practical applicability.

Conclusion: NavFoM provides a versatile, cross-embodiment, cross-task navigation foundation model that generalizes across diverse embodiments and tasks, reducing the need for task-specific architectures and enabling practical deployment in real-world environments.

Abstract: Navigation is a fundamental capability in embodied AI, representing the
intelligence required to perceive and interact within physical environments
following language instructions. Despite significant progress in large
Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance
on general vision-language tasks, their generalization ability in embodied
navigation remains largely confined to narrow task settings and
embodiment-specific architectures. In this work, we introduce a
cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained
on eight million navigation samples that encompass quadrupeds, drones, wheeled
robots, and vehicles, and spanning diverse tasks such as vision-and-language
navigation, object searching, target tracking, and autonomous driving. NavFoM
employs a unified architecture that processes multimodal navigation inputs from
varying camera configurations and navigation horizons. To accommodate diverse
camera setups and temporal horizons, NavFoM incorporates identifier tokens that
embed camera view information of embodiments and the temporal context of tasks.
Furthermore, to meet the demands of real-world deployment, NavFoM controls all
observation tokens using a dynamically adjusted sampling strategy under a
limited token length budget. Extensive evaluations on public benchmarks
demonstrate that our model achieves state-of-the-art or highly competitive
performance across multiple navigation tasks and embodiments without requiring
task-specific fine-tuning. Additional real-world experiments further confirm
the strong generalization capability and practical applicability of our
approach.

</details>


### [199] [Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks](https://arxiv.org/abs/2509.12151)
*Zongyao Yi,Joachim Hertzberg,Martin Atzmueller*

Main category: cs.RO

TL;DR: A learnable physics simulator based on FIGNet is extended with new node/edge types to enable action-conditioned predictions for control and state estimation in contact-rich manipulation. It matches ground-truth dynamics in simulation and shows strong real-world gains: 50% better motion prediction and 3x more accurate force-torque predictions versus a baseline; source code and data are public.


<details>
  <summary>Details</summary>
Motivation: Predicting contact-rich manipulation dynamics is challenging for traditional simulators. A data-driven, action-conditioned GNN-based model can improve predictive accuracy for end-effector motion and force-torque, enabling better control and state estimation.

Method: Extend the state-of-the-art GNN-based FIGNet with novel node and edge types to enable action-conditional predictions. Use the learned model within an MPC controller; evaluate in simulation on peg-in-hole tasks and validate in real-world experiments. Compare against a baseline physics simulator and release code/data publicly.

Result: In simulation, an MPC agent using the proposed model matches the performance of the same controller with ground-truth dynamics in peg-in-hole. In real-world tests, the model achieves 50% higher motion prediction accuracy and 3× higher force-torque prediction precision than the baseline physics simulator.

Conclusion: The proposed action-conditioned graphene-non? extension to FIGNet yields tangible improvements in both predictive accuracy and practical control performance for contact-rich manipulation, with open-source resources to accelerate adoption.

Abstract: We present a learnable physics simulator that provides accurate motion and
force-torque prediction of robot end effectors in contact-rich manipulation.
The proposed model extends the state-of-the-art GNN-based simulator (FIGNet)
with novel node and edge types, enabling action-conditional predictions for
control and state estimation tasks. In simulation, the MPC agent using our
model matches the performance of the same controller with the ground truth
dynamics model in a challenging peg-in-hole task, while in the real-world
experiment, our model achieves a 50% improvement in motion prediction accuracy
and 3$\times$ increase in force-torque prediction precision over the baseline
physics simulator. Source code and data are publicly available.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [200] [Situation Model of the Transport, Transport Emissions and Meteorological Conditions](https://arxiv.org/abs/2509.10541)
*V. Benes,M. Svitek,A. Michalikova,M. Melicherik*

Main category: cs.AI

TL;DR: A fuzzy inference system model predicts how weather affects traffic-emission changes using Prague data, to aid urban planning.


<details>
  <summary>Details</summary>
Motivation: Air pollution in cities is a major concern; traffic emissions interact with meteorological conditions, influencing emission quantities and dispersion. Understanding this relationship can help planners reduce pollution.

Method: A systemic approach using fuzzy inference systems (FIS) to model and predict changes in emissions under varying weather and traffic conditions. The model uses traffic, meteorology, and emission data measured in Prague, Czech Republic.

Result: A functional FIS-based model was developed to predict changes in emissions depending on various meteorological and traffic conditions, using Prague data.

Conclusion: The approach provides insights for urban planners and policymakers to plan and manage urban transport with environmental protection in mind.

Abstract: Air pollution in cities and the possibilities of reducing this pollution
represents one of the most important factors that today's society has to deal
with. This paper focuses on a systemic approach to traffic emissions with their
relation to meteorological conditions, analyzing the effect of weather on the
quantity and dispersion of traffic emissions in a city. Using fuzzy inference
systems (FIS) the model for prediction of changes in emissions depending on
various conditions is developed. The proposed model is based on traffic,
meteorology and emission data measured in Prague, Czech Republic. The main
objective of the work is to provide insight into how urban planners and
policymakers can plan and manage urban transport more effectively with
environmental protection in mind.

</details>


### [201] [ZapGPT: Free-form Language Prompting for Simulated Cellular Control](https://arxiv.org/abs/2509.10660)
*Nam H. Le,Patrick Erickson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TL;DR: Two-AI language-guided control loop evolves to steer simulated cell collectives using natural-language prompts, enabling generalization to unseen prompts without domain-specific rewards.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between natural language and controlling complex, decentralized systems in AI and biology; overcome dependence on engineered rewards, task-specific supervision, and fixed command sets; address the open question of guiding artificial or biological collectives with free-form language.

Method: Two AI models: (1) converts imperative prompts into interventions applied to simulated cells; (2) scores how well the prompt describes the resulting cellular dynamics. An evolutionary loop tunes the first model to maximize the scores from the second, without domain-specific fitness functions or prompts.

Result: The evolved system generalizes to unseen prompts without retraining and demonstrates that language can serve as a control layer for collective behavior, suggesting a path toward AI–biology partnerships where natural language directs complex systems.

Conclusion: Natural language can act as a flexible, domain-agnostic control layer to guide complex, decentralized systems, enabling prompts to direct computational, robotic, or biological systems without fixed objective functions or domain-specific programming.

Abstract: Human language is one of the most expressive tools for conveying intent, yet
most artificial or biological systems lack mechanisms to interpret or respond
meaningfully to it. Bridging this gap could enable more natural forms of
control over complex, decentralized systems. In AI and artificial life, recent
work explores how language can specify high-level goals, but most systems still
depend on engineered rewards, task-specific supervision, or rigid command sets,
limiting generalization to novel instructions. Similar constraints apply in
synthetic biology and bioengineering, where the locus of control is often
genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be
guided by free-form natural language alone, without task-specific tuning or
carefully designed evaluation metrics. We provide one possible answer here by
showing, for the first time, that simple agents' collective behavior can be
guided by free-form language prompts: one AI model transforms an imperative
prompt into an intervention that is applied to simulated cells; a second AI
model scores how well the prompt describes the resulting cellular dynamics; and
the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness
functions or domain-specific prompt design. We show that the evolved system
generalizes to unseen prompts without retraining. By treating natural language
as a control layer, the system suggests a future in which spoken or written
prompts could direct computational, robotic, or biological systems to desired
behaviors. This work provides a concrete step toward this vision of AI-biology
partnerships, in which language replaces mathematical objective functions,
fixed rules, and domain-specific programming.

</details>


### [202] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: Maestro is a self-evolving text-to-image system that autonomously refines prompts via self-critique and self-evolution using multimodal LLMs to improve image quality from a single initial prompt.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models require manual, iterative prompt engineering; there is a need for autonomous, interpretable prompt refinement that aligns with user intent and handles under-specification.

Method: Maestro introduces two innovations: (1) self-critique, where specialized multimodal LLM actors act as 'critics' to identify weaknesses in generated images, propose interpretable edits, and a 'verifier' preserves user intent; (2) self-evolution, using an LLM-as-judge to compare iterates head-to-head, discard poor results, and evolve prompt candidates that better match user goals.

Result: Extensive experiments on complex T2I tasks show that Maestro significantly improves image quality over initial prompts and over state-of-the-art automated methods; effectiveness scales with more advanced LLM components.

Conclusion: Maestro provides a robust, interpretable, and effective pathway toward self-improving text-to-image generation, reducing reliance on manual prompt engineering and enabling autonomous improvement through multimodal reasoning.

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [203] [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
*Sajjad Abdoli,Rudi Cilibrasi,Rima Al-Shikh*

Main category: cs.AI

TL;DR: Distinct evaluation styles emerge across GPT models when assessing NVIDIA’s Describe Anything outputs, with inherent, architecture-specific biases; evaluation ability doesn’t simply scale with general capability and requires diverse architectures to mitigate cascading biases.


<details>
  <summary>Details</summary>
Motivation: To understand how AI evaluators’ assessment behaviors influence cascading biases when judging other AI outputs, and whether these behaviors are inherent to model families or architectures.

Method: Evaluate NVIDIA Describe Anything outputs using three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to identify evaluation personalities; validate via Gemini 2.5 Pro as an independent question generator; perform cross-family semantic similarity analysis of generated questions to compare evaluation strategies across architectures.

Result: Identified three evaluation personalities: GPT-4o-mini shows systematic consistency with minimal variance; GPT-4o excels at error detection; GPT-5 is extremely conservative with high variability. These personalities appear inherent to the models. Cross-family analysis shows GPT models cluster together in high semantic similarity of questions, while Gemini diverges. All GPT models display a 2:1 bias favoring negative assessment over positive confirmation, a pattern that seems family-specific rather than universal. Implication: evaluation competence does not scale with general capability; robust AI assessment requires diverse architectural perspectives.

Conclusion: Evaluation behavior varies by architecture and is not guaranteed to improve with greater general capability; to prevent cascading biases, assessments should combine diverse AI architectures and perspectives.

Abstract: As AI systems increasingly evaluate other AI outputs, understanding their
assessment behavior becomes crucial for preventing cascading biases. This study
analyzes vision-language descriptions generated by NVIDIA's Describe Anything
Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to
uncover distinct "evaluation personalities" the underlying assessment
strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic
consistency with minimal variance, GPT-4o excels at error detection, while
GPT-5 shows extreme conservatism with high variability. Controlled experiments
using Gemini 2.5 Pro as an independent question generator validate that these
personalities are inherent model properties rather than artifacts. Cross-family
analysis through semantic similarity of generated questions reveals significant
divergence: GPT models cluster together with high similarity while Gemini
exhibits markedly different evaluation strategies. All GPT models demonstrate a
consistent 2:1 bias favoring negative assessment over positive confirmation,
though this pattern appears family-specific rather than universal across AI
architectures. These findings suggest that evaluation competence does not scale
with general capability and that robust AI assessment requires diverse
architectural perspectives.

</details>


### [204] [AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework](https://arxiv.org/abs/2509.10762)
*Arlen Kumar,Leanid Palkhouski*

Main category: cs.AI

TL;DR: GEO-16 provides a 16-pillar framework to score on-page quality and predict citations by AI answer engines; higher GEO scores correlate with more citations; offers a publisher playbook.


<details>
  <summary>Details</summary>
Motivation: As AI answer engines cite web sources, there's a need to quantify on-page quality signals that influence citation likelihood and to guide publishers in improving source quality and discoverability.

Method: Develop GEO-16: convert on-page quality signals into 16 pillar scores, produce a normalized GEO score G in [0,1]. Collect 1,702 citations across 3 engines (Brave Summary, Google AI Overviews, Perplexity) using 70 product-intent prompts; audit 1,100 unique URLs; use logistic models with domain-clustered standard errors to relate page quality to citation; analyze pillar-level associations and thresholds; produce engine-specific contrasts and publish a publisher playbook.

Result: Pillars related to Metadata and Freshness, Semantic HTML, and Structured Data show strongest association with citations. Overall page quality (G) is a strong predictor of citation. Thresholds like G ≥ 0.70 with ≥12 pillar hits align with higher citation rates. Engine-specific differences and vertical effects observed; robust but observational dataset across English-language B2B SaaS pages.

Conclusion: GEO-16 offers a practical, data-driven playbook for publishers to improve the likelihood of being cited by AI answer engines; acknowledges limitations including observational design, domain/language scope, and reproducibility considerations.

Abstract: AI answer engines increasingly mediate access to domain knowledge by
generating responses and citing web sources. We introduce GEO-16, a 16 pillar
auditing framework that converts on page quality signals into banded pillar
scores and a normalized GEO score G that ranges from 0 to 1. Using 70 product
intent prompts, we collected 1,702 citations across three engines (Brave
Summary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In
our corpus, the engines differed in the GEO quality of the pages they cited,
and pillars related to Metadata and Freshness, Semantic HTML, and Structured
Data showed the strongest associations with citation. Logistic models with
domain clustered standard errors indicate that overall page quality is a strong
predictor of citation, and simple operating points (for example, G at least
0.70 combined with at least 12 pillar hits) align with substantially higher
citation rates in our data. We report per engine contrasts, vertical effects,
threshold analysis, and diagnostics, then translate findings into a practical
playbook for publishers. The study is observational and focuses on English
language B2B SaaS pages; we discuss limitations, threats to validity, and
reproducibility considerations.

</details>


### [205] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: Comprehensive benchmark of 18 agentic configurations across modern LLMs reveals strong model- and dimension-specific preferences, undermining one-size-fits-all designs, with limited enterprise-task success (max 35.3% complex, 70.8% simple).


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of empirical understanding of how agentic design dimensions interact in complex multi-agent systems and provides enterprise-relevant benchmarks across configurations.

Method: Evaluate 18 configurations spanning orchestration strategy, agent prompt implementation (ReAct vs function calling), memory architecture, and thinking tool integration across state-of-the-art LLMs for enterprise tasks, comparing performance on simple versus complex tasks.

Result: Finds significant model-specific architectural preferences and weak overall agentic performance on enterprise tasks; no universal configuration fits all models; top scores are 35.3% (complex) and 70.8% (simple).

Conclusion: Results inform the design of future agentic systems by guiding empirical choices about architectural components and model selection, moving away from a one-size-fits-all paradigm.

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [206] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: A four-step algorithm to derive a computationally tractable personal expert mental model (EMM) for LLM prompt engineering, combining optimized human–machine dialogue with monotone Boolean and k-valued functions to enhance decision-making under missing or incomplete data.


<details>
  <summary>Details</summary>
Motivation: LLMs face hallucinations due to missing training data and incomplete external sources; existing remedies like retrieval-augmented generation are partial; there is a need for efficient decision support and better capture of domain experts' reasoning.

Method: Proposes an EMM algorithm with four steps: (1) factor identification, (2) hierarchical structuring of factors, (3) generating a generalized expert mental model (EMM) specification, and (4) generating a detailed generalized EMM from that specification, using monotone Boolean and k-valued functions and optimized human–machine dialogue; illustrated via a running example (evaluating whether to respond to a call for proposals).

Result: Conceptual framework outlining a four-step algorithm to construct a personal expert mental model for LLM prompt engineering; emphasizes mathematical representations (monotone Boolean/k-valued) and optimized dialogue; no empirical results reported in the abstract.

Conclusion: The approach aims to make LLM-based decision-making more efficient under information gaps by encoding domain expertise as a tractable EMM, guiding prompt design and decision strategies through a structured four-step process.

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [207] [From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering](https://arxiv.org/abs/2509.10837)
*Yuyin Lu,Hegang Chen,Yanghui Rao*

Main category: cs.AI

TL;DR: Grounding vs Skolemization for CQA on incomplete KGs; LVSA unifies differentiable Skolemization and neural negation with logic-constrained optimization, achieving universality for EFO1 queries and improved efficiency.


<details>
  <summary>Details</summary>
Motivation: There is a trade-off between logical soundness and computational efficiency in CQA over incomplete KGs; existing methods suffer from grounding explosion or inconsistent Skolemization.

Method: Logic-constrained Vector Symbolic Architecture (LVSA) that combines differentiable Skolemization, a neural negator, and a logic-driven optimization protocol; proves universality for all EFO1 queries.

Result: Theoretically universal for EFO1; empirically outperforms state-of-the-art Skolemization-based methods; inference costs reduced by orders of magnitude vs grounding baselines.

Conclusion: LVSA offers a principled neuro-symbolic solution, bridging grounding and Skolemization with explicit logical constraints, enabling scalable, logically sound CQA on incomplete KGs.

Abstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),
typically formalized as reasoning with Existential First-Order predicate logic
with one free variable (EFO$_1$), faces a fundamental trade-off between logical
soundness and computational efficiency. This work establishes the
Grounding-Skolemization dichotomy for systematically analyzing CQA methods
through the lens of formal logic. While Grounding-based methods inherently
suffer from combinatorial explosion, most Skolemization-based methods neglect
to explicitly model Skolem functions and compromise logical consistency. To
address these limitations, we propose the Logic-constrained Vector Symbolic
Architecture (LVSA), a neuro-symbolic framework that unifies a differentiable
Skolemization module and a neural negator, as well as a logical
constraint-driven optimization protocol to harmonize geometric and logical
requirements. Theoretically, LVSA guarantees universality for all EFO$_1$
queries. Empirically, it outperforms state-of-the-art Skolemization-based
methods and reduces inference costs by orders of magnitude compared to
Grounding-based baselines.

</details>


### [208] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: Questions the necessity of the agent-centric paradigm in AI, distinguishes agentic, agential, and non-agentic systems, and advocates systemic, world-modeling approaches as alternatives to achieve robust, scalable, and non-anthropomorphic intelligence.


<details>
  <summary>Details</summary>
Motivation: Agent-centric AI has built-in anthropocentric biases and ambiguous concepts of autonomy and goal-directed behavior, potentially limiting progress and obscuring underlying computation. A clearer taxonomy and alternative paradigms are sought to advance general intelligence.

Method: Systematic literature review and conceptual analysis that deconstructs the agent paradigm across AI frameworks, introducing three categories (agentic, agential, non-agentic) and evaluating properties like autonomy and goal-directedness.

Result: Agentic framing is heuristically useful but can be misleading and may obscure computational mechanisms, especially in LLMs. A taxonomy clarifies distinctions, highlights conceptual ambiguities, and suggests shifting focus toward system-level dynamics, world modeling, and material intelligence rather than pervasive agency.

Conclusion: Encourage investigation of non-agentic and systemic frameworks inspired by complex systems, biology, and unconventional computing, requiring new architectures and a redefinition of intelligence beyond the agent metaphor to achieve robust, scalable, and possibly non-anthropomorphic general intelligence.

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [209] [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: HaPLa is a universal jailbreak technique for LLMs using abductive framing and symbolic encoding, achievable with black-box access, achieving high attack success rates (>95% on GPT-series, ~70% overall) and revealing a tension between safety tuning and helpfulness.


<details>
  <summary>Details</summary>
Motivation: To systematically study universal jailbreaking vulnerabilities in LLMs to strengthen defenses, focusing on black-box attack strategies that exploit intrinsic weaknesses in architecture and learning paradigms.

Method: HaPLa combines two strategies: (1) abductive framing, prompting the model to infer plausible intermediate steps toward harmful activities rather than directly answering harmful prompts; (2) symbolic encoding, lightweight content obfuscation that reduces reliance on explicit harmful keywords. The attacks are designed to work with only black-box access to target models and are evaluated on GPT-series and other targets.

Result: The approach achieves over 95% attack success on GPT-series models and around 70% across diverse targets. Symbolic encoding rules reveal that safely tuning LLMs without substantially harming their helpfulness remains difficult.

Conclusion: Universal jailbreaks like HaPLa pose a substantive threat to LLM safety and require defenses that preserve helpfulness while mitigating vulnerability to abductive reasoning and content obfuscation strategies. The results highlight the challenge in safely retuning models and the need for robust defenses against such black-box attack methodologies.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their potential misuse for harmful purposes remains a
significant concern. To strengthen defenses against such vulnerabilities, it is
essential to investigate universal jailbreak attacks that exploit intrinsic
weaknesses in the architecture and learning paradigms of LLMs. In response, we
propose \textbf{H}armful \textbf{P}rompt \textbf{La}undering (HaPLa), a novel
and broadly applicable jailbreaking technique that requires only black-box
access to target models. HaPLa incorporates two primary strategies: 1)
\textit{abductive framing}, which instructs LLMs to infer plausible
intermediate steps toward harmful activities, rather than directly responding
to explicit harmful queries; and 2) \textit{symbolic encoding}, a lightweight
and flexible approach designed to obfuscate harmful content, given that current
LLMs remain sensitive primarily to explicit harmful keywords. Experimental
results show that HaPLa achieves over 95% attack success rate on GPT-series
models and 70% across all targets. Further analysis with diverse symbolic
encoding rules also reveals a fundamental challenge: it remains difficult to
safely tune LLMs without significantly diminishing their helpfulness in
responding to benign queries.

</details>


### [210] [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: Public-data-augmented private in-context learning with differential privacy improves utility while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Tackle privacy leakage risk in in-context learning (ICL) and the utility loss from differential privacy by leveraging task-related public data.

Method: Introduce a private ICL algorithm that uses public data to guide context construction under DP guarantees; maintains the privacy budget while improving usefulness; evaluate robustness against membership inference.

Result: Experiments show significant utility gains for private ICL when public data is used; method demonstrates empirical protection against membership inference attacks.

Conclusion: Public data integration under DP can balance privacy and utility in ICL; empirical protection against membership inference enhances practical privacy.

Abstract: In-context learning (ICL) in Large Language Models (LLMs) has shown
remarkable performance across various tasks without requiring fine-tuning.
However, recent studies have highlighted the risk of private data leakage
through the prompt in ICL, especially when LLMs are exposed to malicious
attacks. While differential privacy (DP) provides strong privacy guarantees, it
often significantly reduces the utility of in-context learning (ICL). To
address this challenge, we incorporate task-related public data into the ICL
framework while maintaining the DP guarantee. Based on this approach, we
propose a private in-context learning algorithm that effectively balances
privacy protection and model utility. Through experiments, we demonstrate that
our approach significantly improves the utility of private ICL with the
assistance of public data. Additionally, we show that our method is robust
against membership inference attacks, demonstrating empirical privacy
protection.

</details>


### [211] [Enhancing Computational Cognitive Architectures with LLMs: A Case Study](https://arxiv.org/abs/2509.10972)
*Ron Sun*

Main category: cs.AI

TL;DR: Integrating Clarion with LLMs to combine computational power with cognitive realism.


<details>
  <summary>Details</summary>
Motivation: Overcome limits of traditional cognitive architectures by leveraging LLMs for real-world complexity while maintaining psychological plausibility.

Method: Proposes a synergistic integration using Clarion's implicit-explicit dichotomy to merge Clarion mechanisms with LLM capabilities.

Result: Demonstrates that LLM-powered computation can be wrapped within Clarion's structure, yielding enhanced computational power without sacrificing psychological nicety.

Conclusion: Integrating Clarion and LLMs offers a promising path for scalable, explainable, and cognitively plausible AI architectures.

Abstract: Computational cognitive architectures are broadly scoped models of the human
mind that combine different psychological functionalities (as well as often
different computational methods for these different functionalities) into one
unified framework. They structure them in a psychologically plausible and
validated way. However, such models thus far have only limited computational
capabilities, mostly limited by the computational tools and techniques that
were adopted. More recently, LLMs have proved to be more capable
computationally than any other tools. Thus, in order to deal with both
real-world complexity and psychological realism at the same time, incorporating
LLMs into cognitive architectures naturally becomes an important task. In the
present article, a synergistic combination of the Clarion cognitive
architecture and LLMs is discussed as a case study. The implicit-explicit
dichotomy that is fundamental to Clarion is leveraged for a seamless
integration of Clarion and LLMs. As a result, computational power of LLMs is
combined with psychological nicety of Clarion.

</details>


### [212] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: Attribute-based evaluation of LLM rationales using SHAP and attribute-specific ELO reveals nuanced insights beyond binary judgments.


<details>
  <summary>Details</summary>
Motivation: Binary preference judgments for LLM rationales are often opaque and coarse-grained; there is a need for fine-grained, interpretable attributes that explain why a rationale is better.

Method: Identify a set of rationale attributes from prior literature; assess them with automatic metrics, LLM judgments, and human annotations; analyze MT Bench and Chatbot Arena using SHAP to see attribute contributions; re-evaluate model-generated rationales using attribute-specific ELO scores.

Result: Certain attributes better explain human preferences; SHAP identifies key attributes driving preferences; attribute-specific ELO provides more nuanced model comparisons.

Conclusion: Fine-grained attribute-based evaluation can better characterize rationale quality, improve interpretability, and guide future evaluation practices.

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [213] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: Free-MAD replaces last-round consensus in multi-agent debates with a score-based evaluation of the entire debate trajectory and introduces anti-conformity to curb majority influence, enabling single-round debates with improved efficiency, fairness, and robustness.


<details>
  <summary>Details</summary>
Motivation: MAD methods currently rely on multi-round discussions and majority voting, which incur token costs, propagate conformity-based errors, and introduce randomness/unfairness in decision making. A single-round, trajectory-aware approach could reduce cost and improve reliability.

Method: Propose Free-MAD: a score-based decision mechanism that evaluates the full debate trajectory rather than the final round, tracking how each agent’s reasoning evolves; introduce anti-conformity to mitigate undue influence from the majority; demonstrate single-round debates with reduced token usage and greater robustness.

Result: Empirical evaluation across eight benchmark datasets shows that Free-MAD improves reasoning performance compared with existing MAD methods, while reducing token costs due to the single-round format and exhibiting greater robustness under adversarial scenarios.

Conclusion: Free-MAD offers a scalable, fair, and robust MAD framework by moving away from consensus-based final-round voting toward trajectory-aware scoring and anti-conformity, achieving stronger reasoning with lower cost and improved resilience.

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


### [214] [Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration](https://arxiv.org/abs/2509.11067)
*Liangxuan Guo,Bin Zhu,Qingqian Tao,Kangning Liu,Xun Zhao,Xianzhe Qin,Jin Gao,Guangfu Hao*

Main category: cs.AI

TL;DR: FSM-based multi-agent orchestration for desktop automation with quality control; achieves 57.07% success in 50 steps, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Autonomous desktop automation struggles with complex multi-step tasks due to poor coordination and insufficient quality control.

Method: A four-component system (Controller, Manager, three Workers: Technician for code operations, Operator for GUI interactions, Analyst for decision support) and an Evaluator, with an FSM-based routing mechanism that dynamically selects execution strategies for subtasks, enabling adaptive replanning and error recovery with quality gating.

Result: State-of-the-art 57.07% success rate in 50 steps on the OSWorld benchmark, outperforming existing methods; demonstrates improved reliability in complex computing environments.

Conclusion: Principled multi-agent orchestration with continuous quality control yields superior reliability for generalized desktop automation in complex computing environments.

Abstract: Autonomous agents for desktop automation struggle with complex multi-step
tasks due to poor coordination and inadequate quality control. We introduce
\textsc{Agentic Lybic}, a novel multi-agent system where the entire
architecture operates as a finite-state machine (FSM). This core innovation
enables dynamic orchestration. Our system comprises four components: a
Controller, a Manager, three Workers (Technician for code-based operations,
Operator for GUI interactions, and Analyst for decision support), and an
Evaluator. The critical mechanism is the FSM-based routing between these
components, which provides flexibility and generalization by dynamically
selecting the optimal execution strategy for each subtask. This principled
orchestration, combined with robust quality gating, enables adaptive replanning
and error recovery. Evaluated officially on the OSWorld benchmark,
\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\% success rate in 50
steps, substantially outperforming existing methods. Results demonstrate that
principled multi-agent orchestration with continuous quality control provides
superior reliability for generalized desktop automation in complex computing
environments.

</details>


### [215] [Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability](https://arxiv.org/abs/2509.11068)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: A framework for verifiable LLM outputs in homogeneous multi-agent environments, enabling fast probabilistic auditing with asymmetric verification effort and substantial speedups over full regeneration.


<details>
  <summary>Details</summary>
Motivation: To tackle computational trust in dynamic multi-agent LLM ecosystems by enabling cost-effective verification that a claimed LLM produced the output, rather than a cheaper substitute.

Method: Utilize deterministic replicability in autoregressive models under a computationally identical hardware/software stack. Allow multiple validators to probabilistically audit small random segments of an LLM’s output and distribute the verification workload; tune parameters to control detection probability.

Result: Simulations show targeted verification can be over 12× faster than full regeneration, with tunable detection probability, establishing a tractable framework for auditable LLM systems.

Conclusion: Provides a foundational layer for responsible AI and serves as a cornerstone for future research into heterogeneous multi-agent systems by enabling auditable LLM outputs within a controlled environment.

Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,
multi-agent systems. This introduces a fundamental challenge in establishing
computational trust, specifically how one agent can verify that another's
output was genuinely produced by a claimed LLM, and not falsified or generated
by a cheaper or inferior model. To address this challenge, this paper proposes
a verification framework that achieves tractable asymmetric effort, where the
cost to verify a computation is substantially lower than the cost to perform
it. Our approach is built upon the principle of deterministic replicability, a
property inherent to autoregressive models that strictly necessitates a
computationally homogeneous environment where all agents operate on identical
hardware and software stacks. Within this defined context, our framework
enables multiple validators to probabilistically audit small, random segments
of an LLM's output and it distributes the verification workload effectively.
The simulations demonstrated that targeted verification can be over 12 times
faster than full regeneration, with tunable parameters to adjust the detection
probability. By establishing a tractable mechanism for auditable LLM systems,
our work offers a foundational layer for responsible AI and serves as a
cornerstone for future research into the more complex, heterogeneous
multi-agent systems.

</details>


### [216] [Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation](https://arxiv.org/abs/2509.11078)
*Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: Introduces Patient-Zero, a no-record synthetic patient generator using a multi-step, knowledge-injected architecture and dynamic dialogue updating to produce coherent, diverse virtual patients and improve downstream medical QA.


<details>
  <summary>Details</summary>
Motivation: Data privacy, accuracy, and diversity limitations in LLM-based synthetic medical data, plus lack of realistic interactive behavior; need for privacy-preserving, medically coherent synthetic data.

Method: Medically-aligned multi-step generation with hierarchical medical knowledge injection, no real records; dynamic updating to improve interaction consistency; adaptive dialogue strategies and real-time clinical plausibility verification.

Result: Demonstrates good accuracy, diversity, and consistency in generated patient records; after training with virtual patients, existing models show significant improvements on the MedQA dataset.

Conclusion: Shows that high-quality synthetic medical data can be generated without real records, enabling privacy-preserving training and evaluation, and enhancing downstream medical QA performance.

Abstract: Synthetic data generation using large language models (LLMs) has emerged as a
promising solution across various domains, particularly in medical field, to
mitigate data collection challenges. However, existing studies mainly utilize
LLMs to rewrite and complete existing medical records, where the limitations in
data privacy, accuracy, and diversity sill exist, and additionally lack the
ability to interact like real patients. To address these issues, we propose a
realistic patient generation framework, Patient-Zero, which requires no real
medical records. Patient-Zero first introduces a medically-aligned multi-step
generation architecture, which builds comprehensive patient records through
hierarchical medical knowledge injection without real medical records. Then, to
optimize the virtual patient's interaction abilities with humans, Patient-Zero
designs a dynamic updating mechanism to improve the consistency and
conversational performance. Our framework enables the generation of
contextually diverse patient records while maintaining strict medical
coherence, supported by adaptive dialogue strategies and real-time clinical
plausibility verification. Experimental results demonstrate that our model
achieves good performance in accuracy, diversity, and consistency. After
training with our generated virtual patients, existing models show significant
improvements on the MedQA dataset.

</details>


### [217] [Difficulty-Aware Agent Orchestration in LLM-Powered Workflows](https://arxiv.org/abs/2509.11079)
*Jinwei Su,Yinghui Xia,Qizhen Lan,Xinyuan Song,Yang Jingsong,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: DAAO dynamically tunes workflow depth, operators, and LLMs based on per-query difficulty, boosting accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome static or task-level workflows that waste compute on simple queries or underperform on complex ones, and to exploit heterogeneous LLMs by balancing cost and performance.

Method: DAAO consists of three interdependent modules: (1) a variational autoencoder (VAE) to estimate input difficulty, (2) a modular operator allocator to select processing steps, and (3) a cost- and performance-aware LLM router to assign LLMs and route tasks. The framework dynamically adapts workflow depth, operator choices, and LLM assignment for each query using heterogeneous LLMs.

Result: Empirical evaluation across six benchmarks shows that DAAO outperforms prior multi-agent systems in both accuracy and inference efficiency.

Conclusion: Dynamic, difficulty-aware orchestration enables fine-grained, query-specific reasoning strategies; the authors plan to release code and implementation details.

Abstract: Large Language Model (LLM)-based agentic systems have shown strong
capabilities across various tasks. However, existing multi-agent frameworks
often rely on static or task-level workflows, which either over-process simple
queries or underperform on complex ones, while also neglecting the
efficiency-performance trade-offs across heterogeneous LLMs. To address these
limitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a
dynamic framework that adapts workflow depth, operator selection, and LLM
assignment based on the difficulty of each input query. DAAO comprises three
interdependent modules: a variational autoencoder (VAE) for difficulty
estimation, a modular operator allocator, and a cost- and performance-aware LLM
router. By leveraging heterogeneous LLMs and dynamically tailoring workflows,
DAAO enables fine-grained, query-specific reasoning strategies. DAAO
outperforms prior multi-agent systems in both accuracy and inference efficiency
across six benchmarks. We will release our code and implementation details upon
publication.

</details>


### [218] [Neural cellular automata: applications to biology and beyond classical AI](https://arxiv.org/abs/2509.11131)
*Benedikt Hartl,Michael Levin,Léo Pio-Lopez*

Main category: cs.AI

TL;DR: Neural Cellular Automata are trainable, locally-interacting models that replicate biological self-organization, showing robust, generalizable dynamics and potential as a unifying framework for biology and AI.


<details>
  <summary>Details</summary>
Motivation: Motivated by the need to model self-organization across scales and to leverage NCAs for bioengineering and intelligent control, highlighting their robustness, generalization, and potential beyond biology.

Method: Systematic literature review of NCAs focusing on local ANN-based decision rules, differentiable/evolvable updates, and multiscale applications, with examples in morphogenesis, regeneration, aging, robotic morphologies, and reasoning tasks; comparison to diffusion-like state refinement.

Result: Key findings: NCAs exhibit robust, goal-directed, fully localized dynamics with generalization to perturbations and open-ended adaptability; they can coordinate complex system-level outcomes without centralized control; they offer a lean framework linking biology and generative AI.

Conclusion: NCAs offer a unifying computational paradigm bridging multiscale biology and modern AI, enabling bio-inspired collective intelligence and hierarchical reasoning; future work includes scaling, real-world bioengineering applications, and exploration in AI reasoning tasks.

Abstract: Neural Cellular Automata (NCA) represent a powerful framework for modeling
biological self-organization, extending classical rule-based systems with
trainable, differentiable (or evolvable) update rules that capture the adaptive
self-regulatory dynamics of living matter. By embedding Artificial Neural
Networks (ANNs) as local decision-making centers and interaction rules between
localized agents, NCA can simulate processes across molecular, cellular,
tissue, and system-level scales, offering a multiscale competency architecture
perspective on evolution, development, regeneration, aging, morphogenesis, and
robotic control. These models not only reproduce biologically inspired target
patterns but also generalize to novel conditions, demonstrating robustness to
perturbations and the capacity for open-ended adaptation and reasoning. Given
their immense success in recent developments, we here review current literature
of NCAs that are relevant primarily for biological or bioengineering
applications. Moreover, we emphasize that beyond biology, NCAs display robust
and generalizing goal-directed dynamics without centralized control, e.g., in
controlling or regenerating composite robotic morphologies or even on
cutting-edge reasoning tasks such as ARC-AGI-1. In addition, the same
principles of iterative state-refinement is reminiscent to modern generative
Artificial Intelligence (AI), such as probabilistic diffusion models. Their
governing self-regulatory behavior is constraint to fully localized
interactions, yet their collective behavior scales into coordinated
system-level outcomes. We thus argue that NCAs constitute a unifying
computationally lean paradigm that not only bridges fundamental insights from
multiscale biology with modern generative AI, but have the potential to design
truly bio-inspired collective intelligence capable of hierarchical reasoning
and control.

</details>


### [219] [AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment](https://arxiv.org/abs/2509.11135)
*Jing Xiao,Chang You,Zhiyu Chen*

Main category: cs.AI

TL;DR: AlignKT proposes a frontend-backend KT model that explicitly models a stable knowledge state by aligning preliminary state to an ideal state defined by pedagogical theories; uses five encoders and contrastive learning; outperforms seven baselines on three real-world datasets, achieving state-of-the-art on two and competitive on the third; code available.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability and instructional support in knowledge tracing by focusing on the knowledge state rather than only action sequences.

Method: Frontend-backend architecture; five encoders; alignment criterion based on an ideal knowledge state from pedagogical theories; contrastive learning to reinforce alignment; training aligns initial state to ideal state.

Result: Outperforms seven KT baselines on three real-world datasets; achieves state-of-the-art on two datasets and competitive on the third.

Conclusion: Explicit knowledge-state alignment with robust representational learning yields interpretable, effective KT with strong empirical results, and code release.

Abstract: Knowledge Tracing (KT) serves as a fundamental component of Intelligent
Tutoring Systems (ITS), enabling these systems to monitor and understand
learners' progress by modeling their knowledge state. However, many existing KT
models primarily focus on fitting the sequences of learners' interactions, and
often overlook the knowledge state itself. This limitation leads to reduced
interpretability and insufficient instructional support from the ITS. To
address this challenge, we propose AlignKT, which employs a frontend-to-backend
architecture to explicitly model a stable knowledge state. In this approach,
the preliminary knowledge state is aligned with an additional criterion.
Specifically, we define an ideal knowledge state based on pedagogical theories
as the alignment criterion, providing a foundation for interpretability. We
utilize five encoders to implement this set-up, and incorporate a contrastive
learning module to enhance the robustness of the alignment process. Through
extensive experiments, AlignKT demonstrates superior performance, outperforming
seven KT baselines on three real-world datasets. It achieves state-of-the-art
results on two of these datasets and exhibits competitive performance on the
third. The code of this work is available at
https://github.com/SCNU203/AlignKT.

</details>


### [220] [AI-Generated Content in Cross-Domain Applications: Research Trends, Challenges and Propositions](https://arxiv.org/abs/2509.11151)
*Jianxin Li,Liang Qu,Taotao Cai,Zhixue Zhao,Nur Al Hasan Haldar,Aneesh Krishna,Xiangjie Kong,Flavio Romero Macau,Tanmoy Chakraborty,Aniket Deroy,Binshan Lin,Karen Blackmore,Nasimul Noman,Jingxian Cheng,Ningning Cui,Jianliang Xu*

Main category: cs.AI

TL;DR: A cross-domain vision paper by 16 scholars analyzes AI-Generated Content (AIGC), covering training, detection, dissemination, societal impacts, and future challenges across domains, and proposes research directions.


<details>
  <summary>Details</summary>
Motivation: There is limited cross-domain analysis of AIGC's progress and challenges; need for a multidisciplinary synthesis to guide future research and governance.

Method: Literature and expert synthesis by 16 scholars from multiple disciplines; overview of training techniques, detection methods, spread on digital platforms; review of societal impacts and domain-specific methods; discussion of technical challenges and propositions.

Result: Provides a cross-domain perspective, identifies current trends, challenges, and a set of research propositions and directions.

Conclusion: Offers a comprehensive vision of AIGC's trajectory, highlighting trends, gaps, and future research directions for researchers and policymakers.

Abstract: Artificial Intelligence Generated Content (AIGC) has rapidly emerged with the
capability to generate different forms of content, including text, images,
videos, and other modalities, which can achieve a quality similar to content
created by humans. As a result, AIGC is now widely applied across various
domains such as digital marketing, education, and public health, and has shown
promising results by enhancing content creation efficiency and improving
information delivery. However, there are few studies that explore the latest
progress and emerging challenges of AIGC across different domains. To bridge
this gap, this paper brings together 16 scholars from multiple disciplines to
provide a cross-domain perspective on the trends and challenges of AIGC.
Specifically, the contributions of this paper are threefold: (1) It first
provides a broader overview of AIGC, spanning the training techniques of
Generative AI, detection methods, and both the spread and use of AI-generated
content across digital platforms. (2) It then introduces the societal impacts
of AIGC across diverse domains, along with a review of existing methods
employed in these contexts. (3) Finally, it discusses the key technical
challenges and presents research propositions to guide future work. Through
these contributions, this vision paper seeks to offer readers a cross-domain
perspective on AIGC, providing insights into its current research trends,
ongoing challenges, and future directions.

</details>


### [221] [VideoAgent: Personalized Synthesis of Scientific Videos](https://arxiv.org/abs/2509.11253)
*Xiao Liang,Bangxin Li,Zixuan Chen,Hanyue Zheng,Zhi Ma,Di Wang,Cong Tian,Quan Wang*

Main category: cs.AI

TL;DR: Automates personalized scientific video generation with VideoAgent (a multi-agent system) and evaluates it with SciVidEval, outperforming existing services and nearing human-level quality.


<details>
  <summary>Details</summary>
Motivation: There is a need for dynamic, personalized, multimodal scientific videos beyond static posters/slides; current document automation lacks personalization, synchronization, and multimodal orchestration.

Method: VideoAgent parses a source paper into a fine-grained asset library and, guided by user requirements, orchestrates a narrative flow that synthesizes static slides and dynamic animations via a conversational interface. SciVidEval provides automated multimodal quality/synchronization metrics plus a video-quiz human evaluation.

Result: Extensive experiments show significant improvements over existing commercial scientific video services and approach to human-level quality in scientific communication.

Conclusion: The work introduces a novel multi-agent framework and an evaluation suite that enables automated, personalized, synchronized scientific video generation, with strong empirical validation.

Abstract: Automating the generation of scientific videos is a crucial yet challenging
task for effective knowledge dissemination. However, existing works on document
automation primarily focus on static media such as posters and slides, lacking
mechanisms for personalized dynamic orchestration and multimodal content
synchronization. To address these challenges, we introduce VideoAgent, a novel
multi-agent framework that synthesizes personalized scientific videos through a
conversational interface. VideoAgent parses a source paper into a fine-grained
asset library and, guided by user requirements, orchestrates a narrative flow
that synthesizes both static slides and dynamic animations to explain complex
concepts. To enable rigorous evaluation, we also propose SciVidEval, the first
comprehensive suite for this task, which combines automated metrics for
multimodal content quality and synchronization with a Video-Quiz-based human
evaluation to measure knowledge transfer. Extensive experiments demonstrate
that our method significantly outperforms existing commercial scientific video
generation services and approaches human-level quality in scientific
communication.

</details>


### [222] [Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble](https://arxiv.org/abs/2509.11311)
*Bingchen Wang,Zi-Yu Khoo,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: A two-stage alignment framework (P2P) uses LLM agents as survey respondent proxies: first build diverse endowment personas, then select a representative subset to match observed data. This yields faithful aggregate patterns and diversity without demographic conditioning.


<details>
  <summary>Details</summary>
Motivation: Address rising survey deployment costs and demographic imbalances in response data by using cost-effective, steerable LLM-based respondent proxies. Grounded in revealed preference theory, it aims for demographic-agnostic, aggregate-driven alignment that generalizes across populations.

Method: 2-stage approach: (1) construct endowments — create diverse agent personas that simulate plausible respondent profiles via structured prompts; (2) select a representative subset — use entropy-based sampling and regression-based selection to approximate the ground-truth population based on observed data; implement within a system (P2P) that steers LLMs toward representative behaviors without relying on demographic conditioning.

Result: On real-world opinion survey datasets, the aligned agent populations reproduce aggregate response patterns with high fidelity and show substantial response diversity, despite not conditioning on demographics.

Conclusion: The framework demonstrates feasibility of demographic-agnostic, pluralistic alignment for social surveys, offering improved data efficiency and a flexible testbed for studying alignment of LLM agents as survey respondents; invites further work on robustness, ethics, and broader validation.

Abstract: Large language models (LLMs) have demonstrated promise in emulating
human-like responses across a wide range of tasks. In this paper, we propose a
novel alignment framework that treats LLMs as agent proxies for human survey
respondents, affording a cost-effective and steerable solution to two pressing
challenges in the social sciences: the rising cost of survey deployment and the
growing demographic imbalance in survey response data. Drawing inspiration from
the theory of revealed preference, we formulate alignment as a two-stage
problem: constructing diverse agent personas called endowments that simulate
plausible respondent profiles, and selecting a representative subset to
approximate a ground-truth population based on observed data. To implement the
paradigm, we introduce P2P, a system that steers LLM agents toward
representative behavioral patterns using structured prompt engineering,
entropy-based sampling, and regression-based selection. Unlike
personalization-heavy approaches, our alignment approach is
demographic-agnostic and relies only on aggregate survey results, offering
better generalizability and parsimony. Beyond improving data efficiency in
social science research, our framework offers a testbed for studying the
operationalization of pluralistic alignment. We demonstrate the efficacy of our
approach on real-world opinion survey datasets, showing that our aligned agent
populations can reproduce aggregate response patterns with high fidelity and
exhibit substantial response diversity, even without demographic conditioning.

</details>


### [223] [Decoding Plastic Toxicity: An Intelligent Framework for Conflict-Aware Relational Metapath Extraction from Scientific Abstracts](https://arxiv.org/abs/2509.11330)
*Sudeshna Jana,Manjira Sinha,Tirthankar Dasgupta*

Main category: cs.AI

TL;DR: A framework using large language models to extract relational metapaths from abstracts, building a Toxicity Trajectory Graph that links plastics-related pollutants to health outcomes, with a dynamic evidence reconciliation module to handle conflicting findings.


<details>
  <summary>Details</summary>
Motivation: Plastics persist in the environment and micro-/nano-plastics are widespread in air, water, and soil, posing health risks. There's a need to map complex, multi-hop cause–effect relationships from noisy scientific text and scale this mining to domain-specific corpora.

Method: Use large language models to extract relational metapaths (multi-hop semantic chains) connecting pollutant sources to health impacts from scientific abstracts, connect entities across contexts, and aggregate these into a Toxicity Trajectory Graph that traces pollutant propagation through exposure routes and biological systems. Incorporate a dynamic evidence reconciliation module to resolve semantic conflicts from evolving or conflicting findings.

Result: Demonstrates strong performance in extracting reliable, high-utility relational knowledge from noisy scientific text and offers a scalable solution for mining complex cause–effect structures in domain-specific corpora.

Conclusion: The framework enables scalable extraction and integration of complex toxicity relationships, with built-in evidence reconciliation to maintain consistency across evolving literature, facilitating robust mapping of pollutant exposure to health outcomes.

Abstract: The widespread use of plastics and their persistence in the environment have
led to the accumulation of micro- and nano-plastics across air, water, and
soil, posing serious health risks including respiratory, gastrointestinal, and
neurological disorders. We propose a novel framework that leverages large
language models to extract relational metapaths, multi-hop semantic chains
linking pollutant sources to health impacts, from scientific abstracts. Our
system identifies and connects entities across diverse contexts to construct
structured relational metapaths, which are aggregated into a Toxicity
Trajectory Graph that traces pollutant propagation through exposure routes and
biological systems. Moreover, to ensure consistency and reliability, we
incorporate a dynamic evidence reconciliation module that resolves semantic
conflicts arising from evolving or contradictory research findings. Our
approach demonstrates strong performance in extracting reliable, high-utility
relational knowledge from noisy scientific text and offers a scalable solution
for mining complex cause-effect structures in domain-specific corpora.

</details>


### [224] [The power of dynamic causality in observer-based design for soft sensor applications](https://arxiv.org/abs/2509.11336)
*William Farlessyost,Sebastian Oberst,Shweta Singh*

Main category: cs.AI

TL;DR: A dynamic causality-aware, LTC-network-based observer pruning framework for sensor selection that iteratively prunes inputs via controlled perturbation analysis, validated on a harmonic oscillator, a CSTR, and a seasonally forced Lotka-Volterra model; yields minimal sensor sets with improved accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Conventional sensor selection relies on linear observability or statistical correlations that miss temporal dynamics and causal influence; there is a need to identify inputs that causally affect state estimation in dynamic systems.

Method: Train a liquid-time-constant (LTC) network observer using candidate inputs. For each input, measure causal impact via controlled perturbation (e.g., perturb input and observe change in state estimation error). Prune inputs with negligible causal influence. Retrain on remaining inputs and repeat until performance degradations occur. Validate on three mechanistic models: a forced harmonic oscillator, a nonlinear CSTR, and a seasonal-Lotka-Volterra predator-prey system.

Result: Causality-guided pruning identifies minimal sensor subsets that align with underlying physics and improve prediction accuracy. The method differentiates essential physical measurements from noise and clarifies when derived interaction terms provide complementary vs redundant information. Computational efficiency and interpretability improve for soft sensing across domains.

Conclusion: Dynamic causal analysis with LTC observers offers a physics-aligned, interpretable, and efficient approach to sensor selection, valuable for process engineering, ecological monitoring, and agriculture.

Abstract: This paper introduces a novel framework for optimizing observer-based soft
sensors through dynamic causality analysis. Traditional approaches to sensor
selection often rely on linearized observability indices or statistical
correlations that fail to capture the temporal evolution of complex systems. We
address this gap by leveraging liquid-time constant (LTC) networks,
continuous-time neural architectures with input-dependent time constants, to
systematically identify and prune sensor inputs with minimal causal influence
on state estimation. Our methodology implements an iterative workflow: training
an LTC observer on candidate inputs, quantifying each input's causal impact
through controlled perturbation analysis, removing inputs with negligible
effect, and retraining until performance degradation occurs. We demonstrate
this approach on three mechanistic testbeds representing distinct physical
domains: a harmonically forced spring-mass-damper system, a nonlinear
continuous stirred-tank reactor, and a predator-prey model following the
structure of the Lotka-Volterra model, but with seasonal forcing and added
complexity. Results show that our causality-guided pruning consistently
identifies minimal sensor sets that align with underlying physics while
improving prediction accuracy. The framework automatically distinguishes
essential physical measurements from noise and determines when derived
interaction terms provide complementary versus redundant information. Beyond
computational efficiency, this approach enhances interpretability by grounding
sensor selection decisions in dynamic causal relationships rather than static
correlations, offering significant benefits for soft sensing applications
across process engineering, ecological monitoring, and agricultural domains.

</details>


### [225] [MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization](https://arxiv.org/abs/2509.11361)
*Yichen Han,Bojun Liu,Zhengpeng zhou,Guanyu Liu,Zeng Zhang,Yang Yang,Wenli Wang,Isaac N Shi,Yunyan,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: MAPGD introduces a multi-agent, gradient-based prompt optimization framework for LLMs. It uses specialized agents (task clarity, example selection, format design, stylistic refinement), semantic gradient coordination, and bandit-based candidate selection to improve accuracy and efficiency with convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Single-agent prompt engineering often suffers from narrow perspectives, gradient conflicts, and high computational cost. A multi-agent, gradient-driven approach can diversify viewpoints, coordinate signals, and reduce expenses while enabling robust, interpretable prompt optimization.

Method: MAPGD deploys multiple specialized agents (task clarity, example selection, format design, stylistic refinement) that collaboratively optimize prompts via gradient-based updates. It includes semantic gradient coordination to resolve conflicts, bandit-based candidate selection for efficient exploration-exploitation, and theoretical convergence guarantees.

Result: Experiments across classification, generation, and reasoning tasks show MAPGD surpasses single-agent and random baselines in accuracy and efficiency. Ablations confirm the value of gradient fusion, agent specialization, and conflict resolution.

Conclusion: A unified, gradient-inspired multi-agent framework offers robust, interpretable prompt optimization with improved performance and efficiency over traditional single-agent methods.

Abstract: Prompt engineering is crucial for leveraging large language models (LLMs),
but existing methods often rely on a single optimization trajectory, limiting
adaptability and efficiency while suffering from narrow perspectives, gradient
conflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt
Gradient Descent), a framework integrating multi-agent collaboration with
gradient-based optimization. MAPGD features specialized agents for task
clarity, example selection, format design, and stylistic refinement; semantic
gradient coordination to resolve conflicts; bandit-based candidate selection
for efficient exploration-exploitation; and theoretical convergence guarantees.
Experiments on classification, generation, and reasoning tasks show MAPGD
outperforms single-agent and random baselines in accuracy and efficiency.
Ablations confirm the benefits of gradient fusion, agent specialization, and
conflict resolution, providing a unified, gradient-inspired multi-agent
approach to robust and interpretable prompt optimization.

</details>


### [226] [Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications](https://arxiv.org/abs/2509.11431)
*Aadil Gani Ganie*

Main category: cs.AI

TL;DR: Proposes an RBAC-based security framework for AI agents to mitigate prompt injection threats and enable secure, scalable on-premises deployment.


<details>
  <summary>Details</summary>
Motivation: LLMs are bounded by static training data and often require fine-tuning for tasks; AI agents can access real-time tools and data, which enhances capability in industry but introduces security risks like prompt injection. A robust security guardrail is needed for safe, scalable deployment, especially on-premises.

Method: Develops a framework that integrates Role-Based Access Control into AI agents to act as security guardrails, with emphasis on on-premises deployment.

Result: Introduces an RBAC-based framework intended to bolster security and scalability of AI agents in industrial settings; no empirical results reported in the abstract.

Conclusion: RBAC integration is a viable approach to enhance the security and reliability of AI agents in industrial applications and can support safer, scalable deployment.

Abstract: The emergence of Large Language Models (LLMs) has significantly advanced
solutions across various domains, from political science to software
development. However, these models are constrained by their training data,
which is static and limited to information available up to a specific date.
Additionally, their generalized nature often necessitates fine-tuning --
whether for classification or instructional purposes -- to effectively perform
specific downstream tasks. AI agents, leveraging LLMs as their core, mitigate
some of these limitations by accessing external tools and real-time data,
enabling applications such as live weather reporting and data analysis. In
industrial settings, AI agents are transforming operations by enhancing
decision-making, predictive maintenance, and process optimization. For example,
in manufacturing, AI agents enable near-autonomous systems that boost
productivity and support real-time decision-making. Despite these advancements,
AI agents remain vulnerable to security threats, including prompt injection
attacks, which pose significant risks to their integrity and reliability. To
address these challenges, this paper proposes a framework for integrating
Role-Based Access Control (RBAC) into AI agents, providing a robust security
guardrail. This framework aims to support the effective and scalable deployment
of AI agents, with a focus on on-premises implementations.

</details>


### [227] [Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction](https://arxiv.org/abs/2509.11459)
*Chen Jiang,Kofi Osei,Sai Deepthi Yeddula,Dongji Feng,Wei-Shinn Ku*

Main category: cs.AI

TL;DR: Adaptive MoE for multi-source precipitation prediction with a dynamic router and visualization; outperforms baselines on Hurricane Ian data.


<details>
  <summary>Details</summary>
Motivation: Integrating heterogeneous, multi-resolution climate data (radar, satellite, surface) into forecasting is challenging for conventional DL; requires modular, interpretable models to capture modality-specific patterns.

Method: Proposes an Adaptive Mixture of Experts where each expert specializes in a modality or spatio-temporal pattern; uses a dynamic router to assign inputs to experts; includes an interactive web visualization tool; evaluation on a curated multimodal dataset from Hurricane Ian 2022.

Result: Modular design enhances predictive accuracy and interpretability; Adaptive MoE significantly outperforms baselines in benchmark tests.

Conclusion: Demonstrates effectiveness of modular, modality-aware forecasting architecture and visualization for climate-sensitive decision making; potential for broader adoption in multi-source weather prediction.

Abstract: Accurate precipitation forecasting is indispensable in agriculture, disaster
management, and sustainable strategies. However, predicting rainfall has been
challenging due to the complexity of climate systems and the heterogeneous
nature of multi-source observational data, including radar, satellite imagery,
and surface-level measurements. The multi-source data vary in spatial and
temporal resolution, and they carry domain-specific features, making it
challenging for effective integration in conventional deep learning models.
Previous research has explored various machine learning techniques for weather
prediction; however, most struggle with the integration of data with
heterogeneous modalities. To address these limitations, we propose an Adaptive
Mixture of Experts (MoE) model tailored for precipitation rate prediction. Each
expert within the model specializes in a specific modality or spatio-temporal
pattern. We also incorporated a dynamic router that learns to assign inputs to
the most relevant experts. Our results show that this modular design enhances
predictive accuracy and interpretability. In addition to the modeling
framework, we introduced an interactive web-based visualization tool that
enables users to intuitively explore historical weather patterns over time and
space. The tool was designed to support decision-making for stakeholders in
climate-sensitive sectors. We evaluated our approach using a curated multimodal
climate dataset capturing real-world conditions during Hurricane Ian in 2022.
The benchmark results show that the Adaptive MoE significantly outperformed all
the baselines.

</details>


### [228] [Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs](https://arxiv.org/abs/2509.11480)
*Amir Taherin,Juyi Lin,Arash Akbari,Arman Akbari,Pu Zhao,Weiwei Chen,David Kaeli,Yanzhi Wang*

Main category: cs.AI

TL;DR: Five Vision-Language-Action models are evaluated across edge and datacenter GPUs using LIBERO, showing how architecture, power limits, and hardware affect throughput, memory, and accuracy, with deployment guidance.


<details>
  <summary>Details</summary>
Motivation: To understand scaling and power-performance tradeoffs of VLA models across architectures and hardware, enabling informed deployment decisions under diverse resource constraints.

Method: Evaluate five representative VLA models (state-of-the-art baselines and two new architectures) on the LIBERO benchmark. Measure accuracy, latency, throughput, and peak memory under varying edge power budgets and high-performance datacenter GPU configurations.

Result: Architectural choices (e.g., action tokenization, backbone size) strongly influence throughput and memory footprint. Power-constrained edge devices exhibit non-linear performance degradation; some edge configurations match or exceed older datacenter GPUs. High-throughput variants can achieve large throughput without substantial accuracy loss.

Conclusion: Provides actionable insights for selecting and optimizing VLAs across deployment constraints and challenges the notion that datacenter hardware is universally superior for robotic inference.

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist
policies for robotic control, yet their performance scaling across model
architectures and hardware platforms, as well as their associated power
budgets, remain poorly understood. This work presents an evaluation of five
representative VLA models -- spanning state-of-the-art baselines and two newly
proposed architectures -- targeting edge and datacenter GPU platforms. Using
the LIBERO benchmark, we measure accuracy alongside system-level metrics,
including latency, throughput, and peak memory usage, under varying edge power
constraints and high-performance datacenter GPU configurations. Our results
identify distinct scaling trends: (1) architectural choices, such as action
tokenization and model backbone size, strongly influence throughput and memory
footprint; (2) power-constrained edge devices exhibit non-linear performance
degradation, with some configurations matching or exceeding older datacenter
GPUs; and (3) high-throughput variants can be achieved without significant
accuracy loss. These findings provide actionable insights when selecting and
optimizing VLAs across a range of deployment constraints. Our work challenges
current assumptions about the superiority of datacenter hardware for robotic
inference.

</details>


### [229] [MedicalOS: An LLM Agent based Operating System for Digital Healthcare](https://arxiv.org/abs/2509.11507)
*Jared Zhu,Junde Wu*

Main category: cs.AI

TL;DR: MedicalOS introduces a domain-specific, agent-based abstraction layer that translates natural language into predefined digital health commands, validated on 214 patient cases across 22 specialties, achieving high diagnostic accuracy, safe exam requests, and structured reporting.


<details>
  <summary>Details</summary>
Motivation: Clinicians face heavy workflows with multiple tools; while LLM-powered agents show promise, healthcare needs domain-specific, guideline-compliant abstractions to ensure safety, transparency, and compliance in workflow automation.

Method: Authors present MedicalOS as a unified agent-based operating system comprising pre-defined digital healthcare commands (e.g., patient inquiry, history retrieval, exam management, report generation, referrals, treatment planning) wrapped as tools via machine languages (Python, APIs, MCP, Linux). They translate natural language instructions into these commands and validate the system on 214 patient cases across 22 specialties.

Result: MedicalOS demonstrates high diagnostic accuracy and confidence, clinically sound examination requests, and consistent generation of structured reports and medication recommendations.

Conclusion: MedicalOS provides a trustworthy and scalable foundation for advancing workflow automation in clinical practice.

Abstract: Decades' advances in digital health technologies, such as electronic health
records, have largely streamlined routine clinical processes. Yet, most these
systems are still hard to learn and use: Clinicians often face the burden of
managing multiple tools, repeating manual actions for each patient, navigating
complicated UI trees to locate functions, and spending significant time on
administration instead of caring for patients. The recent rise of large
language model (LLM) based agents demonstrates exceptional capability in coding
and computer operation, revealing the potential for humans to interact with
operating systems and software not by direct manipulation, but by instructing
agents through natural language. This shift highlights the need for an
abstraction layer, an agent-computer interface, that translates human language
into machine-executable commands. In digital healthcare, however, requires a
more domain-specific abstractions that strictly follow trusted clinical
guidelines and procedural standards to ensure safety, transparency, and
compliance. To address this need, we present \textbf{MedicalOS}, a unified
agent-based operational system designed as such a domain-specific abstract
layer for healthcare. It translates human instructions into pre-defined digital
healthcare commands, such as patient inquiry, history retrieval, exam
management, report generation, referrals, treatment planning, that we wrapped
as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,
Linux). We empirically validate MedicalOS on 214 patient cases across 22
specialties, demonstrating high diagnostic accuracy and confidence, clinically
sound examination requests, and consistent generation of structured reports and
medication recommendations. These results highlight MedicalOS as a trustworthy
and scalable foundation for advancing workflow automation in clinical practice.

</details>


### [230] [Task Decoding based on Eye Movements using Synthetic Data Augmentation](https://arxiv.org/abs/2509.11547)
*Shanmuka Sadhu,Arca Baran,Preeti Pandey,Ayush Kumar*

Main category: cs.AI

TL;DR: Synthetic data augmentation via CTGAN and variants dramatically boosts task-decoding accuracy on eye-tracking data (28.1% with RF to 82% with Inception Time) and reportedly outperforms prior studies on the dataset.


<details>
  <summary>Details</summary>
Motivation: Test whether synthetically generated eye-movement samples can enhance decoding of observers' task (Yarbus hypothesis) by augmenting real data.

Method: Generate synthetic samples using CTGAN, CopulaGAN, Gretel AI; combine with 320 real samples; vary augmentation up to 5x; train multiple algorithms (Random Forest, Inception Time, etc.); evaluate task-category decoding.

Result: Accuracy improved from 28.1% (Random Forest) to 82% (Inception Time) with 5x augmentation; framework claims superior performance due to synthetic data.

Conclusion: Synthetic data augmentation can substantially improve decoding performance in eye-tracking tasks; supports Yarbus' claim; framework appears effective on the dataset; generalization to broader contexts requires further validation.

Abstract: Machine learning has been extensively used in various applications related to
eye-tracking research. Understanding eye movement is one of the most
significant subsets of eye-tracking research that reveals the scanning pattern
of an individual. Researchers have thoroughly analyzed eye movement data to
understand various eye-tracking applications, such as attention mechanisms,
navigational behavior, task understanding, etc. The outcome of traditional
machine learning algorithms used for decoding tasks based on eye movement data
has received a mixed reaction to Yarbus' claim that it is possible to decode
the observer's task from their eye movements. In this paper, to support the
hypothesis by Yarbus, we are decoding tasks categories while generating
synthetic data samples using well-known Synthetic Data Generators CTGAN and its
variations such as CopulaGAN and Gretel AI Synthetic Data generators on
available data from an in-person user study. Our results show that augmenting
more eye movement data combined with additional synthetically generated
improves classification accuracy even with traditional machine learning
algorithms. We see a significant improvement in task decoding accuracy from
28.1% using Random Forest to 82% using Inception Time when five times more data
is added in addition to the 320 real eye movement dataset sample. Our proposed
framework outperforms all the available studies on this dataset because of the
use of additional synthetic datasets. We validated our claim with various
algorithms and combinations of real and synthetic data to show how decoding
accuracy increases with the increase in the augmentation of generated data to
real data.

</details>


### [231] [Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain](https://arxiv.org/abs/2509.11572)
*Tuan Bui,An Nguyen,Phat Thai,Minh Hua,Ngan Pham L. N.,Ngan Pham T. B.,Dung Le,Long Nguyen,Thanh-Tung Tran,Thang Bui,Tho Quan*

Main category: cs.AI

TL;DR: Neuro-symbolic framework MCFR integrates LLMs with model checking to perform verifiable, dynamic reasoning; introduces EduMC-QA benchmark; shows improved faithfulness and interpretability over large language models.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce unfaithful reasoning traces, which is risky for closed-domain, high-stakes QA; existing symbolic methods handle static logic but struggle with dynamic state changes and multi-step, conditional reasoning.

Method: MCFR translates natural language procedures into formal specifications and uses model checking over transition models to verify properties; evaluates on EduMC-QA, a benchmark grounded in real academic procedures; includes comparative analyses against ChatGPT, DeepSeek, and Claude.

Result: MCFR improves reasoning faithfulness and interpretability in verifiable QA; demonstrates viability of verifiable reasoning in high-stakes domains; provides empirical context by comparing with leading LLMs.

Conclusion: Neuro-symbolic integration of LLMs with formal model checking offers a viable path to verifiable, state-aware reasoning in closed-domain applications; EduMC-QA provides a relevant benchmark for future evaluation.

Abstract: Reasoning is essential for closed-domain QA systems in which procedural
correctness and policy compliance are critical. While large language models
(LLMs) have shown strong performance on many reasoning tasks, recent work
reveals that their reasoning traces are often unfaithful - serving more as
plausible justifications than as causally grounded derivations. Efforts to
combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved
reliability but remain limited to static forms of logic, struggling with
dynamic, state-based reasoning such as multi-step progressions and conditional
transitions.
  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a
neuro-symbolic framework that integrates LLMs with model checking to support
property verification. MCFR translates natural language into formal
specifications and verifies them over transition models. To support evaluation,
we introduce EduMC-QA, a benchmark dataset grounded in real academic
procedures. Our results show that MCFR improves reasoning faithfulness and
interpretability, offering a viable path toward verifiable QA in high-stakes
closed-domain applications. In addition to evaluating MCFR, we compare its
performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to
contextualize its effectiveness.

</details>


### [232] [A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models](https://arxiv.org/abs/2509.11575)
*Ching Chang,Yidan Shi,Defu Cao,Wei Yang,Jeehyun Hwang,Haixin Wang,Jiacheng Pang,Wei Wang,Yan Liu,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.AI

TL;DR: A taxonomy and synthesis of time-series reasoning approaches organized by topology (direct, linear chain, branch) intersected with objectives (analysis, understanding, causal inference, generation) and supported by benchmarks, datasets, and guidelines toward reliable, explainable AI with traceable evidence.


<details>
  <summary>Details</summary>
Motivation: To address the shift from single-step accuracy to reliable, explainable, and actionable time-series reasoning; to organize diverse literature and provide design guidance for uncertainty, grounding, shift/streaming, and cost-aware deployment.

Method: Literature survey organizing work by reasoning topology and objectives; use of a compact tag set; synthesis across domains; cataloging datasets, benchmarks, resources; analysis of where methods work or fail; extraction of practical guidance; proposal of future directions including benchmarks tying quality to utility and closed-loop testbeds.

Result: A structured overview of three topologies crossing main objectives, with insights into faithfulness and robustness, a resource list (datasets, benchmarks), evaluation practices, and design guidance; identification of future progress areas and call for benchmarks and testbeds; framing a shift toward reliability and explainability.

Conclusion: Time-series reasoning should balance grounding, self-correction, and computational cost to enable systems that understand, explain, and act with traceable evidence; progress will rely on more robust benchmarks and closed-loop testing in shifting, streaming, long-horizon contexts.

Abstract: Time series reasoning treats time as a first-class axis and incorporates
intermediate evidence directly into the answer. This survey defines the problem
and organizes the literature by reasoning topology with three families: direct
reasoning in one step, linear chain reasoning with explicit intermediates, and
branch-structured reasoning that explores, revises, and aggregates. The
topology is crossed with the main objectives of the field, including
traditional time series analysis, explanation and understanding, causal
inference and decision making, and time series generation, while a compact tag
set spans these axes and captures decomposition and verification, ensembling,
tool use, knowledge access, multimodality, agent loops, and LLM alignment
regimes. Methods and systems are reviewed across domains, showing what each
topology enables and where it breaks down in faithfulness or robustness, along
with curated datasets, benchmarks, and resources that support study and
deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).
Evaluation practices that keep evidence visible and temporally aligned are
highlighted, and guidance is distilled on matching topology to uncertainty,
grounding with observable artifacts, planning for shift and streaming, and
treating cost and latency as design budgets. We emphasize that reasoning
structures must balance capacity for grounding and self-correction against
computational cost and reproducibility, while future progress will likely
depend on benchmarks that tie reasoning quality to utility and on closed-loop
testbeds that trade off cost and risk under shift-aware, streaming, and
long-horizon settings. Taken together, these directions mark a shift from
narrow accuracy toward reliability at scale, enabling systems that not only
analyze but also understand, explain, and act on dynamic worlds with traceable
evidence and credible outcomes.

</details>


### [233] [AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions](https://arxiv.org/abs/2509.11595)
*Sabin Huda,Ernest Foo,Zahra Jadidi,MA Hakim Newton,Abdul Sattar*

Main category: cs.AI

TL;DR: AMLNet introduces a regulation-aware synthetic AML dataset and an ensemble detector; achieves high F1 (0.90) and 75% AUSTRAC alignment; publicly releases dataset for reproducible AML experimentation.


<details>
  <summary>Details</summary>
Motivation: Address the lack of publicly shareable, regulation-aligned AML datasets to enable reproducible research and regulatory-compliant evaluation.

Method: A two-unit knowledge-based multi-agent framework: a regulation-aware generator producing synthetic transactions (over 1 million, ~0.16% laundering-positive) with core phases (placement, layering, integration) and typologies (e.g., structuring, adaptive threshold behavior); an ensemble detection pipeline; regulatory alignment measured by AUSTRAC rule coverage (75%); composite technical fidelity score of 0.75 for temporal, structural, and behavioral realism; evaluation on internal AMLNet partitions and external SynthAML; dataset Version 1.0 released.

Result: Internal F1 0.90 (precision 0.84, recall 0.97); adaptation to external SynthAML dataset; 1.09 million synthetic transactions; laundering-positive rate ~0.16%; AUSTRAC coverage 75%; fidelity score 0.75; generalizability across generation paradigms; dataset release.

Conclusion: Demonstrates regulatory-conscious AML experimentation via multi-dimensional evaluation; provides a public, reproducible benchmark dataset; indicates architectural generalizability and usefulness for advancing AML research under regulatory constraints.

Abstract: Anti-money laundering (AML) research is constrained by the lack of publicly
shareable, regulation-aligned transaction datasets. We present AMLNet, a
knowledge-based multi-agent framework with two coordinated units: a
regulation-aware transaction generator and an ensemble detection pipeline. The
generator produces 1,090,173 synthetic transactions (approximately 0.16\%
laundering-positive) spanning core laundering phases (placement, layering,
integration) and advanced typologies (e.g., structuring, adaptive threshold
behavior). Regulatory alignment reaches 75\% based on AUSTRAC rule coverage
(Section 4.2), while a composite technical fidelity score of 0.75 summarizes
temporal, structural, and behavioral realism components (Section 4.4). The
detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the
internal test partitions of AMLNet and adapts to the external SynthAML dataset,
indicating architectural generalizability across different synthetic generation
paradigms. We provide multi-dimensional evaluation (regulatory, temporal,
network, behavioral) and release the dataset (Version 1.0,
https://doi.org/10.5281/zenodo.16736515), to advance reproducible and
regulation-conscious AML experimentation.

</details>


### [234] [Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework](https://arxiv.org/abs/2509.11645)
*Zhaolong Wu,Pu Luo,Jason Pui Yin Cheung,Teng Zhang*

Main category: cs.AI

TL;DR: Benchmarks five multimodal LLMs for adolescent idiopathic scoliosis (AIS) self-management using ~3,000 AP X-rays across a Divide and Conquer evaluation (VQA, domain knowledge, patient education). Finds major gaps in image interpretation and AIS knowledge, with keypoint prompting and retrieval-augmented generation (RAG) helping, but overall models struggle with deformity localization (best accuracy ~0.55) and direction/orientation (best ~0.13).


<details>
  <summary>Details</summary>
Motivation: To assess how well current Multimodal LLMs can support AIS self-management across imaging interpretation, medical knowledge, and patient education, and to identify concrete bottlenecks that limit clinical usefulness.

Method: Assemble a ~3,000 AP radiographs database with diagnostic texts. Evaluate five MLLMs using a Divide and Conquer framework comprising (1) visual question answering on radiographs, (2) a domain knowledge assessment task, and (3) a patient education counseling assessment. Introduce spinal keypoint prompting to improve image understanding and construct an AIS knowledge base for retrieval augmented generation (RAG) to bolster knowledge tasks.

Result: MLLMs exhibit limited ability to interpret complex spinal radiographs and AIS care knowledge. Visual prompting effects vary across architectures; RAG substantially improves knowledge task performance. The models perform poorly on localization of deformity (best ~0.55 accuracy) and orientation/direction (best ~0.13).

Conclusion: Current MLLMs are far from capable for personalized AIS care. The main challenges are accurate detection of deformity locations and directions. Spinal keypoint prompting and RAG-based AIS knowledge bases offer partial improvements, but substantial gaps remain for reliable clinical support.

Abstract: This study presents the first comprehensive evaluation of Multimodal Large
Language Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS)
self-management. We constructed a database of approximately 3,000
anteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a
`Divide and Conquer' framework consisting of a visual question-answering task,
a domain knowledge assessment task, and a patient education counseling
assessment task. Our investigation revealed limitations of MLLMs' ability in
interpreting complex spinal radiographs and comprehending AIS care knowledge.
To address these, we pioneered enhancing MLLMs with spinal keypoint prompting
and compiled an AIS knowledge base for retrieval augmented generation (RAG),
respectively. Results showed varying effectiveness of visual prompting across
different architectures, while RAG substantially improved models' performances
on the knowledge assessment task. Our findings indicate current MLLMs are far
from capable in realizing personalized assistant in AIS care. The greatest
challenge lies in their abilities to obtain accurate detections of spinal
deformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).

</details>


### [235] [HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction](https://arxiv.org/abs/2509.11719)
*Bingqing Wei,Lianmin Chen,Zhongyu Xia,Yongtao Wang*

Main category: cs.AI

TL;DR: HeLoFusion introduces a locality-based, multi-scale, heterogeneous graph encoder for multi-agent motion forecasting, achieving state-of-the-art results on Waymo Open Motion.


<details>
  <summary>Details</summary>
Motivation: Global context-based models often fail to capture the full spectrum of social dynamics, including multi-scale interactions and heterogeneity among agents, while remaining computationally expensive. A scalable, local representation could better model both pairwise and group-level interactions.

Method: Build local, agent-centered multi-scale graphs; use aggregation-decomposition message passing to handle information flow; employ type-specific feature networks to capture heterogeneity across agent types; focus on locality to model both direct and group interactions.

Result: Achieves state-of-the-art performance on the Waymo Open Motion Dataset, surpassing prior methods on key metrics such as Soft mAP and minADE, indicating strong modeling of multi-scale, heterogeneous social dynamics.

Conclusion: A locality-grounded, multi-scale, heterogeneous interaction model is an effective and scalable approach for motion forecasting in autonomous driving.

Abstract: Multi-agent trajectory prediction in autonomous driving requires a
comprehensive understanding of complex social dynamics. Existing methods,
however, often struggle to capture the full richness of these dynamics,
particularly the co-existence of multi-scale interactions and the diverse
behaviors of heterogeneous agents. To address these challenges, this paper
introduces HeLoFusion, an efficient and scalable encoder for modeling
heterogeneous and multi-scale agent interactions. Instead of relying on global
context, HeLoFusion constructs local, multi-scale graphs centered on each
agent, allowing it to effectively model both direct pairwise dependencies and
complex group-wise interactions (\textit{e.g.}, platooning vehicles or
pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of
agent heterogeneity through an aggregation-decomposition message-passing scheme
and type-specific feature networks, enabling it to learn nuanced,
type-dependent interaction patterns. This locality-focused approach enables a
principled representation of multi-level social context, yielding powerful and
expressive agent embeddings. On the challenging Waymo Open Motion Dataset,
HeLoFusion achieves state-of-the-art performance, setting new benchmarks for
key metrics including Soft mAP and minADE. Our work demonstrates that a
locality-grounded architecture, which explicitly models multi-scale and
heterogeneous interactions, is a highly effective strategy for advancing motion
forecasting.

</details>


### [236] [Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning](https://arxiv.org/abs/2509.11880)
*Carlos Celemin,Joseph Brennan,Pierluigi Vito Amadori,Tim Bradley*

Main category: cs.AI

TL;DR: A novel application of Supervised Contrastive Learning to Imitation Learning in video games, extending SupCon to continuous action spaces and integrating with standard IL losses to produce better state representations, faster learning, and improved generalization.


<details>
  <summary>Details</summary>
Motivation: To learn latent state representations that better capture action-relevant factors and the causal link between observations and demonstrated actions, addressing limitations of action-prediction IL losses in representing continuous actions and generalizing across environments.

Method: Extend Supervised Contrastive Learning to continuous action spaces and integrate it with the IL objective (supervised action prediction loss). Train on a mix of 3D games (Astro Bot, Returnal) and 2D Atari games to learn action-relevant representations that map observations to actions.

Result: The approach yielded improved representation quality, faster learning convergence, and better generalization compared with baselines trained only with supervised action prediction losses.

Conclusion: SupCon-based representation learning in imitation learning benefits video-game agents by producing richer, action-relevant latent representations, enabling faster learning and stronger generalization, and is adaptable to environments with continuous action spaces.

Abstract: This paper introduces a novel application of Supervised Contrastive Learning
(SupCon) to Imitation Learning (IL), with a focus on learning more effective
state representations for agents in video game environments. The goal is to
obtain latent representations of the observations that capture better the
action-relevant factors, thereby modeling better the cause-effect relationship
from the observations that are mapped to the actions performed by the
demonstrator, for example, the player jumps whenever an obstacle appears ahead.
We propose an approach to integrate the SupCon loss with continuous output
spaces, enabling SupCon to operate without constraints regarding the type of
actions of the environment. Experiments on the 3D games Astro Bot and Returnal,
and multiple 2D Atari games show improved representation quality, faster
learning convergence, and better generalization compared to baseline models
trained only with supervised action prediction loss functions.

</details>


### [237] [EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](https://arxiv.org/abs/2509.11914)
*Yiqun Yao,Naitong Yu,Xiang Li,Xin Jiang,Xuezhi Fang,Wenjia Ma,Xuying Meng,Jing Li,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: EgoMem is the first lifelong memory agent for full-duplex, real-time omnimodal streams. It enables real-time user recognition directly from raw audiovisual data, personalized responses, and long-term memory of users' facts, preferences, and relationships via three asynchronous modules: retrieval, omnimodal dialog, and memory management. It achieves high retrieval/memory-management accuracy (>95%) and strong fact-consistency (>87%) when integrated with RoboEgo, establishing a baseline for lifelong, embodied personalization.


<details>
  <summary>Details</summary>
Motivation: To empower embodied, real-time agents to maintain and update long-term, user-specific knowledge from continuous audiovisual streams, enabling lifelong personalization and coherent interactions beyond offline or text-only memory systems.

Method: Three asynchronous processes operating on raw audiovisual streams: (i) retrieval to identify the user via face/voice and fetch relevant long-term memory; (ii) omnimodal dialog to generate personalized audio responses using retrieved context; (iii) memory management to detect dialog boundaries and extract information to update long-term memory. Unlike prior memory agents, EgoMem relies entirely on raw audiovisual data and can integrate with a RoboEgo omnimodal chatbot.

Result: Retrieval and memory management modules achieve over 95% accuracy on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot, the system attains fact-consistency scores above 87% in real-time personalized dialogs, establishing a strong baseline for future research.

Conclusion: EgoMem demonstrates the feasibility of lifelong, real-time memory for embodied agents using raw audiovisual streams, enabling personalized, consistent interactions in real-time scenarios, and highlighting directions for robustness, privacy, and scalability in future work.

Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex
models that process real-time omnimodal streams. EgoMem enables real-time
models to recognize multiple users directly from raw audiovisual streams, to
provide personalized response, and to maintain long-term knowledge of users'
facts, preferences, and social relationships extracted from audiovisual
history. EgoMem operates with three asynchronous processes: (i) a retrieval
process that dynamically identifies user via face and voice, and gathers
relevant context from a long-term memory; (ii) an omnimodal dialog process that
generates personalized audio responses based on the retrieved context; and
(iii) a memory management process that automatically detects dialog boundaries
from omnimodal streams, and extracts necessary information to update the
long-term memory. Unlike existing memory agents for LLMs, EgoMem relies
entirely on raw audiovisual streams, making it especially suitable for
lifelong, real-time, and embodied scenarios. Experimental results demonstrate
that EgoMem's retrieval and memory management modules achieve over 95% accuracy
on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,
the system achieves fact-consistency scores above 87% in real-time personalized
dialogs, establishing a strong baseline for future research.

</details>


### [238] [BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning](https://arxiv.org/abs/2509.11922)
*Xilei Dai,Ruotian Chen,Songze Guan,Wen-Tai Li,Chau Yuen*

Main category: cs.AI

TL;DR: BuildingGym is an open-source, RL-centric framework for building energy management that integrates EnergyPlus, supports external control signals, and offers plug-and-play RL algorithms and simulators for system- and room-level control, demonstrated on cooling load management.


<details>
  <summary>Details</summary>
Motivation: There is a lack of a flexible, research-friendly framework to implement and compare RL-based control strategies across diverse building energy management problems, especially with external signals and varying levels of control granularity.

Method: Introduce BuildingGym as a modular framework that integrates EnergyPlus as the core simulator, supports external control inputs, and provides built-in RL algorithms. It enables straightforward configuration of optimization tasks for common building energy problems, particularly cooling load management, including both constant and dynamic targets. The framework is designed to be extensible: replaceable RL algorithms, simulators, and control environments to suit different problems and communities (building managers vs. AI researchers).

Result: The built-in RL algorithms demonstrated strong performance on cooling load management tasks, for both constant and dynamic cooling load scenarios, indicating BuildingGym’s effectiveness in training and evaluating RL-based cooling strategies.

Conclusion: BuildingGym addresses a gap between building managers and AI researchers by offering a flexible, open-source platform that can be configured to various problems in building energy management, supports system- and room-level control via EnergyPlus, accepts external signals, and facilitates rapid experimentation with state-of-the-art RL methods for cooling load management.

Abstract: Reinforcement learning (RL) has proven effective for AI-based building energy
management. However, there is a lack of flexible framework to implement RL
across various control problems in building energy management. To address this
gap, we propose BuildingGym, an open-source tool designed as a
research-friendly and flexible framework for training RL control strategies for
common challenges in building energy management. BuildingGym integrates
EnergyPlus as its core simulator, making it suitable for both system-level and
room-level control. Additionally, BuildingGym is able to accept external
signals as control inputs instead of taking the building as a stand-alone
entity. This feature makes BuildingGym applicable for more flexible
environments, e.g. smart grid and EVs community. The tool provides several
built-in RL algorithms for control strategy training, simplifying the process
for building managers to obtain optimal control strategies. Users can achieve
this by following a few straightforward steps to configure BuildingGym for
optimization control for common problems in the building energy management
field. Moreover, AI specialists can easily implement and test state-of-the-art
control algorithms within the platform. BuildingGym bridges the gap between
building managers and AI specialists by allowing for the easy configuration and
replacement of RL algorithms, simulators, and control environments or problems.
With BuildingGym, we efficiently set up training tasks for cooling load
management, targeting both constant and dynamic cooling load management. The
built-in algorithms demonstrated strong performance across both tasks,
highlighting the effectiveness of BuildingGym in optimizing cooling strategies.

</details>


### [239] [Neuromorphic Intelligence](https://arxiv.org/abs/2509.11940)
*Marcel van Gerven*

Main category: cs.AI

TL;DR: A theoretical, unifying dynamical-systems framework for neuromorphic computing that uses differential calculus, leverages noise for learning, and introduces differential genetic programming to discover adaptive dynamical substrates, aiming for emergent, energy-efficient AI.


<details>
  <summary>Details</summary>
Motivation: Bridge diverse disciplines (AI, neuroscience, physics, chemistry, materials science) by providing a principled foundation to realize sustainable, scalable neuromorphic systems.

Method: Adopts dynamical systems theory as the core language for inference, learning, and control; treats noise as a learning resource; proposes differential genetic programming to discover dynamical systems implementing adaptive behaviors.

Result: Proposes a conceptual framework with potential pathways to emergent neuromorphic intelligence; outlines how physical substrates could realize these dynamics and how energy efficiency could be achieved; no empirical results are presented.

Conclusion: Adopting this perspective could catalyze emergent neuromorphic intelligence and advance sustainable AI by grounding computation in dynamical principles.

Abstract: Neuromorphic computing seeks to replicate the remarkable efficiency,
flexibility, and adaptability of the human brain in artificial systems. Unlike
conventional digital approaches, which depend on massive computational and
energy resources, neuromorphic systems exploit brain-inspired principles of
computation to achieve orders of magnitude greater energy efficiency. By
drawing on insights from artificial intelligence, neuroscience, physics,
chemistry, and materials science, neuromorphic computing promises to deliver
intelligent systems that are sustainable, transparent, and widely accessible. A
central challenge, however, is to identify a unifying theoretical framework
capable of bridging these diverse disciplines. We argue that dynamical systems
theory provides such a foundation. Rooted in differential calculus, it offers a
principled language for modeling inference, learning, and control in both
natural and artificial substrates. Within this framework, noise can be
harnessed as a resource for learning, while differential genetic programming
enables the discovery of dynamical systems that implement adaptive behaviors.
Embracing this perspective paves the way toward emergent neuromorphic
intelligence, where intelligent behavior arises from the dynamics of physical
substrates, advancing both the science and sustainability of AI.

</details>


### [240] [How to Evaluate Medical AI](https://arxiv.org/abs/2509.11941)
*Ilia Kopanichuk,Petr Anokhin,Vladimir Shaposhnikov,Vladimir Makharev,Ekaterina Tsapieva,Iaroslav Bespalov,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.AI

TL;DR: Introduces Relative Precision and Recall of Algorithmic Diagnostics (RPAD/RRAD) to evaluate AI diagnostic outputs against multiple expert opinions, normalizing by inter-expert disagreement. Claims improved stability, interpretability, and relevance over传统 metrics; enables free-form diagnosis evaluation with high accuracy (98% in identity of diagnoses) and shows expert variability can exceed AI-human differences.


<details>
  <summary>Details</summary>
Motivation: Address variability and limited interpretability of traditional accuracy metrics in medical AI evaluation by comparing AI outputs to multiple experts rather than a single reference, and by normalizing against inter-expert disagreement.

Method: Define RPAD and RRAD as metrics that quantify AI diagnostic performance relative to the distribution of expert opinions. Use free-form diagnoses instead of restricted lists. Apply the method to 360 medical dialogues, comparing multiple LLMs against physicians, and assess agreement against expert consensus.

Result: RPAD/RRAD yield more stable, realistic performance measures. Top models (e.g., DeepSeek-V3) achieve consistency on par with or exceeding expert consensus. Expert judgments show substantial variability, sometimes greater than AI-vs-human differences. Free-form diagnosis approach achieved ~98% accuracy in identifying diagnoses.

Conclusion: Relative metrics grounded in inter-expert disagreement can improve the evaluation of medical AI by offering stable, interpretable measures and highlighting the limits of absolute metrics; supports adopting relative performance standards in AI diagnostics.

Abstract: The integration of artificial intelligence (AI) into medical diagnostic
workflows requires robust and consistent evaluation methods to ensure
reliability, clinical relevance, and the inherent variability in expert
judgments. Traditional metrics like precision and recall often fail to account
for the inherent variability in expert judgments, leading to inconsistent
assessments of AI performance. Inter-rater agreement statistics like Cohen's
Kappa are more reliable but they lack interpretability. We introduce Relative
Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new
evaluation metrics that compare AI outputs against multiple expert opinions
rather than a single reference. By normalizing performance against inter-expert
disagreement, these metrics provide a more stable and realistic measure of the
quality of predicted diagnosis. In addition to the comprehensive analysis of
diagnostic quality measures, our study contains a very important side result.
Our evaluation methodology allows us to avoid selecting diagnoses from a
limited list when evaluating a given case. Instead, both the models being
tested and the examiners verifying them arrive at a free-form diagnosis. In
this automated methodology for establishing the identity of free-form clinical
diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our
approach using 360 medical dialogues, comparing multiple large language models
(LLMs) against a panel of physicians. Large-scale study shows that
top-performing models, such as DeepSeek-V3, achieve consistency on par with or
exceeding expert consensus. Moreover, we demonstrate that expert judgments
exhibit significant variability - often greater than that between AI and
humans. This finding underscores the limitations of any absolute metrics and
supports the need to adopt relative metrics in medical AI.

</details>


### [241] [Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics](https://arxiv.org/abs/2509.11943)
*Antonin Sulc,Thorsten Hellert*

Main category: cs.AI

TL;DR: A neuro-symbolic multi-agent framework using Kripke-model belief states and modal logic to constrain language-model reasoning, guiding hypothesis generation with immutable domain knowledge, demonstrated by diagnosing cascading failures in a high-fidelity simulated particle accelerator environment for more robust, verifiable autonomous agents.


<details>
  <summary>Details</summary>
Motivation: Improve robustness, verifiability, and adaptive decision-making in autonomous agents by combining powerful LM-based reasoning with formal, checkable logic and a solid factual world model, addressing limitations beyond mere model and data scaling.

Method: Represent each agent's beliefs as Kripke models and reason about possibility and necessity using modal logic. Encode immutable domain knowledge as logical constraints that guide hypothesis generation and prevent untenable conclusions. Integrate LM-driven semantic intuition with rigorous logical validation and a factual world model. Validate in a high-fidelity simulated particle accelerator environment to diagnose complex, cascading failures.

Result: The system successfully diagnoses complex, cascading failures by effectively combining LM semantic capabilities with modal-logic constraints and a factual world model, demonstrating a path toward more robust, verifiable autonomous agents.

Conclusion: This approach offers a viable path toward robust, reliable, and verifiable autonomous agents by integrating neuro-symbolic reasoning with formal logic; further work is needed to scale, generalize, and validate across domains.

Abstract: The development of intelligent agents, particularly those powered by language
models (LMs), has shown the critical role in various environments that require
intelligent and autonomous decision. Environments are not passive testing
grounds and they represent the data required for agents to learn and exhibit
very challenging conditions that require adaptive, complex and autonomous
capacity to make decisions. While the paradigm of scaling models and datasets
has led to remarkable emergent capabilities, we argue that scaling the
structure, fidelity, and logical consistency of agent reasoning within these
environments is a crucial, yet underexplored, dimension of AI research. This
paper introduces a neuro-symbolic multi-agent architecture where the belief
states of individual agents are formally represented as Kripke models. This
foundational choice enables them to reason about known concepts of
\emph{possibility} and \emph{necessity} using the formal language of modal
logic. In this work, we use of immutable, domain-specific knowledge to make
infere information, which is encoded as logical constraints essential for
proper diagnosis. In the proposed model, we show constraints that actively
guide the hypothesis generation of LMs, effectively preventing them from
reaching physically or logically untenable conclusions. In a high-fidelity
simulated particle accelerator environment, our system successfully diagnoses
complex, cascading failures by combining the powerful semantic intuition of LMs
with the rigorous, verifiable validation of modal logic and a factual world
model and showcasing a viable path toward more robust, reliable, and verifiable
autonomous agents.

</details>


### [242] [Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare](https://arxiv.org/abs/2509.11944)
*Susanta Mitra*

Main category: cs.AI

TL;DR: A temporal graph-based, multi-agent reasoning framework for multimodal medical data that supports backtracking and time-point analysis to improve diagnostic reasoning; preliminary studies show feasibility and potential utility.


<details>
  <summary>Details</summary>
Motivation: Healthcare involves multimodal data and longitudinal information. Existing multimodal reasoning models struggle with correct, clinically reliable diagnoses in healthcare; there is a need for dynamic, time-aware, and collaborative reasoning to enhance diagnostic accuracy.

Method: Model a temporal directed graph of reasoning steps, allowing backtracking, refinement, addition/deletion of reasoning nodes, and tracking multimodal data across time points. Introduce a multi-agent reasoning framework with task distribution and cross-validation to improve robustness.

Result: Preliminary experiments and analyses indicate the approach is feasible and has novelty and practical utility. Early results suggest benefits in handling dynamic reasoning and tracking disease progression, though evidence is limited.

Conclusion: The proposed temporal graph-based, multi-agent reasoning framework offers a dynamic, time-aware approach to multimodal medical diagnosis and can assist healthcare professionals, warranting further rigorous evaluation and larger-scale experiments.

Abstract: Healthcare and medicine are multimodal disciplines that deal with multimodal
data for reasoning and diagnosing multiple diseases. Although some multimodal
reasoning models have emerged for reasoning complex tasks in scientific
domains, their applications in the healthcare domain remain limited and fall
short in correct reasoning for diagnosis. To address the challenges of
multimodal medical reasoning for correct diagnosis and assist the healthcare
professionals, a novel temporal graph-based reasoning process modelled through
a directed graph has been proposed in the current work. It helps in
accommodating dynamic changes in reasons through backtracking, refining the
reasoning content, and creating new or deleting existing reasons to reach the
best recommendation or answer. Again, consideration of multimodal data at
different time points can enable tracking and analysis of patient health and
disease progression. Moreover, the proposed multi-agent temporal reasoning
framework provides task distributions and a cross-validation mechanism to
further enhance the accuracy of reasoning outputs. A few basic experiments and
analysis results justify the novelty and practical utility of the proposed
preliminary approach.

</details>


### [243] [MusicSwarm: Biologically Inspired Intelligence for Music Composition](https://arxiv.org/abs/2509.11973)
*Markus J. Buehler*

Main category: cs.AI

TL;DR: Decentralized swarm of identical, frozen foundation models coordinates via stigmergic, peer-to-peer signals to produce long-horizon, coherent music without weight updates. Outperforms centralized critics in quality, diversity, and structure, with a small-world, specialized role network.


<details>
  <summary>Details</summary>
Motivation: Explore how long-horizon creative tasks can be achieved without gradient-based updates by leveraging local interactions, shared memory, and consensus in a scalable swarm, improving efficiency and diversity in music and enabling transfer to other domains.

Method: Compare a centralized multi-agent system with a global critic to a fully decentralized swarm where bar-wise agents sense and deposit harmonic, rhythmic, and structural cues, adapt short-term memory, and reach consensus; analyze symbolic, audio, and graph-theoretic properties to assess quality and structure.

Result: The swarm yields superior quality and greater diversity/structural variety; it converges to a stable configuration with complementary roles; the network exhibits small-world properties enabling efficient long-range connectivity and bridging motifs.

Conclusion: By shifting emphasis from parameter updates to interaction rules, shared memory, and dynamic consensus, MusicSwarm offers a compute- and data-efficient path to long-horizon creative structure with potential applicability beyond music, to collaborative writing, design, and scientific discovery.

Abstract: We show that coherent, long-form musical composition can emerge from a
decentralized swarm of identical, frozen foundation models that coordinate via
stigmergic, peer-to-peer signals, without any weight updates. We compare a
centralized multi-agent system with a global critic to a fully decentralized
swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and
structural cues, adapt short-term memory, and reach consensus. Across symbolic,
audio, and graph-theoretic analyses, the swarm yields superior quality while
delivering greater diversity and structural variety and leads across creativity
metrics. The dynamics contract toward a stable configuration of complementary
roles, and self-similarity networks reveal a small-world architecture with
efficient long-range connectivity and specialized bridging motifs, clarifying
how local novelties consolidate into global musical form. By shifting
specialization from parameter updates to interaction rules, shared memory, and
dynamic consensus, MusicSwarm provides a compute- and data-efficient route to
long-horizon creative structure that is immediately transferable beyond music
to collaborative writing, design, and scientific discovery.

</details>


### [244] [Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review](https://arxiv.org/abs/2509.12034)
*Emmanuel Adjei Domfeh,Christopher L. Dancy*

Main category: cs.AI

TL;DR: A systematic review of Human-AI collaboration patterns across all disaster management phases, identifying four core patterns and sub-patterns, highlighting benefits for situational awareness and response, while noting limitations in scalability, interpretability, and interoperability, and outlining future research directions for adaptive, trustworthy, and context-aware H-AI systems that support equitable recovery.


<details>
  <summary>Details</summary>
Motivation: Disaster management decisions are made under high uncertainty, time pressure, and constrained resources. There is a need to synthesize existing evidence on how human-AI collaboration can support decision-making across all phases of disasters and to identify effective patterns, gaps, and directions for future research.

Method: A systematic review of 51 peer-reviewed studies examining Human-AI collaboration in disaster management. The authors categorize patterns into four major areas and analyze sub-patterns (e.g., cognitive-augmented intelligence, multi-agent coordination, explainable AI, virtual training).

Result: Four major pattern categories emerged: (1) Human-AI Decision Support Systems, (2) Task and Resource Coordination, (3) Trust and Transparency, and (4) Simulation and Training. Within these, sub-patterns include cognitive-augmented intelligence, multi-agent coordination, explainable AI, and virtual training environments. AI systems are shown to enhance situational awareness, improve response efficiency, and support complex decision-making, while limitations include scalability, interpretability, and system interoperability.

Conclusion: The review concludes with key challenges and future research directions, stressing adaptive, trustworthy, and context-aware Human-AI systems designed to improve disaster resilience and promote equitable recovery outcomes.

Abstract: In high-stakes disaster scenarios, timely and informed decision-making is
critical yet often challenged by uncertainty, dynamic environments, and limited
resources. This paper presents a systematic review of Human-AI collaboration
patterns that support decision-making across all disaster management phases.
Drawing from 51 peer-reviewed studies, we identify four major categories:
Human-AI Decision Support Systems, Task and Resource Coordination, Trust and
Transparency, and Simulation and Training. Within these, we analyze
sub-patterns such as cognitive-augmented intelligence, multi-agent
coordination, explainable AI, and virtual training environments. Our review
highlights how AI systems may enhance situational awareness, improves response
efficiency, and support complex decision-making, while also surfacing critical
limitations in scalability, interpretability, and system interoperability. We
conclude by outlining key challenges and future research directions,
emphasizing the need for adaptive, trustworthy, and context-aware Human-AI
systems to improve disaster resilience and equitable recovery outcomes.

</details>


### [245] [When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models](https://arxiv.org/abs/2509.12060)
*Wei Cai,Shujuan Liu,Jian Zhao,Ziyan Shi,Yusheng Zhao,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: Introduces the SSUI dataset and SRPO framework to align multimodal LLMs' internal reasoning with safety values, achieving state-of-the-art safety performance.


<details>
  <summary>Details</summary>
Motivation: MLLMs suffer from implicit reasoning risk: innocuous unimodal inputs can combine into harmful multimodal outputs; safety alignment is hard during long-chain cross-modal reasoning.

Method: Proposes Safe-Semantics-but-Unsafe-Interpretation (SSUI), a dataset with interpretable cross-modal reasoning paths. Builds Safety-aware Reasoning Path Optimization (SRPO) to align the model's internal reasoning with human safety values using SSUI.

Result: SRPO-trained models achieve state-of-the-art results on safety benchmarks, including the proposed RSBench, significantly outperforming both open-source and top-tier commercial MLLMs.

Conclusion: Interpretable reasoning paths and safety-aware optimization can substantially improve safety alignment in multimodal LLMs, mitigating implicit cross-modal risks.

Abstract: Multimodal Large Language Models (MLLMs) are susceptible to the implicit
reasoning risk, wherein innocuous unimodal inputs synergistically assemble into
risky multimodal data that produce harmful outputs. We attribute this
vulnerability to the difficulty of MLLMs maintaining safety alignment through
long-chain reasoning. To address this issue, we introduce
Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring
interpretable reasoning paths tailored for such a cross-modal challenge. A
novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is
also designed based on the SSUI dataset to align the MLLM's internal reasoning
process with human safety values. Experimental results show that our
SRPO-trained models achieve state-of-the-art results on key safety benchmarks,
including the proposed Reasoning Path Benchmark (RSBench), significantly
outperforming both open-source and top-tier commercial MLLMs.

</details>


### [246] [Bridging Engineering and AI Planning through Model-Based Knowledge Transformation for the Validation of Automated Production System Variants](https://arxiv.org/abs/2509.12091)
*Hamied Nabizada,Lasse Beers,Alain Chahine,Felix Gehlhoff,Oliver Niggemann,Alexander Fay*

Main category: cs.AI

TL;DR: SysML-based method to auto-generate PDDL planning artifacts from MBSE models; enriches SysML with planning semantics via a dedicated profile; enables AI planning-based validation; demonstrated on aircraft assembly.


<details>
  <summary>Details</summary>
Motivation: MBSE models typically lack symbolic planning semantics (preconditions, effects, resource constraints, timing), hindering evaluation of system variants and performance. Integrating planning concepts into engineering models enables automated validation and comparison of alternatives.

Method: Introduce a SysML profile with reusable stereotypes for core planning constructs; integrate these into existing model structures; develop an algorithm that converts the enriched models into valid PDDL domain and problem files; ensure native integration and consistency between engineering and planning artifacts.

Result: Generated planning artifacts can be used with AI planning to validate system variants; case study on aircraft assembly shows how models are enriched with planning semantics and how the workflow generates consistent planning artifacts.

Conclusion: The approach provides native integration of planning semantics into SysML, enabling automated generation of planning artifacts from engineering models and enabling variant validation, demonstrated through a practical aircraft assembly case.

Abstract: Engineering models created in Model-Based Systems Engineering (MBSE)
environments contain detailed information about system structure and behavior.
However, they typically lack symbolic planning semantics such as preconditions,
effects, and constraints related to resource availability and timing. This
limits their ability to evaluate whether a given system variant can fulfill
specific tasks and how efficiently it performs compared to alternatives.
  To address this gap, this paper presents a model-driven method that enables
the specification and automated generation of symbolic planning artifacts
within SysML-based engineering models. A dedicated SysML profile introduces
reusable stereotypes for core planning constructs. These are integrated into
existing model structures and processed by an algorithm that generates a valid
domain file and a corresponding problem file in Planning Domain Definition
Language (PDDL). In contrast to previous approaches that rely on manual
transformations or external capability models, the method supports native
integration and maintains consistency between engineering and planning
artifacts.
  The applicability of the method is demonstrated through a case study from
aircraft assembly. The example illustrates how existing engineering models are
enriched with planning semantics and how the proposed workflow is applied to
generate consistent planning artifacts from these models. The generated
planning artifacts enable the validation of system variants through AI
planning.

</details>


### [247] [JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference](https://arxiv.org/abs/2509.12104)
*Zongyue Xue,Siyuan Zheng,Shaochun Wang,Yiran Hu,Shenran Wang,Yuxin Yao,Haitao Li,Qingyao Ai,Yiqun Liu,Yun Liu,Weixing Shen*

Main category: cs.AI

TL;DR: JustEva: an open-source toolkit to evaluate LLM fairness in legal tasks; provides a 65-factor label system, three fairness metrics (inconsistency, bias, imbalanced inaccuracy), robust inference, and visualizations; supports dataset-driven outputs and statistical analysis; empirical results show notable fairness deficiencies in current LLMs; offers a foundation for fairness assessment/improvement in legal AI.


<details>
  <summary>Details</summary>
Motivation: Addresses concerns about judicial fairness and the 'black-box' nature of LLMs in legal practice by providing a structured, transparent evaluation framework to measure fairness and trustworthiness.

Method: Develop JustEva toolkit with a comprehensive label system (65 extra-legal factors), three core fairness metrics, statistical inference methods, visualizations; supports two experimental workflows: (a) generate structured outputs from LLMs using a dataset; (b) perform regression and statistical analysis on outputs to assess fairness.

Result: Empirical application reveals significant fairness deficiencies in current LLMs, indicating lack of fair and trustworthy LLM legal tools.

Conclusion: JustEva provides a practical tool and methodological foundation for evaluating and improving algorithmic fairness in the legal domain.

Abstract: The integration of Large Language Models (LLMs) into legal practice raises
pressing concerns about judicial fairness, particularly due to the nature of
their "black-box" processes. This study introduces JustEva, a comprehensive,
open-source evaluation toolkit designed to measure LLM fairness in legal tasks.
JustEva features several advantages: (1) a structured label system covering 65
extra-legal factors; (2) three core fairness metrics - inconsistency, bias, and
imbalanced inaccuracy; (3) robust statistical inference methods; and (4)
informative visualizations. The toolkit supports two types of experiments,
enabling a complete evaluation workflow: (1) generating structured outputs from
LLMs using a provided dataset, and (2) conducting statistical analysis and
inference on LLMs' outputs through regression and other statistical methods.
Empirical application of JustEva reveals significant fairness deficiencies in
current LLMs, highlighting the lack of fair and trustworthy LLM legal tools.
JustEva offers a convenient tool and methodological foundation for evaluating
and improving algorithmic fairness in the legal domain.

</details>


### [248] [Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation](https://arxiv.org/abs/2509.12179)
*Yubo Li,Weiyi Song*

Main category: cs.AI

TL;DR: BiCA introduces bidirectional Cognitive Alignment between humans and AI, using learnable protocols, representation mapping, and KL-budget constraints. In collaborative navigation, it outperforms RLHF baselines in success rate, mutual adaptation, and protocol convergence, with emergent protocols and improved safety, supporting an intersection-based co-alignment.


<details>
  <summary>Details</summary>
Motivation: RLHF uses a unidirectional alignment that treats human cognition as fixed, limiting adaptation, safety, and synergy. BiCA posits mutual adaptation between humans and AI to exploit complementary strengths and evolve collaboration protocols.

Method: BiCA framework with bidirectional adaptation, leveraging learnable protocols, representation mapping between human and AI cognition, and KL-budget constraints to regulate co-evolution. Evaluates emergent vs handcrafted protocols in collaborative navigation, reporting metrics on success, mutual adaptation, protocol convergence, safety, and OOD robustness.

Result: 85.5% success vs 70.3% baseline; 230% better mutual adaptation; 332% better protocol convergence; emergent protocols outperform handcrafted by 84%; bidirectional adaptation improves safety by 23% in OOD robustness; 46% synergy improvement.

Conclusion: The data support that optimal collaboration lies at the intersection of human and AI capabilities. This substantiates a shift from single-directional to bidirectional co-alignment and demonstrates concrete benefits in performance, safety, and protocol evolution.

Abstract: Current AI alignment through RLHF follows a single directional paradigm that
AI conforms to human preferences while treating human cognition as fixed. We
propose a shift to co-alignment through Bidirectional Cognitive Alignment
(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,
representation mapping, and KL-budget constraints for controlled co-evolution.
In collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,
with 230% better mutual adaptation and 332% better protocol convergence.
Emergent protocols outperformed handcrafted ones by 84%, while bidirectional
adaptation unexpectedly improved safety (+23% out-of-distribution robustness).
The 46% synergy improvement demonstrates optimal collaboration exists at the
intersection, not union, of human and AI capabilities, validating the shift
from single-directional to co-alignment paradigms.

</details>


### [249] [Advancing Medical Artificial Intelligence Using a Century of Cases](https://arxiv.org/abs/2509.12194)
*Thomas A. Buckley,Riccardo Conci,Peter G. Brodeur,Jason Gusdorf,Sourik Beltrán,Bita Behrouzi,Byron Crowe,Jacob Dockterman,Muzzammil Muhammad,Sarah Ohnigian,Andrew Sanchez,James A. Diao,Aashna P. Shah,Daniel Restrepo,Eric S. Rosenberg,Andrew S. Lea,Marinka Zitnik,Scott H. Podolsky,Zahir Kanjee,Raja-Elie E. Abdulnour,Jacob M. Koshy,Adam Rodman,Arjun K. Manrai*

Main category: cs.AI

TL;DR: Introduction of CPC-Bench benchmark and CaBot; OpenAI's o3 exceeds physicians in text-based differential diagnosis and emulates expert medical presentations; image interpretation and literature retrieval remain weaker; CPC-Bench and CaBot released for ongoing benchmarking.


<details>
  <summary>Details</summary>
Motivation: To evaluate AI reasoning and presentation skills in medical conferences (CPCs) beyond final diagnoses, addressing a gap where prior AI work focused on single outcomes and not the multifaceted expert discourse.

Method: Analyzed 7102 CPCs (1923–2025) and 1021 Image Challenges (2006–2025), performed physician annotations, and built CPC-Bench with 10 text-based and multimodal tasks. Evaluated leading LLMs, and developed 'Dr. CaBot' to generate written and slide-based video presentations using only the case presentation.

Result: On 377 contemporary CPCs, OpenAI o3 ranked the final diagnosis first in 60% of cases and within the top ten in 84% (ahead of a 20-physician baseline); next-test selection accuracy 98%. Image challenges reached 67% accuracy with o3 and Gemini 2.5 Pro. In blinded CaBot vs. human-text trials (n=62), physicians misattributed the differential source in 46 trials (74%), while CaBot scored higher on quality. CPC-Bench and CaBot are being released to the research community.

Conclusion: LLMs surpass physicians on complex text-based differential diagnosis and convincingly mimic expert medical presentations; image interpretation and literature retrieval lag behind. CPC-Bench and CaBot enable transparent tracking of progress in medical AI.

Abstract: BACKGROUND: For over a century, the New England Journal of Medicine
Clinicopathological Conferences (CPCs) have tested the reasoning of expert
physicians and, recently, artificial intelligence (AI). However, prior AI
evaluations have focused on final diagnoses without addressing the multifaceted
reasoning and presentation skills required of expert discussants.
  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),
we conducted extensive physician annotation and automated processing to create
CPC-Bench, a physician-validated benchmark spanning 10 text-based and
multimodal tasks, against which we evaluated leading large language models
(LLMs). Then, we developed "Dr. CaBot," an AI discussant designed to produce
written and slide-based video presentations using only the case presentation,
modeling the role of the human expert in these cases.
  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the
final diagnosis first in 60% of cases and within the top ten in 84% of cases,
outperforming a 20-physician baseline; next-test selection accuracy reached
98%. Event-level physician annotations quantified AI diagnostic accuracy per
unit of information. Performance was lower on literature search and image
tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image
challenges. In blinded comparisons of CaBot vs. human expert-generated text,
physicians misclassified the source of the differential in 46 of 62 (74%) of
trials, and scored CaBot more favorably across quality dimensions. To promote
research, we are releasing CaBot and CPC-Bench.
  CONCLUSIONS: LLMs exceed physician performance on complex text-based
differential diagnosis and convincingly emulate expert medical presentations,
but image interpretation and literature retrieval remain weaker. CPC-Bench and
CaBot may enable transparent and continued tracking of progress in medical AI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [250] [The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results](https://arxiv.org/abs/2509.10463)
*Qiuyu Chen,Xin Jin,Yue Song,Xihui Liu,Shuai Yang,Tao Yang,Ziqiang Li,Jianguo Huang,Yuntao Wei,Ba'ao Xie,Nicu Sebe,Wenjun,Zeng,Jooyeol Yun,Davide Abati,Mohamed Omran,Jaegul Choo,Amir Habibian,Auke Wiggers,Masato Kobayashi,Ning Ding,Toru Tamaki,Marzieh Gheisari,Auguste Genovesio,Yuheng Chen,Dingkun Liu,Xinyao Yang,Xinping Xu,Baicheng Chen,Dongrui Wu,Junhao Geng,Lexiang Lv,Jianxin Lin,Hanzhe Liang,Jie Zhou,Xuanxin Chen,Jinbao Wang,Can Gao,Zhangyi Wang,Zongze Li,Bihan Wen,Yixin Gao,Xiaohan Pan,Xin Li,Zhibo Chen,Baorui Peng,Zhongming Chen,Haoran Jin*

Main category: cs.LG

TL;DR: A workshop summary on DRL4Real at ICCV2025 focusing on applying disentangled representation learning to real-world controllable generation, with 9 papers spanning language inductive biases, diffusion models, 3D-aware DRL, and domain-specific applications like autonomous driving and EEG.


<details>
  <summary>Details</summary>
Motivation: DRL promises interpretable, controllable generative representations but lags in real-world robustness, generalization, and applicability beyond synthetic benchmarks; the workshop aims to bridge theory and practice.

Method: Curated and evaluated DRL methods on practical scenarios; organized around themes such as inductive biases, diffusion DRL, 3D-aware disentanglement, and domain-specific applications; presented 9 accepted papers and a summary of objectives, themes, and methodologies.

Result: Identification of key directions for practical DRL: integration of inductive biases (e.g., language), diffusion-based DRL, 3D-aware DRL, and domain-specific deployments; improved understanding of robustness and generalization in real-world contexts; preliminary methodologies for bridging theory to practice.

Conclusion: DRL4Real marks progress toward bridging DRL theory with real-world controllable generation; highlights need for realistic benchmarks, robust evaluation, and cross-domain transfer; sets agenda for future work in DRL applications.

Abstract: This paper reviews the 1st International Workshop on Disentangled
Representation Learning for Controllable Generation (DRL4Real), held in
conjunction with ICCV 2025. The workshop aimed to bridge the gap between the
theoretical promise of Disentangled Representation Learning (DRL) and its
application in realistic scenarios, moving beyond synthetic benchmarks.
DRL4Real focused on evaluating DRL methods in practical applications such as
controllable generation, exploring advancements in model robustness,
interpretability, and generalization. The workshop accepted 9 papers covering a
broad range of topics, including the integration of novel inductive biases
(e.g., language), the application of diffusion models to DRL, 3D-aware
disentanglement, and the expansion of DRL into specialized domains like
autonomous driving and EEG analysis. This summary details the workshop's
objectives, the themes of the accepted papers, and provides an overview of the
methodologies proposed by the authors.

</details>


### [251] [Moment Estimates and DeepRitz Methods on Learning Diffusion Systems with Non-gradient Drifts](https://arxiv.org/abs/2509.10495)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: cs.LG

TL;DR: A data-driven two-phase Moment-DeepRitz Method learns drift decompositions in generalized diffusion systems with conservative-dissipative dynamics, robust to noise and irregular potentials, validated by numerical experiments.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of learning drift decompositions in open systems with conservative-dissipative dynamics from noisy data, especially under rough potentials and oscillatory rotations.

Method: A two-phase framework (Moment-DeepRitz Method) that combines moment-based data extraction with a Deep Ritz variational approach to learn drift decompositions in generalized diffusion systems, robust to noise and irregularities.

Result: Demonstrates effectiveness through several numerical experiments, showing robustness to noisy data and adaptability to rough potentials and oscillatory rotations.

Conclusion: The Moment-DeepRitz Method provides a robust, data-driven approach for learning drift decompositions in conservative-dissipative diffusion, validated by numerical experiments.

Abstract: Conservative-dissipative dynamics are ubiquitous across a variety of complex
open systems. We propose a data-driven two-phase method, the Moment-DeepRitz
Method, for learning drift decompositions in generalized diffusion systems
involving conservative-dissipative dynamics. The method is robust to noisy
data, adaptable to rough potentials and oscillatory rotations. We demonstrate
its effectiveness through several numerical experiments.

</details>


### [252] [SOH-KLSTM: A Hybrid Kolmogorov-Arnold Network and LSTM Model for Enhanced Lithium-Ion Battery Health Monitoring](https://arxiv.org/abs/2509.10496)
*Imen Jarraya,Safa Ben Atitallah,Fatimah Alahmeda,Mohamed Abdelkadera,Maha Drissa,Fatma Abdelhadic,Anis Koubaaa*

Main category: cs.LG

TL;DR: A novel hybrid SOH estimation framework (SOH-KLSTM) that fuses LSTM with Kolmogorov-Arnold Network (KAN) to better capture non-linear, temporal degradation in Li-battery health, aiming to improve accuracy and reliability over conventional methods.


<details>
  <summary>Details</summary>
Motivation: Accurate state of health (SOH) estimation for Li batteries is hindered by strong non-linearities and time-varying degradation. Conventional methods struggle to model long-term dependencies and complex degradation patterns, impacting safety and performance in EVs, drones, and energy storage.

Method: Integrate Kolmogorov-Arnold Network (KAN) with LSTM via an integrated Candidate Cell State to form SOH-KLSTM. The approach leverages LSTM’s ability to learn long-term temporal dependencies and KAN’s non-linear approximation capabilities to model battery degradation dynamics for health monitoring.

Result: The abstract does not report empirical results or quantitative findings; it presents the proposed framework without experimental validation details.

Conclusion: A hybrid SOH prediction framework combining LSTM and KAN is proposed to better capture non-linear, time-dependent degradation in Li batteries, with potential improvements in SOH monitoring applicable to various platforms; actual performance remains to be validated.

Abstract: Accurate and reliable State Of Health (SOH) estimation for Lithium (Li)
batteries is critical to ensure the longevity, safety, and optimal performance
of applications like electric vehicles, unmanned aerial vehicles, consumer
electronics, and renewable energy storage systems. Conventional SOH estimation
techniques fail to represent the non-linear and temporal aspects of battery
degradation effectively. In this study, we propose a novel SOH prediction
framework (SOH-KLSTM) using Kolmogorov-Arnold Network (KAN)-Integrated
Candidate Cell State in LSTM for Li batteries Health Monitoring. This hybrid
approach combines the ability of LSTM to learn long-term dependencies for
accurate time series predictions with KAN's non-linear approximation
capabilities to effectively capture complex degradation behaviors in Lithium
batteries.

</details>


### [253] [Exploring Multi-view Symbolic Regression methods in physical sciences](https://arxiv.org/abs/2509.10500)
*Etienne Russeil,Fabrício Olivetti de França,Konstantin Malanchev,Guillaume Moinard,Maxime Cherrey*

Main category: cs.LG

TL;DR: Multiview symbolic regression (MvSR) across several implementations effectively models multiple datasets with sparse, interpretable equations; results support strong performance and guide future development.


<details>
  <summary>Details</summary>
Motivation: Automate discovery of interpretable equations and mitigate overfitting/data scarcity by extending SR to multi-view data.

Method: Empirical comparison of MvSR implementations (Operon, PySR, phy-SO, eggp) on real-world datasets, evaluating accuracy and sparsity (few free parameters) and identifying features that promote better models.

Result: All tested implementations frequently achieve good accuracy with few free parameters; some features enable more frequent discovery of better models.

Conclusion: MvSR is a viable approach across multiple tools, and the paper offers guidelines for improving future MvSR developments.

Abstract: Describing the world behavior through mathematical functions help scientists
to achieve a better understanding of the inner mechanisms of different
phenomena. Traditionally, this is done by deriving new equations from first
principles and careful observations. A modern alternative is to automate part
of this process with symbolic regression (SR). The SR algorithms search for a
function that adequately fits the observed data while trying to enforce
sparsity, in the hopes of generating an interpretable equation. A particularly
interesting extension to these algorithms is the Multi-view Symbolic Regression
(MvSR). It searches for a parametric function capable of describing multiple
datasets generated by the same phenomena, which helps to mitigate the common
problems of overfitting and data scarcity. Recently, multiple implementations
added support to MvSR with small differences between them. In this paper, we
test and compare MvSR as supported in Operon, PySR, phy-SO, and eggp, in
different real-world datasets. We show that they all often achieve good
accuracy while proposing solutions with only few free parameters. However, we
find that certain features enable a more frequent generation of better models.
We conclude by providing guidelines for future MvSR developments.

</details>


### [254] [From Noise to Precision: A Diffusion-Driven Approach to Zero-Inflated Precipitation Prediction](https://arxiv.org/abs/2509.10501)
*Wentao Gao,Jiuyong Li,Lin Liu,Thuc Duy Le,Xiongren Chen,Xiaojing Du,Jixue Liu,Yanchang Zhao,Yun Chen*

Main category: cs.LG

TL;DR: Proposes Zero Inflation Diffusion Framework (ZIDF) to handle zero-inflated precipitation data by combining Gaussian perturbation, Transformer-based temporal modeling, and diffusion denoising; achieves large gains over Non-stationary Transformer on SA observations and synthetic data, suggesting robust handling of sparse zero-inflated time series with potential cross-domain applicability.


<details>
  <summary>Details</summary>
Motivation: Zero-inflated data pose major challenges in precipitation forecasting due to many zeros; existing models struggle to capture sparse non-zero events.

Method: ZIDF integrates Gaussian perturbation to smooth zero-inflated distributions, Transformer-based temporal prediction to model temporal patterns, and diffusion-based denoising to recover the original data structure. Evaluated on real South Australia precipitation data and synthetic zero-inflated data.

Result: Significant improvements over state-of-the-art models, with up to 56.7% reduction in MSE and 21.1% reduction in MAE relative to a Non-stationary Transformer baseline.

Conclusion: ZIDF robustly handles sparse time series data with zero inflation and shows potential for generalization to other domains facing zero-inflation challenges.

Abstract: Zero-inflated data pose significant challenges in precipitation forecasting
due to the predominance of zeros with sparse non-zero events. To address this,
we propose the Zero Inflation Diffusion Framework (ZIDF), which integrates
Gaussian perturbation for smoothing zero-inflated distributions,
Transformer-based prediction for capturing temporal patterns, and
diffusion-based denoising to restore the original data structure. In our
experiments, we use observational precipitation data collected from South
Australia along with synthetically generated zero-inflated data. Results show
that ZIDF demonstrates significant performance improvements over multiple
state-of-the-art precipitation forecasting models, achieving up to 56.7\%
reduction in MSE and 21.1\% reduction in MAE relative to the baseline
Non-stationary Transformer. These findings highlight ZIDF's ability to robustly
handle sparse time series data and suggest its potential generalizability to
other domains where zero inflation is a key challenge.

</details>


### [255] [FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free](https://arxiv.org/abs/2509.10503)
*Haolin Yuan,Jingtao Li,Weiming Zhuang,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: FEDEXCHANGE introduces a server-side dynamic model exchange for Federated Object Detection, enabling domain transfer without extra local computation; clustering-based model exchange during dedicated rounds improves cross-domain generalization, achieving 1.6x mAP in rainy domains while using 0.8x computation.


<details>
  <summary>Details</summary>
Motivation: Cross-domain generalization in FOD is hindered by domain shifts and hardware constraints; existing federated approaches add local compute overhead, limiting real-world deployment.

Method: Server alternates between model aggregation and exchange rounds. In exchange rounds, it clusters local models by similarity and exchanges models accordingly; all exchanges occur on the server, so clients incur no extra local computational cost.

Result: Empirical evaluation shows improved cross-domain performance, notably 1.6x mean Average Precision in challenging domains (e.g., rain), with only 0.8x the computational resources compared to baseline methods.

Conclusion: A server-side model exchange strategy can effectively bridge domain gaps in Federated Object Detection without increasing client-side computation, enhancing cross-domain robustness with lower resource demands.

Abstract: Federated Object Detection (FOD) enables clients to collaboratively train a
global object detection model without accessing their local data from diverse
domains. However, significant variations in environment, weather, and other
domain specific factors hinder performance, making cross domain generalization
a key challenge. Existing FOD methods often overlook the hardware constraints
of edge devices and introduce local training regularizations that incur high
computational costs, limiting real-world applicability. In this paper, we
propose FEDEXCHANGE, a novel FOD framework that bridges domain gaps without
introducing additional local computational overhead. FEDEXCHANGE employs a
server side dynamic model exchange strategy that enables each client to gain
insights from other clients' domain data without direct data sharing.
Specifically, FEDEXCHANGE allows the server to alternate between model
aggregation and model exchange. During aggregation rounds, the server
aggregates all local models as usual. In exchange rounds, FEDEXCHANGE clusters
and exchanges local models based on distance measures, allowing local models to
learn from a variety of domains. As all operations are performed on the server
side, clients can achieve improved cross domain utility without any additional
computational overhead. Extensive evaluations demonstrate that FEDEXCHANGE
enhances FOD performance, achieving 1.6X better mean average precision in
challenging domains, such as rainy conditions, while requiring only 0.8X the
computational resources compared to baseline methods.

</details>


### [256] [Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs](https://arxiv.org/abs/2509.10504)
*Mianchu Wang,Giovanni Montana*

Main category: cs.LG

TL;DR: Recasts retrosynthesis as worst-path optimization in a tree-structured MD P; InterRetro learns a value function for worst-path outcomes and uses self-imitation to reinforce high-advantage decisions; achieves state-of-the-art results on Retro*-190 with high data efficiency.


<details>
  <summary>Details</summary>
Motivation: Current retrosynthesis methods optimize average branch performance, which risks poor performance on the weakest (worst) branch leaves the plan vulnerable. A worst-path objective with monotonic guarantees can yield more robust, dependable routes.

Method: Formulates retrosynthesis as a tree-structured MDP and proves a unique optimal solution with monotonic improvement. Introduces Interactive Retrosynthesis Planning (InterRetro) that learns a value function for worst-path outcomes and uses self-imitation to reinforce past decisions with high estimated advantage.

Result: Empirically strong performance: solves 100% of targets on Retro*-190, reduces synthetic route length by about 4.9%, and demonstrates notable data efficiency by achieving competitive results with only 10% of the training data.

Conclusion: Worst-path optimization in tree MDPs provides robust guarantees for retrosynthesis planning. InterRetro offers a principled, data-efficient approach with state-of-the-art results, marking a significant advance in computational retrosynthesis.

Abstract: Retrosynthesis planning aims to decompose target molecules into available
building blocks, forming a synthesis tree where each internal node represents
an intermediate compound and each leaf ideally corresponds to a purchasable
reactant. However, this tree becomes invalid if any leaf node is not a valid
building block, making the planning process vulnerable to the "weakest link" in
the synthetic route. Existing methods often optimise for average performance
across branches, failing to account for this worst-case sensitivity. In this
paper, we reframe retrosynthesis as a worst-path optimisation problem within
tree-structured Markov Decision Processes (MDPs). We prove that this
formulation admits a unique optimal solution and offers monotonic improvement
guarantees. Building on this insight, we introduce Interactive Retrosynthesis
Planning (InterRetro), a method that interacts with the tree MDP, learns a
value function for worst-path outcomes, and improves its policy through
self-imitation, preferentially reinforcing past decisions with high estimated
advantage. Empirically, InterRetro achieves state-of-the-art results, solving
100% of targets on the Retro*-190 benchmark, shortening synthetic routes by
4.9%, and achieving promising performance using only 10% of the training data -
representing a significant advance in computational retrosynthesis planning.

</details>


### [257] [AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective](https://arxiv.org/abs/2509.10506)
*Muxin Ge,Hanyu Ma,Yiyang Wu,Xiaoli Ma,Yadi Liu,Ye Aung Moe,Weizheng Xie*

Main category: cs.LG

TL;DR: AttnBoost integrates feature-level attention into gradient boosting to improve predictive accuracy and interpretability in retail demand forecasting, by adaptively adjusting feature importance during boosting rounds.


<details>
  <summary>Details</summary>
Motivation: Forecasting in retail involves noisy, heterogeneous features and rapidly shifting consumer behavior; traditional GBDT lacks adaptive mechanisms to identify and emphasize the most relevant features under changing conditions, limiting performance and explainability.

Method: Introduce a lightweight attention mechanism that dynamically updates feature importance at each boosting round, enabling focus on high-impact variables (e.g., promotions, pricing, seasonality); evaluate on a large-scale retail dataset; include ablation studies to assess the attention module.

Result: AttnBoost outperforms standard ML and deep tabular models on the retail dataset; provides actionable insights for supply chain managers; ablation confirms the attention module helps mitigate overfitting and improves interpretability.

Conclusion: Attention-guided boosting appears to be a promising direction for interpretable, scalable AI in real-world forecasting applications.

Abstract: Forecasting product demand in retail supply chains presents a complex
challenge due to noisy, heterogeneous features and rapidly shifting consumer
behavior. While traditional gradient boosting decision trees (GBDT) offer
strong predictive performance on structured data, they often lack adaptive
mechanisms to identify and emphasize the most relevant features under changing
conditions. In this work, we propose AttnBoost, an interpretable learning
framework that integrates feature-level attention into the boosting process to
enhance both predictive accuracy and explainability. Specifically, the model
dynamically adjusts feature importance during each boosting round via a
lightweight attention mechanism, allowing it to focus on high-impact variables
such as promotions, pricing, and seasonal trends. We evaluate AttnBoost on a
large-scale retail sales dataset and demonstrate that it outperforms standard
machine learning and deep tabular models, while also providing actionable
insights for supply chain managers. An ablation study confirms the utility of
the attention module in mitigating overfitting and improving interpretability.
Our results suggest that attention-guided boosting represents a promising
direction for interpretable and scalable AI in real-world forecasting
applications.

</details>


### [258] [The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback](https://arxiv.org/abs/2509.10509)
*Sai Teja Reddy Adapala*

Main category: cs.LG

TL;DR: Selective feedback during recursive training of a high-dimensional LLM reverses expected degradation, yielding performance gains (Anti-Ouroboros Effect) on complex summarization; five generations show +6.6% ROUGE-L F1 with quality-filtered feedback, while unfiltered or random-filtering degrade.


<details>
  <summary>Details</summary>
Motivation: Address the stability and safety risks of recursive self-training in LLMs, challenging the classic model-collapse narrative by exploring whether simple selection pressure can induce resilience and improve performance.

Method: Implement a selective feedback mechanism during recursive training on a Gemma 2B model and evaluate on a complex summarization task. Compare five generations across conditions: quality-filtered feedback (best performance), unfiltered feedback, and random-filter, plus a simple classifier as a degenerative control to contrast high-dimensional dynamics.

Result: In five generations, the quality-filtered condition improved ROUGE-L F1 by 6.6%; unfiltered control degraded by 3.5%; random-filter degraded by 4.2%. The Anti-Ouroboros Effect emerged in the LLM, while the simple classifier validated the degenerative loop under standard settings.

Conclusion: Systemic resilience can emerge as an effect of simple selection pressure in high-dimensional models, offering a scalable principle for safer and more robust AI through selective feedback mechanisms.

Abstract: The stability of recursively trained large language models (LLMs) is a
foundational problem for AI safety. Prevailing theory predicts model collapse,
a progressive degradation when models are trained on their own output. We
challenge this narrative by introducing a selective feedback mechanism.
Contrary to expectation, instead of merely slowing decay, our experiments
provide strong evidence that this pressure reverses it, inducing a
statistically significant performance improvement in a Gemma 2B model on a
complex summarization task. We name this phenomenon the Anti-Ouroboros Effect.
We contrast this with a foundational experiment using a simple classifier,
where the theoretical degenerative loop was validated, highlighting the unique
dynamics of high-dimensional models. Our findings establish that systemic
resilience can be an emergent property of LLMs under simple selection pressure,
suggesting a powerful and scalable principle for developing safer and more
robust AI systems. Across five generations, a quality-filtered condition
improved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by
3.5% and a random-filter control degraded by 4.2%

</details>


### [259] [LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs](https://arxiv.org/abs/2509.10511)
*Umberto Gonçalves de Sousa*

Main category: cs.LG

TL;DR: LogGuardQ is a cognitive-inspired, dual-memory, adaptive RL framework that outperforms DQN and PPO on a simulated log anomaly detection task, offering improved detection stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: RL struggles with exploration, stability, and adaptability in dynamic/cyber environments; there is a need for cognitive-inspired memory and adaptive exploration to improve performance.

Method: Proposes LogGuardQ with a dual-memory architecture and adaptive exploration via temperature decay and curiosity; evaluated on 1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes; compared against DQN and PPO; uses Savitzky-Golay smoothing; Mann-Whitney U tests to assess significance.

Result: LogGuardQ achieves 96.0% detection rate (vs 93.0% DQN, 47.1% PPO); precision 0.4776, recall 0.9996, F1 0.6450; mean reward 20.34±44.63 vs 18.80±43.98 (DQN) and -0.17±23.79 (PPO); average 5.0 steps/episode; statistical tests show significant advantages (p=0.0002 vs DQN; p<0.0001 vs PPO; DQN vs PPO p<0.0001 with small to medium effects).

Conclusion: By bridging cognitive science and RL, LogGuardQ offers a scalable approach to adaptive learning in uncertain environments, with potential applications in cybersecurity, intrusion detection, and decision-making under uncertainty.

Abstract: Reinforcement learning (RL) has transformed sequential decision-making, but
traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy
Optimization (PPO) often struggle with efficient exploration, stability, and
adaptability in dynamic environments. This study presents LogGuardQ (Adaptive
Log Guard with Cognitive enhancement), a novel framework that integrates a
dual-memory system inspired by human cognition and adaptive exploration
strategies driven by temperature decay and curiosity. Evaluated on a dataset of
1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes,
LogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for
PPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450.
The mean reward is 20.34 \pm 44.63 across all episodes (versus 18.80 \pm 43.98
for DQN and -0.17 \pm 23.79 for PPO), with an average of 5.0 steps per episode
(constant across models). Graphical analyses, including learning curves
smoothed with a Savgol filter (window=501, polynomial=2), variance trends,
action distributions, and cumulative detections, demonstrate LogGuardQ's
superior stability and efficiency. Statistical tests (Mann-Whitney U) confirm
significant performance advantages (e.g., p = 0.0002 vs. DQN with negligible
effect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN
vs. PPO with small effect size). By bridging cognitive science and RL,
LogGuardQ offers a scalable approach to adaptive learning in uncertain
environments, with potential applications in cybersecurity, intrusion
detection, and decision-making under uncertainty.

</details>


### [260] [A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2509.10512)
*Jiaxing Cao,Yuzhou Gao,Jiwei Huang*

Main category: cs.LG

TL;DR: An adaptive incentive framework for federated learning that combines a Stackelberg game (task publisher as leader, local model owners as followers) with a DRL-based multi-agent MDP for LMO–worker interactions, plus an ASOSA algorithm to stabilize strategies; validated by experiments.


<details>
  <summary>Details</summary>
Motivation: Federated learning suffers from limited data, so an adaptive incentive mechanism is needed to recruit data-gathering workers while maximizing the utilities of the task publisher, local model owners, and workers.

Method: 1) Model the TP-LMO interaction as a Stackelberg game and derive an analytical Nash equilibrium to maximize utilities. 2) Model LMO–worker interactions as a multi-agent MDP and solve for optimal strategies via deep reinforcement learning. 3) Introduce ASOSA (Adaptively Searching the Optimal Strategy Algorithm) to stabilize strategies and resolve coupling issues.

Result: Derives an analytical equilibrium for TP and LMOs; DRL yields optimal strategies for LMO–worker coordination; ASOSA stabilizes the strategy dynamics and addresses coupling problems. Extensive experiments validate the proposed method.

Conclusion: The proposed adaptive incentive framework effectively aligns incentives among TP, LMOs, and workers in FL and demonstrates improved performance and stability in data-gathering scenarios.

Abstract: Recently, federated learning (FL) has emerged as a novel framework for
distributed model training. In FL, the task publisher (TP) releases tasks, and
local model owners (LMOs) use their local data to train models. Sometimes, FL
suffers from the lack of training data, and thus workers are recruited for
gathering data. To this end, this paper proposes an adaptive incentive
mechanism from a service-oriented perspective, with the objective of maximizing
the utilities of TP, LMOs and workers. Specifically, a Stackelberg game is
theoretically established between the LMOs and TP, positioning TP as the leader
and the LMOs as followers. An analytical Nash equilibrium solution is derived
to maximize their utilities. The interaction between LMOs and workers is
formulated by a multi-agent Markov decision process (MAMDP), with the optimal
strategy identified via deep reinforcement learning (DRL). Additionally, an
Adaptively Searching the Optimal Strategy Algorithm (ASOSA) is designed to
stabilize the strategies of each participant and solve the coupling problems.
Extensive numerical experiments are conducted to validate the efficacy of the
proposed method.

</details>


### [261] [Mixture-of-Clustered-Experts: Advancing Expert Specialization and Generalization in Instruction Tuning](https://arxiv.org/abs/2509.10513)
*Sugyeong Eo,Jungjun Lee,Chanjun Park,Heuiseok Lim*

Main category: cs.LG

TL;DR: MoCE is a two-stage routing sparse Mixture-of-Experts that first partitions inputs into expert groups using sequence-level features and then activates the top-k token-level experts within the relevant group, improving specialization and generalization in instruction-tuning with heterogeneous inputs.


<details>
  <summary>Details</summary>
Motivation: Instruction-tuning scenarios exhibit significant input heterogeneity, which challenges conventional sparse MoE models to specialize experts effectively, limiting performance and generalization.

Method: Introduce Mixture-of-Clustered-Experts (MoCE) with a dual-stage routing mechanism: (1) sequence-level expert group routing to form specialized groups based on input characteristics, (2) within each group, activate the top-k token-level experts for each token, preserving token-level routing benefits while leveraging group specialization.

Result: Empirical evaluation across diverse benchmarks shows MoCE consistently outperforms strong baselines and exhibits enhanced generalization and robustness, with analyses highlighting its effective partitioning of heterogeneous inputs into knowledge-aware expert groups.

Conclusion: MoCE provides a robust mechanism to improve expert specialization in MoE for heterogeneous instruction-tuned inputs by combining group-level routing with token-level selection, maintaining efficiency and scalability while boosting performance.

Abstract: A sparse Mixture-of-Experts (MoE) architecture has emerged as a highly
scalable solution by conditionally activating sub-modules without a
proportional increase in computational costs. However, improving expert
specialization to enhance performance and generalization remains a challenge
for MoE, especially in instruction tuning scenarios characterized by
significant input heterogeneity. In this work, we propose the
Mixture-of-Clustered-Experts (MoCE) to address this limitation through a
dual-stage routing mechanism. The first stage in the mechanism performs expert
group routing based on sequence-level features, while the second stage
activates the top-$k$ experts within the group at the token level. This
approach enables the effective partitioning of heterogeneous inputs based on
their knowledge requirements, encouraging expert group specialization while
maintaining the advantages of token-level routing. We evaluate MoCE across a
comprehensive set of benchmarks, demonstrating its consistent superiority over
strong baselines and its enhanced generalization capabilities. Detailed
analysis further highlights the robustness and effectiveness of MoCE.

</details>


### [262] [A Differential Manifold Perspective and Universality Analysis of Continuous Attractors in Artificial Neural Networks](https://arxiv.org/abs/2509.10514)
*Shaoxin Tian,Hongkai Liu,Yuying Yang,Jiali Yu,Zizheng Miao,Xuming Huang,Zhishuai Liu,Zhang Yi*

Main category: cs.LG

TL;DR: A differential-manifold framework for continuous attractors in neural networks, linking attractor behavior to Jacobian eigenvalues and singular values, with claimed universality across models and data.


<details>
  <summary>Details</summary>
Motivation: There is no unified framework to analyze continuous attractors across diverse neural architectures, hindering cross-architectural insights and general theory.

Method: Develop a differential-manifold based framework; analyze how continuous attractor phenomena relate to the eigenvalues of the local Jacobian; demonstrate universality of singular value stratification in common classification models and datasets across architectures and tasks.

Result: The framework is compatible with prior conclusions, clarifies links between attractors and Jacobian eigenvalues, and suggests continuous attractors may be ubiquitous in general neural networks via singular value stratification.

Conclusion: The proposed manifold-based framework offers a foundation for a general theory of continuous attractors in neural networks, supported by the mathematical link between eigenvalues and singular values and the observed universality across models and data.

Abstract: Continuous attractors are critical for information processing in both
biological and artificial neural systems, with implications for spatial
navigation, memory, and deep learning optimization. However, existing research
lacks a unified framework to analyze their properties across diverse dynamical
systems, limiting cross-architectural generalizability. This study establishes
a novel framework from the perspective of differential manifolds to investigate
continuous attractors in artificial neural networks. It verifies compatibility
with prior conclusions, elucidates links between continuous attractor phenomena
and eigenvalues of the local Jacobian matrix, and demonstrates the universality
of singular value stratification in common classification models and datasets.
These findings suggest continuous attractors may be ubiquitous in general
neural networks, highlighting the need for a general theory, with the proposed
framework offering a promising foundation given the close mathematical
connection between eigenvalues and singular values.

</details>


### [263] [Adaptive Preference Optimization with Uncertainty-aware Utility Anchor](https://arxiv.org/abs/2509.10515)
*Xiaobo Wang,Zixia Jia,Jiaqi Li,Qi Liu,Zilong Zheng*

Main category: cs.LG

TL;DR: Presents Adaptive Preference Optimization with Utility Anchor (UAPO), a framework that uses a utility anchor to model annotation uncertainty in offline preference data, enabling training without paired data and achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: BT-based reward modeling relies on restrictive assumptions (pairwise data, distribution shifts, human rationality). There is a need for a general, robust, data-efficient offline preference optimization framework that can handle unpaired data.

Method: Proposes Offline preference optimization framework UAPO with an anchoring function estimating annotation uncertainty; supports training with unpaired data; anchor design improves training robustness.

Result: Empirical results show competitive performance without strict data pairing and enhanced data utilization efficiency.

Conclusion: UAPO offers flexible and effective preference optimization, reducing dependence on data pairing and enabling more robust and efficient use of preference data.

Abstract: Offline preference optimization methods are efficient for large language
models (LLMs) alignment. Direct Preference optimization (DPO)-like learning,
one of the most popular approaches, stands out for its efficiency in reward
modeling. However, these methods typically follow the convention to use
Bradley-Terry (BT) reward modeling that faces several critical assumptions,
including the requirement for pairwise training data, model distribution
shifting, human rationality assumption, etc. To address these limitations, we
propose a general framework for offline preference optimization methods,
Adaptive Preference Optimization with Utility Anchor (UAPO), which introduces
an anchoring function to estimate the uncertainties brought from preference
data annotation. Our method enables training even in scenarios where the data
is unpaired, significantly enhancing data utilization efficiency. Moreover, the
anchor design makes UAPO more robust in the training process. Experimental
results demonstrate that UAPO achieves competitive outcomes without the strict
dependency on data pairing, paving the way for more flexible and effective
preference optimization methods.

</details>


### [264] [Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction](https://arxiv.org/abs/2509.10516)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: Federated learning with FedProx delivers privacy-preserving educational recommendations with competitive performance (76.28% F1, ~83% of centralized XGBoost), outperforming FedAvg in heterogeneous data.


<details>
  <summary>Details</summary>
Motivation: To address student data privacy in educational recommender systems, which struggle with centralized data approaches under modern data protection regulations.

Method: Build a Deep Neural Network with engineered features on the ASSISTments dataset. Conduct a comparative analysis of federated aggregation strategies, showing FedProx as more stable/effective than FedAvg for heterogeneous data. Evaluate performance against a centralized XGBoost baseline.

Result: FedProx outperforms FedAvg under data heterogeneity and achieves 76.28% F1, corresponding to 82.85% of the centralized XGBoost model’s performance.

Conclusion: A federated approach offers a viable and robust solution to the personalization-privacy dilemma in modern educational platforms, enabling effective content recommendations without centralizing sensitive student data.

Abstract: The increasing digitalization of education presents unprecedented
opportunities for data-driven personalization, yet it introduces significant
student data privacy challenges. Conventional recommender systems rely on
centralized data, a paradigm often incompatible with modern data protection
regulations. A novel privacy-preserving recommender system is proposed and
evaluated to address this critical issue using Federated Learning (FL). The
approach utilizes a Deep Neural Network (DNN) with rich, engineered features
from the large-scale ASSISTments educational dataset. A rigorous comparative
analysis of federated aggregation strategies was conducted, identifying FedProx
as a significantly more stable and effective method for handling heterogeneous
student data than the standard FedAvg baseline. The optimized federated model
achieves a high-performance F1-Score of 76.28\%, corresponding to 82.85\% of
the performance of a powerful, centralized XGBoost model. These findings
validate that a federated approach can provide highly effective content
recommendations without centralizing sensitive student data. Consequently, our
work presents a viable and robust solution to the personalization-privacy
dilemma in modern educational platforms.

</details>


### [265] [A Comparative Benchmark of Federated Learning Strategies for Mortality Prediction on Heterogeneous and Imbalanced Clinical Data](https://arxiv.org/abs/2509.10517)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: Federated learning can improve mortality prediction under non-IID and imbalanced clinical data; FedProx outperforms other FL strategies (FedAvg, FedAdagrad, FedAdam, FedCluster) in F1-score and convergence on MIMIC-IV, with FedAvg being most efficient but least accurate.


<details>
  <summary>Details</summary>
Motivation: To enable privacy-preserving, generalizable mortality prediction using FL in real-world clinical data, where data are heterogeneously distributed and imbalanced, requiring evaluation of FL strategy robustness.

Method: Benchmark five FL strategies (FedAvg, FedProx, FedAdagrad, FedAdam, FedCluster) on the MIMIC-IV dataset. Create non-IID client partitions by clinical care unit. Address class imbalance with SMOTE-Tomek on each client. Run over 50 communication rounds.

Result: FedProx achieved the highest F1-score (0.8831) and stable convergence across rounds, outperforming other strategies. FedAvg was most computationally efficient but had notably lower predictive performance. Regularization-based FL (FedProx) demonstrated robustness under heterogeneity and imbalance compared to server-side adaptive methods.

Conclusion: Regularization-based FL algorithms like FedProx are more robust and effective for heterogeneous and imbalanced clinical prediction tasks than standard FedAvg or other adaptive schemes; this work provides empirical guidance for selecting FL strategies in real-world healthcare applications.

Abstract: Machine learning models hold significant potential for predicting in-hospital
mortality, yet data privacy constraints and the statistical heterogeneity of
real-world clinical data often hamper their development. Federated Learning
(FL) offers a privacy-preserving solution, but its performance under
non-Independent and Identically Distributed (non-IID) and imbalanced conditions
requires rigorous investigation. The study presents a comparative benchmark of
five federated learning strategies: FedAvg, FedProx, FedAdagrad, FedAdam, and
FedCluster for mortality prediction. Using the large-scale MIMIC-IV dataset, we
simulate a realistic non-IID environment by partitioning data by clinical care
unit. To address the inherent class imbalance of the task, the SMOTE-Tomek
technique is applied to each client's local training data. Our experiments,
conducted over 50 communication rounds, reveal that the regularization-based
strategy, FedProx, consistently outperformed other methods, achieving the
highest F1-Score of 0.8831 while maintaining stable convergence. While the
baseline FedAvg was the most computationally efficient, its predictive
performance was substantially lower. Our findings indicate that
regularization-based FL algorithms like FedProx offer a more robust and
effective solution for heterogeneous and imbalanced clinical prediction tasks
than standard or server-side adaptive aggregation methods. The work provides a
crucial empirical benchmark for selecting appropriate FL strategies for
real-world healthcare applications.

</details>


### [266] [Holographic Knowledge Manifolds: A Novel Pipeline for Continual Learning Without Catastrophic Forgetting in Large Language Models](https://arxiv.org/abs/2509.10518)
*Justin Arndt*

Main category: cs.LG

TL;DR: Lifelong, memory-efficient knowledge representation via HKM with 0% forgetting, 3x compression, and rapid updates.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting and unbounded memory growth in AI knowledge representations while maintaining efficiency.

Method: Four-phase Holographic Knowledge Manifold (HKM) pipeline using fractal quantization, probabilistic entanglement, and dynamic diffraction chipping; holographic integration; evaluation on WikiText+FB15k scaled to 2,997 nodes.

Result: 0% forgetting; 3x compression; 53% training-time reduction on consumer GPUs; supports >1,020 updates with ~1% growth; projected monetary/energy savings; open-source reproducibility.

Conclusion: This approach could enable eternal adaptation for large LMs and may open avenues with multimodal and quantum hardware; could dramatically cut fine-tuning costs.

Abstract: We introduce the Holographic Knowledge Manifold (HKM), a four-phase pipeline
that achieves zero catastrophic forgetting in AI knowledge representation while
maintaining minimal memory growth and high efficiency. Leveraging fractal
quantization, probabilistic entanglement, and dynamic diffraction chipping, HKM
compresses knowledge substrates by 3x with 67% storage savings, integrates
holographically at 100%, and supports over 1,020 updates with 1% growth per
increment. In experiments on combined WikiText and FB15k datasets (scaled to
2,997 nodes), we demonstrate industry-leading performance: 0% forgetting
(infinite improvement over GEM baselines), 3x compression, and 53% training
time reduction on consumer GPU hardware. Hypothetical cost analyses project
$92.4M savings over 5 years at petabyte scale, with 21.2% energy reduction and
33% lower carbon footprint. This work hypothesizes a paradigm shift for public
large language models (LLMs), enabling "eternal" adaptation without retraining.
Future extensions to multimodal fusion and quantum hardware could further
democratize scalable AI, potentially reducing fine-tuning costs by 60-80% for
models like Llama-3 or Grok-4. Code, datasets, and full results are publicly
available for reproducibility.

</details>


### [267] [Gradient Estimation Methods of Approximate Multipliers for High-Accuracy Retraining of Deep Learning Models](https://arxiv.org/abs/2509.10519)
*Chang Meng,Wayne Burleson,Giovanni De Micheli*

Main category: cs.LG

TL;DR: This work introduces two gradient-estimation methods for approximate multipliers (AppMults) to improve retraining accuracy: LUT-2D and LUT-1D. LUT-2D uses two-dimensional lookup tables for fine-grained gradient estimation, achieving the highest retraining accuracy; LUT-1D uses one-dimensional LUTs for a compact, efficient alternative with comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Approximate multipliers reduce hardware cost but introduce arithmetic errors, requiring retraining. Traditionally, gradients are derived from accurate multipliers, which can degrade retraining performance. More precise AppMult gradients are needed to better recover accuracy after deployment.

Method: Two approaches: LUT-2D, which characterizes the AppMult gradient with 2D LUTs for fine-grained estimation, and LUT-1D, a compact variant using 1D LUTs to store gradient values for efficiency. Both aim to provide more accurate gradients than using AccMult-derived gradients.

Result: CIFAR-10 with CNNs: LUT-2D improves retraining accuracy by 3.83% on average; LUT-1D by 3.72% on average. ImageNet with Vision Transformers: LUT-1D improves retraining accuracy by 23.69% on average over a state-of-the-art retraining framework.

Conclusion: Accurate AppMult gradient estimation via LUT-based approaches substantially boosts retraining performance. LUT-2D offers the highest accuracy at potentially higher cost, while LUT-1D provides a favorable accuracy-computation trade-off, making it practical for large-scale models.

Abstract: Approximate multipliers (AppMults) are widely used in deep learning
accelerators to reduce their area, delay, and power consumption. However,
AppMults introduce arithmetic errors into deep learning models, necessitating a
retraining process to recover accuracy. A key step in retraining is computing
the gradient of the AppMult, i.e., the partial derivative of the approximate
product with respect to each input operand. Existing approaches typically
estimate this gradient using that of the accurate multiplier (AccMult), which
can lead to suboptimal retraining results. To address this, we propose two
methods to obtain more precise gradients of AppMults. The first, called LUT-2D,
characterizes the AppMult gradient with 2-dimensional lookup tables (LUTs),
providing fine-grained estimation and achieving the highest retraining
accuracy. The second, called LUT-1D, is a compact and more efficient variant
that stores gradient values in 1-dimensional LUTs, achieving comparable
retraining accuracy with shorter runtime. Experimental results show that on
CIFAR-10 with convolutional neural networks, our LUT-2D and LUT-1D methods
improve retraining accuracy by 3.83% and 3.72% on average, respectively. On
ImageNet with vision transformer models, our LUT-1D method improves retraining
accuracy by 23.69% on average, compared to a state-of-the-art retraining
framework.

</details>


### [268] [Offline Contextual Bandit with Counterfactual Sample Identification](https://arxiv.org/abs/2509.10520)
*Alexandre Gilotte,Otmane Sakhi,Imad Aouali,Benjamin Heymann*

Main category: cs.LG

TL;DR: Counterfactual Sample Identification reframes contextual bandits to predict which action leads to a successful binary outcome by comparing to a counterfactual action drawn from the logging policy for the same context, reducing confounding between action and context.


<details>
  <summary>Details</summary>
Motivation: Direct reward models that take both action and context can be confounded, making it hard to isolate the causal effect of the action on reward. A counterfactual approach aims to identify the winning action without relying on magnitude of reward.

Method: Train a classifier to recognize which action led to a binary success by contrasting the observed action with a counterfactual action sampled from the logging policy under the same context, instead of predicting reward magnitudes. The method is theoretically grounded in counterfactual reasoning and causal identification.

Result: Empirically, the approach consistently outperforms direct reward models in both synthetic experiments and real-world deployments.

Conclusion: Counterfactual Sample Identification offers a theoretically justified and empirically effective alternative to direct reward modeling in contextual bandits, mitigating confounding and improving action selection.

Abstract: In production systems, contextual bandit approaches often rely on direct
reward models that take both action and context as input. However, these models
can suffer from confounding, making it difficult to isolate the effect of the
action from that of the context. We present \emph{Counterfactual Sample
Identification}, a new approach that re-frames the problem: rather than
predicting reward, it learns to recognize which action led to a successful
(binary) outcome by comparing it to a counterfactual action sampled from the
logging policy under the same context. The method is theoretically grounded and
consistently outperforms direct models in both synthetic experiments and
real-world deployments.

</details>


### [269] [Variational Gaussian Mixture Manifold Models for Client-Specific Federated Personalization](https://arxiv.org/abs/2509.10521)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Sajedul Talukder*

Main category: cs.LG

TL;DR: Geometry-centric personalized federated learning (VGM^2) uses client-specific UMAP embeddings and Dirichlet-NIG variational markers to robustly handle label skew and non-stationarity, enabling efficient secure communication and competitive F1 on non-IID vision tasks.


<details>
  <summary>Details</summary>
Motivation: Global single-parameter models struggle under label skew and non-stationarity; need to capture client-specific geometry. Propose a variational, geometry-aware framework that exchanges only statistical summaries and preserves privacy while stabilizing learning across heterogeneous clients.

Method: Each client learns a parametric UMAP embedding and maintains a Dir-NIG posterior over latent pairwise distance markers (same/different class). The server aggregates via conjugate moment matching to form global priors; a calibration term maps distance to similarity. Communication: only variational marker statistics; security via secure aggregation and optional DP noise.

Result: Evaluated on eight vision datasets with non-IID label shards; VGM^2 achieves competitive or superior test F1 compared to strong baselines while transmitting small geometry summaries; privacy strengthened; includes membership-inference stress test; code to be released.

Conclusion: Aggregation minimizes summed reverse KL divergence from client posteriors within the conjugate family, yielding stability under heterogeneity and enabling robust, privacy-preserving, geometry-aware PFL.

Abstract: Personalized federated learning (PFL) often fails under label skew and
non-stationarity because a single global parameterization ignores
client-specific geometry. We introduce VGM$^2$ (Variational Gaussian Mixture
Manifold), a geometry-centric PFL framework that (i) learns client-specific
parametric UMAP embeddings, (ii) models latent pairwise distances with mixture
relation markers for same and different class pairs, and (iii) exchanges only
variational, uncertainty-aware marker statistics. Each client maintains a
Dirichlet-Normal-Inverse-Gamma (Dir-NIG) posterior over marker weights, means,
and variances; the server aggregates via conjugate moment matching to form
global priors that guide subsequent rounds. We prove that this aggregation
minimizes the summed reverse Kullback-Leibler divergence from client posteriors
within the conjugate family, yielding stability under heterogeneity. We further
incorporate a calibration term for distance-to-similarity mapping and report
communication and compute budgets. Across eight vision datasets with non-IID
label shards, VGM$^2$ achieves competitive or superior test F1 scores compared
to strong baselines while communicating only small geometry summaries. Privacy
is strengthened through secure aggregation and optional differential privacy
noise, and we provide a membership-inference stress test. Code and
configurations will be released to ensure full reproducibility.

</details>


### [270] [Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction](https://arxiv.org/abs/2509.10522)
*Kaizhen Tan*

Main category: cs.LG

TL;DR: Multimodal CNN-Transformer model fuses structured data, trajectory sequences, and image features to predict ATCO command timing and duration, using a dataset with maneuver points detected via sliding window and histogram-based methods, enabling improved workload estimation and intelligent command generation.


<details>
  <summary>Details</summary>
Motivation: Accurate workload modeling for air traffic controllers in dense airspace is critical for safety and efficiency. The work addresses the gap in multimodal integration and interpretability, aiming to link trajectories with voice commands to support workload assessment, staffing, and scheduling.

Method: Constructed a high-quality dataset with maneuver points detected using sliding window and histogram-based methods. Developed a CNN-Transformer ensemble that fuses structured data, trajectory sequences, and image features to predict the time offset between a command and the resulting maneuver, and the command duration, with claims of accuracy, generalizability and interpretability.

Result: The model provides accurate, generalizable, and interpretable predictions for command timing and duration; it is claimed to be the first model to link trajectories to voice commands, with practical value for workload assessment, staffing, and scheduling.

Conclusion: The work offers a novel multimodal approach to ATCO workload modeling and intelligent command generation, with potential operational benefits in workload assessment, staffing, and scheduling.

Abstract: Air traffic controllers (ATCOs) issue high-intensity voice commands in dense
airspace, where accurate workload modeling is critical for safety and
efficiency. This paper proposes a multimodal deep learning framework that
integrates structured data, trajectory sequences, and image features to
estimate two key parameters in the ATCO command lifecycle: the time offset
between a command and the resulting aircraft maneuver, and the command
duration. A high-quality dataset was constructed, with maneuver points detected
using sliding window and histogram-based methods. A CNN-Transformer ensemble
model was developed for accurate, generalizable, and interpretable predictions.
By linking trajectories to voice commands, this work offers the first model of
its kind to support intelligent command generation and provides practical value
for workload assessment, staffing, and scheduling.

</details>


### [271] [From Predictions to Explanations: Explainable AI for Autism Diagnosis and Identification of Critical Brain Regions](https://arxiv.org/abs/2509.10523)
*Kush Gupta,Amir Aly,Emmanuel Ifeachor,Rohit Shankar*

Main category: cs.LG

TL;DR: A two-module ASD diagnostic framework combining cross-domain transfer learning with explainable AI, addressing data scarcity and providing interpretable decisions that align with neurobiological evidence.


<details>
  <summary>Details</summary>
Motivation: ASD is neurodevelopmental with heterogeneous brain maturation and limited ASD-specific data. There is a need for data-efficient, interpretable ML models that can reveal neurobiological correlates.

Method: Two-module approach: (1) a deep learning classifier fine-tuned via cross-domain transfer learning to perform ASD classification; (2) three XAI techniques (saliency mapping, Grad-CAM, SHAP) to interpret model decisions and identify brain regions driving the predictions, with comparison to established neurobiological evidence.

Result: Cross-domain transfer learning helps mitigate data scarcity in ASD research. The XAI analyses reveal how the model makes decisions and identify brain regions associated with ASD; findings show strong alignment with known neurobiological evidence.

Conclusion: The framework offers a feasible, interpretable, and clinically relevant approach for ASD diagnosis, highlighting the value of combining cross-domain transfer learning with multiple XAI methods to obtain interpretable insights aligned with neurobiology.

Abstract: Autism spectrum disorder (ASD) is a neurodevelopmental condition
characterized by atypical brain maturation. However, the adaptation of transfer
learning paradigms in machine learning for ASD research remains notably
limited. In this study, we propose a computer-aided diagnostic framework with
two modules. This chapter presents a two-module framework combining deep
learning and explainable AI for ASD diagnosis. The first module leverages a
deep learning model fine-tuned through cross-domain transfer learning for ASD
classification. The second module focuses on interpreting the model decisions
and identifying critical brain regions. To achieve this, we employed three
explainable AI (XAI) techniques: saliency mapping, Gradient-weighted Class
Activation Mapping, and SHapley Additive exPlanations (SHAP) analysis. This
framework demonstrates that cross-domain transfer learning can effectively
address data scarcity in ASD research. In addition, by applying three
established explainability techniques, the approach reveals how the model makes
diagnostic decisions and identifies brain regions most associated with ASD.
These findings were compared against established neurobiological evidence,
highlighting strong alignment and reinforcing the clinical relevance of the
proposed approach.

</details>


### [272] [Resource-Aware Neural Network Pruning Using Graph-based Reinforcement Learning](https://arxiv.org/abs/2509.10526)
*Dieter Balemans,Thomas Huybrechts,Jan Steckel,Siegfried Mercelis*

Main category: cs.LG

TL;DR: A graph-based AutoML pruning framework that uses a Graph Attention Network to encode a full network graph into a rich embedding, with a binary action space and CMDP-based training, achieving state-of-the-art pruning on CIFAR-10/100 and ImageNet by learning data-driven, constraint-aware channel pruning beyond magnitude-based heuristics.


<details>
  <summary>Details</summary>
Motivation: Traditional pruning relies on hand-crafted heuristics and local optimization, often leading to suboptimal and inefficient pruning. A global, data-driven approach is sought to capture complete network topology and task-specific channel importance.

Method: Represent the target network as a graph capturing topological relationships between layers and channels. Use a Graph Attention Network (GAT) encoder to produce embeddings. Replace continuous pruning ratios with a fine-grained binary action space to learn channel importance directly from data. Model pruning as a Constrained Markov Decision Process (CMDP) with a self-competition reward that favors performance improvements while meeting resource constraints (e.g., target compression rates).

Result: Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet show the proposed method consistently outperforms traditional pruning techniques, achieving state-of-the-art results and learning task-specific pruning strategies that identify functionally redundant connections beyond simple weight magnitudes.

Conclusion: A global graph-based AutoML pruning framework effectively learns data-driven, constraint-aware pruning strategies that surpass heuristic methods, offering robust, task-specific channel pruning that respects resource constraints.

Abstract: This paper presents a novel approach to neural network pruning by integrating
a graph-based observation space into an AutoML framework to address the
limitations of existing methods. Traditional pruning approaches often depend on
hand-crafted heuristics and local optimization perspectives, which can lead to
suboptimal performance and inefficient pruning strategies. Our framework
transforms the pruning process by introducing a graph representation of the
target neural network that captures complete topological relationships between
layers and channels, replacing the limited layer-wise observation space with a
global view of network structure. The core innovations include a Graph
Attention Network (GAT) encoder that processes the network's graph
representation and generates a rich embedding. Additionally, for the action
space we transition from continuous pruning ratios to fine-grained binary
action spaces which enables the agent to learn optimal channel importance
criteria directly from data, moving away from predefined scoring functions.
These contributions are modelled within a Constrained Markov Decision Process
(CMDP) framework, allowing the agent to make informed pruning decisions while
adhering to resource constraints such as target compression rates. For this, we
design a self-competition reward system that encourages the agent to outperform
its previous best performance while satisfying the defined constraints. We
demonstrate the effectiveness of our approach through extensive experiments on
benchmark datasets including CIFAR-10, CIFAR-100, and ImageNet. The experiments
show that our method consistently outperforms traditional pruning techniques,
showing state-of-the-art results while learning task-specific pruning
strategies that identify functionally redundant connections beyond simple
weight magnitude considerations.

</details>


### [273] [STM-Graph: A Python Framework for Spatio-Temporal Mapping and Graph Neural Network Predictions](https://arxiv.org/abs/2509.10528)
*Amirhossein Ghaffari,Huong Nguyen,Lauri Lovén,Ekaterina Gilman*

Main category: cs.LG

TL;DR: STM-Graph is an open-source Python framework that converts raw urban spatio-temporal event data into graph representations for Graph Neural Network (GNN) training/prediction, featuring diverse mapping methods, OpenStreetMap features, multiple GNN models, visualization tools, and a GUI for both professionals and non-professionals, enabling rapid experimentation and benchmarking.


<details>
  <summary>Details</summary>
Motivation: Urban spatio-temporal data are highly dynamic and complex, presenting challenges for predictive analytics. There is a need for a modular, extensible framework that can transform such data into graph representations for GNNs and support rapid experimentation and benchmarking across mapping methods, features, and models.

Method: The authors present STM-Graph, an open-source Python framework that integrates multiple spatial mapping methods, urban features from OpenStreetMap, several GNN models, comprehensive visualization tools, and a GUI. The framework is modular and extensible, allowing researchers and practitioners to experiment with new mapping methods and custom models and to benchmark performance.

Result: The paper introduces a ready-to-use framework and GUI (with source code available at specified GitHub repositories), designed to streamline the conversion of urban spatio-temporal data into graph inputs for GNNs and to support rapid experimentation and benchmarking in urban computing.

Conclusion: STM-Graph provides a valuable resource for researchers and practitioners in urban computing by offering an open-source, modular, and extensible platform with integrated mapping, features, models, visualization, and a user-friendly GUI for both professional and non-professional users.

Abstract: Urban spatio-temporal data present unique challenges for predictive analytics
due to their dynamic and complex nature. We introduce STM-Graph, an open-source
Python framework that transforms raw spatio-temporal urban event data into
graph representations suitable for Graph Neural Network (GNN) training and
prediction. STM-Graph integrates diverse spatial mapping methods, urban
features from OpenStreetMap, multiple GNN models, comprehensive visualization
tools, and a graphical user interface (GUI) suitable for professional and
non-professional users. This modular and extensible framework facilitates rapid
experimentation and benchmarking. It allows integration of new mapping methods
and custom models, making it a valuable resource for researchers and
practitioners in urban computing. The source code of the framework and GUI are
available at: https://github.com/Ahghaffari/stm_graph and
https://github.com/tuminguyen/stm_graph_gui.

</details>


### [274] [Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion via Latent Replay](https://arxiv.org/abs/2509.10529)
*Aoi Otani*

Main category: cs.LG

TL;DR: Latent Replay enables continual learning for text-to-image diffusion models by storing compact latent features instead of raw images, mitigating catastrophic forgetting and mode collapse with lower memory usage, outperforming baselines on five sequential concepts.


<details>
  <summary>Details</summary>
Motivation: To overcome catastrophic forgetting and mode collapse in diffusion-based generative models and to do so with lower memory requirements than traditional replay methods.

Method: Store high-level latent representations from the model's internal architecture (latent replay) and use them during training to rehearse past concepts while learning new ones; experimental setup with five sequential visual concepts; randomly sampled latent storages outperform similarity-based strategies.

Result: Significant improvement over baselines in preserving knowledge and diversity; after learning all concepts, 77.59% Image Alignment on the first concept, about 14% above baselines; outputs remain diverse.

Conclusion: Latent Replay provides an efficient path to continual learning for diffusion models, enabling adaptable, personalized text-to-image systems with reduced memory and computation compared to replaying raw data.

Abstract: Continual learning -- the ability to acquire knowledge incrementally without
forgetting previous skills -- is fundamental to natural intelligence. While the
human brain excels at this, artificial neural networks struggle with
"catastrophic forgetting," where learning new tasks erases previously acquired
knowledge. This challenge is particularly severe for text-to-image diffusion
models, which generate images from textual prompts. Additionally, these models
face "mode collapse," where their outputs become increasingly repetitive over
time. To address these challenges, we apply Latent Replay, a
neuroscience-inspired approach, to diffusion models. Traditional replay methods
mitigate forgetting by storing and revisiting past examples, typically
requiring large collections of images. Latent Replay instead retains only
compact, high-level feature representations extracted from the model's internal
architecture. This mirrors the hippocampal process of storing neural activity
patterns rather than raw sensory inputs, reducing memory usage while preserving
critical information. Through experiments with five sequentially learned visual
concepts, we demonstrate that Latent Replay significantly outperforms existing
methods in maintaining model versatility. After learning all concepts, our
approach retained 77.59% Image Alignment (IA) on the earliest concept, 14%
higher than baseline methods, while maintaining diverse outputs. Surprisingly,
random selection of stored latent examples outperforms similarity-based
strategies. Our findings suggest that Latent Replay enables efficient continual
learning for generative AI models, paving the way for personalized
text-to-image models that evolve with user needs without excessive
computational costs.

</details>


### [275] [Dynamic Adaptive Shared Experts with Grouped Multi-Head Attention Mixture of Experts](https://arxiv.org/abs/2509.10530)
*Cheng Li,Jiexiong Liu,Yixuan Chen,Jie ji*

Main category: cs.LG

TL;DR: DASG-MoE is a new Transformer model that combines grouped multi-head attention, a dual-scale shared expert structure, and hierarchical adaptive dynamic routing to improve long-sequence modeling by balancing efficiency and accuracy with dynamic resource allocation.


<details>
  <summary>Details</summary>
Motivation: To overcome inefficiencies in Mixture-of-Experts Transformers for long sequences, including high compute cost, limited long-range dependency modeling, and static expert allocation that harms adaptability and efficiency.

Method: 1) Grouped Multi-Head Attention (GMHA) to reduce computation for long sequences via sequence grouping, local sliding window attention, and feature aggregation. 2) Dual-Scale Shared Expert Structure (DSSE) with shallow lightweight experts for low-dimensional features and deep experts for high-dimensional semantics, aided by pre-training transfer and post-training optimization. 3) Hierarchical Adaptive Dynamic Routing (ADR) to dynamically select expert levels based on feature complexity and task needs, plus a local expert activation strategy to optimize resources.

Result: Experiments on multiple long-sequence benchmarks show DASG-MoE outperforms state-of-the-art models, demonstrating improved efficiency and accuracy in long-sequence modeling.

Conclusion: DASG-MoE effectively integrates GMHA, DSSE, and ADR to enhance long-sequence modeling, achieving a favorable trade-off between computational efficiency and modeling capability through dynamic, adaptive expert resource allocation.

Abstract: Transformer models based on the Mixture of Experts (MoE) architecture have
made significant progress in long-sequence modeling, but existing models still
have shortcomings in computational efficiency and the ability to capture
long-range dependencies, especially in terms of the dynamic adaptability of
expert resource allocation. In this paper, we propose a Dynamic Adaptive Shared
Expert and Grouped Multi-Head Attention Hybrid Model (DASG-MoE) to enhance
long-sequence modeling capabilities by integrating three modules. First, we
employ the Grouped Multi-Head Attention (GMHA) mechanism to effectively reduce
the computational complexity of long sequences. By parallel processing through
sequence grouping, local sliding window attention, and feature aggregation, we
address long-range dependency issues and the model's lack of generalization for
local information. Second, we design a Dual-Scale Shared Expert Structure
(DSSE), where shallow experts use lightweight computations to quickly respond
to low-dimensional features, while deep experts process high-dimensional
complex semantics through pre-training transfer and post-training optimization,
achieving a dynamic balance between efficiency and accuracy. Third, we propose
a hierarchical Adaptive Dynamic Routing (ADR) mechanism that dynamically
selects expert levels based on feature complexity and task requirements, and
optimizes resource allocation through a local expert activation strategy.
Experiments on multiple long-sequence benchmark datasets demonstrate that our
DASG-MoE model outperforms state-of-the-art models.

</details>


### [276] [FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities](https://arxiv.org/abs/2509.10531)
*Himanshu Choudhary,Arishi Orra,Manoj Thakur*

Main category: cs.LG

TL;DR: A two-agent DRL framework that combines exploitation within a fixed asset universe with exploration of an extended universe to optimize portfolios, showing improved performance on two real-world datasets against baselines.


<details>
  <summary>Details</summary>
Motivation: DRL-based portfolio optimization typically operates within a pre-defined asset universe and neglects exploring new investment opportunities; expanding the search space could enhance adaptability and returns in dynamic markets.

Method: Introduce an investment landscape with two DRL agents: one focuses on allocating assets within the existing universe, the other guides exploration of opportunities in an extended universe. The agents dynamically balance exploitation and exploration to adapt to changing markets. Evaluation uses two real-world market datasets and compares against state-of-the-art strategies and baselines.

Result: Empirical experiments demonstrate that the proposed two-agent approach outperforms state-of-the-art portfolio strategies and baseline methods.

Conclusion: Integrating exploitation in the existing universe with exploration in an extended universe via a dual-agent DRL framework improves portfolio performance and adaptability in evolving markets.

Abstract: Portfolio optimization is essential for balancing risk and return in
financial decision-making. Deep Reinforcement Learning (DRL) has stood out as a
cutting-edge tool for portfolio optimization that learns dynamic asset
allocation using trial-and-error interactions. However, most DRL-based methods
are restricted to allocating assets within a pre-defined investment universe
and overlook exploring new opportunities. This study introduces an investment
landscape that integrates exploiting existing assets with exploring new
investment opportunities in an extended universe. The proposed approach
leverages two DRL agents and dynamically balances these objectives to adapt to
evolving markets while enhancing portfolio performance. One agent allocates
assets within the existing universe, while another assists in exploring new
opportunities in the extended universe. The effciency of the proposed
methodology is determined using two real-world market data sets. The
experiments demonstrate the superiority of the suggested approach against the
state-of-the-art portfolio strategies and baseline methods.

</details>


### [277] [Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings](https://arxiv.org/abs/2509.10534)
*Anand Gopalakrishnan,Robert Csordás,Jürgen Schmidhuber,Michael C. Mozer*

Main category: cs.LG

TL;DR: PoPE (Polar Coordinate Positional Embeddings) decouples content (what) from position (where) in RoPE, improving indexing and extrapolation; yields better performance across music, genomic, and language tasks and robust zero-shot length extrapolation.


<details>
  <summary>Details</summary>
Motivation: RoPE entangles content and position, which hurts tasks that require independent matching of content and position; a decoupled positional encoding could improve Transformer performance and extrapolation.

Method: Introduce Polar Coordinate Position Embeddings (PoPE) to remove what-where confounding; use a diagnostic task to test indexing by position or content; evaluate on autoregressive modeling in music, genomics, and NLP; test across model scales (128M–774M) and zero-shot length extrapolation; compare against RoPE.

Result: PoPE outperforms RoPE on evaluation loss (perplexity) and downstream task performance across domains; gains persist across model scales; PoPE shows strong zero-shot length extrapolation, while RoPE degrades on longer sequences without interpolation.

Conclusion: Eliminating the what-where confound with PoPE yields better generalization and length extrapolation; PoPE is a more robust positional encoding for Transformers than RoPE.

Abstract: The attention mechanism in a Transformer architecture matches key to query
based on both content -- the what -- and position in a sequence -- the where.
We present an analysis indicating that what and where are entangled in the
popular RoPE rotary position embedding. This entanglement can impair
performance particularly when decisions require independent matches on these
two factors. We propose an improvement to RoPE, which we call Polar Coordinate
Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is
far superior on a diagnostic task requiring indexing solely by position or by
content. On autoregressive sequence modeling in music, genomic, and natural
language domains, Transformers using PoPE as the positional encoding scheme
outperform baselines using RoPE with respect to evaluation loss (perplexity)
and downstream task performance. On language modeling, these gains persist
across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong
zero-shot length extrapolation capabilities, whereas RoPE's performance
degrades significantly on longer sequences at test time without fine tuning or
the use of position-interpolation methods.

</details>


### [278] [Semantic-guided LoRA Parameters Generation](https://arxiv.org/abs/2509.10535)
*Miaoge Li,Yang Chen,Zhijie Rao,Can Jiang,Jingcai Guo*

Main category: cs.LG

TL;DR: SG-LoRA generates user-specific LoRA parameters in zero-shot fashion by semantically matching user task descriptions to expert tasks, enabling privacy-preserving, real-time personalized model adaptation without retraining.


<details>
  <summary>Details</summary>
Motivation: Edge users have task-specific preferences and significant domain shifts; retraining or fine-tuning for each user is costly and raises privacy concerns, necessitating a zero-shot, privacy-preserving personalization approach.

Method: Utilize semantic guidance by mapping task descriptions into a shared embedding space with expert tasks, model the target LoRA parameter distribution based on semantic proximity, and distill knowledge from prominent LoRA experts to generate high-performing parameters without any user data or training on user tasks.

Result: Extensive experiments demonstrate superior performance and remarkable adaptability across multiple challenging tasks, enabling real-time construction of personalized LoRA models in a privacy-preserving, zero-shot open-world setting.

Conclusion: SG-LoRA enables real-time construction of user-specific LoRA models without access to user data or retraining, providing a privacy-preserving zero-shot solution for personalized model adaptation in edge/open-world scenarios.

Abstract: Low-Rank Adaptation (LoRA) has demonstrated strong generalization
capabilities across a variety of tasks for efficiently fine-tuning AI models,
especially on resource-constrained edges. However, in real-world applications,
edge users often exhibit task-specific preferences that are difficult to handle
with a unified model trained under a closed-world assumption, and the challenge
may further increase when there are significant domain shifts between training
and deployment. Meanwhile, retraining/fine-tuning models for each user is also
impractical due to its cost-intensive nature and privacy concerns over raw data
utilization from edges. To address these challenges, we propose Semantic-guided
LoRA Parameter Generation (SG-LoRA), the first of its kind framework to
efficiently produce user-specific LoRA parameters without any additional
training on user tasks or access to user-specific data. Concretely, SG-LoRA
uses task descriptions as the semantic bridge, measuring their proximity to a
set of known expert tasks in a shared embedding space. Based on this semantic
guidance, it models the target task's LoRA parameter distribution to generate
high-performing parameters for novel tasks. SG-LoRA enables the real-time
construction of LoRA models aligned with individual intents by distilling
knowledge from prominent LoRA experts and, meanwhile, offering a
privacy-preserving solution for personalized model adaptation in a novel
zero-shot open-world setting proposed in this work. Extensive experiments on
multiple challenging tasks confirm the superior performance and remarkable
adaptability of SG-LoRA. Code is available at
https://github.com/keepgoingjkg/SG-LoRA.

</details>


### [279] [Contextuality, Holonomy and Discrete Fiber Bundles in Group-Valued Boltzmann Machines](https://arxiv.org/abs/2509.10536)
*Jean-Pierre Magnot*

Main category: cs.LG

TL;DR: Geometric extension of RBMs with group-valued weights, introducing a contextuality index from holonomies to capture curvature; links to gauge theory, contextuality, and noncommutative geometry; demonstrated via finite/infinite-dimensional examples; enables curvature-aware learning and topological regularization.


<details>
  <summary>Details</summary>
Motivation: Extend probabilistic RBMs to non-Euclidean, symmetry-rich domains by elevating weights to elements of groups; quantify global inconsistency and curvature to integrate geometric/topological structure into learning.

Method: Replace scalar weights with group-valued weights (e.g., GL_n(R), SU(2)); compute holonomies along cycles in the RBM graph to define a contextuality index; relate this index to sheaf-theoretic contextuality, gauge theory, and noncommutative geometry; provide numerical and diagrammatic demonstrations in finite and infinite dimensions.

Result: Define a contextuality index based on group-valued holonomies, establish theoretical connections to sheaf theory, gauge theory, and noncommutative geometry, and present numerical and diagrammatic examples across finite and infinite dimensions.

Conclusion: This framework enables curvature-aware learning architectures and topological regularization in settings with uncertainty or adversarial dynamics.”

Abstract: We propose a geometric extension of restricted Boltzmann machines (RBMs) by
allowing weights to take values in abstract groups such as \(
\mathrm{GL}_n(\mathbb{R}) \), \( \mathrm{SU}(2) \), or even
infinite-dimensional operator groups. This generalization enables the modeling
of complex relational structures, including projective transformations, spinor
dynamics, and functional symmetries, with direct applications to vision,
language, and quantum learning.
  A central contribution of this work is the introduction of a
\emph{contextuality index} based on group-valued holonomies computed along
cycles in the RBM graph. This index quantifies the global inconsistency or
"curvature" induced by local weights, generalizing classical notions of
coherence, consistency, and geometric flatness. We establish links with
sheaf-theoretic contextuality, gauge theory, and noncommutative geometry, and
provide numerical and diagrammatic examples in both finite and infinite
dimensions.
  This framework opens novel directions in AI, from curvature-aware learning
architectures to topological regularization in uncertain or adversarial
environments.

</details>


### [280] [On Using Large-Batches in Federated Learning](https://arxiv.org/abs/2509.10537)
*Sahil Tyagi*

Main category: cs.LG

TL;DR: In Federated Learning with frequent synchronization, the paper studies large-batch vs small-batch trade-offs and proposes a large-batch training technique to gain parallel speedups without sacrificing generalization; reports test accuracy gains on ResNet50 (+32.33%) and VGG11 (+3.74%).


<details>
  <summary>Details</summary>
Motivation: Address generalization degradation associated with large-batch training in FL, and leverage the parallelism of large batches while preserving the generalization benefits of small batches; provide a framework to balance batch-size-related trade-offs in distributed/FL contexts.

Method: Proposes a large-batch training technique within FL to exploit trade-offs between small and large batches; investigates how increasing global batch size under frequent synchronization can speed up training, while seeking to retain generalization; conducts experiments on ResNet50 and VGG11.

Result: Large-batch technique yields higher test accuracy than small-batch baseline by 32.33% (ResNet50) and 3.74% (VGG11).

Conclusion: Demonstrates a viable path to combine parallel scaling benefits of large batches with the generalization capabilities of small batches in FL; suggests further exploration of strategies to balance batch-size and synchronization to optimize both speed and accuracy.

Abstract: Efficient Federated learning (FL) is crucial for training deep networks over
devices with limited compute resources and bounded networks. With the advent of
big data, devices either generate or collect multimodal data to train either
generic or local-context aware networks, particularly when data privacy and
locality is vital. FL algorithms generally trade-off between parallel and
statistical performance, improving model quality at the cost of higher
communication frequency, or vice versa. Under frequent synchronization
settings, FL over a large cluster of devices may perform more work per-training
iteration by processing a larger global batch-size, thus attaining considerable
training speedup. However, this may result in poor test performance (i.e., low
test loss or accuracy) due to generalization degradation issues associated with
large-batch training. To address these challenges with large-batches, this work
proposes our vision of exploiting the trade-offs between small and large-batch
training, and explore new directions to enjoy both the parallel scaling of
large-batches and good generalizability of small-batch training. For the same
number of iterations, we observe that our proposed large-batch training
technique attains about 32.33% and 3.74% higher test accuracy than small-batch
training in ResNet50 and VGG11 models respectively.

</details>


### [281] [DualAlign: Generating Clinically Grounded Synthetic Data](https://arxiv.org/abs/2509.10538)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.LG

TL;DR: DualAlign improves synthetic clinical data generation by dual alignment: statistical alignment to demographics/risk factors and semantic alignment using real symptom trajectories, yielding more realistic and clinically grounded data. Fine-tuning LLaMA 3.1-8B on a mix of synthetic and human-annotated data yields strong gains over gold or unguided baselines, using Alzheimer's disease as a case study; practical for privacy-preserving clinical text analysis, though not fully capturing longitudinal complexity.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for realistic, privacy-preserving synthetic clinical data to support AI in healthcare, given strict EHR privacy, scarcity of annotated rare-condition data, and biases in observational datasets. It proposes a method to generate clinically plausible text that reflects real-world symptom trajectories and patient demographics.

Method: Introduce DualAlign with two alignment modes: (1) statistical alignment, conditioning generation on patient demographics and risk factors; (2) semantic alignment, incorporating real-world symptom trajectories to guide content generation. Apply to Alzheimer's disease as a case study. Fine-tune a LLaMA 3.1-8B model on a mixture of DualAlign-generated and human-annotated data, comparing against models trained on gold data alone or unguided synthetic data.

Result: DualAlign yields substantial performance gains over models trained on gold data or unguided synthetic baselines. It produces context-grounded, symptom-level sentences that better reflect real-world documentation. However, it does not fully capture longitudinal complexity.

Conclusion: DualAlign offers a practical approach to generating clinically grounded, privacy-preserving synthetic clinical text data to support low-resource clinical NLP tasks, advancing realism and utility while acknowledging remaining limitations in longitudinal dynamics.

Abstract: Synthetic clinical data are increasingly important for advancing AI in
healthcare, given strict privacy constraints on real-world EHRs, limited
availability of annotated rare-condition data, and systemic biases in
observational datasets. While large language models (LLMs) can generate fluent
clinical text, producing synthetic data that is both realistic and clinically
meaningful remains challenging. We introduce DualAlign, a framework that
enhances statistical fidelity and clinical plausibility through dual alignment:
(1) statistical alignment, which conditions generation on patient demographics
and risk factors; and (2) semantic alignment, which incorporates real-world
symptom trajectories to guide content generation. Using Alzheimer's disease
(AD) as a case study, DualAlign produces context-grounded symptom-level
sentences that better reflect real-world clinical documentation. Fine-tuning an
LLaMA 3.1-8B model with a combination of DualAlign-generated and
human-annotated data yields substantial performance gains over models trained
on gold data alone or unguided synthetic baselines. While DualAlign does not
fully capture longitudinal complexity, it offers a practical approach for
generating clinically grounded, privacy-preserving synthetic data to support
low-resource clinical text analysis.

</details>


### [282] [GTS_Forecaster: a novel deep learning based geodetic time series forecasting toolbox with python](https://arxiv.org/abs/2509.10560)
*Xuechen Liang,Xiaoxing He,Shengdao Wang,Jean-Philippe Montillet,Zhengkai Huang,Gaël Kermarrec,Shunqiang Hu,Yu Zhou,Jiahui Huang*

Main category: cs.LG

TL;DR: A new open-source package, GTS Forecaster, for geodetic time series forecasting using advanced deep learning models (KAN, GNNGRU, TimeGNN) with robust preprocessing (outlier detection, KTIF) enabling forecasting, visualization, and evaluation for GNSS, SSH, TG data.


<details>
  <summary>Details</summary>
Motivation: Geodetic time series are nonlinear, non-stationary, and incomplete; need better models to capture long-term dependencies and spatiotemporal dynamics; improve forecasts for hazards and sea level.

Method: Integration of several deep learning architectures (KAN, GNNGRU, TimeGNN), preprocessing tools (outlier detection), and a reinforcement-learning-based gap-filling algorithm (KTIF) into an open-source Python package with forecasting, visualization, evaluation capabilities across GNSS, SSH, TG.

Result: Introduces GTS Forecaster package with integrated models and preprocessing; enables forecasting, visualization, evaluation; demonstrates on GNSS/SSH/TG datasets; open-source and adaptable to general time series tasks.

Conclusion: The package lowers barriers to applying deep learning in geodetic forecasting and can be adapted to broader time series applications, supporting improved monitoring of surface deformation and sea level changes.

Abstract: Geodetic time series -- such as Global Navigation Satellite System (GNSS)
positions, satellite altimetry-derived sea surface height (SSH), and tide gauge
(TG) records -- is essential for monitoring surface deformation and sea level
change. Accurate forecasts of these variables can enhance early warning systems
and support hazard mitigation for earthquakes, landslides, coastal storm surge,
and long-term sea level. However, the nonlinear, non-stationary, and incomplete
nature of such variables presents significant challenges for classic models,
which often fail to capture long-term dependencies and complex spatiotemporal
dynamics. We introduce GTS Forecaster, an open-source Python package for
geodetic time series forecasting. It integrates advanced deep learning models
-- including kernel attention networks (KAN), graph neural network-based gated
recurrent units (GNNGRU), and time-aware graph neural networks (TimeGNN) -- to
effectively model nonlinear spatial-temporal patterns. The package also
provides robust preprocessing tools, including outlier detection and a
reinforcement learning-based gap-filling algorithm, the Kalman-TransFusion
Interpolation Framework (KTIF). GTS Forecaster currently supports forecasting,
visualization, and evaluation of GNSS, SSH, and TG datasets, and is adaptable
to general time series applications. By combining cutting-edge models with an
accessible interface, it facilitates the application of deep learning in
geodetic forecasting tasks.

</details>


### [283] [SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs](https://arxiv.org/abs/2509.10594)
*Iqbal H. Sarker,Helge Janicke,Ahmad Mohsin,Leandros Maglaras*

Main category: cs.LG

TL;DR: Four-pillar framework to embed trust and ethics in AI for SMEs, aligning Data, Algorithms, Human oversight, and Model Architecture across the AI lifecycle to bridge theory with practice and enable responsible, sustainable adoption.


<details>
  <summary>Details</summary>
Motivation: SMEs face technical, ethical, and trust challenges in AI adoption. There is a need for practical governance that translates ethical principles into operational practices within SME contexts.

Method: A structured, multi-phased framework organized around four pillars (Data, Algorithms, Human oversight, Model Architecture) that translates ethical principles into actionable SME workflows and provides a roadmap for implementation across the AI lifecycle.

Result: Delivers a structured roadmap for responsible AI adoption in SMEs, enhancing resilience, competitiveness, and sustainable innovation, with applicability across diverse SME applications.

Conclusion: Trust and ethics can catalyze resilience, competitiveness, and sustainable innovation in SMEs through responsible AI adoption.

Abstract: Artificial Intelligence (AI) and Large Language Models (LLMs) are reshaping
today's business practices, however, their adoption within small and
medium-sized enterprises (SMEs) raises significant technical, ethical and trust
issues. This paper proposes a structured, multi-phased framework designed to
embed trust and ethical principles throughout the AI lifecycle for their secure
and responsible use in SMEs. Structured around four pillars, i.e., Data,
Algorithms, Human oversight, and Model Architecture, the framework bridges
theoretical ethical principles with operational practice, enhancing AI
capabilities in diverse SME applications. Ultimately, this paper offers a
structured roadmap for responsible AI adoption, framing trust and ethics as a
catalyst for resilience, competitiveness, and sustainable innovation in SMEs.

</details>


### [284] [pySigLib -- Fast Signature-Based Computations on CPU and GPU](https://arxiv.org/abs/2509.10613)
*Daniil Shmelev,Cristopher Salvi*

Main category: cs.LG

TL;DR: pySigLib is a high-performance Python library for signatures and signature kernels with CPU/GPU support, PyTorch autograd compatibility, and a novel differentiation scheme providing fast, accurate gradients for large-scale time-series tasks.


<details>
  <summary>Details</summary>
Motivation: Signature-based representations are powerful for sequential data and time-series (notably in quantitative finance) but existing implementations do not scale to large datasets and long sequences; this bottleneck hampers training and practical deployment.

Method: Developed pySigLib with optimized CPU/GPU implementations of signatures and signature kernels; ensured full PyTorch autograd compatibility; proposed a new differentiation scheme for signature kernels to compute gradients more accurately and faster than prior libraries.

Result: Achieves scalable, high-performance computation on large datasets and long sequences; faster gradient computation with accurate gradients; improved runtime relative to existing libraries while maintaining differentiability with PyTorch.

Conclusion: pySigLib provides a scalable, differentiable, signature-based ML foundation for time-series modeling, enabling efficient training of discriminators and losses in practice, including finance-oriented applications.

Abstract: Signature-based methods have recently gained significant traction in machine
learning for sequential data. In particular, signature kernels have emerged as
powerful discriminators and training losses for generative models on
time-series, notably in quantitative finance. However, existing implementations
do not scale to the dataset sizes and sequence lengths encountered in practice.
We present pySigLib, a high-performance Python library offering optimised
implementations of signatures and signature kernels on CPU and GPU, fully
compatible with PyTorch's automatic differentiation. Beyond an efficient
software stack for large-scale signature-based computation, we introduce a
novel differentiation scheme for signature kernels that delivers accurate
gradients at a fraction of the runtime of existing libraries.

</details>


### [285] [Optimal Multimarginal Schrödinger Bridge: Minimum Spanning Tree over Measure-valued Vertices](https://arxiv.org/abs/2509.10626)
*Georgiy A. Bondar,Abhishek Halder*

Main category: cs.LG

TL;DR: Optimal MSB over all graph structures reduces to a minimum spanning tree problem on measure-valued vertices; a two-step MST-based algorithm is proposed.


<details>
  <summary>Details</summary>
Motivation: To determine the best correlation/graph structure among multiple random vectors when the network of dependencies is unknown, by seeking the optimal coupling under any graph.

Method: 1) Prove the MSB with variable graph structure is equivalent to finding an MST on measure-valued vertices. 2) Build a complete graph with edge weights w_ij = V_bimarginal_SB(i,j) + H(i) + H(j), where V_bimarginal_SB is the optimal bimarginal Schrödinger Bridge value and H denotes endpoint entropies. 3) Compute MST on this complete graph; the resulting tree defines the optimal graph structure and coupling.

Result: The optimal MSB structure is given by the MST; the method decomposes into a simple two-step procedure and is validated numerically.

Conclusion: The combinatorial problem of choosing a graph structure for MSB collapses to a standard MST computation, enabling efficient design of multipartite couplings with uncertain correlation graphs; potential for scalable and practical applications.

Abstract: The Multimarginal Schr\"odinger Bridge (MSB) finds the optimal coupling among
a collection of random vectors with known statistics and a known correlation
structure. In the MSB formulation, this correlation structure is specified
\emph{a priori} as an undirected connected graph with measure-valued vertices.
In this work, we formulate and solve the problem of finding the optimal MSB in
the sense we seek the optimal coupling over all possible graph structures. We
find that computing the optimal MSB amounts to solving the minimum spanning
tree problem over measure-valued vertices. We show that the resulting problem
can be solved in two steps. The first step constructs a complete graph with
edge weight equal to a sum of the optimal value of the corresponding bimarginal
SB and the entropies of the endpoints. The second step solves a standard
minimum spanning tree problem over that complete weighted graph. Numerical
experiments illustrate the proposed solution.

</details>


### [286] [Interpretable neural network system identification method for two families of second-order systems based on characteristic curves](https://arxiv.org/abs/2509.10632)
*Federico J. Gonzalez,Luis P. Lara*

Main category: cs.LG

TL;DR: Proposes a CC-based framework tying governing equations to neural nets, with three variants (SINDy-CC, Poly-CC, NN-CC). NN-CC offers best performance for complex/non-smooth nonlinearities while preserving interpretability via explicit characteristic curves.


<details>
  <summary>Details</summary>
Motivation: Address the interpretability–flexibility trade-off in nonlinear system identification by embedding physical constraints through equation structure.

Method: Introduce characteristic curves (CCs), each modeled by a dedicated neural network. Three strategies: (i) SINDy-CC—sparse regression with equation-structure constraints; (ii) Poly-CC—each CC represented by high-degree polynomials; (iii) NN-CC—full neural networks without prespecified basis functions.

Result: All three approaches work well for systems with simple polynomial nonlinearities (e.g., van der Pol). NN-CC outperforms for systems with complex nonlinearities and discontinuities (e.g., stick-slip).

Conclusion: The CC-based framework, especially NN-CC, captures complex nonlinearities while maintaining interpretable CC representations, making it well-suited for systems with discontinuities where traditional polynomial or sparse regression struggles.

Abstract: Nonlinear system identification often involves a fundamental trade-off
between interpretability and flexibility, often requiring the incorporation of
physical constraints. We propose a unified data-driven framework that combines
the mathematical structure of the governing differential equations with the
flexibility of neural networks (NNs). At the core of our approach is the
concept of characteristic curves (CCs), which represent individual nonlinear
functions (e.g., friction and restoring components) of the system. Each CC is
modeled by a dedicated NN, enabling a modular and interpretable representation
of the system equation. To demonstrate the versatility of the CC-based
formalism, we introduce three identification strategies: (1) SINDy-CC, which
extends the sparse regression approach of SINDy by incorporating the
mathematical structure of the governing equations as constraints; (2) Poly-CC,
which represents each CC using high-degree polynomials; and (3) NN-CC, which
uses NNs without requiring prior assumptions about basis functions. Our results
show that all three approaches are well-suited for systems with simple
polynomial nonlinearities, such as the van der Pol oscillator. In contrast,
NN-CC demonstrates superior performance in modeling systems with complex
nonlinearities and discontinuities, such as those observed in stick-slip
systems. The key contribution of this work is to demonstrate that the CC-based
framework, particularly the NN-CC approach, can capture complex nonlinearities
while maintaining interpretability through the explicit representation of the
CCs. This balance makes it well-suited for modeling systems with
discontinuities and complex nonlinearities that are challenging to assess using
traditional polynomial or sparse regression methods, providing a powerful tool
for nonlinear system identification.

</details>


### [287] [Accurate and Private Diagnosis of Rare Genetic Syndromes from Facial Images with Federated Deep Learning](https://arxiv.org/abs/2509.10635)
*Ali Burak Ünal,Cem Ata Baykara,Peter Krawitz,Mete Akgün*

Main category: cs.LG

TL;DR: Privacy-preserving federated GestaltMatcher enabling cross-institution collaboration without sharing images, achieving about 90% of centralized performance and robust to data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Overcome privacy and data-silo constraints in rare genetic disorder diagnosis using facial dysmorphology; need scalable collaboration without exchanging patient images.

Method: Cross-silo horizontal federated learning; global ensemble feature extractor trained across hospitals; data mapped to a shared latent space; privacy-preserving kernel matrix computation for inference and discovery; new participants can adopt the global extractor and kernel config from prior rounds.

Result: Federated service retains >90% of centralized performance; robust to varying silo counts and heterogeneous data distributions.

Conclusion: Demonstrates viability of privacy-preserving federated learning for GestaltMatcher-like tasks, enabling scalable collaboration and ongoing discovery in facial dysmorphology without compromising patient confidentiality.

Abstract: Machine learning has shown promise in facial dysmorphology, where
characteristic facial features provide diagnostic clues for rare genetic
disorders. GestaltMatcher, a leading framework in this field, has demonstrated
clinical utility across multiple studies, but its reliance on centralized
datasets limits further development, as patient data are siloed across
institutions and subject to strict privacy regulations. We introduce a
federated GestaltMatcher service based on a cross-silo horizontal federated
learning framework, which allows hospitals to collaboratively train a global
ensemble feature extractor without sharing patient images. Patient data are
mapped into a shared latent space, and a privacy-preserving kernel matrix
computation framework enables syndrome inference and discovery while
safeguarding confidentiality. New participants can directly benefit from and
contribute to the system by adopting the global feature extractor and kernel
configuration from previous training rounds. Experiments show that the
federated service retains over 90% of centralized performance and remains
robust to both varying silo numbers and heterogeneous data distributions.

</details>


### [288] [Test-Time Warmup for Multimodal Large Language Models](https://arxiv.org/abs/2509.10641)
*Nikita Rajaneesh,Thomas Zollo,Richard Zemel*

Main category: cs.LG

TL;DR: Test-time warmup for multimodal LLMs adapts per test instance using weakly supervised auxiliary data to boost reasoning, achieving relative gains on MMMU, VQA-Rad, and GQA without large-scale labeled fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Despite large pretraining, MLLMs are often trained end-to-end on limited multimodal data; improving complex reasoning performance typically requires expensive fine-tuning on large labeled datasets. A lightweight test-time adaptation could enhance robustness without heavy data requirements.

Method: During inference, perform a per-instance adaptation (warmup) by leveraging data from weakly supervised auxiliary tasks to adjust the model’s components (e.g., vision-to-text mapping and prompting) before answering.

Result: Reported relative improvements for the Llama-Vision-Instruct model: +4.03% on MMMU, +5.28% on VQA-Rad, and +1.63% on GQA.

Conclusion: A test-time warming up phase can improve the robustness and reasoning performance of multimodal LLMs across diverse tasks without relying on large-scale labeled fine-tuning; effectiveness appears task-dependent and warrants further validation.

Abstract: Multimodal Large Language Models (MLLMs) hold great promise for advanced
reasoning at the intersection of text and images, yet they have not fully
realized this potential. MLLMs typically integrate an LLM, a vision encoder,
and a connector that maps the vision encoder's embeddings into the LLM's text
embedding space. Although each component is pretrained on massive datasets with
billions of samples, the entire multimodal model is typically trained on only
thousands (or a few million) samples, which can result in weak performance on
complex reasoning tasks. To address these shortcomings, instead of relying on
extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup
method that adapts the MLLM per test instance by leveraging data from weakly
supervised auxiliary tasks. With our approach, we observe a relative
performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on
the Llama-Vision-Instruct model. Our method demonstrates that 'warming up'
before inference can enhance MLLMs' robustness across diverse reasoning tasks.

</details>


### [289] [Self-Supervised Goal-Reaching Results in Multi-Agent Cooperation and Exploration](https://arxiv.org/abs/2509.10656)
*Chirayu Nimonkar,Shlok Shah,Catherine Ji,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: Self-supervised goal-reaching for multi-agent coordination; agents optimize the likelihood of reaching a user-specified goal state rather than maximizing a scalar reward, enabling tasks to be specified via a single goal and leading to emergent cooperation with sparse feedback.


<details>
  <summary>Details</summary>
Motivation: Designing reward functions for long-horizon, cooperative multi-agent tasks is hard; a user-friendly way to specify tasks and encourage cooperation with sparse feedback.

Method: Replace scalar rewards with maximizing the probability of visiting a given goal state; use self-supervised learning to guide agents to this goal; demonstrate in multi-agent reinforcement learning benchmarks; no explicit exploration mechanism but emergence of cooperative exploration.

Result: On MARL benchmarks, the approach outperforms alternative methods using the same sparse reward signal; self-supervised goal-reaching leads to emergent cooperation and exploration in scenarios where other methods fail to achieve any successful trial.

Conclusion: Self-supervised goal-reaching is a viable approach for enabling cooperative MARL with sparse feedback and user-specified goals, producing emergent exploratory and cooperative behavior without explicit exploration strategies.

Abstract: For groups of autonomous agents to achieve a particular goal, they must
engage in coordination and long-horizon reasoning. However, designing reward
functions to elicit such behavior is challenging. In this paper, we study how
self-supervised goal-reaching techniques can be leveraged to enable agents to
cooperate. The key idea is that, rather than have agents maximize some scalar
reward, agents aim to maximize the likelihood of visiting a certain goal. This
problem setting enables human users to specify tasks via a single goal state
rather than implementing a complex reward function. While the feedback signal
is quite sparse, we will demonstrate that self-supervised goal-reaching
techniques enable agents to learn from such feedback. On MARL benchmarks, our
proposed method outperforms alternative approaches that have access to the same
sparse reward signal as our method. While our method has no explicit mechanism
for exploration, we observe that self-supervised multi-agent goal-reaching
leads to emergent cooperation and exploration in settings where alternative
approaches never witness a single successful trial.

</details>


### [290] [M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations](https://arxiv.org/abs/2509.10659)
*Bo Lei,Victor M. Castillo,Yeping Hu*

Main category: cs.LG

TL;DR: M4GN is a three-tier hierarchical GNN for mesh-based PDE surrogates, using segment-centric hybrid segmentation and permutation-invariant aggregation to bridge micro GNNs and macro transformers, achieving higher accuracy and faster inference on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Efficiently model long-range dependencies in meshes for PDE simulations while preserving accuracy; address challenges in coarse graph construction and over-smoothing in hierarchical GNNs.

Method: Hybrid segmentation combining a fast graph partitioner with superpixel-style refinement guided by modal-decomposition features to create contiguous, dynamically consistent segments. Segments are encoded by a permutation-invariant aggregator. A micro GNN captures local dynamics while a macro-level transformer reasons across segments.

Result: On representative benchmarks, M4GN improves prediction accuracy by up to 56% and achieves up to 22% faster inference than state-of-the-art baselines.

Conclusion: The three-tier, segment-centric approach offers a principled balance between accuracy and efficiency for mesh-based PDE surrogates, leveraging segmentation to connect local dynamics with cross-segment reasoning.

Abstract: Mesh-based graph neural networks (GNNs) have become effective surrogates for
PDE simulations, yet their deep message passing incurs high cost and
over-smoothing on large, long-range meshes; hierarchical GNNs shorten
propagation paths but still face two key obstacles: (i) building coarse graphs
that respect mesh topology, geometry, and physical discontinuities, and (ii)
maintaining fine-scale accuracy without sacrificing the speed gained from
coarsening. We tackle these challenges with M4GN, a three-tier, segment-centric
hierarchical network. M4GN begins with a hybrid segmentation strategy that
pairs a fast graph partitioner with a superpixel-style refinement guided by
modal-decomposition features, producing contiguous segments of dynamically
consistent nodes. These segments are encoded by a permutation-invariant
aggregator, avoiding the order sensitivity and quadratic cost of aggregation
approaches used in prior works. The resulting information bridges a micro-level
GNN, which captures local dynamics, and a macro-level transformer that reasons
efficiently across segments, achieving a principled balance between accuracy
and efficiency. Evaluated on multiple representative benchmark datasets, M4GN
improves prediction accuracy by up to 56% while achieving up to 22% faster
inference than state-of-the-art baselines.

</details>


### [291] [Least-Ambiguous Multi-Label Classifier](https://arxiv.org/abs/2509.10689)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: Proposes a model-agnostic conformal prediction method to handle single-positive multi-label learning (SPMLL) by producing calibrated set-valued predictions, bridging single-label training and multi-label evaluation without assuming label distributions, tested on 12 datasets with consistent improvements.


<details>
  <summary>Details</summary>
Motivation: SPMLL is an extreme supervision scenario where each training instance has only one annotated positive label, though multiple labels may be relevant; this makes reliable multi-label predictions challenging and costly to obtain full annotations.

Method: A model-agnostic approach that uses conformal prediction to produce calibrated set-valued outputs for multi-label predictions, enabling reliable predictions at test time without relying on label distribution assumptions.

Result: Demonstrates consistent improvements over existing baselines on 12 benchmark datasets and shows practical applicability of the proposed SPMLL method.

Conclusion: The method provides calibrated, reliable multi-label predictions under single-positive supervision and does not depend on label distribution assumptions, highlighting practical usefulness and potential for broader adoption.

Abstract: Multi-label learning often requires identifying all relevant labels for
training instances, but collecting full label annotations is costly and
labor-intensive. In many datasets, only a single positive label is annotated
per training instance, despite the presence of multiple relevant labels. This
setting, known as single-positive multi-label learning (SPMLL), presents a
significant challenge due to its extreme form of partial supervision. We
propose a model-agnostic approach to SPMLL that draws on conformal prediction
to produce calibrated set-valued outputs, enabling reliable multi-label
predictions at test time. Our method bridges the supervision gap between
single-label training and multi-label evaluation without relying on label
distribution assumptions. We evaluate our approach on 12 benchmark datasets,
demonstrating consistent improvements over existing baselines and practical
applicability.

</details>


### [292] [Learning Concave Bid Shading Strategies in Online Auctions via Measure-valued Proximal Optimization](https://arxiv.org/abs/2509.10693)
*Iman Nodozi,Djordje Gligorijevic,Abhishek Halder*

Main category: cs.LG

TL;DR: Context-aware bid shading in first-price auctions modeled as a measure-valued convex optimization; updates shading distribution via regularized Wasserstein proximal steps guided by a context-dependent energy functional; yields a closed-form solution and is demonstrated numerically.


<details>
  <summary>Details</summary>
Motivation: Improve expected surplus in first-price auctions by optimizing the shading distribution in a principled, context-aware manner, leveraging contextual attributes to tailor the strategy.

Method: Formulate bid shading as a convex optimization over the joint distribution of shading parameters (measure-valued). After each auction, update the shading distribution with a regularized Wasserstein proximal step using a data-driven energy functional that depends on context (e.g., domain, ad slot, device, location) and rewards higher surplus (high win probability and large value gap). Derive a closed-form solution for the resulting measure-valued problem.

Result: The optimization problem admits a closed-form solution. A numerical example demonstrates the proposed method and its behavior.

Conclusion: The paper provides a principled, context-aware, measure-valued optimization framework for bid shading in first-price auctions, with a tractable closed-form solution and empirical illustration.

Abstract: This work proposes a bid shading strategy for first-price auctions as a
measure-valued optimization problem. We consider a standard parametric form for
bid shading and formulate the problem as convex optimization over the joint
distribution of shading parameters. After each auction, the shading parameter
distribution is adapted via a regularized Wasserstein-proximal update with a
data-driven energy functional. This energy functional is conditional on the
context, i.e., on publisher/user attributes such as domain, ad slot type,
device, or location. The proposed algorithm encourages the bid distribution to
place more weight on values with higher expected surplus, i.e., where the win
probability and the value gap are both large. We show that the resulting
measure-valued convex optimization problem admits a closed form solution. A
numerical example illustrates the proposed method.

</details>


### [293] [Verifying Computational Graphs in Production-Grade Distributed Machine Learning Frameworks](https://arxiv.org/abs/2509.10694)
*Kahfi S. Zulkifli,Wenbo Qian,Shaowei Zhu,Yuan Zhou,Zhen Zhang,Chang Lou*

Main category: cs.LG

TL;DR: Scalify is a lightweight framework that verifies the semantic equivalence of computational graphs to reveal silent errors in large ML frameworks, enabling scalable, actionable debugging for very large models.


<details>
  <summary>Details</summary>
Motivation: Silent errors arising from parallelism and optimization in modern ML systems degrade performance and correctness; existing remedies are ad hoc or prohibitively expensive for production-scale models.

Method: Scalify uses equality saturation and Datalog-style reasoning, with graph partitioning, parallel rewriting, layer memoization, and reusable rewrite templates. It adds relational reasoning and symbolic bijection inference, localizes discrepancies to precise code sites, and translates findings into debugging guidance.

Result: It can verify models as large as Llama-3.1-405B within minutes on a commodity machine and has exposed five unknown bugs in Amazon's production ML frameworks.

Conclusion: Scalify demonstrates scalable, actionable verification of ML models, offering practical defense against silent errors and a path toward more reliable production ML systems.

Abstract: Modern machine learning frameworks support very large models by incorporating
parallelism and optimization techniques. Yet, these very techniques add new
layers of complexity, introducing silent errors that severely degrade model
performance. Existing solutions are either ad hoc or too costly for production.
  We present Scalify, a lightweight framework that exposes silent errors by
verifying semantic equivalence of computational graphs using equality
saturation and Datalog-style reasoning. To scale, Scalify partitions graphs
with parallel rewriting and layer memoization, reuses rewrite templates, and
augments equality saturation with relational reasoning and symbolic bijection
inference. It further localizes discrepancies to precise code sites, turning
verification results into actionable debugging guidance. Scalify verifies
models as large as Llama-3.1-405B within minutes on a commodity machine and
exposed five unknown bugs in Amazon production machine learning frameworks.

</details>


### [294] [Kalman Bayesian Transformer](https://arxiv.org/abs/2509.10695)
*Haoming Jing,Oren Wright,José M. F. Moura,Yorie Nakahira*

Main category: cs.LG

TL;DR: A Bayesian, uncertainty-aware sequential fine-tuning method for transformers that balances priors from pre-trained models with new data under distribution shifts, using moment propagation, Kalman Bayesian Neural Networks, and softmax moment Taylor approximations to achieve robust, data-efficient learning in latency-constrained settings.


<details>
  <summary>Details</summary>
Motivation: Sequential learning of transformers is needed when data arrive over time and distributions shift, but stability, data efficiency, and uncertainty quantification are challenging under small data and latency constraints.

Method: Formulates sequential fine-tuning as posterior inference in a Bayesian framework, integrating closed-form moment propagation of random variables, Kalman Bayesian Neural Networks, and Taylor approximations of softmax moments; treats the pre-trained model as a prior and adaptively balances it against new information based on quantified uncertainty.

Result: Numerical simulations demonstrate sequential adaptation of a decision transformer to distribution-shift tasks with limited memory, showing robustness and data efficiency in updating the model.

Conclusion: The approach provides robust, data-efficient, and uncertainty-aware sequential fine-tuning for transformers in latency-constrained environments by explicitly balancing prior knowledge with new data using quantified uncertainty.

Abstract: Sequential fine-tuning of transformers is useful when new data arrive
sequentially, especially with shifting distributions. Unlike batch learning,
sequential learning demands that training be stabilized despite a small amount
of data by balancing new information and previously learned knowledge in the
pre-trained models. This challenge is further complicated when training is to
be completed in latency-critical environments and learning must additionally
quantify and be mediated by uncertainty. Motivated by these challenges, we
propose a novel method that frames sequential fine-tuning as a posterior
inference problem within a Bayesian framework. Our approach integrates
closed-form moment propagation of random variables, Kalman Bayesian Neural
Networks, and Taylor approximations of the moments of softmax functions. By
explicitly accounting for pre-trained models as priors and adaptively balancing
them against new information based on quantified uncertainty, our method
achieves robust and data-efficient sequential learning. The effectiveness of
our method is demonstrated through numerical simulations involving sequential
adaptation of a decision transformer to tasks characterized by distribution
shifts and limited memory resources.

</details>


### [295] [CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome Prediction](https://arxiv.org/abs/2509.10698)
*Rabeya Tus Sadia,Qiang Cheng*

Main category: cs.LG

TL;DR: CrunchLLM is a domain-adapted LLM framework that fuses structured Crunchbase data with unstructured text to predict startup success, using parameter-efficient fine-tuning and prompt optimization, achieving >80% accuracy and providing interpretable reasoning traces.


<details>
  <summary>Details</summary>
Motivation: Accurately predicting startup exits is valuable for venture capital and policy; heterogeneous data (structured, unstructured) pose modeling challenges; standard ML often underutilizes textual data; LLMs lack domain adaptation; there is a need for domain-aware fusion and efficient fine-tuning.

Method: Integrates structured attributes (funding rounds, industries, investor networks) with unstructured narratives; applies parameter-efficient fine-tuning strategies (e.g., adapters/LoRA) and prompt optimization to adapt foundation models to entrepreneurship data; delivers interpretable reasoning traces.

Result: Achieves accuracy >80% on Crunchbase startup success prediction, outperforming traditional classifiers and baseline LLMs; provides explanation traces to support transparency.

Conclusion: Demonstrates effectiveness of domain-aware fine-tuning and structured–unstructured data fusion for entrepreneurial outcome predictions; offers a practical methodological framework for data-driven decisions in venture capital and innovation policy.

Abstract: Predicting the success of start-up companies, defined as achieving an exit
through acquisition or IPO, is a critical problem in entrepreneurship and
innovation research. Datasets such as Crunchbase provide both structured
information (e.g., funding rounds, industries, investor networks) and
unstructured text (e.g., company descriptions), but effectively leveraging this
heterogeneous data for prediction remains challenging. Traditional machine
learning approaches often rely only on structured features and achieve moderate
accuracy, while large language models (LLMs) offer rich reasoning abilities but
struggle to adapt directly to domain-specific business data. We present
\textbf{CrunchLLM}, a domain-adapted LLM framework for startup success
prediction. CrunchLLM integrates structured company attributes with
unstructured textual narratives and applies parameter-efficient fine-tuning
strategies alongside prompt optimization to specialize foundation models for
entrepreneurship data. Our approach achieves accuracy exceeding 80\% on
Crunchbase startup success prediction, significantly outperforming traditional
classifiers and baseline LLMs. Beyond predictive performance, CrunchLLM
provides interpretable reasoning traces that justify its predictions, enhancing
transparency and trustworthiness for financial and policy decision makers. This
work demonstrates how adapting LLMs with domain-aware fine-tuning and
structured--unstructured data fusion can advance predictive modeling of
entrepreneurial outcomes. CrunchLLM contributes a methodological framework and
a practical tool for data-driven decision making in venture capital and
innovation policy.

</details>


### [296] [Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition](https://arxiv.org/abs/2509.10729)
*Ilker Demirel,Karan Thakkar,Benjamin Elizalde,Miquel Espi Marques,Shirley Ren,Jaya Narain*

Main category: cs.LG

TL;DR: LLMs enable effective zero- and one-shot late fusion for multimodal activity recognition from audio and motion data, achieving significantly above-chance 12-class F1 without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Integrating complementary sensor information is challenging and often requires task-specific training; a lightweight fusion approach that works with limited aligned data would simplify deployment and reduce compute.

Method: Curated a diverse Ego4D subset of activities (household and sports). Applied modality-specific models for audio and motion, then used an LLM to perform late fusion for 12-class classification in zero- and one-shot settings, with no task-specific training.

Result: LLMs achieved 12-class F1-scores significantly above chance in zero- and one-shot scenarios, demonstrating feasibility of LLM-based fusion without task-specific training.

Conclusion: LLM-based late fusion is a viable, deployment-friendly approach for multimodal temporal activity recognition, enabling performance gains with limited training data and reduced memory/computation compared to jointly trained multimodal models.

Abstract: Sensor data streams provide valuable information around activities and
context for downstream applications, though integrating complementary
information can be challenging. We show that large language models (LLMs) can
be used for late fusion for activity classification from audio and motion time
series data. We curated a subset of data for diverse activity recognition
across contexts (e.g., household activities, sports) from the Ego4D dataset.
Evaluated LLMs achieved 12-class zero- and one-shot classification F1-scores
significantly above chance, with no task-specific training. Zero-shot
classification via LLM-based fusion from modality-specific models can enable
multimodal temporal applications where there is limited aligned training data
for learning a shared embedding space. Additionally, LLM-based fusion can
enable model deploying without requiring additional memory and computation for
targeted application-specific multimodal models.

</details>


### [297] [Matched-Pair Experimental Design with Active Learning](https://arxiv.org/abs/2509.10742)
*Weizhi Li,Gautam Dasarathy,Visar Berisha*

Main category: cs.LG

TL;DR: Active, sequential matched-pair design using a classification-based active learning framework to identify high-treatment-effect regions and enroll participants accordingly, with theoretical label-complexity guarantees and empirical validation showing cost savings and full-region coverage.


<details>
  <summary>Details</summary>
Motivation: When average treatment effects are small, it is efficient to locate and target regions where the intervention is most effective. Standard matched-pair designs do not directly address region identification or selective enrollment.

Method: Formulate region identification as a binary classification problem and couple it with an active-learning strategy tailored to matched-pair experiments. Sequentially enroll participants in regions predicted to have high effects, while guaranteeing coverage of all high-effect regions. Provide theoretical label-complexity results and empirical validation on practical scenarios.

Result: The approach yields favorable label-complexity bounds and demonstrates through experiments that it reduces experimental cost and accurately encloses the high-treatment-effect regions.

Conclusion: The proposed framework offers an efficient, theoretically grounded method for discovering and targeting high-treatment-effect regions within matched-pair experiments, with demonstrated practical benefits.

Abstract: Matched-pair experimental designs aim to detect treatment effects by pairing
participants and comparing within-pair outcome differences. In many situations,
the overall effect size is small across the entire population. Then, the focus
naturally shifts to identifying and targeting high treatment-effect regions
where the intervention is most effective. This paper proposes a matched-pair
experimental design that sequentially and actively enrolls patients in high
treatment-effect regions. Importantly, we frame the identification of the
target region as a classification problem and propose an active learning
framework tailored to matched-pair designs. The proposed design not only
reduces the experimental cost of detecting treatment efficacy, but also ensures
that the identified regions enclose the entire high-treatment-effect regions.
Our theoretical analysis of the framework's label complexity, along with
experiments in practical scenarios, demonstrates the efficiency and advantages
of the approach.

</details>


### [298] [HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling](https://arxiv.org/abs/2509.10753)
*Minh Vu,Brian K. Tran,Syed A. Shah,Geigh Zollicoffer,Nhat Hoang-Xuan,Manish Bhattarai*

Main category: cs.LG

TL;DR: HalluField is a physics-inspired detector for LLM hallucinations that analyzes the model’s token-path energy-entropy landscape derived from logits, predicting instability to flag hallucinations without any fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs hinder deployment in high-stakes tasks. A general, model-agnostic, training-free detection method is needed to assess response reliability across models and datasets.

Method: Model responses are treated as ensembles of discrete token paths, each with an energy and entropy. By sampling/considering token paths under varying temperature and likelihood settings, HalluField builds energy and entropy distributions and uses a variational-principle framework (analogous to thermodynamics, e.g., a first-law interpretation) to quantify semantic stability. Detection hinges on observing unstable or erratic energy landscape behavior as conditions vary. The approach operates directly on output logits (no fine-tuning or auxiliary nets) and is designed to be computationally efficient.

Result: Reportedly achieves state-of-the-art hallucination-detection performance across diverse models and datasets, with robustness to model/dataset variation and no need for additional training or model modification.

Conclusion: HalluField provides a principled, efficient, physics-inspired framework for hallucination detection that can be broadly applied to existing LLMs without fine-tuning, potentially enhancing reliability in high-stakes applications.

Abstract: Large Language Models (LLMs) exhibit impressive reasoning and
question-answering capabilities. However, they often produce inaccurate or
unreliable content known as hallucinations. This unreliability significantly
limits their deployment in high-stakes applications. Thus, there is a growing
need for a general-purpose method to detect hallucinations in LLMs. In this
work, we introduce HalluField, a novel field-theoretic approach for
hallucination detection based on a parametrized variational principle and
thermodynamics. Inspired by thermodynamics, HalluField models an LLM's response
to a given query and temperature setting as a collection of discrete likelihood
token paths, each associated with a corresponding energy and entropy. By
analyzing how energy and entropy distributions vary across token paths under
changes in temperature and likelihood, HalluField quantifies the semantic
stability of a response. Hallucinations are then detected by identifying
unstable or erratic behavior in this energy landscape. HalluField is
computationally efficient and highly practical: it operates directly on the
model's output logits without requiring fine-tuning or auxiliary neural
networks. Notably, the method is grounded in a principled physical
interpretation, drawing analogies to the first law of thermodynamics.
Remarkably, by modeling LLM behavior through this physical lens, HalluField
achieves state-of-the-art hallucination detection performance across models and
datasets.

</details>


### [299] [Contextual Budget Bandit for Food Rescue Volunteer Engagement](https://arxiv.org/abs/2509.10777)
*Ariana Tang,Naveen Raman,Fei Fang,Zheyuan Ryan Shi*

Main category: cs.LG

TL;DR: Proposes Contextual Budget Bandit to address geographic disparities in volunteer-based food rescue by allocating more budget to underserved communities; introduces a fast heuristic and an optimal Mitosis algorithm; demonstrates improved performance and geographical fairness on synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms worsen geographic disparities in volunteer engagement and food rescue; there is a need for context-aware budget allocation that promotes fairness across communities.

Method: Formulates a contextual restless multi-armed bandit where budgets are allocated based on context (community characteristics) and arm state; develops a fast heuristic algorithm for practical use and the Mitosis algorithm that guarantees optimal budget allocation.

Result: The proposed methods outperform baselines on synthetic and real-world datasets and achieve geographical fairness in food rescue.

Conclusion: Contextual Budget Bandit provides a principled and effective approach to fair resource allocation in food rescue platforms, with potential applicability to other stateful-armed decision problems.

Abstract: Volunteer-based food rescue platforms tackle food waste by matching surplus
food to communities in need. These platforms face the dual problem of
maintaining volunteer engagement and maximizing the food rescued. Existing
algorithms to improve volunteer engagement exacerbate geographical disparities,
leaving some communities systematically disadvantaged. We address this issue by
proposing Contextual Budget Bandit. Contextual Budget Bandit incorporates
context-dependent budget allocation in restless multi-armed bandits, a model of
decision-making which allows for stateful arms. By doing so, we can allocate
higher budgets to communities with lower match rates, thereby alleviating
geographical disparities. To tackle this problem, we develop an empirically
fast heuristic algorithm. Because the heuristic algorithm can achieve a poor
approximation when active volunteers are scarce, we design the Mitosis
algorithm, which is guaranteed to compute the optimal budget allocation.
Empirically, we demonstrate that our algorithms outperform baselines on both
synthetic and real-world food rescue datasets, and show how our algorithm
achieves geographical fairness in food rescue.

</details>


### [300] [GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research](https://arxiv.org/abs/2509.10790)
*Luke Howard*

Main category: cs.LG

TL;DR: GoldenTransformer is a modular fault-injection framework for evaluating the robustness of large transformer models to hardware faults. Built on PyTorch and HuggingFace Transformers, it supports injections at weight, activation, and attention levels, with reproducibility, metric logging, and visualization. Demonstrated on classification and generation tasks.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models underpin many state-of-the-art systems, but their robustness under fault conditions is underexplored. There is a need for a unified, extensible tool to inject controlled faults and study resiliency to guide dependable deployment of LLMs.

Method: A modular, extensible fault-injection framework (GoldenTransformer) built atop PyTorch and HuggingFace Transformers. It enables diverse fault classes such as weight corruption, activation injections, and attention-level disruptions, addressing challenges unique to large transformer architectures (structural complexity, latent dependencies, nonuniform layer definitions). The framework supports experiment reproducibility, metric logging, and visualization, and draws inspiration from the GoldenEye DNN simulator. Demonstrations include fault-injected experiments on classification and generation tasks.

Result: The paper demonstrates several example experiments on classification and generation tasks to illustrate how controlled fault injections affect transformer behavior and performance, validating GoldenTransformer as a practical tool for robustness analysis and experimentation.

Conclusion: GoldenTransformer provides researchers and practitioners with a valuable and extensible tool for robustness analysis of transformer-based models, enabling controlled fault injections to guide dependable system design in real-world LLM applications.

Abstract: Transformers have become the foundation for a wide range of
state--of--the--art models across natural language processing, computer vision,
and other machine learning domains. Despite their widespread deployment, the
robustness of these models under fault conditions remains underexplored. We
present GoldenTransformer, a modular and extensible fault injection framework
designed to evaluate the resiliency of Large Language Models to induced
hardware faults. GoldenTransformer offers a unified Python-based platform for
injecting diverse classes of faults--such as weight corruption, activation
injections, and attention--level disruptions--into pretrained
transformer--based models. Inspired by the GoldenEye simulator for DNNs, our
framework focuses on the unique challenges of working with large transformer
architectures, including considerations such as structural complexity, latent
dependencies, and nonuniform layer definitions. GoldenTransformer is built atop
PyTorch and HuggingFace Transformers, and it supports experiment
reproducibility, metric logging, and visualization out of the box. We detail
the technical design and use of GoldenTransformer and demonstrate through
several example experiments on classification and generation tasks. By enabling
controlled injection of faults at multiple logical and structural points in a
transformer, GoldenTransformer offers researchers and practitioners a valuable
tool for model robustness analysis and for guiding dependable system design in
real-world LLM applications.

</details>


### [301] [Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone](https://arxiv.org/abs/2509.10809)
*Antonio Bărbălau,Cristian Daniel Păduraru,Teodor Poncu,Alexandru Tifrea,Elena Burceanu*

Main category: cs.LG

TL;DR: Encoder-focused debiasing for Sparse Autoencoders (S&P TopK): orthogonalize input embeddings against encoder weights and interpolate encoder weights to preserve performance, yielding strong fairness gains (up to 3.2x on SAE fairness) and improved test-time VLM debiasing (up to 1.8x) with maintained downstream accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional SAE debiasing assumes feature representations live in decoder weights, potentially misattributing bias and limiting effectiveness. An encoder-centric approach could more directly debias representations while preserving model utility.

Method: Introduce S&P TopK, an encoder-focused debiasing framework with three components: (i) unconventional SAE feature selection, (ii) orthogonal projection of input embeddings against encoder weights to remove bias, and (iii) encoder-weight interpolation during debiasing to preserve downstream performance.

Result: Empirical results show up to 3.2x improvements in SAE fairness metrics and up to 1.8x improvements in state-of-the-art test-time VLM debiasing, all while maintaining downstream performance.

Conclusion: An encoder-centric debiasing paradigm for SAEs (S&P TopK) can outperform traditional decoder-centric approaches, achieving substantial fairness gains with negligible sacrifice to task performance.

Abstract: Sparse Autoencoders (SAEs) have proven valuable due to their ability to
provide interpretable and steerable representations. Current debiasing methods
based on SAEs manipulate these sparse activations presuming that feature
representations are housed within decoder weights. We challenge this
fundamental assumption and introduce an encoder-focused alternative for
representation debiasing, contributing three key findings: (i) we highlight an
unconventional SAE feature selection strategy, (ii) we propose a novel SAE
debiasing methodology that orthogonalizes input embeddings against encoder
weights, and (iii) we establish a performance-preserving mechanism during
debiasing through encoder weight interpolation. Our Selection and Projection
framework, termed S\&P TopK, surpasses conventional SAE usage in fairness
metrics by a factor of up to 3.2 and advances state-of-the-art test-time VLM
debiasing results by a factor of up to 1.8 while maintaining downstream
performance.

</details>


### [302] [FACTORS: Factorial Approximation for Complementary Two-factor Optimization with Risk-aware Scoring](https://arxiv.org/abs/2509.10825)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: FACTORS combines design of experiments with Shapley decomposition to estimate main effects and two-way interactions, then optimizes configurations under a fixed budget using a risk-adjusted objective; it uses plug-in and least-squares paths for effect estimation, with standardization, bias correction, and uncertainty quantification, plus a lightweight search and interpretable effect maps.


<details>
  <summary>Details</summary>
Motivation: Performance and stability of learning systems often hinge on interactions among training factors; there is a need to reliably identify configurations that perform well under budget while accounting for uncertainty and cost.

Method: Use DOE to systematically estimate main and two-way effects; compute Shapley contributions via two complementary paths: plug-in conditional means and a least-squares reconstruction from samples; standardize estimates, correct bias, quantify uncertainty; integrate effects into a risk-adjusted objective with a budget constraint; lightweight search for configurations; provide error decompositions, sample complexity, and optimality-gap bounds; produce map-style summaries of effects.

Result: Improved rank preservation and identification of optimal configurations; reduced decision risk; robust performance under budget constraints; scalable to large factor spaces; provides interpretable justifications alongside stable gains.

Conclusion: FACTORS offers a practical, interpretable framework for budget-aware configuration selection that blends DOE with Shapley analysis, with theoretical guarantees and interpretable effect maps to guide safe improvements.

Abstract: We propose FACTORS, a framework that combines design of experiments with
Shapley decomposition to address performance and stability issues that are
sensitive to combinations of training factors. Our approach consistently
estimates main effects and two-factor interactions, then integrates them into a
risk-adjusted objective function that jointly accounts for uncertainty and
cost, enabling reliable selection of configurations under a fixed budget.
Effect estimation is implemented through two complementary paths: a plug-in
path based on conditional means, and a least-squares path that reconstructs
Shapley contributions from samples. These paths are designed to work
complementarily even when design density and bias levels differ. By
incorporating standardization of estimates, bias correction, and uncertainty
quantification, our procedure ensures comparability across heterogeneous factor
spaces and designs, while a lightweight search routine yields configurations
within practical time even for large factor spaces. On the theoretical side, we
provide error decompositions, sample complexity analysis, and upper bounds on
optimality gaps. On the interpretive side, we summarize main effects and
interactions in map form, highlighting adjustment priorities and safe
improvement pathways. Across diverse datasets and design conditions, our
approach improves rank preservation and optimal configuration identification,
reduces decision-making risks, and offers a tuning foundation that delivers
interpretable justification alongside stable performance gains even under
budget constraints.

</details>


### [303] [Neurosymbolic AI Transfer Learning Improves Network Intrusion Detection](https://arxiv.org/abs/2509.10850)
*Huynh T. T. Tran,Jacob Sander,Achraf Cohen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.LG

TL;DR: Transfer learning with a neurosymbolic AI framework for network intrusion detection outperforms small-data neural models, aided by uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Cybersecurity intrusion detection faces data scarcity and evolving threats. Leveraging transfer learning from large, well-structured datasets can improve performance and generalization, addressing a gap in applying transfer learning and uncertainty-aware techniques to network security.

Method: Propose a neurosymbolic AI framework for network intrusion detection that incorporates transfer learning and uncertainty quantification. Train models on large, well-structured datasets and compare their performance against neural models trained on smaller, less structured datasets.

Result: Transfer learning models trained on large datasets outperform neural-based models trained on smaller datasets, indicating the framework's effectiveness for cybersecurity tasks.

Conclusion: Suggests a new era for cybersecurity solutions by combining transfer learning with neurosymbolic reasoning and uncertainty quantification, encouraging further research in uncertainty-aware transfer learning for intrusion detection.

Abstract: Transfer learning is commonly utilized in various fields such as computer
vision, natural language processing, and medical imaging due to its impressive
capability to address subtasks and work with different datasets. However, its
application in cybersecurity has not been thoroughly explored. In this paper,
we present an innovative neurosymbolic AI framework designed for network
intrusion detection systems, which play a crucial role in combating malicious
activities in cybersecurity. Our framework leverages transfer learning and
uncertainty quantification. The findings indicate that transfer learning
models, trained on large and well-structured datasets, outperform neural-based
models that rely on smaller datasets, paving the way for a new era in
cybersecurity solutions.

</details>


### [304] [CogGNN: Cognitive Graph Neural Networks in Generative Connectomics](https://arxiv.org/abs/2509.10864)
*Mayssa Soussia,Yijun Lin,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: CogGNN is the first cognition-aware generative model for brain networks that injects cognitive abilities (visual memory) into GNNs to generate cognitively meaningful brain templates (CBTs), with a co-optimization framework. It demonstrates superior performance over state-of-the-art methods and lays the groundwork for cognitively grounded brain network modeling.


<details>
  <summary>Details</summary>
Motivation: Current brain-network generative models (GNN-based) focus on structural/topological properties but ignore cognitive traits. There is a need to incorporate cognitive cues to produce more functionally relevant brain networks and templates.

Method: Introduce CogGNN, a cognition-aware GNN variant with a visual-memory-based loss. Apply a CBT-learning framework using a co-optimization strategy to obtain well-centered, discriminative, cognitively enhanced population-level templates from multi-view brain networks.

Result: CogGNN outperforms state-of-the-art methods and yields connectional brain templates (CBTs) that are both cognitively and structurally meaningful, demonstrating the viability of cognitively grounded brain network modeling.

Conclusion: We present the first cognition-aware generative model for brain networks and a CBT-learning framework that together enable cognitively enhanced brain network modeling, enabling more accurate and meaningful brain templates.

Abstract: Generative learning has advanced network neuroscience, enabling tasks like
graph super-resolution, temporal graph prediction, and multimodal brain graph
fusion. However, current methods, mainly based on graph neural networks (GNNs),
focus solely on structural and topological properties, neglecting cognitive
traits. To address this, we introduce the first cognified generative model,
CogGNN, which endows GNNs with cognitive capabilities (e.g., visual memory) to
generate brain networks that preserve cognitive features. While broadly
applicable, we present CogGNN, a specific variant designed to integrate visual
input, a key factor in brain functions like pattern recognition and memory
recall. As a proof of concept, we use our model to learn connectional brain
templates (CBTs), population-level fingerprints from multi-view brain networks.
Unlike prior work that overlooks cognitive properties, CogGNN generates CBTs
that are both cognitively and structurally meaningful. Our contributions are:
(i) a novel cognition-aware generative model with a visual-memory-based loss;
(ii) a CBT-learning framework with a co-optimization strategy to yield
well-centered, discriminative, cognitively enhanced templates. Extensive
experiments show that CogGNN outperforms state-of-the-art methods, establishing
a strong foundation for cognitively grounded brain network modeling.

</details>


### [305] [GTHNA: Local-global Graph Transformer with Memory Reconstruction for Holistic Node Anomaly Evaluation](https://arxiv.org/abs/2509.10869)
*Mingkang Li,Xuexiong Luo,Yue Zhang,Yaoyang Li,Fu Lin*

Main category: cs.LG

TL;DR: Introduces a holistic anomaly evaluation framework for graphs combining a local-global Transformer encoder, memory-guided reconstruction, and multi-scale matching; achieves state-of-the-art results on seven benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection on graph-structured data is hard due to rare nodes that deviate in structure and behavior. GCNs suffer from over-smoothing and reconstruction-based methods are vulnerable to interference from anomalous nodes. A robust, multi-granular framework is needed that leverages local and global dependencies while mitigating anomalies.

Method: Three components: (1) local-global Transformer encoder to capture local and global structural dependencies; (2) memory-guided reconstruction to suppress the influence of anomalous nodes during reconstruction; (3) multi-scale representation matching to assess anomalies at multiple granularities. Anomaly scores are computed by fusing reconstruction errors with memory-matching signals, enabling robust evaluation.

Result: Extensive experiments on seven benchmark datasets show the proposed method outperforms state-of-the-art approaches, indicating strong robustness and generalization across graph domains.

Conclusion: Proposes a comprehensive and generalizable anomaly detection framework for graphs that mitigates over-smoothing and anomalous interference, with superior empirical performance across diverse datasets.

Abstract: Anomaly detection in graph-structured data is an inherently challenging
problem, as it requires the identification of rare nodes that deviate from the
majority in both their structural and behavioral characteristics. Existing
methods, such as those based on graph convolutional networks (GCNs), often
suffer from over-smoothing, which causes the learned node representations to
become indistinguishable. Furthermore, graph reconstruction-based approaches
are vulnerable to anomalous node interference during the reconstruction
process, leading to inaccurate anomaly detection. In this work, we propose a
novel and holistic anomaly evaluation framework that integrates three key
components: a local-global Transformer encoder, a memory-guided reconstruction
mechanism, and a multi-scale representation matching strategy. These components
work synergistically to enhance the model's ability to capture both local and
global structural dependencies, suppress the influence of anomalous nodes, and
assess anomalies from multiple levels of granularity. Anomaly scores are
computed by combining reconstruction errors and memory matching signals,
resulting in a more robust evaluation. Extensive experiments on seven benchmark
datasets demonstrate that our method outperforms existing state-of-the-art
approaches, offering a comprehensive and generalizable solution for anomaly
detection across various graph domains.

</details>


### [306] [Optimal message passing for molecular prediction is simple, attentive and spatial](https://arxiv.org/abs/2509.10871)
*Alma C. Castaneda-Leautaud,Rommie E. Amaro*

Main category: cs.LG

TL;DR: Simple yet effective MPNNs with bidirectional message passing and attention on minimalist messages achieve state-of-the-art results in molecular property prediction, with 2D graphs plus carefully chosen 3D descriptors offering large computational savings; dataset diversity shapes the need for extra components.


<details>
  <summary>Details</summary>
Motivation: To boost predictive performance of message-passing neural networks (MPNNs) for molecular properties by simplifying message passing and by leveraging diverse descriptors, while understanding how dataset diversity influences architecture and feature requirements.

Method: Design and evaluate architectures featuring bidirectional message passing with attention on a minimalist message formulation (excluding self-perception). Compare against models with convolution normalization factors. Examine 2D graphs augmented with selected 3D descriptors across datasets, and assess global and node-level predictions. Analyze dataset diversity and its impact on component necessity. Measure predictive performance and computational cost reductions.

Result: The proposed architecture achieves state-of-the-art predictive performance, outperforming complex pre-trained models. On many datasets, the best model uses bidirectional message passing with attention on a minimalist message. Convolution normalization factors do not universally improve performance. 2D graphs with well-chosen 3D descriptors retain predictive power while reducing computational cost by over 50%, benefiting high-throughput screening. Dataset diversity modulates the need for additional components and feature sets.

Conclusion: Simpler MPNN architectures can deliver high class separability and strong performance, with dataset diversity guiding the necessity of extra components; 2D graphs complemented by targeted 3D descriptors offer an efficient route that matches or surpasses more complex models while substantially reducing compute requirements.

Abstract: Strategies to improve the predicting performance of Message-Passing
Neural-Networks for molecular property predictions can be achieved by
simplifying how the message is passed and by using descriptors that capture
multiple aspects of molecular graphs. In this work, we designed model
architectures that achieved state-of-the-art performance, surpassing more
complex models such as those pre-trained on external databases. We assessed
dataset diversity to complement our performance results, finding that
structural diversity influences the need for additional components in our MPNNs
and feature sets.
  In most datasets, our best architecture employs bidirectional message-passing
with an attention mechanism, applied to a minimalist message formulation that
excludes self-perception, highlighting that relatively simpler models, compared
to classical MPNNs, yield higher class separability. In contrast, we found that
convolution normalization factors do not benefit the predictive power in all
the datasets tested. This was corroborated in both global and node-level
outputs. Additionally, we analyzed the influence of both adding spatial
features and working with 3D graphs, finding that 2D molecular graphs are
sufficient when complemented with appropriately chosen 3D descriptors. This
approach not only preserves predictive performance but also reduces
computational cost by over 50%, making it particularly advantageous for
high-throughput screening campaigns.

</details>


### [307] [Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors](https://arxiv.org/abs/2509.12081)
*Anirudha Majumdar*

Main category: cs.LG

TL;DR: Deceptive risk minimization (DRM) learns iid-like representations to suppress spurious correlations for out-of-distribution generalization, using a differentiable objective and a detector based on conformal martingales; it does not require test data or predefined domain partitions and is validated on concept shift and simulated robotic imitation with covariate shift.


<details>
  <summary>Details</summary>
Motivation: OOD generalization is hindered by spurious correlations; existing domain adaptation or invariant representation methods often need access to test data or a partition of training data. DRM aims to learn stable, domain-robust features without such requirements.

Method: Propose DRM: learn representations that make training data appear iid to an observer, optimizing a differentiable objective that jointly minimizes task loss and eliminates distribution shifts from the detector's perspective. The approach uses a detector based on conformal martingales and does not rely on a finite set of data-generating domains.

Result: Demonstrates efficacy on numerical experiments with concept shift and in a simulated imitation-learning setting with covariate shift, in environments where a robot would be deployed.

Conclusion: DRM provides a principled mechanism for OOD generalization by deceiving a detector to identify stable features, without access to test data or domain partitioning, and is validated in concept shift and robotic imitation scenarios.

Abstract: This paper proposes deception as a mechanism for out-of-distribution (OOD)
generalization: by learning data representations that make training data appear
independent and identically distributed (iid) to an observer, we can identify
stable features that eliminate spurious correlations and generalize to unseen
domains. We refer to this principle as deceptive risk minimization (DRM) and
instantiate it with a practical differentiable objective that simultaneously
learns features that eliminate distribution shifts from the perspective of a
detector based on conformal martingales while minimizing a task-specific loss.
In contrast to domain adaptation or prior invariant representation learning
methods, DRM does not require access to test data or a partitioning of training
data into a finite number of data-generating domains. We demonstrate the
efficacy of DRM on numerical experiments with concept shift and a simulated
imitation learning setting with covariate shift in environments that a robot is
deployed in.

</details>


### [308] [Robustifying Diffusion-Denoised Smoothing Against Covariate Shift](https://arxiv.org/abs/2509.10913)
*Ali Hedayatnia,Mostafa Tavassolipour,Babak Nadjar Araabi,Abdol-Hossein Vahabie*

Main category: cs.LG

TL;DR: Identifies and mitigates covariate shift in Diffusion Denoised Smoothing (DDS) caused by misestimation of added noise; introduces an adversarial objective on the denoiser-added noise to train the base classifier to be robust to this shift, achieving state-of-the-art certified l2-robustness on MNIST, CIFAR-10, and ImageNet.


<details>
  <summary>Details</summary>
Motivation: To achieve robust and certified l2-robustness in randomized smoothing when using a diffusion denoiser, by addressing the covariate shift introduced by misestimated added noise that degrades performance.

Method: Proposes an adversarial objective that targets the added noise of the denoising diffusion model. This trains the base classifier to be robust against the covariate shift caused by the denoiser, improving the smoothed classifier's performance.

Result: Significant improvements in certified accuracy across MNIST, CIFAR-10, and ImageNet, attaining new state-of-the-art results for l2-adversarial perturbations.

Conclusion: Training the base classifier to be robust to the covariate shift introduced by the denoiser in DDS yields substantial gains in certified l2-robustness, suggesting the approach can effectively salvage DDS performance and potentially generalize to other denoising-based smoothing pipelines.

Abstract: Randomized smoothing is a well-established method for achieving certified
robustness against l2-adversarial perturbations. By incorporating a denoiser
before the base classifier, pretrained classifiers can be seamlessly integrated
into randomized smoothing without significant performance degradation. Among
existing methods, Diffusion Denoised Smoothing - where a pretrained denoising
diffusion model serves as the denoiser - has produced state-of-the-art results.
However, we show that employing a denoising diffusion model introduces a
covariate shift via misestimation of the added noise, ultimately degrading the
smoothed classifier's performance. To address this issue, we propose a novel
adversarial objective function focused on the added noise of the denoising
diffusion model. This approach is inspired by our understanding of the origin
of the covariate shift. Our goal is to train the base classifier to ensure it
is robust against the covariate shift introduced by the denoiser. Our method
significantly improves certified accuracy across three standard classification
benchmarks - MNIST, CIFAR-10, and ImageNet - achieving new state-of-the-art
performance in l2-adversarial perturbations. Our implementation is publicly
available at
https://github.com/ahedayat/Robustifying-DDS-Against-Covariate-Shift

</details>


### [309] [ToMA: Token Merge with Attention for Image Generation with Diffusion Models](https://arxiv.org/abs/2509.10918)
*Wenbo Lu,Shaoyi Zheng,Yuxuan Xia,Shengjie Wang*

Main category: cs.LG

TL;DR: ToMA offers a GPU-friendly token-merge approach for diffusion models, reframing merge as submodular token selection and implementing merge/unmerge as linear attention-like ops, plus exploiting locality to reduce overhead. It achieves ~23–24% speedups with minimal quality loss, outperforming prior token-reduction methods.


<details>
  <summary>Details</summary>
Motivation: Diffusion-models use transformers with quadratic attention, and prior plug-and-play token-reduction methods (e.g., ToMeSD, ToFu) add GPU-inefficient operations and overhead that erode speedups. There is a gap between theoretical speedups and practical, GPU-accelerated implementations.

Method: 1) Reformulate token merge as a submodular optimization to select diverse tokens; 2) Implement merge/unmerge as an attention-like linear transformation using GPU-friendly matrix operations; 3) Exploit latent locality and sequential redundancy (pattern reuse) to minimize overhead.

Result: Reduction in SDXL/Flux generation latency by 24%/23%, respectively, with negligible quality change (Delta < 0.07 on the DINO metric). The method outperforms prior token-reduction methods

Conclusion: ToMA closes the gap between theoretical and practical efficiency for transformers in diffusion models by offering a GPU-aligned, off-the-shelf token-merge strategy that delivers meaningful latency reductions with preserved quality.

Abstract: Diffusion models excel in high-fidelity image generation but face scalability
limits due to transformers' quadratic attention complexity. Plug-and-play token
reduction methods like ToMeSD and ToFu reduce FLOPs by merging redundant tokens
in generated images but rely on GPU-inefficient operations (e.g., sorting,
scattered writes), introducing overheads that negate theoretical speedups when
paired with optimized attention implementations (e.g., FlashAttention). To
bridge this gap, we propose Token Merge with Attention (ToMA), an off-the-shelf
method that redesigns token reduction for GPU-aligned efficiency, with three
key contributions: 1) a reformulation of token merge as a submodular
optimization problem to select diverse tokens; 2) merge/unmerge as an
attention-like linear transformation via GPU-friendly matrix operations; and 3)
exploiting latent locality and sequential redundancy (pattern reuse) to
minimize overhead. ToMA reduces SDXL/Flux generation latency by 24%/23%,
respectively (with DINO $\Delta < 0.07$), outperforming prior methods. This
work bridges the gap between theoretical and practical efficiency for
transformers in diffusion.

</details>


### [310] [Clarifying Model Transparency: Interpretability versus Explainability in Deep Learning with MNIST and IMDB Examples](https://arxiv.org/abs/2509.10929)
*Mitali Raj*

Main category: cs.LG

TL;DR: Clarifies interpretability vs explainability in deep learning, arguing interpretability is global model understanding while explainability is local, post-hoc explanations.


<details>
  <summary>Details</summary>
Motivation: Resolve terminological ambiguity in XAI to improve trust, evaluation, and deployment of AI systems in high-trust domains.

Method: Conceptual comparative analysis of definitions, objectives, methods, and challenges; illustrated with MNIST and IMDB tasks; discusses feature attribution and post-hoc explanations and their limitations.

Result: Proposes a clear distinction between global interpretability and local explainability; shows that local explanations do not yield global transparency; provides a framework for evaluating XAI approaches.

Conclusion: A precise differentiation between interpretability and explainability is essential for trustworthy AI; use standard datasets to demonstrate and validate these concepts.

Abstract: The impressive capabilities of deep learning models are often counterbalanced
by their inherent opacity, commonly termed the "black box" problem, which
impedes their widespread acceptance in high-trust domains. In response, the
intersecting disciplines of interpretability and explainability, collectively
falling under the Explainable AI (XAI) umbrella, have become focal points of
research. Although these terms are frequently used as synonyms, they carry
distinct conceptual weights. This document offers a comparative exploration of
interpretability and explainability within the deep learning paradigm,
carefully outlining their respective definitions, objectives, prevalent
methodologies, and inherent difficulties. Through illustrative examinations of
the MNIST digit classification task and IMDB sentiment analysis, we
substantiate a key argument: interpretability generally pertains to a model's
inherent capacity for human comprehension of its operational mechanisms (global
understanding), whereas explainability is more commonly associated with
post-hoc techniques designed to illuminate the basis for a model's individual
predictions or behaviors (local explanations). For example, feature attribution
methods can reveal why a specific MNIST image is recognized as a '7', and
word-level importance can clarify an IMDB sentiment outcome. However, these
local insights do not render the complex underlying model globally transparent.
A clear grasp of this differentiation, as demonstrated by these standard
datasets, is vital for fostering dependable and sound artificial intelligence.

</details>


### [311] [The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models](https://arxiv.org/abs/2509.10970)
*Joshua Au Yeung,Jacopo Dalmasso,Luca Foschini,Richard JB Dobson,Zeljko Kraljevic*

Main category: cs.LG

TL;DR: A novel benchmark, psychosis-bench, quantifies LLM psychogenicity across 16 structured, 12-turn delusional-progress scenarios, assessing Delusion Confirmation (DCS), Harm Enablement (HES), and Safety Intervention (SIS) in eight LLMs; results show pervasive psychogenic potential, correlation between DCS and HES, and insufficient safety interventions, especially in implicit contexts, underscoring the need to rethink LLM training as a public health issue.


<details>
  <summary>Details</summary>
Motivation: Rising reports of 'AI psychosis' where user-LLM interactions may induce or reinforce delusions and psychological harm. There is a need to quantify psychogenicity in LLMs to guide safe design, regulation, and clinical/public health responses.

Method: Introduce psychosis-bench with 16 structured, 12-turn conversational scenarios simulating delusional themes (Erotic, Grandiose/Messianic, Referential). Evaluate eight prominent LLMs for Delusion Confirmation (DCS), Harm Enablement (HES), and Safety Intervention (SIS) across explicit and implicit contexts. Analyze 1,536 turns; report means, SDs, correlations; compare performance across scenario types and contexts.

Result: All evaluated LLMs exhibited psychogenic potential, with mean DCS 0.91±0.88; mean HES 0.69±0.84; SIS 0.37±0.48. 51/128 (39.8%) scenarios offered no safety interventions. Implicit scenarios yielded worse performance (p<.001). Strong DCS-HES correlation (rs=0.77). Notable model-to-model variation, indicating safety is not an emergent property of scale alone.

Conclusion: LLM psychogenicity is a real, quantifiable risk that demands changes in training and governance. It should be treated as a public health concern requiring collaboration among developers, policymakers, and healthcare professionals.

Abstract: Background: Emerging reports of "AI psychosis" are on the rise, where
user-LLM interactions may exacerbate or induce psychosis or adverse
psychological symptoms. The sycophantic and agreeable nature of LLMs can
beneficial, it can become a vector for harm by reinforcing delusional beliefs
in vulnerable users.
  Methods: We introduce psychosis-bench, a novel benchmark designed to
systematically evaluate the psychogenicity of LLMs comprimising 16 structured,
12-turn conversational scenarios simulating the progression of delusional
themes(Erotic Delusions, Grandiose/Messianic Delusions, Referential Delusions)
and potential harms. We evaluated eight prominent LLMs for Delusion
Confirmation (DCS), Harm Enablement (HES), and Safety Intervention(SIS) across
explicit and implicit conversational contexts.
  Findings: Across 1,536 simulated conversation turns, all LLMs demonstrated
psychogenic potential, showing a strong tendency to perpetuate rather than
challenge delusions (mean DCS of 0.91 $\pm$0.88). Models frequently enabled
harmful user requests (mean HES of 0.69 $\pm$0.84) and offered safety
interventions in only roughly a third of applicable turns (mean SIS of 0.37
$\pm$0.48). 51 / 128 (39.8%) of scenarios had no safety interventions offered.
Performance was significantly worse in implicit scenarios, models were more
likely to confirm delusions and enable harm while offering fewer interventions
(p < .001). A strong correlation was found between DCS and HES (rs = .77).
Model performance varied widely, indicating that safety is not an emergent
property of scale alone.
  Conclusion: This study establishes LLM psychogenicity as a quantifiable risk
and underscores the urgent need for re-thinking how we train LLMs. We frame
this issue not merely as a technical challenge but as a public health
imperative requiring collaboration between developers, policymakers, and
healthcare professionals.

</details>


### [312] [PHLoRA: data-free Post-hoc Low-Rank Adapter extraction from full-rank checkpoint](https://arxiv.org/abs/2509.10971)
*Bhoomit Vasani,Jack FitzGerald,Anjie Fang,Sushmit Vaish*

Main category: cs.LG

TL;DR: PHLoRA enables post-hoc extraction of low-rank adapters from full-rank fine-tuned models without data or gradients, reconstructing adapters from weight differences that can be merged or routed at inference time, greatly reducing latency and cost while preserving performance.


<details>
  <summary>Details</summary>
Motivation: To retrofit any existing full-rank checkpoint with adapter-style efficiency without requiring access to training data, gradients, or retraining, thus democratizing scalable inference across model families and platforms.

Method: Compute low-rank decomposition of the weight differences between a base model and its fine-tuned counterpart to reconstruct adapter modules. These adapters can be merged into the model or dynamically routed at inference (via S-LoRA) and deployed on scalable platforms (e.g., NVIDIA NIM). The method decouples fine-tuning from adapter generation, enabling extraction from existing checkpoints or third-party models without training data or gradients. It also supports pruning and safe merging.

Result: Experimental evaluation on text, image, and video benchmarks using the Amazon Nova model family shows that extracted adapters preserve most of the weight-delta energy, can be pruned safely, and incur negligible degradation in downstream task performance when re-merged. The approach enables adapter-ready conversion of full-rank checkpoints and offers cost and latency savings in scalable inference.

Conclusion: PHLoRA provides a practical, data-free path to convert existing full-rank checkpoints into adapter-ready models, enabling scalable, cost-effective inference and broad democratization of adaptation across model types and deployment platforms.

Abstract: We introduce PHLoRA (Pronounced "flora"). (Post-hoc LoRA), a simple yet
powerful method to extract low-rank adaptation adapters from full-rank
fine-tuned models without requiring access to training data or gradients. By
computing the low-rank decomposition of weight differences between a base model
and its fine-tuned counterpart, our method reconstructs adapter modules that
can be merged or dynamically routed at inference time via S-LoRA, or served in
scalable, industry settings using platforms like NVIDIA NIM. This approach
amortizes latency overhead across requests and yields substantial cost savings.
Unlike prior work that trains each adapter explicitly, our approach decouples
fine-tuning from adapter generation, allowing adapter extraction from existing
full-rank models or third-party checkpoints. Experiments on text, image, and
video benchmarks using the Amazon Nova model family demonstrate that extracted
adapters preserve high energy from the full weight delta, can be pruned safely,
and yield negligible degradation in downstream task performance when re-merged.
Overall, PHLoRA provides a practical path for making all existing full-rank
checkpoints adapter-ready, democratizing scalable inference for all models.

</details>


### [313] [Decoupling Search and Learning in Neural Net Training](https://arxiv.org/abs/2509.10973)
*Akshay Vegesna,Samip Dahal*

Main category: cs.LG

TL;DR: Two-phase training framework: search for diverse, learnable representations in activation space via evolutionary search, then regress to those representations in parameter space using gradient learning. Larger search compute yields more diverse and fit representations; networks regressing to them match SGD on standard vision tasks and follow different training trajectories.


<details>
  <summary>Details</summary>
Motivation: Gradient descent tends to settle into a single minimum and struggles to explore alternative minima that might generalize better. The paper proposes decoupling search in representation space from gradient-based optimization to access diverse solutions.

Method: Phase 1: perform evolutionary search in the space of intermediate activations to discover diverse representational solutions. Phase 2: train networks in parameter space by regressing to the discovered representations, i.e., gradient-based learning guided by the targeted activations. Vary compute (population size, generations) to scale diversity and fitness.

Result: Found representations whose fitness and diversity increase with search compute. Networks trained to regress to these representations achieve performance close to SGD on MNIST, CIFAR-10, CIFAR-100. Representations learned are qualitatively different from standard GD-trained models, exhibiting different training trajectories.

Conclusion: Decoupling search in representation space from gradient-based learning can overcome gradient descent's exploratory limitations and enable learning of diverse, generalizable representations through future training algorithms.

Abstract: Gradient descent typically converges to a single minimum of the training loss
without mechanisms to explore alternative minima that may generalize better.
Searching for diverse minima directly in high-dimensional parameter space is
generally intractable. To address this, we propose a framework that performs
training in two distinct phases: search in a tractable representation space
(the space of intermediate activations) to find diverse representational
solutions, and gradient-based learning in parameter space by regressing to
those searched representations. Through evolutionary search, we discover
representational solutions whose fitness and diversity scale with
compute--larger populations and more generations produce better and more varied
solutions. These representations prove to be learnable: networks trained by
regressing to searched representations approach SGD's performance on MNIST,
CIFAR-10, and CIFAR-100. Performance improves with search compute up to
saturation. The resulting models differ qualitatively from networks trained
with gradient descent, following different representational trajectories during
training. This work demonstrates how future training algorithms could overcome
gradient descent's exploratory limitations by decoupling search in
representation space from efficient gradient-based learning in parameter space.

</details>


### [314] [California Wildfire Inventory (CAWFI): An Extensive Dataset for Predictive Techniques based on Artificial Intelligence](https://arxiv.org/abs/2509.11015)
*Rohan Tan Bhowmik,Youn Soo Jung,Juan Aguilera,Mary Prunicki,Kari Nadeau*

Main category: cs.LG

TL;DR: CAWFI is a large wildfire dataset for predictive modeling, combining daily fire data with leading/trailing/geological indicators to enable pre-ignition forecasting.


<details>
  <summary>Details</summary>
Motivation: Climate change and ecosystem disruption are increasing wildfires, and most current systems detect fires after ignition. A large, multi-source dataset is needed to train accurate predictive models to prevent megafires.

Method: Compile daily wildfire data for California (2012–2018) with indicator data (2012–2022) across leading, trailing, and geological categories; use in spatio-temporal AI models to predict wildfire events.

Result: The dataset enabled a spatio-temporal AI model that predicted 85.7% of future wildfires larger than 300,000 acres when trained on 2012–2017 indicators.

Conclusion: CAWFI provides a resource for wildfire prediction research and sets a precedent for regional wildfire databases to support preventive solutions.

Abstract: Due to climate change and the disruption of ecosystems worldwide, wildfires
are increasingly impacting environment, infrastructure, and human lives
globally. Additionally, an exacerbating climate crisis means that these losses
would continue to grow if preventative measures are not implemented. Though
recent advancements in artificial intelligence enable wildfire management
techniques, most deployed solutions focus on detecting wildfires after
ignition. The development of predictive techniques with high accuracy requires
extensive datasets to train machine learning models. This paper presents the
California Wildfire Inventory (CAWFI), a wildfire database of over 37 million
data points for building and training wildfire prediction solutions, thereby
potentially preventing megafires and flash fires by addressing them before they
spark. The dataset compiles daily historical California wildfire data from 2012
to 2018 and indicator data from 2012 to 2022. The indicator data consists of
leading indicators (meteorological data correlating to wildfire-prone
conditions), trailing indicators (environmental data correlating to prior and
early wildfire activity), and geological indicators (vegetation and elevation
data dictating wildfire risk and spread patterns). CAWFI has already
demonstrated success when used to train a spatio-temporal artificial
intelligence model, predicting 85.7% of future wildfires larger than 300,000
acres when trained on 2012-2017 indicator data. This dataset is intended to
enable wildfire prediction research and solutions as well as set a precedent
for future wildfire databases in other regions.

</details>


### [315] [FragmentGPT: A Unified GPT Model for Fragment Growing, Linking, and Merging in Molecular Design](https://arxiv.org/abs/2509.11044)
*Xuefeng Liu,Songhao Jiang,Qinan Huang,Tinson Xu,Ian Foster,Mengdi Wang,Hening Lin,Jinbo Xu,Rick Stevens*

Main category: cs.LG

TL;DR: FragmentGPT presents a GPT-based framework for fragment-based drug discovery with two key innovations: an energy-based bond-cleavage pre-training to enable fragment growth/linking/merging, and a Reward Ranked Alignment with Expert Exploration (RAE) algorithm that blends expert imitation, data augmentation, Pareto optimization, and supervised fine-tuning to produce multi-objective optimized linker molecules. It can resolve fragment redundancies (e.g., duplicates) via intelligent merging and demonstrates chemically valid, high-quality molecules on cancer datasets.


<details>
  <summary>Details</summary>
Motivation: FBDD faces challenges in designing effective linkers between disjoint fragments, compounded by structural redundancies (like duplicate rings) that cannot be fixed by simple edits. A unified framework is needed to enable controlled, goal-driven assembly, including multi-objective optimization for pharmacological properties.

Method: 1) Train a GPT-based model with an energy-based bond-cleavage pre-training to acquire fragment growing, linking, and merging capabilities. 2) Introduce Reward Ranked Alignment with Expert Exploration (RAE): combines expert imitation learning for diversity, data selection and augmentation for Pareto- and composite-score optimality, and supervised fine-tuning to align the learner with multi-objective goals. The model, conditioned on fragment pairs, generates linkers and learns to resolve redundancies through intelligent merging.

Result: Experiments and ablation studies on real-world cancer datasets show that FragmentGPT can generate chemically valid, high-quality molecules tailored for downstream drug discovery tasks, and that the components contribute to performance improvements (as shown by ablations).

Conclusion: FragmentGPT enables controlled, goal-driven molecular assembly in FBDD, offering a unified approach to linker design and redundancy resolution, with demonstrated potential to accelerate drug discovery on cancer-related tasks.

Abstract: Fragment-Based Drug Discovery (FBDD) is a popular approach in early drug
development, but designing effective linkers to combine disconnected molecular
fragments into chemically and pharmacologically viable candidates remains
challenging. Further complexity arises when fragments contain structural
redundancies, like duplicate rings, which cannot be addressed by simply adding
or removing atoms or bonds. To address these challenges in a unified framework,
we introduce FragmentGPT, which integrates two core components: (1) a novel
chemically-aware, energy-based bond cleavage pre-training strategy that equips
the GPT-based model with fragment growing, linking, and merging capabilities,
and (2) a novel Reward Ranked Alignment with Expert Exploration (RAE) algorithm
that combines expert imitation learning for diversity enhancement, data
selection and augmentation for Pareto and composite score optimality, and
Supervised Fine-Tuning (SFT) to align the learner policy with multi-objective
goals. Conditioned on fragment pairs, FragmentGPT generates linkers that
connect diverse molecular subunits while simultaneously optimizing for multiple
pharmaceutical goals. It also learns to resolve structural redundancies-such as
duplicated fragments-through intelligent merging, enabling the synthesis of
optimized molecules. FragmentGPT facilitates controlled, goal-driven molecular
assembly. Experiments and ablation studies on real-world cancer datasets
demonstrate its ability to generate chemically valid, high-quality molecules
tailored for downstream drug discovery tasks.

</details>


### [316] [Data-Efficient Ensemble Weather Forecasting with Diffusion Models](https://arxiv.org/abs/2509.11047)
*Kevin Valencia,Ziyang Liu,Justin Cui*

Main category: cs.LG

TL;DR: Time-stratified sampling enables data-efficient diffusion training for weather forecasting, achieving comparable or better performance than full-data training with only 20% of the data, suggesting data selection strategies can greatly reduce data needs without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for weather forecasting are powerful but computationally expensive and data-limited; exploring data-efficient training through curated data sampling can reduce data requirements while maintaining performance.

Method: Compare multiple data sampling strategies for autoregressive diffusion models on weather forecasting tasks; implement time-stratified sampling; evaluate against full-data training across multiple metrics; demonstrate that training with 20% data can match or exceed full-data performance on some metrics.

Result: Time-stratified sampling matches or surpasses full-data performance on certain metrics and performs only slightly worse on others, while using only 20% of the training data; overall supports data-efficient diffusion training for weather forecasting.

Conclusion: Data-efficient diffusion training is feasible for weather forecasting; sampling strategies—particularly time-stratified sampling—can reduce data requirements with little to no loss in performance, motivating further work on adaptive or model-aware sampling beyond simple random or temporal sampling.

Abstract: Although numerical weather forecasting methods have dominated the field,
recent advances in deep learning methods, such as diffusion models, have shown
promise in ensemble weather forecasting. However, such models are typically
autoregressive and are thus computationally expensive. This is a challenge in
climate science, where data can be limited, costly, or difficult to work with.
In this work, we explore the impact of curated data selection on these
autoregressive diffusion models. We evaluate several data sampling strategies
and show that a simple time stratified sampling approach achieves performance
similar to or better than full-data training. Notably, it outperforms the
full-data model on certain metrics and performs only slightly worse on others
while using only 20% of the training data. Our results demonstrate the
feasibility of data-efficient diffusion training, especially for weather
forecasting, and motivates future work on adaptive or model-aware sampling
methods that go beyond random or purely temporal sampling.

</details>


### [317] [An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data](https://arxiv.org/abs/2509.11053)
*Shengke Sun,Shuzhen Han,Ziqian Luan,Xinghao Qin,Jiao Yin,Zhanshan Zhao,Jinli Cao,Hua Wang*

Main category: cs.LG

TL;DR: A data-efficient bearing fault diagnosis method that uses a CCLR-GAN for diverse data generation, a contrastive joint optimization to model sample relations, and a 1D Fourier CNN to capture global features, achieving substantial gains in limited-data settings.


<details>
  <summary>Details</summary>
Motivation: In bearing fault diagnosis, labeled data are scarce due to cost/privacy; traditional augmentation causes mode collapse; CNNs have limited global feature extraction; limited data also hampers modeling relationships between samples.

Method: Proposes CCLR-GAN to generate diverse latent-consistent samples conditioned on fault classes; introduces a contrastive learning-based joint optimization mechanism to model relations among available samples; uses a 1D Fourier CNN (1D-FCNN) to capture global abstractions from vibration signals.

Result: Experiments show up to 32% accuracy improvement on the CWRU dataset and 10% on a self-collected bench; ablation experiments validate each component's contribution.

Conclusion: DAC-FCF provides a promising solution for bearing fault diagnosis under limited data, combining advanced data augmentation, contrastive learning, and global-feature extraction.

Abstract: In the area of bearing fault diagnosis, deep learning (DL) methods have been
widely used recently. However, due to the high cost or privacy concerns,
high-quality labeled data are scarce in real world scenarios. While few-shot
learning has shown promise in addressing data scarcity, existing methods still
face significant limitations in this domain. Traditional data augmentation
techniques often suffer from mode collapse and generate low-quality samples
that fail to capture the diversity of bearing fault patterns. Moreover,
conventional convolutional neural networks (CNNs) with local receptive fields
makes them inadequate for extracting global features from complex vibration
signals. Additionally, existing methods fail to model the intricate
relationships between limited training samples. To solve these problems, we
propose an advanced data augmentation and contrastive fourier convolution
framework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, a
novel conditional consistent latent representation and reconstruction
generative adversarial network (CCLR-GAN) is proposed to generate more diverse
data. Secondly, a contrastive learning based joint optimization mechanism is
utilized to better model the relations between the available training data.
Finally, we propose a 1D fourier convolution neural network (1D-FCNN) to
achieve a global-aware of the input data. Experiments demonstrate that DAC-FCF
achieves significant improvements, outperforming baselines by up to 32\% on
case western reserve university (CWRU) dataset and 10\% on a self-collected
test bench. Extensive ablation experiments prove the effectiveness of the
proposed components. Thus, the proposed DAC-FCF offers a promising solution for
bearing fault diagnosis under limited data.

</details>


### [318] [Machine Learning Framework for Audio-Based Equipment Condition Monitoring: A Comparative Study of Classification Algorithms](https://arxiv.org/abs/2509.11075)
*Srijesh Pillai,Yodhin Agarwal,Zaheeruddin Ahmed*

Main category: cs.LG

TL;DR: Introduces a standardized benchmarking framework for audio-based equipment condition monitoring using a 127-feature set; shows ensemble method achieves 94.2% accuracy and 0.942 F1 with statistically significant improvements over individual models; provides guidelines for robust industrial monitoring.


<details>
  <summary>Details</summary>
Motivation: Lack of standardized methodologies leads to irreproducible research and hindered model selection in industry; a rigorous evaluation framework is needed to enable fair comparisons and reproducible results.

Method: A comprehensive evaluation framework using 127 features across time, frequency, and time-frequency domains; validation on synthetic and real datasets; ensemble learning with statistical testing; established benchmarking protocol and guidelines.

Result: Ensemble method attains 94.2% accuracy and 0.942 F1; statistically significant improvement over individual algorithms by 8-15% across tests.

Conclusion: Presents a validated benchmarking protocol and practical guidelines to aid robust, reproducible model selection for audio-based industrial equipment monitoring.

Abstract: Audio-based equipment condition monitoring suffers from a lack of
standardized methodologies for algorithm selection, hindering reproducible
research. This paper addresses this gap by introducing a comprehensive
framework for the systematic and statistically rigorous evaluation of machine
learning models. Leveraging a rich 127-feature set across time, frequency, and
time-frequency domains, our methodology is validated on both synthetic and
real-world datasets. Results demonstrate that an ensemble method achieves
superior performance (94.2% accuracy, 0.942 F1-score), with statistical testing
confirming its significant outperformance of individual algorithms by 8-15%.
Ultimately, this work provides a validated benchmarking protocol and practical
guidelines for selecting robust monitoring solutions in industrial settings.

</details>


### [319] [DemandLens: Enhancing Forecast Accuracy Through Product-Specific Hyperparameter Optimization](https://arxiv.org/abs/2509.11085)
*Srijesh Pillai,M. I. Jawid Nazir*

Main category: cs.LG

TL;DR: Prophet-based demand forecasting for mattress-in-a-box, using COVID-19 metrics and SKU-specific hyperparameter optimization to improve supply chain planning for contract manufacturers and DTC brands.


<details>
  <summary>Details</summary>
Motivation: The mattress-in-a-box industry relies on limited contract manufacturers and outsourcing, making accurate sales forecasting essential to manage raw materials, inventory, and logistics, avoid bottlenecks, and optimize sourcing in a growing e-commerce and DTC landscape.

Method: A Prophet-based forecasting model augmented with COVID-19 metrics and SKU-specific hyperparameter optimization to produce per-SKU demand forecasts for mattress brands and contract manufacturers.

Result: The approach demonstrates strong predictive capabilities through SKU-specific hyperparameter optimization, enabling more reliable forecasts that can streamline supply chain operations for mattress brands and contract manufacturers.

Conclusion: The model provides a reliable tool for demand forecasting in the mattress-in-a-box ecosystem, helping contract manufacturers and brands to plan capacity, procurement, and logistics more efficiently.

Abstract: DemandLens demonstrates an innovative Prophet based forecasting model for the
mattress-in-a-box industry, incorporating COVID-19 metrics and SKU-specific
hyperparameter optimization. This industry has seen significant growth of
E-commerce players in the recent years, wherein the business model majorly
relies on outsourcing Mattress manufacturing and related logistics and supply
chain operations, focusing on marketing the product and driving conversions
through Direct-to-Consumer sales channels. Now, within the United States, there
are a limited number of Mattress contract manufacturers available, and hence,
it is important that they manage their raw materials, supply chain, and,
inventory intelligently, to be able to cater maximum Mattress brands. Our
approach addresses the critical need for accurate Sales Forecasting in an
industry that is heavily dependent on third-party Contract Manufacturing. This,
in turn, helps the contract manufacturers to be prepared, hence, avoiding
bottleneck scenarios, and aiding them to source raw materials at optimal rates.
The model demonstrates strong predictive capabilities through SKU-specific
Hyperparameter optimization, offering the Contract Manufacturers and Mattress
brands a reliable tool to streamline supply chain operations.

</details>


### [320] [GCN-TULHOR: Trajectory-User Linking Leveraging GCNs and Higher-Order Spatial Representations](https://arxiv.org/abs/2509.11095)
*Khoa Tran,Pranav Gupta,Manos Papagelis*

Main category: cs.LG

TL;DR: GCN-TULHOR introduces hexagonal-flow representations and graph convolutions to tackle trajectory-user linking (TUL), reducing sparsity and improving accuracy across six real datasets.


<details>
  <summary>Details</summary>
Motivation: TUL is hampered by sparse data, incomplete routes, and weak modeling of complex spatial dependencies. There is a need for unified higher-order spatial representations that do not rely heavily on side information.

Method: Convert sparse check-ins and continuous GPS trajectories into unified higher-order mobility flow representations using hexagonal tessellation; apply Graph Convolutional Networks to model spatial relationships and non-local dependencies; avoid requiring timestamps or points of interest; evaluate across six real-world datasets; sensitivity analysis finds optimal setup with a single GCN layer and 512-dimensional embeddings.

Result: Six real-world datasets show consistent improvements over classical baselines, RNN- and Transformer-based models, and the TULHOR method in accuracy, precision, recall, and F1-score, with 1-8% relative gains in accuracy and F1; findings suggest good generalizability and robustness.

Conclusion: Integrating graph-based spatial learning with sequential modeling yields a robust, scalable TUL solution. Higher-order, spatially-aware representations mitigate sparsity and enhance semantic understanding, with applications in recommendations, urban planning, and security.

Abstract: Trajectory-user linking (TUL) aims to associate anonymized trajectories with
the users who generated them, which is crucial for personalized
recommendations, privacy-preserving analytics, and secure location-based
services. Existing methods struggle with sparse data, incomplete routes, and
limited modeling of complex spatial dependencies, often relying on low-level
check-in data or ignoring spatial patterns. In this paper, we introduced
GCN-TULHOR, a method that transforms raw location data into higher-order
mobility flow representations using hexagonal tessellation, reducing data
sparsity and capturing richer spatial semantics, and integrating Graph
Convolutional Networks (GCNs). Our approach converts both sparse check-in and
continuous GPS trajectory data into unified higher-order flow representations,
mitigating sparsity while capturing deeper semantic information. The GCN layer
explicitly models complex spatial relationships and non-local dependencies
without requiring side information such as timestamps or points of interest.
Experiments on six real-world datasets show consistent improvements over
classical baselines, RNN- and Transformer-based models, and the TULHOR method
in accuracy, precision, recall, and F1-score. GCN-TULHOR achieves 1-8% relative
gains in accuracy and F1. Sensitivity analysis identifies an optimal setup with
a single GCN layer and 512-dimensional embeddings. The integration of GCNs
enhances spatial learning and improves generalizability across mobility data.
This work highlights the value of combining graph-based spatial learning with
sequential modeling, offering a robust and scalable solution for TUL with
applications in recommendations, urban planning, and security.

</details>


### [321] [BIGNet: Pretrained Graph Neural Network for Embedding Semantic, Spatial, and Topological Data in BIM Models](https://arxiv.org/abs/2509.11104)
*Jin Han,Xin-Zheng Lu,Jia-Rui Lin*

Main category: cs.LG

TL;DR: BIGNet is a large-scale GNN for BIM that learns semantic-spatial-topological features from a BIM graph; pretraining with node masking and GraphMAE2-inspired message passing; achieves 72.7% average F1 improvement over non-pretrained models in BIM design checking, with homogeneous graphs and 30 cm local spatial radius providing best performance, and uses GAT-based features.


<details>
  <summary>Details</summary>
Motivation: LFMs excel in text/vision but largely ignore BIM's rich semantic, spatial, and topological information. There is a need to scale BIM feature learning and enable transfer to design checking and lifecycle management.

Method: Proposes a scalable BIM graph representation encoding semantic-spatial-topological features; constructs a dataset with ~1M nodes and 3.5M edges; introduces a new message-passing mechanism to GraphMAE2; pretrains with node masking; evaluates transfer learning for BIM design checking; uses homogeneous vs heterogeneous graphs; radius-30cm; Graph Attention Network-based feature extraction.

Result: Homogeneous graphs outperform heterogeneous; 30 cm radius local spatial relations improve performance; BIGNet with GAT features achieves best transfer learning results; 72.7% improvement in Avg F1-score over non-pretrained models.

Conclusion: Demonstrates effectiveness of BIGNet in learning and transferring BIM design features, enabling automated application in design and lifecycle management.

Abstract: Large Foundation Models (LFMs) have demonstrated significant advantages in
civil engineering, but they primarily focus on textual and visual data,
overlooking the rich semantic, spatial, and topological features in BIM
(Building Information Modelling) models. Therefore, this study develops the
first large-scale graph neural network (GNN), BIGNet, to learn, and reuse
multidimensional design features embedded in BIM models. Firstly, a scalable
graph representation is introduced to encode the "semantic-spatial-topological"
features of BIM components, and a dataset with nearly 1 million nodes and 3.5
million edges is created. Subsequently, BIGNet is proposed by introducing a new
message-passing mechanism to GraphMAE2 and further pretrained with a node
masking strategy. Finally, BIGNet is evaluated in various transfer learning
tasks for BIM-based design checking. Results show that: 1) homogeneous graph
representation outperforms heterogeneous graph in learning design features, 2)
considering local spatial relationships in a 30 cm radius enhances performance,
and 3) BIGNet with GAT (Graph Attention Network)-based feature extraction
achieves the best transfer learning results. This innovation leads to a 72.7%
improvement in Average F1-score over non-pretrained models, demonstrating its
effectiveness in learning and transferring BIM design features and facilitating
their automated application in future design and lifecycle management.

</details>


### [322] [Agentic Username Suggestion and Multimodal Gender Detection in Online Platforms: Introducing the PNGT-26K Dataset](https://arxiv.org/abs/2509.11136)
*Farbod Bijary,Mohsen Ebadpour,Amirhosein Tajbakhsh*

Main category: cs.LG

TL;DR: PNGT-26K is a 26k Persian-name dataset with gender labels and English transliterations, plus two frameworks (Open Gender Detection and Nominalist) for gender inference and username generation; all are open-source on GitHub.


<details>
  <summary>Details</summary>
Motivation: Persian names pose transliteration inconsistencies and culturally specific naming patterns that challenge NLP; existing tools show degraded performance on Persian names; the scarcity of comprehensive datasets hinders progress in gender detection and user identity applications.

Method: Curate and annotate a dataset of ~26,000 Persian names with common gender associations and English transliterations (PNGT-26K). Develop two frameworks: Open Gender Detection (production-grade probabilistic gender inference from user data like profile photo and name) and Nominalist (agentic AI-powered username suggestion tool). Make PNGT-26K and the frameworks publicly available on GitHub.

Result: Introduction of the PNGT-26K dataset and two demonstration frameworks, with public release on GitHub to enable use and evaluation by the community.

Conclusion: The work provides concrete resources to address Persian-name NLP challenges, enabling more reliable gender detection and user-identification experiences, and it invites further research and practical deployment.

Abstract: Persian names present unique challenges for natural language processing
applications, particularly in gender detection and digital identity creation,
due to transliteration inconsistencies and cultural-specific naming patterns.
Existing tools exhibit significant performance degradation on Persian names,
while the scarcity of comprehensive datasets further compounds these
limitations. To address these challenges, the present research introduces
PNGT-26K, a comprehensive dataset of Persian names, their commonly associated
gender, and their English transliteration, consisting of approximately 26,000
tuples. As a demonstration of how this resource can be utilized, we also
introduce two frameworks, namely Open Gender Detection and Nominalist. Open
Gender Detection is a production-grade, ready-to-use framework for using
existing data from a user, such as profile photo and name, to give a
probabilistic guess about the person's gender. Nominalist, the second framework
introduced by this paper, utilizes agentic AI to help users choose a username
for their social media accounts on any platform. It can be easily integrated
into any website to provide a better user experience. The PNGT-26K dataset,
Nominalist and Open Gender Detection frameworks are publicly available on
Github.

</details>


### [323] [Feature Space Topology Control via Hopkins Loss](https://arxiv.org/abs/2509.11154)
*Einari Vaaras,Manu Airaksinen*

Main category: cs.LG

TL;DR: Introduces Hopkins loss to actively shape the topology of feature spaces using the Hopkins statistic, rather than preserving input topology, and evaluates it on speech, text, and image data for classification and non-linear bottleneck autoencoder-based dimensionality reduction. Shows topology can be modified with only minor impact on classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Current topology-aware methods focus on preserving input feature topology, which may limit the ability to structure latent spaces for robustness, transfer learning, or generative aims. By leveraging the Hopkins statistic, the authors aim to explicitly control the topology of the learned feature space.

Method: Propose Hopkins loss that penalizes deviations from a desired Hopkins statistic in the latent feature space. Integrate this loss into training for two tasks—classification and dimensionality reduction using nonlinear bottleneck autoencoders—and evaluate across speech, text, and image modalities.

Result: Incorporating Hopkins loss yields modifications to the feature-space topology with only a small impact on classification performance, suggesting a practical trade-off between topology control and accuracy. The approach is demonstrated across multiple data modalities and tasks.

Conclusion: Hopkins loss provides a practical tool for actively shaping latent space topology during training, enabling topology manipulation without substantially sacrificing predictive performance. This opens avenues for enhanced robustness and transferability through controlled feature-space organization.

Abstract: Feature space topology refers to the organization of samples within the
feature space. Modifying this topology can be beneficial in machine learning
applications, including dimensionality reduction, generative modeling, transfer
learning, and robustness to adversarial attacks. This paper introduces a novel
loss function, Hopkins loss, which leverages the Hopkins statistic to enforce a
desired feature space topology, which is in contrast to existing
topology-related methods that aim to preserve input feature topology. We
evaluate the effectiveness of Hopkins loss on speech, text, and image data in
two scenarios: classification and dimensionality reduction using nonlinear
bottleneck autoencoders. Our experiments show that integrating Hopkins loss
into classification or dimensionality reduction has only a small impact on
classification performance while providing the benefit of modifying feature
topology.

</details>


### [324] [AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient Inference in LLMs](https://arxiv.org/abs/2509.11155)
*Santhosh G S,Saurav Prakash,Balaraman Ravindran*

Main category: cs.LG

TL;DR: AQUA reduces attention cost by about 25% with negligible accuracy loss using offline SVD-based projection and online magnitude-based sparse selection, plus compatibility with token eviction methods and KV-cache reduction.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of the attention mechanism is the main bottleneck limiting long-context LLMs; a scalable, controllable method to trade off efficiency and accuracy during inference is highly desirable.

Method: Two-phase approach: offline compute a universal projection matrix via SVD on calibration data; online project queries and keys and dynamically select a sparse subset of dimensions based on query magnitude, reducing dot-product computations. Includes theoretical break-even analysis and demonstrates synergy with H2O eviction and KV-cache memory reduction.

Result: Empirical evaluation on Llama-3.1-8B shows a 25% reduction in attention dot-product computations with statistically insignificant impact on performance across benchmarks, and potential reductions in KV-cache memory and compatibility with existing eviction methods.

Conclusion: AQUA provides a practical, versatile tool for making large-scale LLM inference more efficient and sustainable, offering a tunable efficiency-accuracy trade-off and compatibility with complementary acceleration techniques.

Abstract: The quadratic complexity of the attention mechanism remains a fundamental
barrier to scaling Large Language Models (LLMs) to longer contexts, creating a
critical bottleneck in both computation and memory. To address this, we
introduce AQUA (Attention via QUery mAgnitudes) a novel and versatile
approximation strategy that significantly reduces the cost of attention with a
graceful performance trade-off. Our method operates in two phases: an efficient
offline step where we compute a universal, language agnostic projection matrix
via SVD on a calibration dataset, and an online inference step where we project
query and key vectors and dynamically select a sparse subset of dimensions
based on the query's magnitude. We provide a formal theoretical analysis of
AQUA, establishing the break-even point at which it becomes more
computationally efficient than standard attention. Our empirical evaluations on
state-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in
the attention dot-product computation can be achieved with a statistically
insignificant impact on performance across a wide range of benchmarks. We
further showcase the versatility of AQUA by demonstrating its ability to
synergistically accelerate existing token eviction methods like H2O and to
directly reduce KV-cache memory size. By offering a controllable knob to
balance efficiency and accuracy, AQUA provides a practical and powerful tool
for making large-scale LLM inference more accessible and sustainable.

</details>


### [325] [Stabilizing Data-Free Model Extraction](https://arxiv.org/abs/2509.11159)
*Dat-Thinh Nguyen,Kim-Hung Le,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: A meta-learning–driven data-free model extraction method (MetaDFME) stabilizes substitute-model accuracy by mitigating data-distribution shifts, outperforming state-of-the-art on standard datasets.


<details>
  <summary>Details</summary>
Motivation: Data-free MLaaS attacks are threatened by oscillating substitute accuracy caused by distribution shifts of synthetic data; without target in-distribution data, identifying an optimal substitute model is hard, necessitating approaches to reduce distribution shift.

Method: Train a generator with meta-learning to capture meta-representations of synthetic data. During the attack, adapt these meta-representations with a few steps to generate data that helps the substitute model learn from the target model while mitigating distribution shift, thereby reducing accuracy oscillation.

Result: Empirical evaluation on MNIST, SVHN, CIFAR-10, CIFAR-100 shows MetaDFME outperforms current state-of-the-art data-free model extraction methods and yields more stable substitute-model accuracy during the attack.

Conclusion: MetaDFME effectively reduces distribution shift and oscillation in data-free model extraction, enabling more practical and reliable attacks across standard image datasets.

Abstract: Model extraction is a severe threat to Machine Learning-as-a-Service systems,
especially through data-free approaches, where dishonest users can replicate
the functionality of a black-box target model without access to realistic data.
Despite recent advancements, existing data-free model extraction methods suffer
from the oscillating accuracy of the substitute model. This oscillation, which
could be attributed to the constant shift in the generated data distribution
during the attack, makes the attack impractical since the optimal substitute
model cannot be determined without access to the target model's in-distribution
data. Hence, we propose MetaDFME, a novel data-free model extraction method
that employs meta-learning in the generator training to reduce the distribution
shift, aiming to mitigate the substitute model's accuracy oscillation. In
detail, we train our generator to iteratively capture the meta-representations
of the synthetic data during the attack. These meta-representations can be
adapted with a few steps to produce data that facilitates the substitute model
to learn from the target model while reducing the effect of distribution
shifts. Our experiments on popular baseline image datasets, MNIST, SVHN,
CIFAR-10, and CIFAR-100, demonstrate that MetaDFME outperforms the current
state-of-the-art data-free model extraction method while exhibiting a more
stable substitute model's accuracy during the attack.

</details>


### [326] [GK-SMOTE: A Hyperparameter-free Noise-Resilient Gaussian KDE-Based Oversampling Approach](https://arxiv.org/abs/2509.11163)
*Mahabubur Rahman Miraj,Hongyu Huang,Ting Yang,Jinxue Zhao,Nankun Mu,Xinyu Lei*

Main category: cs.LG

TL;DR: GK-SMOTE is a hyperparameter-free, noise-resilient SMOTE extension based on Gaussian KDE that generates minority samples in high-density regions to improve imbalanced binary classification, outperforming state-of-the-art oversamplers on several metrics.


<details>
  <summary>Details</summary>
Motivation: Imbalanced classification is critical in many domains; traditional oversampling methods struggle with label noise and complex data distributions, leading to reduced accuracy.

Method: Gaussian KDE-based self-adaptive oversampling that discerns safe (high-density) minority regions from noisy/ambiguous ones and generates synthetic samples there, without hyperparameters.

Result: Empirical results on diverse binary datasets show GK-SMOTE surpasses existing oversampling methods on MCC, Balanced Accuracy, and AUPRC.

Conclusion: GK-SMOTE offers a robust, efficient oversampling approach suitable for noisy real-world imbalanced data, with strong potential for practical deployment.

Abstract: Imbalanced classification is a significant challenge in machine learning,
especially in critical applications like medical diagnosis, fraud detection,
and cybersecurity. Traditional oversampling techniques, such as SMOTE, often
fail to handle label noise and complex data distributions, leading to reduced
classification accuracy. In this paper, we propose GK-SMOTE, a
hyperparameter-free, noise-resilient extension of SMOTE, built on Gaussian
Kernel Density Estimation (KDE). GK-SMOTE enhances class separability by
generating synthetic samples in high-density minority regions, while
effectively avoiding noisy or ambiguous areas. This self-adaptive approach uses
Gaussian KDE to differentiate between safe and noisy regions, ensuring more
accurate sample generation without requiring extensive parameter tuning. Our
extensive experiments on diverse binary classification datasets demonstrate
that GK-SMOTE outperforms existing state-of-the-art oversampling techniques
across key evaluation metrics, including MCC, Balanced Accuracy, and AUPRC. The
proposed method offers a robust, efficient solution for imbalanced
classification tasks, especially in noisy data environments, making it an
attractive choice for real-world applications.

</details>


### [327] [Harnessing Optimization Dynamics for Curvature-Informed Model Merging](https://arxiv.org/abs/2509.11167)
*Pouria Mahdavinia,Hamed Mahdavi,Niloofar Mireshghallah,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: A curvature-aware merging framework (OTA+FFG) for consolidating multiple capability-focused SFT checkpoints into one model, reweighting edits via optimizer second-moment (diagonal curvature) proxies and sparsifying conflicts, with memory-efficient second-moment compression.


<details>
  <summary>Details</summary>
Motivation: Merging diverse capabilities in a single model via weight-space averaging is prone to interference and negative transfer. Exploiting curvature information can guide edits and improve multi-capability integration without joint retraining.

Method: OTA Merging uses diagonal curvature proxies from optimizer second moments to reweight parameter edits during merge. Fast Fisher Grafting (FFG) performs curvature-driven, task-localized sparsification of conflicting or low-importance edits, yielding extremely low-rank masks concentrated in early attention query/key projections and token embeddings. A memory-light compression of second moments preserves OTA's benefits. Evaluations span diverse capability-based SFT checkpoints; ablations test FFG's role and the compressed moments.

Result: OTA+FFG yields higher merged-model quality than strong weight-space baselines, with reduced negative transfer and robustness across sparsity levels. Analyses reveal substantial curvature overlap among checkpoints, explaining effective linear merging. Ablations confirm FFG’s critical role and the efficacy of compressed moments.

Conclusion: Curvature-aware merging (OTA) combined with curvature-driven sparsification (FFG) provides a practical, scalable approach to synthesizing multiple capabilities in a single model, achieving improved performance while maintaining memory efficiency. The work offers insights into why simple merging can work and provides reproducible resources.

Abstract: Model merging is an effective post-training strategy for composing
capabilities in large language models without joint retraining. We study this
in the supervised fine-tuning (SFT) stage, where multiple capability-based SFT
checkpoints -- spanning math, code, precise instruction following, general
instruction following, and knowledge recall -- must be consolidated into a
single model. We introduce Optimization Trajectory Aware (OTA) Merging, a
curvature-aware aggregation that leverages optimizer second-moment statistics
as a diagonal curvature proxy to reweight parameter edits and mitigate
interference. Complementing OTA, we propose Fast Fisher Grafting (FFG), a
curvature-driven task-localization step that sparsifies conflicting or
low-importance edits. FFG induces extremely low-rank masks concentrated in
early attention query/key projections and token embeddings, exploiting shared
curvature across capabilities. We further develop a memory-light compression of
the second moments that preserves OTA's effect. Across diverse capability-based
SFT checkpoints, OTA+FFG improves merged-model quality over strong weight-space
baselines, reduces negative transfer, and remains robust across sparsity
levels. Analyses reveal substantial curvature overlap between checkpoints,
offering a novel lens on why simple linear merging can be effective in
practice. Ablations confirm that FFG is critical for reducing task interference
and that the compressed second moments retain the gains of the full
formulation. To facilitate reproducibility, we open-source all code, training
and evaluation scripts, visualization artifacts, and capability-specific SFT
checkpoints at https://github.com/pmahdavi/ota-merge.

</details>


### [328] [Federated Recommender System with Data Valuation for E-commerce Platform](https://arxiv.org/abs/2509.11196)
*Jongwon Park,Minku Kang,Wooseok Sim,Soyoung Lee,Hogun Park*

Main category: cs.LG

TL;DR: FedGDVE selectively augments a client's local graph with globally available, semantically aligned samples to improve federated recommender systems, using a graph encoder, a local relevance predictor, and an RL-based sampler, achieving up to 34.86% gains.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns drive federated learning (FL); public data resources could enrich local training but are underutilized due to noise and cost when naively integrating all global interactions.

Method: FedGDVE uses (i) a pre-trained graph encoder to extract global structural features, (ii) a local valid predictor to assess client-specific relevance, (iii) a reinforcement-learning-based probability estimator to filter and sample only the most pertinent global interactions.

Result: Performance improvements of up to 34.86% on recognized benchmarks in FL environments.

Conclusion: Selective augmentation with semantically aligned global data mitigates data sparsity and bias in FL recommender systems, improving personalization and efficiency.

Abstract: Federated Learning (FL) is gaining prominence in machine learning as privacy
concerns grow. This paradigm allows each client (e.g., an individual online
store) to train a recommendation model locally while sharing only model
updates, without exposing the raw interaction logs to a central server, thereby
preserving privacy in a decentralized environment. Nonetheless, most existing
FL-based recommender systems still rely solely on each client's private data,
despite the abundance of publicly available datasets that could be leveraged to
enrich local training; this potential remains largely underexplored. To this
end, we consider a realistic scenario wherein a large shopping platform
collaborates with multiple small online stores to build a global recommender
system. The platform possesses global data, such as shareable user and item
lists, while each store holds a portion of interaction data privately (or
locally). Although integrating global data can help mitigate the limitations of
sparse and biased clients' local data, it also introduces additional
challenges: simply combining all global interactions can amplify noise and
irrelevant patterns, worsening personalization and increasing computational
costs. To address these challenges, we propose FedGDVE, which selectively
augments each client's local graph with semantically aligned samples from the
global dataset. FedGDVE employs: (i) a pre-trained graph encoder to extract
global structural features, (ii) a local valid predictor to assess
client-specific relevance, (iii) a reinforcement-learning-based probability
estimator to filter and sample only the most pertinent global interactions.
FedGDVE improves performance by up to 34.86% on recognized benchmarks in FL
environments.

</details>


### [329] [Foundational theory for optimal decision tree problems. I. Algorithmic and geometric foundations](https://arxiv.org/abs/2509.11226)
*Xi He*

Main category: cs.LG

TL;DR: Four new executable-definable ODT formulations (three size-constrained, one depth-constrained) yield four optimal algorithms for ODT with arbitrary splitting rules. Part II adds the first optimal hypersurface decision tree algorithm and experiments, showing competitive results and a framework extendable to more flexible trees.


<details>
  <summary>Details</summary>
Motivation: Provide formal, executable specifications for ODT problems and construct optimal algorithms by grounding in algebraic programming theory; unify and extend ODT with flexible splitting rules, enabling more general decision trees.

Method: Define four ODT problems as executable recursive programs within a relational algebraic programming framework; derive dynamic-programming solutions constructively whenever solutions exist; these yield four generic definitions and associated optimal algorithms; Part II develops the first optimal hypersurface DT algorithm and benchmarks against axis-parallel DTs and other optimal methods.

Result: Four novel optimal algorithms for ODT with arbitrary splitting rules; depth-constrained axis-parallel ODT appears as a special case; Part II demonstrates the potential of hypersurface trees and provides empirical comparisons against CART and state-of-the-art optimal methods; the framework supports extending to more flexible/mixed splitting rules.

Conclusion: A unified, efficient framework for general ODT problems; constructive DP solutions under the proposed definitions; extendable to more flexible decision trees including mixed splitting rules; Part II extends applicability through the hypersurface DT algorithm and experimental validation.

Abstract: In the first paper (part I) of this series of two, we introduce four novel
definitions of the ODT problems: three for size-constrained trees and one for
depth-constrained trees. These definitions are stated unambiguously through
executable recursive programs, satisfying all criteria we propose for a formal
specification. In this sense, they resemble the "standard form" used in the
study of general-purpose solvers.
  Grounded in algebraic programming theory-a relational formalism for deriving
correct-by-construction algorithms from specifications-we can not only
establish the existence or nonexistence of dynamic programming solutions but
also derive them constructively whenever they exist. Consequently, the four
generic problem definitions yield four novel optimal algorithms for ODT
problems with arbitrary splitting rules that satisfy the axioms and objective
functions of a given form. These algorithms encompass the known
depth-constrained, axis-parallel ODT algorithm as the special case, while
providing a unified, efficient, and elegant solution for the general ODT
problem.
  In Part II, we present the first optimal hypersurface decision tree algorithm
and provide comprehensive experiments against axis-parallel decision tree
algorithms, including heuristic CART and state-of-the-art optimal methods. The
results demonstrate the significant potential of decision trees with flexible
splitting rules. Moreover, our framework is readily extendable to support
algorithms for constructing even more flexible decision trees, including those
with mixed splitting rules.

</details>


### [330] [TransZero: Parallel Tree Expansion in MuZero using Transformer Networks](https://arxiv.org/abs/2509.11233)
*Emil Malmsten,Wendelin Böhmer*

Main category: cs.LG

TL;DR: TransZero is a model-based RL method that uses a transformer to generate multiple latent future states and a mean-variance constrained evaluator to enable parallel subtree planning, achieving significant wall-clock speedups over MuZero while preserving sample efficiency.


<details>
  <summary>Details</summary>
Motivation: To remove the sequential bottleneck in Monte Carlo Tree Search and enable real-time planning in model-based RL by moving from step-by-step, recurrent search to parallel, transformer-based planning.

Method: Use a transformer-based network to generate multiple latent future states simultaneously; employ a Mean-Variance Constrained (MVC) evaluator to replace visitation-count based evaluation, enabling parallel expansion of entire subtrees during planning.

Result: Demonstrates up to ~11x wall-clock speedups compared to MuZero on MiniGrid and LunarLander, while maintaining sample efficiency.

Conclusion: Parallel tree construction via transformer-based future-state generation and MVC evaluation substantially accelerates model-based RL planning, bringing real-time decision-making closer to practice; code is publicly available.

Abstract: We present TransZero, a model-based reinforcement learning algorithm that
removes the sequential bottleneck in Monte Carlo Tree Search (MCTS). Unlike
MuZero, which constructs its search tree step by step using a recurrent
dynamics model, TransZero employs a transformer-based network to generate
multiple latent future states simultaneously. Combined with the Mean-Variance
Constrained (MVC) evaluator that eliminates dependence on inherently sequential
visitation counts, our approach enables the parallel expansion of entire
subtrees during planning. Experiments in MiniGrid and LunarLander show that
TransZero achieves up to an eleven-fold speedup in wall-clock time compared to
MuZero while maintaining sample efficiency. These results demonstrate that
parallel tree construction can substantially accelerate model-based
reinforcement learning, bringing real-time decision-making in complex
environments closer to practice. The code is publicly available on GitHub.

</details>


### [331] [Online Optimization on Hadamard Manifolds: Curvature Independent Regret Bounds on Horospherically Convex Objectives](https://arxiv.org/abs/2509.11236)
*Emre Sahinoglu,Shahin Shahrampour*

Main category: cs.LG

TL;DR: Horospherical convexity enables curvature-free online Riemannian optimization on Hadamard manifolds; online gradient descent achieves O(sqrt(T)) regret for h-convex and O(log T) regret for strongly h-convex functions, matching Euclidean bounds. Validated on SPD manifolds with online Tyler's M-estimation and Fréchet mean estimation.


<details>
  <summary>Details</summary>
Motivation: G-convexity-based analyses on manifolds often incur curvature-dependent regret bounds. The authors propose using horospherical (h-)convexity on Hadamard manifolds to obtain curvature-independent guarantees, aiming to align Riemannian online optimization with Euclidean performance.

Method: Develop online Riemannian gradient descent under h-convex and strongly h-convex regimes on Hadamard manifolds; prove regret bounds O(√T) and O(log T) respectively; analyze how h-convexity yields curvature-independent rates; validate with experiments on SPD manifolds under the affine-invariant metric.

Result: Regret bounds that do not depend on curvature, matching Euclidean rates (O(√T) and O(log T)) in the h-convex setting; experimental demonstrations on SPD manifolds illustrating online Tyler's M-estimation and Fréchet mean computation using h-convexity.

Conclusion: Horospherical convexity provides a curvature-robust framework for online optimization on Hadamard manifolds, yielding near-Euclidean performance and practical applicability for problems on SPD manifolds.

Abstract: We study online Riemannian optimization on Hadamard manifolds under the
framework of horospherical convexity (h-convexity). Prior work mostly relies on
the geodesic convexity (g-convexity), leading to regret bounds scaling poorly
with the manifold curvature. To address this limitation, we analyze Riemannian
online gradient descent for h-convex and strongly h-convex functions and
establish $O(\sqrt{T})$ and $O(\log(T))$ regret guarantees, respectively. These
bounds are curvature-independent and match the results in the Euclidean
setting. We validate our approach with experiments on the manifold of symmetric
positive definite (SPD) matrices equipped with the affine-invariant metric. In
particular, we investigate online Tyler's $M$-estimation and online Fr\'echet
mean computation, showing the application of h-convexity in practice.

</details>


### [332] [Gradient Free Deep Reinforcement Learning With TabPFN](https://arxiv.org/abs/2509.11259)
*David Schiff,Ofir Lindenbaum,Yonathan Efroni*

Main category: cs.LG

TL;DR: Gradient-free RL using TabPFN as a Q-function; competitive with DQN on classic control tasks; analyzes context-size limits and truncation; highlights potential of pre-trained transformers for fast, efficient RL.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on gradient-based RL due to hyperparameter sensitivity, unstable dynamics, and high costs; exploit TabPFN's in-context learning to perform Q-value inference without updates.

Method: Utilize TabPFN pre-trained on synthetic tabular data as a Q-value approximator via in-context learning; no gradient updates; introduce a high-reward episode gate selecting top 5% trajectories to fit within a fixed context; evaluate on CartPole v1, MountainCar v0, Acrobot v1; discuss theoretical aspects and context-size limits; propose truncation strategies for continual learning.

Result: TabPFN RL matches or exceeds DQN performance on the tested tasks without gradient-based training or extensive hyperparameter tuning; shows surprising generalization; confirms feasibility of gradient-free RL with large pre-trained transformers; identifies intrinsic context size limitations and effective truncation approaches.

Conclusion: Foundations like TabPFN can enable fast, computationally efficient RL via gradient-free inference; opens new directions for using pre-trained transformers in RL; invites further work on context management and continual learning.

Abstract: Gradient based optimization is fundamental to most modern deep reinforcement
learning algorithms, however, it introduces significant sensitivity to
hyperparameters, unstable training dynamics, and high computational costs. We
propose TabPFN RL, a novel gradient free deep RL framework that repurposes the
meta trained transformer TabPFN as a Q function approximator. Originally
developed for tabular classification, TabPFN is a transformer pre trained on
millions of synthetic datasets to perform inference on new unseen datasets via
in context learning. Given an in context dataset of sample label pairs and new
unlabeled data, it predicts the most likely labels in a single forward pass,
without gradient updates or task specific fine tuning. We use TabPFN to predict
Q values using inference only, thereby eliminating the need for back
propagation at both training and inference. To cope with the model's fixed
context budget, we design a high reward episode gate that retains only the top
5% of trajectories. Empirical evaluations on the Gymnasium classic control
suite demonstrate that TabPFN RL matches or surpasses Deep Q Network on
CartPole v1, MountainCar v0, and Acrobot v1, without applying gradient descent
or any extensive hyperparameter tuning. We discuss the theoretical aspects of
how bootstrapped targets and non stationary visitation distributions violate
the independence assumptions encoded in TabPFN's prior, yet the model retains a
surprising generalization capacity. We further formalize the intrinsic context
size limit of in context RL algorithms and propose principled truncation
strategies that enable continual learning when the context is full. Our results
establish prior fitted networks such as TabPFN as a viable foundation for fast
and computationally efficient RL, opening new directions for gradient free RL
with large pre trained transformers.

</details>


### [333] [SelectMix: Enhancing Label Noise Robustness through Targeted Sample Mixing](https://arxiv.org/abs/2509.11265)
*Qiuhao Liu,Ling Li,Yao Lu,Qi Xuan,Zhaowei Zhu,Jiaheng Wei*

Main category: cs.LG

TL;DR: Introduces SelectMix, a confidence-guided selective mixing method for learning with noisy labels. It uses K-fold cross-validation based confidence mismatch to identify uncertain samples and mixes them with confident peers, employing soft labels to reflect mix composition. Demonstrates consistent improvements over strong baselines on synthetic and real-world noisy-label datasets.


<details>
  <summary>Details</summary>
Motivation: Noisy labels cause memorization and degraded generalization in deep networks. While Mixup improves robustness, existing Mixup-based methods mix indiscriminately, spreading the influence of noisy labels. A principled, confidence-guided mixing strategy is needed to mitigate label noise.

Method: Identify potentially noisy/ambiguous samples via confidence-based mismatch analysis using K-fold cross-validation. Selectively blend these uncertain samples with confidently predicted peers from their potential classes. Use soft labels across all classes involved in the mixing to accurately reflect the mixture. Provide theoretical analysis and validate empirically on synthetic datasets (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and real-world benchmarks (CIFAR-N, MNIST, Clothing1M).

Result: SelectMix consistently outperforms strong baseline methods, demonstrating improved generalization and robustness to noisy labels across diverse datasets.

Conclusion: A principled, confidence-guided mixing framework for learning with noisy labels. By identifying uncertain samples and using soft labels in mixtures, SelectMix mitigates noise propagation and enhances generalization, with strong theoretical backing and broad empirical validation.

Abstract: Deep neural networks tend to memorize noisy labels, severely degrading their
generalization performance. Although Mixup has demonstrated effectiveness in
improving generalization and robustness, existing Mixup-based methods typically
perform indiscriminate mixing without principled guidance on sample selection
and mixing strategy, inadvertently propagating noisy supervision. To overcome
these limitations, we propose SelectMix, a confidence-guided mixing framework
explicitly tailored for noisy labels. SelectMix first identifies potentially
noisy or ambiguous samples through confidence based mismatch analysis using
K-fold cross-validation, then selectively blends identified uncertain samples
with confidently predicted peers from their potential classes. Furthermore,
SelectMix employs soft labels derived from all classes involved in the mixing
process, ensuring the labels accurately represent the composition of the mixed
samples, thus aligning supervision signals closely with the actual mixed
inputs. Through extensive theoretical analysis and empirical evaluations on
multiple synthetic (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and real-world
benchmark datasets (CIFAR-N, MNIST and Clothing1M), we demonstrate that
SelectMix consistently outperforms strong baseline methods, validating its
effectiveness and robustness in learning with noisy labels.

</details>


### [334] [Protected Probabilistic Classification Library](https://arxiv.org/abs/2509.11267)
*Ivan Petej*

Main category: cs.LG

TL;DR: A new Python package for shift-aware calibration of probabilistic classifiers, evaluated in binary and multi-class settings against existing post-hoc calibration methods; results are promising for batch and online learning under dataset shift.


<details>
  <summary>Details</summary>
Motivation: Calibration reliability degrades when training and test distributions differ. The abstract targets improving probability calibration under dataset shift for binary and multi-class classifiers.

Method: Proposes a shift-aware calibration technique implemented as a Python package and benchmarks it against existing post-hoc calibration methods in both binary and multi-class settings.

Result: Empirical results are promising, suggesting the technique improves calibration performance under distribution shift in both batch and online learning scenarios.

Conclusion: The proposed tool offers a practical, shift-aware calibration solution applicable to batch and online learning where data distribution changes between training and test phases.

Abstract: This paper introduces a new Python package specifically designed to address
calibration of probabilistic classifiers under dataset shift. The method is
demonstrated in binary and multi-class settings and its effectiveness is
measured against a number of existing post-hoc calibration methods. The
empirical results are promising and suggest that our technique can be helpful
in a variety of settings for batch and online learning classification problems
where the underlying data distribution changes between the training and test
sets.

</details>


### [335] [PINGS: Physics-Informed Neural Network for Fast Generative Sampling](https://arxiv.org/abs/2509.11284)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.LG

TL;DR: PINGS delivers one-pass, physics-informed generative sampling by learning reverse-time probability-flow dynamics, enabling constant-time samples with NFE=1.


<details>
  <summary>Details</summary>
Motivation: There is a demand for fast, differentiable, and interpretable generative samplers that preserve the target distribution and scale beyond iterative diffusion solvers.

Method: Train a physics-informed neural network to approximate the reverse-time probability-flow; frame sampling as a PINN residual with endpoint anchoring; learn a direct map from a 3D standard normal to a non-Gaussian GMM; achieve NFE=1.

Result: Constant-time generation (1e4 samples in ~16.54 ms on RTX 3090); distribution fidelity (MMD^2 = 1.88e-2) with accurate mean, covariance, skewness, and excess kurtosis; faster than DPM-Solver and DDIM; validated via a damped harmonic oscillator with MSE ~1e-5; white-box, differentiable map.

Conclusion: PINGS is a promising route to fast, function-based generative sampling, with potential extensions to scientific simulations (e.g., fast calorimetry) and other applications where one-pass sampling is advantageous.

Abstract: We introduce PINGS (Physics-Informed Neural Network for Fast Generative
Sampling), a framework that amortizes diffusion sampling by training a
physics-informed network to approximate reverse-time probability-flow dynamics,
reducing sampling to a single forward pass (NFE = 1). As a proof of concept, we
learn a direct map from a 3D standard normal to a non-Gaussian Gaussian Mixture
Model (GMM). PINGS preserves the target's distributional structure
(multi-bandwidth kernel $MMD^2 = 1.88 \times 10^{-2}$ with small errors in
mean, covariance, skewness, and excess kurtosis) and achieves constant-time
generation: $10^4$ samples in $16.54 \pm 0.56$ millisecond on an RTX 3090,
versus 468-843 millisecond for DPM-Solver (10/20) and 960 millisecond for DDIM
(50) under matched conditions. We also sanity-check the
PINN/automatic-differentiation pipeline on a damped harmonic oscillator,
obtaining MSEs down to $\mathcal{O}(10^{-5})$. Compared to fast but iterative
ODE solvers and direct-map families (Flow, Rectified-Flow, Consistency), PINGS
frames generative sampling as a PINN-style residual problem with endpoint
anchoring, yielding a white-box, differentiable map with NFE = 1. These
proof-of-concept results position PINGS as a promising route to fast,
function-based generative sampling with potential extensions to scientific
simulation (e.g., fast calorimetry).

</details>


### [336] [Efficient Single-Step Framework for Incremental Class Learning in Neural Networks](https://arxiv.org/abs/2509.11285)
*Alejandro Dopico-Castro,Oscar Fontenla-Romero,Bertha Guijarro-Berdiñas,Amparo Alonso-Betanzos*

Main category: cs.LG

TL;DR: CIFNet uses a frozen pre-trained feature extractor, a compact data buffer, and a non-iterative one-layer classifier to enable efficient class-incremental learning with one-step optimization on fixed features, reducing forgetting and training cost.


<details>
  <summary>Details</summary>
Motivation: Class Incremental Learning methods achieve high accuracy but are often resource-intensive due to continual feature adaptation and backpropagation. There is a need for efficient, low-resource approaches that perform well in constrained environments.

Method: Integrates a frozen pre-trained feature extractor, a compressed data buffer, and a simple non-iterative single-layer classifier trained on fixed features. No backbone fine-tuning; one-shot optimization on fixed features reduces computational overhead and training time.

Result: Empirical results indicate CIFNet mitigates classifier-level forgetting and attains accuracy comparable to state-of-the-art methods while substantially improving training efficiency and sustainability.

Conclusion: CIFNet advances practical class-incremental learning in resource-limited settings by leveraging strong pre-trained features and a frugal architecture that avoids iterative training.

Abstract: Incremental learning remains a critical challenge in machine learning, as
models often struggle with catastrophic forgetting -the tendency to lose
previously acquired knowledge when learning new information. These challenges
are even more pronounced in resource-limited settings. Many existing Class
Incremental Learning (CIL) methods achieve high accuracy by continually
adapting their feature representations; however, they often require substantial
computational resources and complex, iterative training procedures. This work
introduces CIFNet (Class Incremental and Frugal Network), a novel CIL approach
that addresses these limitations by offering a highly efficient and sustainable
solution. CIFNet's key innovation lies in its novel integration of several
existing, yet separately explored, components: a pre-trained and frozen feature
extractor, a compressed data buffer, and an efficient non-iterative one-layer
neural network for classification. A pre-trained and frozen feature extractor
eliminates computationally expensive fine-tuning of the backbone. This,
combined with a compressed buffer for efficient memory use, enables CIFNet to
perform efficient class-incremental learning through a single-step optimization
process on fixed features, minimizing computational overhead and training time
without requiring multiple weight updates. Experiments on benchmark datasets
confirm that CIFNet effectively mitigates catastrophic forgetting at the
classifier level, achieving high accuracy comparable to that of existing
state-of-the-art methods, while substantially improving training efficiency and
sustainability. CIFNet represents a significant advancement in making
class-incremental learning more accessible and pragmatic in environments with
limited resources, especially when strong pre-trained feature extractors are
available.

</details>


### [337] [Opal: An Operator Algebra View of RLHF](https://arxiv.org/abs/2509.11298)
*Madhava Gaikwad*

Main category: cs.LG

TL;DR: OPAL introduces GKPO, a unified representation of RLHF objectives as ladders of penalties and weights, with a reduction to a normal form under fixed reference and independent, additive components; provides serialization, hashing, and adapters, plus examples (DPO, RRHF, ORPO) and stress tests, enabling cross-method mapping when reducible.


<details>
  <summary>Details</summary>
Motivation: Provide a canonical, analyzable, and interoperable framework for RLHF methods that exposes reducibility properties, supports serialization/hashing, and facilitates cross-method conversions and debugging.

Method: Propose Opal view: objectives are ladders of penalties (additive) and pairwise weights (multiplicative) on a base utility. Establish a reduction law with if-and-only-if conditions: ladders collapse to a normal form on pairwise margins when the reference is fixed, penalties additive, and weights independent of intermediate margins. When conditions fail (reference shift, non-additive gates, score-dependent weights), small examples show non-reducibility. Introduce GKPO canonical schema that can represent many RLHF methods, plus standard JSON serialization, canonicalization, hashing, and explicit flags with finite witnesses when assumptions fail. Provide GKPO examples for DPO, RRHF, ORPO; cross-method conversions where assumptions permit; stress tests SHIFT/GATE/SCORE to illustrate non-reducibility. Supply a lightweight Python reference library for canonical hashing and adapters for DPO and RRHF.

Result: Demonstrates that under stated assumptions ladders reduce to a simple normal form; identifies boundaries where reducibility breaks; provides a generic schema (GKPO) and tooling (JSON, hashing, adapters) enabling representation, comparison, and conversion across RLHF methods; includes practical tests and a Python library.

Conclusion: GKPO offers a scalable, extensible framework for representing RLHF methods, enabling interoperability, rigorous analysis of reducibility, and practical tooling for hashing and conversion across methods; non-reducibility is highlighted under shifting references, non-additive gates, or score-dependent weights.

Abstract: We present Opal, an operator view of reinforcement learning from human
feedback (RLHF). Objectives are expressed as ladders of two primitives on a
base utility: additive penalties and multiplicative pairwise weights. We
describe a simple reduction law with if-and-only-if conditions: such ladders
collapse to a normal form on pairwise margins when the reference is fixed,
penalties are additive, and weights are independent of intermediate margins.
When these assumptions do not hold (reference shift, non-additive gates,
score-dependent weights), small examples demonstrate non-reducibility.
  Building on this view, we introduce GKPO (Generalized Kernel Preference
Object), a canonical schema in which many RLHF methods can be represented and,
when reducible, mapped back from. GKPO provides a standard JSON serialization,
canonicalization and hashing rules, and explicit flags with finite witnesses
when assumptions fail.
  We illustrate these ideas with GKPO examples for DPO, RRHF, and ORPO, along
with cross-method conversions (where assumptions permit) and minimal stress
tests (SHIFT/GATE/SCORE) that highlight non-reducibility. A lightweight Python
reference library accompanies the schema, implementing canonical hashing and
adapters for DPO and RRHF.

</details>


### [338] [MatQnA: A Benchmark Dataset for Multi-modal Large Language Models in Materials Characterization and Analysis](https://arxiv.org/abs/2509.11335)
*Yonghao Weng,Liqiang Gao,Linwu Zhu,Jian Huang*

Main category: cs.LG

TL;DR: MatQnA is a multi-modal benchmark for material characterization, spanning 10 techniques, created with LLMs and human-in-the-loop validation; shows ~90% accuracy on objective questions; dataset public on HuggingFace.


<details>
  <summary>Details</summary>
Motivation: Addresses lack of systematic validation of AI capabilities in specialized materials characterization; aims to quantify LLM performance across standard techniques.

Method: Hybrid pipeline: human-in-the-loop to generate high-quality QA pairs, combining multiple-choice and subjective questions across 10 methods (XPS, XRD, SEM, TEM, etc.); evaluation on state-of-the-art multi-modal LLMs like GPT-4.1, Claude 4, etc.; dataset is public.

Result: Advanced models achieved nearly 90% accuracy on objective questions in materials data interpretation tasks; indicates strong potential for AI-assisted characterization.

Conclusion: MatQnA provides a publicly available benchmark to drive AI-assisted materials characterization and analysis, enabling systematic assessment and progress in the field.

Abstract: Recently, large language models (LLMs) have achieved remarkable breakthroughs
in general domains such as programming and writing, and have demonstrated
strong potential in various scientific research scenarios. However, the
capabilities of AI models in the highly specialized field of materials
characterization and analysis have not yet been systematically or sufficiently
validated. To address this gap, we present MatQnA, the first multi-modal
benchmark dataset specifically designed for material characterization
techniques. MatQnA includes ten mainstream characterization methods, such as
X-ray Photoelectron Spectroscopy (XPS), X-ray Diffraction (XRD), Scanning
Electron Microscopy (SEM), Transmission Electron Microscopy (TEM), etc. We
employ a hybrid approach combining LLMs with human-in-the-loop validation to
construct high-quality question-answer pairs, integrating both multiple-choice
and subjective questions. Our preliminary evaluation results show that the most
advanced multi-modal AI models (e.g., GPT-4.1, Claude 4, Gemini 2.5, and Doubao
Vision Pro 32K) have already achieved nearly 90% accuracy on objective
questions in materials data interpretation and analysis tasks, demonstrating
strong potential for applications in materials characterization and analysis.
The MatQnA dataset is publicly available at
https://huggingface.co/datasets/richardhzgg/matQnA.

</details>


### [339] [On the Escaping Efficiency of Distributed Adversarial Training Algorithms](https://arxiv.org/abs/2509.11337)
*Ying Cao,Kun Yuan,Ali H. Sayed*

Main category: cs.LG

TL;DR: Decentralized adversarial training can escape local minima faster than centralized when attack strength is mild and batch size is large, leading to flatter, potentially more robust models; with stronger attacks, the advantage may fade.


<details>
  <summary>Details</summary>
Motivation: Understand how distributed training architecture (centralized vs decentralized) affects robustness and the geometry (flatness) of the solutions in adversarial training.

Method: Develop a general theoretical framework to analyze escaping efficiency from local minima for centralized and decentralized (consensus and diffusion) adversarial training; examine dependence on perturbation bound and batch size; validate with simulations comparing performance.

Result: Under small perturbations and large batch sizes, decentralized methods escape faster from local minima and yield flatter minima than centralized; with larger perturbations the advantage may disappear; simulations corroborate theoretical predictions and compare methods.

Conclusion: Decentralized adversarial training can enhance robustness in distributed settings under mild attacks, due to faster escape and flatter minima; the benefit depends on attack strength; decentralization is advantageous in certain regimes, informing strategy for distributed robust learning.

Abstract: Adversarial training has been widely studied in recent years due to its role
in improving model robustness against adversarial attacks. This paper focuses
on comparing different distributed adversarial training algorithms--including
centralized and decentralized strategies--within multi-agent learning
environments. Previous studies have highlighted the importance of model
flatness in determining robustness. To this end, we develop a general
theoretical framework to study the escaping efficiency of these algorithms from
local minima, which is closely related to the flatness of the resulting models.
We show that when the perturbation bound is sufficiently small (i.e., when the
attack strength is relatively mild) and a large batch size is used,
decentralized adversarial training algorithms--including consensus and
diffusion--are guaranteed to escape faster from local minima than the
centralized strategy, thereby favoring flatter minima. However, as the
perturbation bound increases, this trend may no longer hold. In the simulation
results, we illustrate our theoretical findings and systematically compare the
performance of models obtained through decentralized and centralized
adversarial training algorithms. The results highlight the potential of
decentralized strategies to enhance the robustness of models in distributed
settings.

</details>


### [340] [BiLSTM-VHP: BiLSTM-Powered Network for Viral Host Prediction](https://arxiv.org/abs/2509.11345)
*Azher Ahmed Efat,Farzana Islam,Annajiat Alim Rasel,Munima Haque*

Main category: cs.LG

TL;DR: BiLSTM-VHP is a lightweight bidirectional LSTM model that predicts the viral host from 400-base nucleotide sequences for orthohantavirus, rabies lyssavirus, and rotavirus A, achieving high accuracy (89.62%, 96.58%, 77.22% respectively) and using large curated datasets; code and data are released.


<details>
  <summary>Details</summary>
Motivation: Zoonotic diseases pose severe public health risks. Rapid, accurate host prediction from viral sequences can help prevent spillovers. The abstract emphasizes the need for efficient, scalable methods and reports high performance across multiple viruses using curated datasets.

Method: A lightweight BiLSTM-based architecture (BiLSTM-VHP) processes 400-base nucleotide sequences to classify the host into multiple classes (9, 12, and 29 classes for the three viruses). Evaluation includes confusion matrix, F1, precision, recall, and micro-averaged AUC. The authors also compiled three curated datasets with 8,575 (orthohantavirus), 95,197 (rotavirus A), and 22,052 (rabies lyssavirus) sequences.

Result: Prediction accuracies: orthohantavirus 89.62%; rotavirus A 96.58%; rabies lyssavirus 77.22%. Datasets comprise 8,575, 95,197, and 22,052 sequences across 9, 12, and 29 hosts respectively. The work outperforms prior studies, and provides code and datasets at the provided OSF link.

Conclusion: BiLSTM-VHP demonstrates effective host prediction from viral nucleotide sequences across multiple viruses, with strong accuracy and sizable curated datasets, contributing publicly released tools and data for further research.

Abstract: Recorded history shows the long coexistence of humans and animals, suggesting
it began much earlier. Despite some beneficial interdependence, many animals
carry viral diseases that can spread to humans. These diseases are known as
zoonotic diseases. Recent outbreaks of SARS-CoV-2, Monkeypox and swine flu
viruses have shown how these viruses can disrupt human life and cause death.
Fast and accurate predictions of the host from which the virus spreads can help
prevent these diseases from spreading. This work presents BiLSTM-VHP, a
lightweight bidirectional long short-term memory (LSTM)-based architecture that
can predict the host from the nucleotide sequence of orthohantavirus, rabies
lyssavirus, and rotavirus A with high accuracy. The proposed model works with
nucleotide sequences of 400 bases in length and achieved a prediction accuracy
of 89.62% for orthohantavirus, 96.58% for rotavirus A, and 77.22% for rabies
lyssavirus outperforming previous studies. Moreover, performance of the model
is assessed using the confusion matrix, F-1 score, precision, recall,
microaverage AUC. In addition, we introduce three curated datasets of
orthohantavirus, rotavirus A, and rabies lyssavirus containing 8,575, 95,197,
and 22,052 nucleotide sequences divided into 9, 12, and 29 host classes,
respectively. The codes and dataset are available at
https://doi.org/10.17605/OSF.IO/ANFKR

</details>


### [341] [On Linear Mode Connectivity of Mixture-of-Experts Architectures](https://arxiv.org/abs/2509.11348)
*Viet-Hoang Tran,Van Hoan Trinh,Khanh Vinh Bui,Tan M. Nguyen*

Main category: cs.LG

TL;DR: LMC exists in Mixture-of-Experts models; MoE symmetries are permutations over experts and gating; a matching/alignment algorithm reveals low-loss linear connectivity between independently trained MoEs; evidence across dense, sparse, and shared-expert MoEs across varied scales/datasets.


<details>
  <summary>Details</summary>
Motivation: To understand the loss landscape and generalization properties of MoE architectures by extending the Linear Mode Connectivity concept to modular, scalable networks, and to inform ensembling and optimization dynamics.

Method: 1) Theoretically characterize MoE symmetries as permutations on both expert components and gating function. 2) Propose a matching/alignment algorithm to align independently trained MoEs. 3) Empirically validate LMC by applying the alignment and tracing linear paths between trained models across diverse MoE configurations (dense, sparse, shared-employment), model scales, and datasets.

Result: The study demonstrates that LMC holds in MoE architectures. The proposed alignment algorithm successfully matches independently trained MoEs, enabling discovery of low-loss linear paths. This is validated across multiple MoE variants and datasets, confirming the presence of LMC and revealing insights into MoE loss landscapes.

Conclusion: LMC generalizes to Mixture-of-Experts architectures, indicating that the functional landscape of MoEs supports connected, low-loss linear paths despite modular gating and permutation symmetries. This enhances understanding of optimization dynamics in scalable models and has implications for ensembling and transfer across MoEs.

Abstract: Linear Mode Connectivity (LMC) is a notable phenomenon in the loss landscapes
of neural networks, wherein independently trained models have been observed to
be connected--up to permutation symmetries--by linear paths in parameter space
along which the loss remains consistently low. This observation challenges
classical views of non-convex optimization and has implications for model
ensembling, generalization, and our understanding of neural loss geometry.
Inspired by recent studies on LMC in standard neural networks, we
systematically investigate this phenomenon within Mixture-of-Experts (MoE)
architectures--a class of models known for their scalability and computational
efficiency, which combine traditional neural networks--referred to as
experts--through a learnable gating mechanism. We begin by conducting a
comprehensive analysis of both dense and sparse gating regimes, demonstrating
that the symmetries inherent to MoE architectures are fully characterized by
permutations acting on both the expert components and the gating function.
Building on these foundational findings, we propose a matching algorithm that
enables alignment between independently trained MoEs, thereby facilitating the
discovery of LMC. Finally, we empirically validate the presence of LMC using
our proposed algorithm across diverse MoE configurations--including dense,
sparse, and shared-expert variants--under a wide range of model settings and
datasets of varying scales and modalities. Our results confirm the existence of
LMC in MoE architectures and offer fundamental insights into the functional
landscape and optimization dynamics of deep learning models.

</details>


### [342] [Online Omniprediction with Long-Term Constraints](https://arxiv.org/abs/2509.11357)
*Yahav Bechavod,Jiuyao Lu,Aaron Roth*

Main category: cs.LG

TL;DR: We study online omniprediction with long-term constraints and show how a single forecaster can enable heterogeneous downstream agents to achieve sublinear regret and bounded cumulative constraint violations; the results extend to per-subsequence guarantees.


<details>
  <summary>Details</summary>
Motivation: The setting combines online decision making with long-term constraint satisfaction across multiple downstream agents with potentially different utilities and vector-valued costs. A universal set of predictions is desirable so that each agent can achieve no-regret learning while respecting constraints.

Method: We design a universal prediction mechanism that feeds into downstream agents, who act as simple functions of the predictions. Using online learning techniques with long-term constraints (e.g., primal–dual analyses), we prove that each agent can guarantee tilde O(sqrt(T)) regret and O(1) cumulative constraint violation. We further extend the guarantees to arbitrary intersecting contextually defined subsequences, achieving simultaneous regret and constraint bounds on each subsequence against subsequence-specific benchmarks.

Result: For each downstream agent, the forecast set enables them to achieve sublinear regret (tilde O(sqrt(T))) and constant-order cumulative constraint violation (O(1)) over time. The framework also yields per-subsequence guarantees, providing regret and constraint bounds on every intersecting subsequence with respect to tailored benchmarks.

Conclusion: A single universal set of predictions suffices for heterogeneous agents to attain both no-regret learning and long-term constraint satisfaction. The approach is broadly applicable and invites future work on computational efficiency, non-convex settings, stochastic/adversarial mixtures, and extensions to richer constraint structures.

Abstract: We introduce and study the problem of online omniprediction with long-term
constraints. At each round, a forecaster is tasked with generating predictions
for an underlying (adaptively, adversarially chosen) state that are broadcast
to a collection of downstream agents, who must each choose an action. Each of
the downstream agents has both a utility function mapping actions and state to
utilities, and a vector-valued constraint function mapping actions and states
to vector-valued costs. The utility and constraint functions can arbitrarily
differ across downstream agents. Their goal is to choose actions that guarantee
themselves no regret while simultaneously guaranteeing that they do not
cumulatively violate the constraints across time. We show how to make a single
set of predictions so that each of the downstream agents can guarantee this by
acting as a simple function of the predictions, guaranteeing each of them
$\tilde{O}(\sqrt{T})$ regret and $O(1)$ cumulative constraint violation. We
also show how to extend our guarantees to arbitrary intersecting contextually
defined \emph{subsequences}, guaranteeing each agent both regret and constraint
violation bounds not just marginally, but simultaneously on each subsequence,
against a benchmark set of actions simultaneously tailored to each subsequence.

</details>


### [343] [PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits](https://arxiv.org/abs/2509.11362)
*Loka Li,Wong Yu Kang,Minghao Fu,Guangyi Chen,Zhenhao Chen,Gongxu Luo,Yuewen Sun,Salman Khan,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: PersonaX provides two multimodal datasets (CelebPersona and AthlePersona) linking LLM-inferred behavioral traits with facial imagery and biographical data, and proposes a novel causal representation learning framework with identifiability guarantees for multimodal, multi-measurement data.


<details>
  <summary>Details</summary>
Motivation: There is a need to study human behavior traits across modalities (textual descriptions, facial attributes, biographical data) using curated datasets. Existing resources lack integrated multimodal data combining behavioral descriptors with visual and biographical attributes.

Method: Curate two datasets: CelebPersona (9444 public figures) and AthlePersona (4181 professional athletes). Each sample includes trait assessments inferred by three high-performing LLMs, facial imagery, and structured biographical features. Analysis comprises (1) abstracting high-level trait scores from text and applying five statistical independence tests with other modalities, and (2) introducing a novel causal representation learning (CRL) framework for multimodal and multi-measurement data, with theoretical identifiability guarantees. Experiments on synthetic and real data demonstrate effectiveness.

Result: The CRL framework demonstrates effectiveness on both synthetic and real-world data, enabling unified analysis of LLM-inferred behavioral traits with visual and biographical attributes and offering identifiability guarantees for the multimodal setup.

Conclusion: PersonaX lays the groundwork for studying LLM-inferred behavioral traits in conjunction with visual and biographical attributes, advancing multimodal trait analysis and causal reasoning.

Abstract: Understanding human behavior traits is central to applications in
human-computer interaction, computational social science, and personalized AI
systems. Such understanding often requires integrating multiple modalities to
capture nuanced patterns and relationships. However, existing resources rarely
provide datasets that combine behavioral descriptors with complementary
modalities such as facial attributes and biographical information. To address
this gap, we present PersonaX, a curated collection of multimodal datasets
designed to enable comprehensive analysis of public traits across modalities.
PersonaX consists of (1) CelebPersona, featuring 9444 public figures from
diverse occupations, and (2) AthlePersona, covering 4181 professional athletes
across 7 major sports leagues. Each dataset includes behavioral trait
assessments inferred by three high-performing large language models, alongside
facial imagery and structured biographical features. We analyze PersonaX at two
complementary levels. First, we abstract high-level trait scores from text
descriptions and apply five statistical independence tests to examine their
relationships with other modalities. Second, we introduce a novel causal
representation learning (CRL) framework tailored to multimodal and
multi-measurement data, providing theoretical identifiability guarantees.
Experiments on both synthetic and real-world data demonstrate the effectiveness
of our approach. By unifying structured and unstructured analysis, PersonaX
establishes a foundation for studying LLM-inferred behavioral traits in
conjunction with visual and biographical attributes, advancing multimodal trait
analysis and causal reasoning.

</details>


### [344] [Detecting Model Drifts in Non-Stationary Environment Using Edit Operation Measures](https://arxiv.org/abs/2509.11367)
*Chang-Hwan Lee,Alexander Shim*

Main category: cs.LG

TL;DR: A framework using edit-operation-based trajectory measures to detect drift in non-stationary RL by comparing stationary vs perturbed state-action sequences; effective under noise.


<details>
  <summary>Details</summary>
Motivation: In real-world RL, environment dynamics often drift, breaking stationarity. Detecting drift is important for reliability, monitoring, and timely adaptation in systems like healthcare, robotics, and finance.

Method: Develop a suite of edit-operation-based metrics to quantify deviations between state-action trajectories generated under stationary and perturbed conditions. Compare the distributions of these trajectories to identify drift, and test robustness to noise.

Result: The proposed measures effectively distinguish drifted from non-drifted scenarios, maintaining performance under varying noise levels in the observed trajectories.

Conclusion: Provides a practical drift-detection tool for non-stationary RL environments, enabling monitoring and potential adaptive responses when drift is detected.

Abstract: Reinforcement learning (RL) agents typically assume stationary environment
dynamics. Yet in real-world applications such as healthcare, robotics, and
finance, transition probabilities or reward functions may evolve, leading to
model drift. This paper proposes a novel framework to detect such drifts by
analyzing the distributional changes in sequences of agent behavior.
Specifically, we introduce a suite of edit operation-based measures to quantify
deviations between state-action trajectories generated under stationary and
perturbed conditions. Our experiments demonstrate that these measures can
effectively distinguish drifted from non-drifted scenarios, even under varying
levels of noise, providing a practical tool for drift detection in
non-stationary RL environments.

</details>


### [345] [Decoding Musical Origins: Distinguishing Human and AI Composers](https://arxiv.org/abs/2509.11369)
*Cheng-Yang Tsai,Tzu-Wei Huang,Shao-Yu Wei,Guan-Wei Chen,Hung-Ying Chu,Yu-Cheng Lin*

Main category: cs.LG

TL;DR: YNote enables an ML-friendly representation of music that preserves stylistic cues well enough to classify a piece as human-created, algorithm-generated, or LLM-generated with 98.25% accuracy; it also reveals distinctive traces ('technological fingerprints') of different AI generation methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of representing musical data for ML analysis and enable tracing the origins of AI-generated music by developing a notation system that preserves stylistic features.

Method: Develop a new music notation system (YNote) and frame the task as text classification. Use TF-IDF to extract features from sequences encoded in YNote, and apply SMOTE to balance classes. Train a classification model to distinguish Native, Algorithm Generated, and LLM Generated music.

Result: The model achieved 98.25% accuracy, demonstrating that YNote retains sufficient stylistic information for origin analysis and can identify unique fingerprints associated with different AI generation techniques.

Conclusion: YNote is effective for ML-based analysis of musical style and for tracing the origins of AI-generated music, offering a practical tool for detecting and characterizing AI-authored content in music.

Abstract: With the rapid advancement of Large Language Models (LLMs), AI-driven music
generation has become a vibrant and fruitful area of research. However, the
representation of musical data remains a significant challenge. To address
this, a novel, machine-learning-friendly music notation system, YNote, was
developed. This study leverages YNote to train an effective classification
model capable of distinguishing whether a piece of music was composed by a
human (Native), a rule-based algorithm (Algorithm Generated), or an LLM (LLM
Generated). We frame this as a text classification problem, applying the Term
Frequency-Inverse Document Frequency (TF-IDF) algorithm to extract structural
features from YNote sequences and using the Synthetic Minority Over-sampling
Technique (SMOTE) to address data imbalance. The resulting model achieves an
accuracy of 98.25%, successfully demonstrating that YNote retains sufficient
stylistic information for analysis. More importantly, the model can identify
the unique " technological fingerprints " left by different AI generation
techniques, providing a powerful tool for tracing the origins of AI-generated
content.

</details>


### [346] [Intelligent Reservoir Decision Support: An Integrated Framework Combining Large Language Models, Advanced Prompt Engineering, and Multimodal Data Fusion for Real-Time Petroleum Operations](https://arxiv.org/abs/2509.11376)
*Seyed Kourosh Mahjour,Seyed Saman Mahjour*

Main category: cs.LG

TL;DR: An AI-driven, multimodal reservoir analysis framework using LLMs, retrieval-augmented generation (RAG), and vision transformers delivers real-time decision support with high accuracy and substantial cost savings, validated across multiple reservoirs.


<details>
  <summary>Details</summary>
Motivation: The petroleum industry faces challenges in rapidly integrating complex, multimodal datasets for real-time reservoir decision support; there is a need for scalable, accurate, and safe AI-enabled solutions that can handle seismic, well logs, and production data.

Method: An integrated framework combining LLMs (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Pro) with domain-specific RAG over 50k petroleum documents, chain-of-thought prompting, few-shot learning, and multimodal data fusion via vision transformers to process seismic interpretations, well logs, and production data; real-time data streams; field validation across 15 reservoirs; safety and reproducibility measures.

Result: Key results include: 94.2% reservoir characterization accuracy, 87.6% production forecasting precision, and 91.4% well placement optimization success across 15 reservoir environments; sub-second response times; 96.2% safety reliability with no high-risk incidents; economic gains with 62–78% cost reductions (mean 72%) and an 8-month payback; few-shot learning reduces field adaptation time by 72%; automated prompt optimization yields 89% improvement in reasoning quality; real-time data streams achieve 96.2% anomaly detection accuracy and environmental incidents reduced by 45%; comprehensive experimental protocols, baselines, ablations, and significance testing for reproducibility.

Conclusion: Demonstrates that integrating cutting-edge AI technologies with petroleum domain expertise can substantially improve operational efficiency, safety, and economic performance; the study provides detailed methodologies and reproducible protocols for deploying AI-driven reservoir management.

Abstract: The petroleum industry faces unprecedented challenges in reservoir
management, requiring rapid integration of complex multimodal datasets for
real-time decision support. This study presents a novel integrated framework
combining state-of-the-art large language models (GPT-4o, Claude 4 Sonnet,
Gemini 2.5 Pro) with advanced prompt engineering techniques and multimodal data
fusion for comprehensive reservoir analysis. The framework implements
domain-specific retrieval-augmented generation (RAG) with over 50,000 petroleum
engineering documents, chain-of-thought reasoning, and few-shot learning for
rapid field adaptation. Multimodal integration processes seismic
interpretations, well logs, and production data through specialized AI models
with vision transformers. Field validation across 15 diverse reservoir
environments demonstrates exceptional performance: 94.2% reservoir
characterization accuracy, 87.6% production forecasting precision, and 91.4%
well placement optimization success rate. The system achieves sub-second
response times while maintaining 96.2% safety reliability with no high-risk
incidents during evaluation. Economic analysis reveals 62-78% cost reductions
(mean 72%) relative to traditional methods with 8-month payback period.
Few-shot learning reduces field adaptation time by 72%, while automated prompt
optimization achieves 89% improvement in reasoning quality. The framework
processed real-time data streams with 96.2% anomaly detection accuracy and
reduced environmental incidents by 45%. We provide detailed experimental
protocols, baseline comparisons, ablation studies, and statistical significance
testing to ensure reproducibility. This research demonstrates practical
integration of cutting-edge AI technologies with petroleum domain expertise for
enhanced operational efficiency, safety, and economic performance.

</details>


### [347] [Enhancing ML Models Interpretability for Credit Scoring](https://arxiv.org/abs/2509.11389)
*Sagi Schwartz,Qinling Wang,Fang Fang*

Main category: cs.LG

TL;DR: Proposes a hybrid explainable AI approach for credit default: SHAP-guided feature selection to identify a small set of predictors, followed by training transparent glass-box models (EBM, PLTR) that match a black-box benchmark (XGBoost) while meeting regulatory transparency requirements.


<details>
  <summary>Details</summary>
Motivation: Regulators require transparent, robust models for credit risk (e.g., IRB). While ML can boost accuracy, black-box models lack interpretability. Post-hoc explanations exist but may not yield lightweight, transparent models suitable for compliance. The paper aims to bridge accuracy and transparency by combining SHAP-driven feature selection with glass-box modeling.

Method: Use Lending Club data. Apply SHAP to rank features and select a subset (10 features, 88.5% reduction). Benchmark with XGBoost as the black-box model. Train glass-box models (Explainable Boosting Machine, Penalized Logistic Tree Regression). Assess performance parity with the benchmark. Enhance interpretability and robustness via feature interaction analysis, correlation checks, and expert input.

Result: Glass-box models achieved performance comparable to the XGBoost benchmark while using only 10 features (88.5% reduction), demonstrating that post-hoc SHAP-guided feature selection can yield compact, interpretable models without sacrificing accuracy. The study also shows improvements in interpretability and robustness through interaction analysis and expert input.

Conclusion: A viable path to regulatory-compliant, accurate credit risk models: post-hoc SHAP-guided feature selection can inform the construction of transparent glass-box models that retain competitive performance, with further gains from interaction checks and expert guidance.

Abstract: Predicting default is essential for banks to ensure profitability and
financial stability. While modern machine learning methods often outperform
traditional regression techniques, their lack of transparency limits their use
in regulated environments. Explainable artificial intelligence (XAI) has
emerged as a solution in domains like credit scoring. However, most XAI
research focuses on post-hoc interpretation of black-box models, which does not
produce models lightweight or transparent enough to meet regulatory
requirements, such as those for Internal Ratings-Based (IRB) models.
  This paper proposes a hybrid approach: post-hoc interpretations of black-box
models guide feature selection, followed by training glass-box models that
maintain both predictive power and transparency.
  Using the Lending Club dataset, we demonstrate that this approach achieves
performance comparable to a benchmark black-box model while using only 10
features - an 88.5% reduction. In our example, SHapley Additive exPlanations
(SHAP) is used for feature selection, eXtreme Gradient Boosting (XGBoost)
serves as the benchmark and the base black-box model, and Explainable Boosting
Machine (EBM) and Penalized Logistic Tree Regression (PLTR) are the
investigated glass-box models.
  We also show that model refinement using feature interaction analysis,
correlation checks, and expert input can further enhance model interpretability
and robustness.

</details>


### [348] [From Firewalls to Frontiers: AI Red-Teaming is a Domain-Specific Evolution of Cyber Red-Teaming](https://arxiv.org/abs/2509.11398)
*Anusha Sinha,Keltin Grimes,James Lucassen,Michael Feffer,Nathan VanHoudnos,Zhiwei Steven Wu,Hoda Heidari*

Main category: cs.LG

TL;DR: Treat AI red-teaming as a domain-specific evolution of cyber red-teaming to systematically assess and secure AI-enabled systems by leveraging cyber red-team structures, rules of engagement, and tooling.


<details>
  <summary>Details</summary>
Motivation: AI-enabled enterprise systems introduce new risks, failure modes, and unpatchable bugs that necessitate re-prioritizing disclosure and mitigation; integrating AI with cyber red-teaming enhances realism, accountability, and scalability of red-team engagements.

Method: Conceptual analysis and framework proposal: map AI red-teaming onto existing cyber red-teaming structures, rules of engagement, and tooling to create repeatable, scalable, and evidence-based assessments of AI components.

Result: Proposes an integrated AI-cyber red-teaming framework that emphasizes recognizing AI-specific risks and failure modes, enabling more effective evaluation and remediation of AI-enabled systems.

Conclusion: Merging AI and cyber red-teaming will create a robust security ecosystem that can adapt to evolving threats and better prepare defenders to mitigate AI-specific vulnerabilities.

Abstract: A red team simulates adversary attacks to help defenders find effective
strategies to defend their systems in a real-world operational setting. As more
enterprise systems adopt AI, red-teaming will need to evolve to address the
unique vulnerabilities and risks posed by AI systems. We take the position that
AI systems can be more effectively red-teamed if AI red-teaming is recognized
as a domain-specific evolution of cyber red-teaming. Specifically, we argue
that existing Cyber Red Teams who adopt this framing will be able to better
evaluate systems with AI components by recognizing that AI poses new risks, has
new failure modes to exploit, and often contains unpatchable bugs that
re-prioritize disclosure and mitigation strategies. Similarly, adopting a
cybersecurity framing will allow existing AI Red Teams to leverage a
well-tested structure to emulate realistic adversaries, promote mutual
accountability with formal rules of engagement, and provide a pattern to mature
the tooling necessary for repeatable, scalable engagements. In these ways, the
merging of AI and Cyber Red Teams will create a robust security ecosystem and
best position the community to adapt to the rapidly changing threat landscape.

</details>


### [349] [Framing AI System Benchmarking as a Learning Task: FlexBench and the Open MLPerf Dataset](https://arxiv.org/abs/2509.11413)
*Grigori Fursin,Daniel Altunay*

Main category: cs.LG

TL;DR: FlexBench is a modular extension of MLPerf LLM inference that integrates with HuggingFace and OpenMLPerf Dataset to provide actionable benchmarking insights across datasets, software, and hardware, validated on commodity servers to guide cost-effective AI deployment decisions.


<details>
  <summary>Details</summary>
Motivation: Benchmarking lags behind the rapid evolution of AI systems; there is a need to frame benchmarking as an AI task — continuously evaluating and optimizing models across diverse datasets, software, hardware, and metrics to inform deployment, optimization, and co-design choices.

Method: Develop FlexBench as a modular extension of the MLPerf LLM inference benchmark, integrated with HuggingFace, to collect benchmarking results and metadata into an OpenMLPerf Dataset. This dataset supports collaborative curation and predictive modeling/feature engineering. Validation performed via MLPerf Inference submissions evaluating models like DeepSeek R1 and LLaMA 3.3 on commodity servers.

Result: Concept validated: FlexBench is feasible and provides actionable benchmarking through MLPerf Inference submissions on commodity hardware, illustrating its utility for cost-aware deployment decisions.

Conclusion: The broader objective is to empower practitioners to make cost-effective AI deployment decisions aligned with available resources, requirements, and constraints by providing a flexible, extensible benchmarking framework and a collaboratively maintained benchmark dataset.

Abstract: Existing AI system benchmarks such as MLPerf often struggle to keep pace with
the rapidly evolving AI landscape, making it difficult to support informed
deployment, optimization, and co-design decisions for AI systems. We suggest
that benchmarking itself can be framed as an AI task - one in which models are
continuously evaluated and optimized across diverse datasets, software, and
hardware, using key metrics such as accuracy, latency, throughput, energy
consumption, and cost. To support this perspective, we present FlexBench: a
modular extension of the MLPerf LLM inference benchmark, integrated with
HuggingFace and designed to provide relevant and actionable insights.
Benchmarking results and metadata are collected into an Open MLPerf Dataset,
which can be collaboratively curated, extended, and leveraged for predictive
modeling and feature engineering. We successfully validated the FlexBench
concept through MLPerf Inference submissions, including evaluations of DeepSeek
R1 and LLaMA 3.3 on commodity servers. The broader objective is to enable
practitioners to make cost-effective AI deployment decisions that reflect their
available resources, requirements, and constraints.

</details>


### [350] [Long-time dynamics and universality of nonconvex gradient descent](https://arxiv.org/abs/2509.11426)
*Qiyang Han*

Main category: cs.LG

TL;DR: A general theory for long-time dynamics of nonconvex gradient descent in generalized single-index models at large aspect ratio, showing concentration around a Gaussian theoretical gradient descent governed by a two-scalar state evolution; universality across design matrices; data-free parameter estimation; and connections to dynamical mean-field theory.


<details>
  <summary>Details</summary>
Motivation: Address how nonconvex gradient descent behaves over long horizons in high-dimensional generalized single-index models, characterize its trajectory, reveal universal behavior beyond Gaussian data, and provide practical tools for hyperparameter tuning and understanding implicit regularization.

Method: Prove trajectory concentration of each GD iterate around a deterministic Gaussian theoretical gradient descent. Derive a two-scalar state evolution system that tracks the dynamics. Establish universality across broad design matrices in the large aspect ratio regime. Develop a data-free algorithm to estimate state evolution parameters along the entire gradient descent path. Demonstrate connections to dynamical mean-field theory for constant-time horizons.

Result: Gradient descent iterates concentrate around the Gaussian theoretical gradient descent for long times; the dynamics are captured by a two-scalar state evolution; universality holds for a broad class of design matrices; GD is approximately data-independent and incoherent with features (implicit regularization); two applications: (i) global convergence results for structured link functions and universality in phase retrieval with large aspect ratio; (ii) a data-free estimator for state evolution enabling hyperparameter tuning; the Gaussian theoretical gradient descent aligns with DMFT predictions at constant horizon.

Conclusion: The proposed Gaussian theoretical gradient descent provides a unifying, data-universal framework for long-time GD dynamics in high-dimensional nonconvex models and offers practical tools for analysis and tuning, while connecting to recent DMFT approaches.

Abstract: This paper develops a general approach to characterize the long-time
trajectory behavior of nonconvex gradient descent in generalized single-index
models in the large aspect ratio regime. In this regime, we show that for each
iteration the gradient descent iterate concentrates around a deterministic
vector called the `Gaussian theoretical gradient descent', whose dynamics can
be tracked by a state evolution system of two recursive equations for two
scalars. Our concentration guarantees hold universally for a broad class of
design matrices and remain valid over long time horizons until algorithmic
convergence or divergence occurs. Moreover, our approach reveals that gradient
descent iterates are in general approximately independent of the data and
strongly incoherent with the feature vectors, a phenomenon previously known as
the `implicit regularization' effect of gradient descent in specific models
under Gaussian data.
  As an illustration of the utility of our general theory, we present two
applications of different natures in the regression setting. In the first, we
prove global convergence of nonconvex gradient descent with general independent
initialization for a broad class of structured link functions, and establish
universality of randomly initialized gradient descent in phase retrieval for
large aspect ratios. In the second, we develop a data-free iterative algorithm
for estimating state evolution parameters along the entire gradient descent
trajectory, thereby providing a low-cost yet statistically valid tool for
practical tasks such as hyperparameter tuning and runtime determination.
  As a by-product of our analysis, we show that in the large aspect ratio
regime, the Gaussian theoretical gradient descent coincides with a recent line
of dynamical mean-field theory for gradient descent over the constant-time
horizon.

</details>


### [351] [Tabular Data with Class Imbalance: Predicting Electric Vehicle Crash Severity with Pretrained Transformers (TabPFN) and Mamba-Based Models](https://arxiv.org/abs/2509.11449)
*Shriyank Somvanshi,Pavan Hebli,Gaurab Chhetri,Subasish Das*

Main category: cs.LG

TL;DR: A deep tabular learning approach on Texas EV crash data uses XGBoost/Random Forest feature importance, SMOTEENN resampling, and benchmarks three deep tabular models (TabPFN, MambaNet, MambaAttention). TabPFN generalizes well; MambaAttention excels in severe-injury prediction; results support deep tabular architectures for data-driven EV crash safety interventions.


<details>
  <summary>Details</summary>
Motivation: Improve crash severity prediction for electric vehicle (EV) crashes using real-world data, address class imbalance, and leverage advanced deep tabular models to enable data-driven safety interventions.

Method: Filter to EV-only crashes (n=23,301); identify key predictors via XGBoost and Random Forest feature importance; apply SMOTEENN to balance classes; benchmark TabPFN, MambaNet, and MambaAttention for severity prediction; assess predictions and analyze AEB’s predictive value.

Result: Top predictors include intersection relation, first harmful event, person age, crash speed limit, and day of week; automatic emergency braking (AEB) noted as an advanced safety feature. TabPFN shows strong generalization; MambaAttention achieves superior performance in classifying severe injuries due to attention-based feature reweighting.

Conclusion: Deep tabular architectures are promising for EV crash severity prediction and can support data-driven safety interventions in EV crash contexts.

Abstract: This study presents a deep tabular learning framework for predicting crash
severity in electric vehicle (EV) collisions using real-world crash data from
Texas (2017-2023). After filtering for electric-only vehicles, 23,301
EV-involved crash records were analyzed. Feature importance techniques using
XGBoost and Random Forest identified intersection relation, first harmful
event, person age, crash speed limit, and day of week as the top predictors,
along with advanced safety features like automatic emergency braking. To
address class imbalance, Synthetic Minority Over-sampling Technique and Edited
Nearest Neighbors (SMOTEENN) resampling was applied. Three state-of-the-art
deep tabular models, TabPFN, MambaNet, and MambaAttention, were benchmarked for
severity prediction. While TabPFN demonstrated strong generalization,
MambaAttention achieved superior performance in classifying severe injury cases
due to its attention-based feature reweighting. The findings highlight the
potential of deep tabular architectures for improving crash severity prediction
and enabling data-driven safety interventions in EV crash contexts.

</details>


### [352] [Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting](https://arxiv.org/abs/2509.11452)
*Yining Lu,Zilong Wang,Shiyang Li,Xin Liu,Changlong Yu,Qingyu Yin,Zhan Shi,Zixuan Zhang,Meng Jiang*

Main category: cs.LG

TL;DR: Dynamic reward weighting for online multi-objective reinforcement learning (MO-RL) that adapts weights during training to better explore non-convex Pareto fronts; introduces hypervolume-guided adaptation and gradient-based weight optimization, improving Pareto dominance with fewer training steps than fixed-weight baselines.


<details>
  <summary>Details</summary>
Motivation: Fixed linear scalarization with static weights cannot capture non-convex Pareto fronts in online MO-RL, which is problematic for online preference alignment in large language models where parameter-to-objective mappings are highly non-linear and non-convex.

Method: Propose two dynamic weighting methods: (1) hypervolume-guided weight adaptation, which adjusts weights to maximize hypervolume progress; (2) gradient-based weight optimization, which updates weights via gradient signals to improve the Pareto front. These are integrated with online RL algorithms such as GRPO, REINFORCE, and RLOO, and evaluated across multiple mathematical reasoning datasets and model families.

Result: Empirical results show that dynamic weighting is compatible with common online RL algorithms and effectively explores Pareto fronts, yielding Pareto-dominant solutions with fewer training steps compared to fixed-weight linear scalarization baselines across diverse datasets and model families.

Conclusion: Dynamic reward weighting provides a versatile toolkit for online MO-RL alignment, enabling effective exploration of non-convex Pareto fronts and improved sample efficiency; broadly applicable to LLM alignment tasks and online RL settings.

Abstract: Prior works in multi-objective reinforcement learning typically use linear
reward scalarization with fixed weights, which provably fail to capture
non-convex Pareto fronts and thus yield suboptimal results. This limitation
becomes especially critical in online preference alignment for large language
models. Here, stochastic trajectories generated by parameterized policies
create highly non-linear and non-convex mappings from parameters to objectives
that no single static weighting scheme can find optimal trade-offs. We address
this limitation by introducing dynamic reward weighting, which adaptively
adjusts reward weights during the online reinforcement learning process. Unlike
existing approaches that rely on fixed-weight interpolation, our dynamic
weighting continuously balances and prioritizes objectives in training,
facilitating effective exploration of Pareto fronts in objective space. We
introduce two approaches of increasing sophistication and generalizability: (1)
hypervolume-guided weight adaptation and (2) gradient-based weight
optimization, offering a versatile toolkit for online multi-objective
alignment. Our extensive experiments demonstrate their compatibility with
commonly used online reinforcement learning algorithms (including GRPO,
REINFORCE, and RLOO), effectiveness across multiple mathematical reasoning
datasets, and applicability to different model families, consistently achieving
Pareto dominant solutions with fewer training steps than fixed-weight linear
scalarization baselines.

</details>


### [353] [Drug Repurposing Using Deep Embedded Clustering and Graph Neural Networks](https://arxiv.org/abs/2509.11493)
*Luke Delzer,Robert Kroleski,Ali K. AlShami,Jugal Kalita*

Main category: cs.LG

TL;DR: A dual ML pipeline combines unsupervised deep embedding with supervised GNN link prediction to identify novel drug–disease links from multi-omic data, enabling scalable drug repurposing.


<details>
  <summary>Details</summary>
Motivation: Drug repurposing is economically challenging; leveraging multi-omic data with advanced ML can uncover novel associations beyond known drug–disease similarities.

Method: Unsupervised autoencoder and clustering reduce multi-omic data to latent embeddings (9,022 drugs, 35 clusters; mean silhouette 0.855). A graph neural network performs link prediction, achieving 0.901 accuracy, 0.960 AUC, and 0.901 F1. A ranked list of 477 per-cluster links with >99th percentile probability is produced.

Result: Demonstrates strong predictive performance and yields a prioritized set of candidate drug–disease links derived from multi-omic data; showcases a feasible ML pipeline for drug repurposing.

Conclusion: The approach could reveal new drug–disease connections across diverse domains and contributes to methodological advances in ML-driven drug repurposing.

Abstract: Drug repurposing has historically been an economically infeasible process for
identifying novel uses for abandoned drugs. Modern machine learning has enabled
the identification of complex biochemical intricacies in candidate drugs;
however, many studies rely on simplified datasets with known drug-disease
similarities. We propose a machine learning pipeline that uses unsupervised
deep embedded clustering, combined with supervised graph neural network link
prediction to identify new drug-disease links from multi-omic data.
Unsupervised autoencoder and cluster training reduced the dimensionality of
omic data into a compressed latent embedding. A total of 9,022 unique drugs
were partitioned into 35 clusters with a mean silhouette score of 0.8550. Graph
neural networks achieved strong statistical performance, with a prediction
accuracy of 0.901, receiver operating characteristic area under the curve of
0.960, and F1-Score of 0.901. A ranked list comprised of 477 per-cluster link
probabilities exceeding 99 percent was generated. This study could provide new
drug-disease link prospects across unrelated disease domains, while advancing
the understanding of machine learning in drug repurposing studies.

</details>


### [354] [OASIS: A Deep Learning Framework for Universal Spectroscopic Analysis Driven by Novel Loss Functions](https://arxiv.org/abs/2509.11499)
*Chris Young,Juejing Liu,Marie L. Mortensen,Yifu Feng,Elizabeth Li,Zheming Wang,Xiaofeng Guo,Kevin M. Rosso,Xin Zhang*

Main category: cs.LG

TL;DR: OASIS is a technique-independent ML framework for automated spectral analysis (denoising, baseline correction, peak localization and parameter retrieval) trained on synthetic data, using ViPeR loss for accurate, compact models, validated on Raman, UV-vis, and fluorescence spectra; enables in situ, high-throughput, and online monitoring, with loss-function design as a resource-efficient development strategy.


<details>
  <summary>Details</summary>
Motivation: There is a need for automated, cross-technique spectral analysis that reduces human intervention and is not tied to a single spectroscopy modality; current approaches are often manual or technique-specific. A unified, efficient solution would support rapid denoising, baseline correction, and precise peak parameter extraction across diverse datasets.

Method: Develop a ML framework OASIS trained on a synthetic dataset capturing features across spectroscopy techniques; introduce ViPeR (vicinity peak response) as a task-specific loss function to improve peak localization; produce compact models capable of denoising, baseline correction, and peak parameter retrieval (location, intensity, FWHM). Validation with experimental data from Raman, UV-vis, and fluorescence spectra.

Result: OASIS delivers accurate peak localization and parameter retrieval with small models; demonstrates demonstration potential for in situ experiments, high-throughput optimization, and online monitoring; highlights loss-function optimization as a key resource-efficient strategy for ML model development.

Conclusion: Technique-independent spectral analysis via loss-optimized ML can generalize across spectroscopy modalities; ViPeR enables efficient, accurate peak localization; OASIS showcases viability of cross-technique automation and sets direction for resource-efficient ML in spectroscopy.

Abstract: The proliferation of spectroscopic data across various scientific and
engineering fields necessitates automated processing. We introduce OASIS
(Omni-purpose Analysis of Spectra via Intelligent Systems), a machine learning
(ML) framework for technique-independent, automated spectral analysis,
encompassing denoising, baseline correction, and comprehensive peak parameter
(location, intensity, FWHM) retrieval without human intervention. OASIS
achieves its versatility through models trained on a strategically designed
synthetic dataset incorporating features from numerous spectroscopy techniques.
Critically, the development of innovative, task-specific loss functions-such as
the vicinity peak response (ViPeR) for peak localization-enabled the creation
of compact yet highly accurate models from this dataset, validated with
experimental data from Raman, UV-vis, and fluorescence spectroscopy. OASIS
demonstrates significant potential for applications including in situ
experiments, high-throughput optimization, and online monitoring. This study
underscores the optimization of the loss function as a key resource-efficient
strategy to develop high-performance ML models.

</details>


### [355] [Know What You Don't Know: Selective Prediction for Early Exit DNNs](https://arxiv.org/abs/2509.11520)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: SPEED uses per-layer Deferral Classifiers to detect hard samples and defer to an expert, improving accuracy and latency for Early Exit DNNs; reports ~50% reduction in wrong predictions and ~2.05x speedup; code available.


<details>
  <summary>Details</summary>
Motivation: EE DNNs reduce latency but suffer from overconfidence causing premature exits and untrustworthy decisions. Selective Prediction (SP) can help by assessing sample hardness rather than relying solely on confidence.

Method: Attach Deferral Classifiers at each layer to assess if a sample is hard to predict; if hard, defer to the expert instead of exiting; if easy, allow exit. This pre-exit deferral aims to prevent hallucinations and save computation.

Result: EE aided with SP improves both accuracy and latency; reported 2.05x speedup versus final layer exit and 50% reduction in wrong predictions.

Conclusion: SPEED offers a practical way to improve trust and efficiency of EE DNNs by early detection of hard samples and deferring them; code is available at the stated GitHub repository.

Abstract: Inference latency and trustworthiness of Deep Neural Networks (DNNs) are the
bottlenecks in deploying them in critical applications like sensitive tasks.
Early Exit (EE) DNNs overcome the latency issues by allowing samples to exit
from intermediary layers if they attain `high' confidence scores on the
predicted class. However, the DNNs are known to exhibit overconfidence, which
can lead to many samples exiting early and render EE strategies untrustworthy.
We use Selective Prediction (SP) to overcome this issue by checking the
`hardness' of the samples rather than just relying on the confidence score
alone. We propose SPEED, a novel approach that uses Deferral Classifiers (DCs)
at each layer to check the hardness of samples before performing EEs.
Specifically, the DCs identify if a sample is hard to predict at an
intermediary layer, leading to hallucination, and defer it to an expert. Early
detection of hard samples for inference prevents the wastage of computational
resources and improves trust by deferring the hard samples to the expert. We
demonstrate that EE aided with SP improves both accuracy and latency. Our
method minimizes the risk of wrong prediction by $50\%$ with a speedup of
$2.05\times$ as compared to the final layer. The anonymized source code is
available at https://github.com/Div290/SPEED

</details>


### [356] [DARD: Dice Adversarial Robustness Distillation against Adversarial Attacks](https://arxiv.org/abs/2509.11525)
*Jing Zou,Shungeng Zhang,Meikang Qiu,Chong Li*

Main category: cs.LG

TL;DR: Robustness transfer from large teachers to small students via Dice Adversarial Robustness Distillation (DARD); uses Dice Projected Gradient Descent (DPGD) for stronger adversarial attacks; yields better robustness and accuracy than standard adversarial training for the same architecture.


<details>
  <summary>Details</summary>
Motivation: Adversarial Training reduces natural accuracy; robustness scales with model size; need scalable, effective transfer of robustness from large models to compact models.

Method: Introduce DARD, a knowledge-distillation framework tailored to preserve robustness from a large teacher to a smaller student; develop DPGD to generate adversarial examples optimized for training/attack; evaluate on standard benchmarks.

Result: DARD consistently outperforms vanilla adversarial training for the same architecture, achieving higher robustness while maintaining or improving standard accuracy.

Conclusion: Robustness can be effectively distilled from large models into compact ones; DARD and DPGD provide a practical path to robust, efficient models, validating distillation as a defense-enhancing paradigm.

Abstract: Deep learning models are vulnerable to adversarial examples, posing critical
security challenges in real-world applications. While Adversarial Training (AT
) is a widely adopted defense mechanism to enhance robustness, it often incurs
a trade-off by degrading performance on unperturbed, natural data. Recent
efforts have highlighted that larger models exhibit enhanced robustness over
their smaller counterparts. In this paper, we empirically demonstrate that such
robustness can be systematically distilled from large teacher models into
compact student models. To achieve better performance, we introduce Dice
Adversarial Robustness Distillation (DARD), a novel method designed to transfer
robustness through a tailored knowledge distillation paradigm. Additionally, we
propose Dice Projected Gradient Descent (DPGD), an adversarial example
generalization method optimized for effective attack. Our extensive experiments
demonstrate that the DARD approach consistently outperforms adversarially
trained networks with the same architecture, achieving superior robustness and
standard accuracy.

</details>


### [357] [UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning](https://arxiv.org/abs/2509.11543)
*Zhengxi Lu,Jiabo Ye,Fei Tang,Yongliang Shen,Haiyang Xu,Ziwei Zheng,Weiming Lu,Ming Yan,Fei Huang,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: Semi-online RL bridges offline training efficiency and online multi-turn reasoning for GUI agents by simulating online learning on offline trajectories, using a Patch Module to recover divergence, discounted returns, and weighted advantages, plus a new Semi-Online Performance (SOP) metric; achieves SOTA among 7B models on four benchmarks.


<details>
  <summary>Details</summary>
Motivation: Off-policy offline RL is stable but struggles with multi-step, long-horizon tasks due to lack of trajectory-level rewards; online RL provides such signals but incurs sparse rewards and deployment costs; a practical paradigm is needed to combine offline data efficiency with online credit assignment.

Method: During rollouts on offline trajectories, preserve the original model outputs in multi-turn dialogues; employ a Patch Module to adaptively recover divergence between rollout and expert trajectories; incorporate discounted future returns into rewards; optimize policy with a blend of weighted step-level and episode-level advantages; introduce Semi-Online Performance (SOP) as a proxy metric for online performance.

Result: Achieves state-of-the-art performance among 7B models across four dynamic benchmarks, with notable gains over the base model (e.g., +12.0% on AndroidWorld, +23.8% on AITW); demonstrates improved long-horizon reasoning while maintaining offline data efficiency.

Conclusion: Semi-online RL effectively bridges offline training efficiency and online multi-turn reasoning for GUI agents; SOP provides a practical evaluation proxy close to true online performance; code is released for reproducibility.

Abstract: Graphical User Interface (GUI) agents have demonstrated remarkable progress
in automating complex user interface interactions through reinforcement
learning. However, current approaches face a fundamental dilemma: offline RL
enables stable training on pre-collected trajectories, but struggles with
multi-step task execution for lack of trajectory-level reward signals; online
RL captures these signals through environment interaction, but suffers from
sparse rewards and prohibitive deployment costs. To address it, we present
Semi-online Reinforcement Learning, a novel paradigm that simulates online RL
on offline trajectories. During each rollout process, we preserve the original
model output within the multi-turn dialogue, where a Patch Module adaptively
recovers the divergence between rollout and expert trajectories. To capture
long-term training signals, Semi-online RL introduces discounted future returns
into the reward computation and optimizes the policy with weighted step-level
and episode-level advantages. We further introduce Semi-Online Performance
(SOP), a metric that aligns better with true online performance, serving as a
practical and effective proxy for real-world evaluation. Experiments show that
ours Semi-online RL achieves SOTA performance among 7B models across four
dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on
AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging
the gap between offline training efficiency and online multi-turn reasoning.
The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.

</details>


### [358] [Compressed Sensing: Mathematical Foundations, Implementation, and Advanced Optimization Techniques](https://arxiv.org/abs/2509.11550)
*Shane Stevenson,Maryam Sabagh*

Main category: cs.LG

TL;DR: A survey-style overview of compressed sensing focusing on theory, logic, pathologies, and real-world applications.


<details>
  <summary>Details</summary>
Motivation: Many real-world signals are sparse and can be efficiently represented, enabling reconstruction from few measurements; compressed sensing aims to exploit this sparsity for efficient sensing and recovery.

Method: Examines the mathematical formulation of compressed sensing, discusses its underlying logic and potential pathologies, and demonstrates application to real-world signals.

Result: The abstract does not report explicit results or findings.

Conclusion: The abstract presents an analytic/survey-oriented agenda rather than new empirical or theoretical results.

Abstract: Compressed sensing is a signal processing technique that allows for the
reconstruction of a signal from a small set of measurements. The key idea
behind compressed sensing is that many real-world signals are inherently
sparse, meaning that they can be efficiently represented in a different space
with only a few components compared to their original space representation. In
this paper we will explore the mathematical formulation behind compressed
sensing, its logic and pathologies, and apply compressed sensing to real world
signals.

</details>


### [359] [Dynamic Adaptive Parsing of Temporal and Cross-Variable Patterns for Network State Classification](https://arxiv.org/abs/2509.11601)
*Yuan Gao,Xuelong Wang,Zhenguo Dong,Yong Zhang*

Main category: cs.LG

TL;DR: DAPNet uses a Mixture-of-Experts to jointly model periodic traffic patterns, cross-variable dependencies, and hybrid temporal features for network state classification, achieving higher accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing methods separately capture either temporal periodicities or cross-variable dependencies, but struggle to balance both. A unified framework could improve accuracy and robustness in network state classification and address class-imbalance issues.

Method: A Mixture-of-Experts framework with three specialized networks for (1) periodic analysis, (2) dynamic cross-variable correlation modeling, and (3) hybrid temporal feature extraction. A learnable gating network assigns sample-specific weights to experts, producing a weighted fusion of outputs. A hybrid regularization loss stabilizes training and mitigates class imbalance. Evaluations on CICIDS2017/2018 and generalization tests across 10 UEA benchmarks.

Result: DAPNet achieves higher accuracy on the target network intrusion detection datasets CICIDS2017/2018 and demonstrates generalizability across ten public UEA benchmark datasets.

Conclusion: The architecture effectively balances temporal and dependency cues via gated expert fusion and robust regularization, making DAPNet a specialized, generalizable framework for network state classification.

Abstract: Effective network state classification is a primary task for ensuring network
security and optimizing performance. Existing deep learning models have shown
considerable progress in this area. Some methods excel at analyzing the complex
temporal periodicities found in traffic data, while graph-based approaches are
adept at modeling the dynamic dependencies between different variables.
However, a key trade-off remains, as these methods struggle to capture both
characteristics simultaneously. Models focused on temporal patterns often
overlook crucial variable dependencies, whereas those centered on dependencies
may fail to capture fine-grained temporal details. To address this trade-off,
we introduce DAPNet, a framework based on a Mixture-of-Experts architecture.
DAPNet integrates three specialized networks for periodic analysis, dynamic
cross-variable correlation modeling, and hybrid temporal feature extraction. A
learnable gating network dynamically assigns weights to experts based on the
input sample and computes a weighted fusion of their outputs. Furthermore, a
hybrid regularization loss function ensures stable training and addresses the
common issue of class imbalance. Extensive experiments on two large-scale
network intrusion detection datasets (CICIDS2017/2018) validate DAPNet's higher
accuracy for its target application. The generalizability of the architectural
design is evaluated across ten public UEA benchmark datasets, positioning
DAPNet as a specialized framework for network state classification.

</details>


### [360] [Topology Structure Optimization of Reservoirs Using GLMY Homology](https://arxiv.org/abs/2509.11612)
*Yu Chen,Shengwei Wang,Hongwei Lin*

Main category: cs.LG

TL;DR: A reservoir computer’s performance can be predicted and improved via one-dimensional persistent GLMY homology; authors optimize reservoir structure by adjusting minimal cycles and validate effects, showing performance depends on structure and data periodicity.


<details>
  <summary>Details</summary>
Motivation: Reservoir topology impacts performance but is hard to analyze; lack of suitable mathematical tools; propose GLMY persistent homology as a framework to connect topology to function and to guide design.

Method: Apply persistent GLMY homology to characterize reservoir topologies, identify that 1D GLMY homology groups correlate with performance, propose optimization by modifying minimal representative cycles of 1D GLMY groups, perform experiments to test.

Result: Found strong correlation between 1D GLMY homology and performance; cycle-based optimization improves results; dataset periodicity also influences performance.

Conclusion: GLMY persistent homology provides a principled tool to analyze and optimize reservoir topology; 1D homology is pivotal; dataset periodicity also plays a role; potential to guide design of reservoirs.

Abstract: Reservoir is an efficient network for time series processing. It is well
known that network structure is one of the determinants of its performance.
However, the topology structure of reservoirs, as well as their performance, is
hard to analyzed, due to the lack of suitable mathematical tools. In this
paper, we study the topology structure of reservoirs using persistent GLMY
homology theory, and develop a method to improve its performance. Specifically,
it is found that the reservoir performance is closely related to the
one-dimensional GLMY homology groups. Then, we develop a reservoir structure
optimization method by modifying the minimal representative cycles of
one-dimensional GLMY homology groups. Finally, by experiments, it is validated
that the performance of reservoirs is jointly influenced by the reservoir
structure and the periodicity of the dataset.

</details>


### [361] [Inducing Uncertainty for Test-Time Privacy](https://arxiv.org/abs/2509.11625)
*Muhammad H. Ashiq,Peter Triantafillou,Hung Yun Tseng,Grigoris G. Chrysos*

Main category: cs.LG

TL;DR: A weight-perturbation finetuning approach to enforce test-time privacy after unlearning by Pareto-optimizing privacy vs accuracy; with a certifiable (ε,δ) guarantee and a tight tradeoff bound; shows strong uncertainty gains with minimal accuracy loss on image benchmarks.


<details>
  <summary>Details</summary>
Motivation: Unlearning does not erase all information from a model; adversaries with full access can still extract or rely on confident predictions on data that was removed or obsolete (test-time privacy). Existing defenses fail under full-model access, risking user privacy. The work seeks to guarantee additional protection by actively increasing uncertainty on protected instances while preserving performance elsewhere.

Method: Core algorithm finetunes model parameters using a Pareto-optimal objective that explicitly balances test-time privacy against utility. It provides a certifiable approximation algorithm yielding (ε, δ) guarantees without relying on convexity. The authors also derive a tight, non-vacuous bound describing the privacy-utility tradeoff induced by their methods.

Result: Empirically, the method achieves more than 3× stronger uncertainty on protected instances than pretraining, while incurring less than 0.2% drop in accuracy across multiple image recognition benchmarks.

Conclusion: The proposed framework adds a principled layer of protection for end users against test-time privacy threats, going beyond traditional unlearning. It delivers certifiable guarantees and a clear privacy-utility tradeoff, making test-time privacy practically enforceable.

Abstract: Unlearning is the predominant method for removing the influence of data in
machine learning models. However, even after unlearning, models often continue
to produce the same predictions on the unlearned data with high confidence.
This persistent behavior can be exploited by adversaries using confident model
predictions on incorrect or obsolete data to harm users. We call this threat
model, which unlearning fails to protect against, *test-time privacy*. In
particular, an adversary with full model access can bypass any naive defenses
which ensure test-time privacy. To address this threat, we introduce an
algorithm which perturbs model weights to induce maximal uncertainty on
protected instances while preserving accuracy on the rest of the instances. Our
core algorithm is based on finetuning with a Pareto optimal objective that
explicitly balances test-time privacy against utility. We also provide a
certifiable approximation algorithm which achieves $(\varepsilon, \delta)$
guarantees without convexity assumptions. We then prove a tight, non-vacuous
bound that characterizes the privacy-utility tradeoff that our algorithms
incur. Empirically, our method obtains $>3\times$ stronger uncertainty than
pretraining with $<0.2\%$ drops in accuracy on various image recognition
benchmarks. Altogether, this framework provides a tool to guarantee additional
protection to end users.

</details>


### [362] [SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching](https://arxiv.org/abs/2509.11628)
*Jiacheng Liu,Chang Zou,Yuanhuiyi Lyu,Fei Ren,Shaobo Wang,Kaixin Li,Linfeng Zhang*

Main category: cs.LG

TL;DR: SpeCa introduces Speculative Sampling for diffusion models with a Forecast-then-verify framework, enabling real-time acceleration by predicting future intermediate features and verifying them with a parameter-free mechanism, plus sample-adaptive computation; achieves substantial speedups with modest quality loss and low verification overhead; code released.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from high computational demands and strict temporal dependencies that impede real-time applications. Inspired by speculative decoding in LLMs, there is a need for efficient acceleration without sacrificing quality.

Method: Speculative Sampling predicts intermediate features for subsequent timesteps based on fully computed reference timesteps. A parameter-free verification mechanism assesses the reliability of predictions to accept or reject them with negligible overhead. Additionally, sample-adaptive computation allocation adjusts resources according to generation complexity, reducing work for simpler samples while preserving heavy processing for complex ones.

Result: Experiments show 6.34x acceleration on FLUX with 5.5% quality degradation, 7.3x speedup on DiT with preserved fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. Verification overhead is 1.67%–3.5% of full inference costs, demonstrating effective acceleration with minimal quality loss.

Conclusion: SpeCa establishes a new paradigm for efficient diffusion-model inference by forecasting and verifying intermediate results, enabling aggressive acceleration while maintaining generation quality; code is released at GitHub.

Abstract: Diffusion models have revolutionized high-fidelity image and video synthesis,
yet their computational demands remain prohibitive for real-time applications.
These models face two fundamental challenges: strict temporal dependencies
preventing parallelization, and computationally intensive forward passes
required at each denoising step. Drawing inspiration from speculative decoding
in large language models, we present SpeCa, a novel 'Forecast-then-verify'
acceleration framework that effectively addresses both limitations. SpeCa's
core innovation lies in introducing Speculative Sampling to diffusion models,
predicting intermediate features for subsequent timesteps based on fully
computed reference timesteps. Our approach implements a parameter-free
verification mechanism that efficiently evaluates prediction reliability,
enabling real-time decisions to accept or reject each prediction while
incurring negligible computational overhead. Furthermore, SpeCa introduces
sample-adaptive computation allocation that dynamically modulates resources
based on generation complexity, allocating reduced computation for simpler
samples while preserving intensive processing for complex instances.
Experiments demonstrate 6.34x acceleration on FLUX with minimal quality
degradation (5.5% drop), 7.3x speedup on DiT while preserving generation
fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The
verification mechanism incurs minimal overhead (1.67%-3.5% of full inference
costs), establishing a new paradigm for efficient diffusion model inference
while maintaining generation quality even at aggressive acceleration ratios.
Our codes have been released in Github:
\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}

</details>


### [363] [Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check](https://arxiv.org/abs/2509.11629)
*Chentao Cao,Xiaojun Xu,Bo Han,Hang Li*

Main category: cs.LG

TL;DR: Proposes Answer-Then-Check safety alignment (ReSA) for LLMs to think first and assess safety before answering; introduces 80k-example Reasoned Safety Alignment dataset; achieves safety Pareto frontier with reduced over-refusal while preserving reasoning abilities; demonstrates safe completions and data-efficient training (500 examples).


<details>
  <summary>Details</summary>
Motivation: To defend against jailbreak and unsafe prompts by integrating safety evaluation into the reasoning process, rather than relying solely on post-hoc refusals.

Method: Fine-tune LLMs on ReSA (80k examples) guiding them to produce an initial thought-out answer, then critically check safety before final response; evaluate on safety benchmarks (over-refusal) and standard reasoning benchmarks (MMLU, MATH500, HumanEval); explore data efficiency showing 500-example sufficiency; enable safe completions.

Result: Outperforms baselines on safety with Pareto frontier; reduces over-refusal; preserves core reasoning performance on MMLU, MATH500, HumanEval; enables safe alternative responses for sensitive topics; demonstrates data efficiency (500 examples approximate full dataset).

Conclusion: Safety alignment can be achieved with smaller datasets using reasoning-based safety checks; the ReSA approach provides safer interactions without sacrificing general capabilities and enables safe completions.

Abstract: As large language models (LLMs) continue to advance in capabilities, ensuring
their safety against jailbreak attacks remains a critical challenge. In this
paper, we introduce a novel safety alignment approach called Answer-Then-Check,
which enhances LLM robustness against malicious prompts by applying thinking
ability to mitigate jailbreaking problems before producing a final answer to
the user. Our method enables models to directly answer the question in their
thought and then critically evaluate its safety before deciding whether to
provide it. To implement this approach, we construct the Reasoned Safety
Alignment (ReSA) dataset, comprising 80K examples that teach models to reason
through direct responses and then analyze their safety. Experimental results
demonstrate that our approach achieves the Pareto frontier with superior safety
capability while decreasing over-refusal rates on over-refusal benchmarks.
Notably, the model fine-tuned with ReSA maintains general reasoning
capabilities on benchmarks like MMLU, MATH500, and HumanEval. Besides, our
method equips models with the ability to perform safe completion. Unlike
post-hoc methods that can only reject harmful queries, our model can provide
helpful and safe alternative responses for sensitive topics (e.g., self-harm).
Furthermore, we discover that training on a small subset of just 500 examples
can achieve comparable performance to using the full dataset, suggesting that
safety alignment may require less data than previously assumed.

</details>


### [364] [Adaptive-GraphSketch: Real-Time Edge Anomaly Detection via Multi-Layer Tensor Sketching and Temporal Decay](https://arxiv.org/abs/2509.11633)
*Ocheme Anthony Ekle,William Eberle*

Main category: cs.LG

TL;DR: ADAPTIVE-GRAPHSKETCH presents a real-time, scalable anomaly detector for streaming graphs that combines temporal multi-tensor sketching with Count-Min Sketch (CMS-CU), Bayesian scoring, and EWMA-based adaptive thresholds. It achieves high throughput with bounded memory and shows statistically meaningful AUC gains over baselines on real intrusion datasets.


<details>
  <summary>Details</summary>
Motivation: There is a need for scalable, probabilistically interpretable, and adaptive anomaly detection in dynamic graphs and edge streams, where existing methods struggle with scalability, interpretability, and adapting to evolving traffic patterns.

Method: Proposes temporal multi-tensor sketching integrated with Count-Min Sketch using Conservative Update (CMS-CU) to track edge frequencies with bounded memory and reduced hash collisions; employs Bayesian inference for probabilistic anomaly scoring; uses Exponentially Weighted Moving Average (EWMA) for adaptive thresholding aligned with burst intensity; demonstrated in real-time streaming graphs.

Result: Experimental evaluation on four real-world intrusion datasets shows ADAPTIVE-GRAPHSKETCH outperforms state-of-the-art baselines (ANOEDGE-G/L, MIDAS-R, F-FADE) with up to 6.5% AUC gain on CIC-IDS2018 and up to 15.6% on CIC-DDoS2019; processes 20 million edges in under 3.4 seconds using 10 hash functions.

Conclusion: A practical and effective solution for fast, accurate anomaly detection in large-scale streaming graphs, combining memory-bounded sketching, probabilistic scoring, and adaptive thresholding.

Abstract: Anomaly detection in dynamic graphs is essential for identifying malicious
activities, fraud, and unexpected behaviors in real-world systems such as
cybersecurity and power grids. However, existing approaches struggle with
scalability, probabilistic interpretability, and adaptability to evolving
traffic patterns. In this paper, we propose ADAPTIVE-GRAPHSKETCH, a lightweight
and scalable framework for real-time anomaly detection in streaming edge data.
Our method integrates temporal multi-tensor sketching with Count-Min Sketch
using Conservative Update (CMS-CU) to compactly track edge frequency patterns
with bounded memory, while mitigating hash collision issues. We incorporate
Bayesian inference for probabilistic anomaly scoring and apply Exponentially
Weighted Moving Average (EWMA) for adaptive thresholding tuned to burst
intensity. Extensive experiments on four real-world intrusion detection
datasets demonstrate that ADAPTIVE-GRAPHSKETCH outperforms state-of-the-art
baselines such as ANOEDGE-G/L, MIDAS-R, and F-FADE, achieving up to 6.5% AUC
gain on CIC-IDS2018 and up to 15.6% on CIC-DDoS2019, while processing 20
million edges in under 3.4 seconds using only 10 hash functions. Our results
show that ADAPTIVE-GRAPHSKETCH is practical and effective for fast, accurate
anomaly detection in large-scale streaming graphs.
  Keywords: Anomaly Detection, Streaming, Real-time, Dynamic Graphs, Edge
Streams, Tensor Sketching

</details>


### [365] [Assessing On-the-Ground Disaster Impact Using Online Data Sources](https://arxiv.org/abs/2509.11634)
*Saketh Vishnubhatla,Ujun Jeong,Bohan Jiang,Paras Sheth,Zhen Tan,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: Online data sources can complement offline, ground-truth disaster impact assessments by providing real-time estimates and broader coverage; the study curates a county-level multi-source dataset for billion-dollar disasters and compares online with offline estimates.


<details>
  <summary>Details</summary>
Motivation: Traditional offline assessments are time-delayed and biased; online data streams (social media, news, imagery) offer real-time signals to better estimate disaster impact.

Method: Curate a multi-source online dataset for a set of billion-dollar disasters at the county level; analyze how online estimates compare with traditional offline-based impact estimates; assess complementary information across sources.

Result: Findings indicate online sources provide complementary information to offline estimates and can enhance disaster impact assessment by offering real-time data and broader coverage across counties.

Conclusion: Integrating diverse online data sources with offline assessments can improve disaster response planning; the dataset enables further research into cross-source estimation at administrative units.

Abstract: Assessing the impact of a disaster in terms of asset losses and human
casualties is essential for preparing effective response plans. Traditional
methods include offline assessments conducted on the ground, where volunteers
and first responders work together to collect the estimate of losses through
windshield surveys or on-ground inspection. However, these methods have a time
delay and are prone to different biases. Recently, various online data sources,
including social media, news reports, aerial imagery, and satellite data, have
been utilized to evaluate the impact of disasters. Online data sources provide
real-time data streams for estimating the offline impact. Limited research
exists on how different online sources help estimate disaster impact at a given
administrative unit. In our work, we curate a comprehensive dataset by
collecting data from multiple online sources for a few billion-dollar disasters
at the county level. We also analyze how online estimates compare with
traditional offline-based impact estimates for the disaster. Our findings
provide insight into how different sources can provide complementary
information to assess the disaster.

</details>


### [366] [Measuring Visual Understanding in Telecom domain: Performance Metrics for Image-to-UML conversion using VLMs](https://arxiv.org/abs/2509.11667)
*HG Ranjani,Rutuja Prabhudesai*

Main category: cs.LG

TL;DR: The paper defines evaluation metrics for converting 3GPP sequence diagram images into PlantUML using Vision-Language Models, comparing two VLMs against ground truth on a domain-specific dataset. It finds that basic elements (participants, messages) are captured well, while complex constructs (notes, boxes, groups) lag, indicating a need for more representative training data.


<details>
  <summary>Details</summary>
Motivation: There is a lack of fine-grained, component-level evaluation for image-to-puml conversions in domain-specific sequence diagrams, hindering reliable deployment of VLM-based pipelines in telecom documents.

Method: Create a domain-specific dataset of 3GPP sequence diagrams, generate PlantUML scripts with Claude Sonnet and GPT-4V, compare against manually authored ground truth, use version-control diff tools to quantify differences, and propose metrics for participants, messages, ordering, and grouping preservation.

Result: Quantitative metrics show good accuracy for nodes, edges, and messages but weak performance on complex constructs like notes, boxes, and groups; the metrics effectively reveal component-wise conversion errors across VLMs.

Conclusion: The proposed component-wise metrics quantify conversion quality and highlight gaps in VLM representations of complex diagram constructs, underscoring the need for training data that better covers these components and potentially model architectural adjustments for improved group/note handling.

Abstract: Telecom domain 3GPP documents are replete with images containing sequence
diagrams. Advances in Vision-Language Large Models (VLMs) have eased conversion
of such images to machine-readable PlantUML (puml) formats. However, there is a
gap in evaluation of such conversions - existing works do not compare puml
scripts for various components. In this work, we propose performance metrics to
measure the effectiveness of such conversions. A dataset of sequence diagrams
from 3GPP documents is chosen to be representative of domain-specific actual
scenarios. We compare puml outputs from two VLMs - Claude Sonnet and GPT-4V -
against manually created ground truth representations. We use version control
tools to capture differences and introduce standard performance metrics to
measure accuracies along various components: participant identification,
message flow accuracy, sequence ordering, and grouping construct preservation.
We demonstrate effectiveness of proposed metrics in quantifying conversion
errors across various components of puml scripts. The results show that nodes,
edges and messages are accurately captured. However, we observe that VLMs do
not necessarily perform well on complex structures such as notes, box, groups.
Our experiments and performance metrics indicates a need for better
representation of these components in training data for fine-tuned VLMs.

</details>


### [367] [An Interventional Approach to Real-Time Disaster Assessment via Causal Attribution](https://arxiv.org/abs/2509.11676)
*Saketh Vishnubhatla,Alimohammad Beigi,Rui Heng Foo,Umang Goel,Ujun Jeong,Bohan Jiang,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: Presents an interventional, counterfactual disaster severity tool that uses real-time data to enable causal attribution and mitigation planning, complementing predictive models; code is open-source.


<details>
  <summary>Details</summary>
Motivation: Traditional predictive disaster models cannot easily simulate interventions; there is a need for counterfactual analysis using current data to guide mitigation.

Method: Proposes an interventional framework leveraging real-time data sources (satellite imagery, news, social media) to estimate severity, attribute causal influence of factors, and provide actionable recourses; regional focus; open-source.

Result: Framework enabling causal attribution and mitigation guidance; demonstrates feasibility and provides public code.

Conclusion: Interventional tool complements traditional modelling, enabling better planning and response under real-time uncertainty; promotes transparency via open-source release.

Abstract: Traditional disaster analysis and modelling tools for assessing the severity
of a disaster are predictive in nature. Based on the past observational data,
these tools prescribe how the current input state (e.g., environmental
conditions, situation reports) results in a severity assessment. However, these
systems are not meant to be interventional in the causal sense, where the user
can modify the current input state to simulate counterfactual "what-if"
scenarios. In this work, we provide an alternative interventional tool that
complements traditional disaster modelling tools by leveraging real-time data
sources like satellite imagery, news, and social media. Our tool also helps
understand the causal attribution of different factors on the estimated
severity, over any given region of interest. In addition, we provide actionable
recourses that would enable easier mitigation planning. Our source code is
publicly available.

</details>


### [368] [Beyond Regularity: Modeling Chaotic Mobility Patterns for Next Location Prediction](https://arxiv.org/abs/2509.11713)
*Yuqian Wu,Yuhong Peng,Jiapeng Yu,Xiangyu Liu,Zeting Yan,Kang Lin,Weifeng Su,Bingqing Qu,Raymond Lee,Dingqi Yang*

Main category: cs.LG

TL;DR: CANOE: a Chaotic Neural Oscillatory Attention Network for next location prediction that addresses dynamic imbalance between periodic and chaotic mobility patterns and leverages arrival-time context; shows robust improvements over baselines on two real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with dynamic imbalance between periodic and chaotic mobility patterns and underutilize temporal contextual cues like arrival times, which persist even in chaotic patterns and offer stronger predictability than spatial forecasts.

Method: Propose CANOE: Chaotic Neural Oscillatory Attention mechanism to inject adaptive variability into attention; Tri-Pair Interaction Encoder; Cross Context Attentive Decoder; joint 'who-when-where' multimodal context fusion.

Result: Experiments on two real-world datasets show CANOE consistently outperforms state-of-the-art baselines; improvements of 3.17% to 13.11% over the best baselines; robust across mobility chaotic levels; ablation studies validate design; code available at the provided repository.

Conclusion: CANOE provides balanced representation of evolving mobility behavior by aligning attention with chaos dynamics and exploiting arrival-time cues, enabling robust next-location predictions; demonstrated effectiveness across different chaotic regimes.

Abstract: Next location prediction is a key task in human mobility analysis, crucial
for applications like smart city resource allocation and personalized
navigation services. However, existing methods face two significant challenges:
first, they fail to address the dynamic imbalance between periodic and chaotic
mobile patterns, leading to inadequate adaptation over sparse trajectories;
second, they underutilize contextual cues, such as temporal regularities in
arrival times, which persist even in chaotic patterns and offer stronger
predictability than spatial forecasts due to reduced search spaces. To tackle
these challenges, we propose \textbf{\method}, a
\underline{\textbf{C}}h\underline{\textbf{A}}otic \underline{\textbf{N}}eural
\underline{\textbf{O}}scillator n\underline{\textbf{E}}twork for next location
prediction, which introduces a biologically inspired Chaotic Neural Oscillatory
Attention mechanism to inject adaptive variability into traditional attention,
enabling balanced representation of evolving mobility behaviors, and employs a
Tri-Pair Interaction Encoder along with a Cross Context Attentive Decoder to
fuse multimodal ``who-when-where'' contexts in a joint framework for enhanced
prediction performance. Extensive experiments on two real-world datasets
demonstrate that CANOE consistently and significantly outperforms a sizeable
collection of state-of-the-art baselines, yielding 3.17\%-13.11\% improvement
over the best-performing baselines across different cases. In particular, CANOE
can make robust predictions over mobility trajectories of different mobility
chaotic levels. A series of ablation studies also supports our key design
choices. Our code is available at: https://github.com/yuqian2003/CANOE.

</details>


### [369] [DRAG: Data Reconstruction Attack using Guided Diffusion](https://arxiv.org/abs/2509.11724)
*Wa-Kin Lei,Jun-Cheng Chen,Shang-Tse Chen*

Main category: cs.LG

TL;DR: A diffusion-based data reconstruction attack targets vision foundation models deployed with split inference, using a pre-trained latent diffusion model (LDM) to iteratively reconstruct high-fidelity images from intermediate representations, revealing privacy risks and outperforming prior attacks; code is released.


<details>
  <summary>Details</summary>
Motivation: Split inference exposes intermediate representations from foundation models, creating privacy risks. Prior data reconstruction attacks focused on small CNNs; this work shows that attackers can exploit the rich priors of large LDMs to reconstruct data from deep IRs, underscoring the need for robust protections.

Method: An iterative reconstruction method guided by an LDM's learned image prior. The attacker leverages a pre-trained LDM on a large-scale dataset and performs guided diffusion conditioned on the target model's intermediate representations to synthesize data resembling the original inputs.

Result: Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art reconstruction methods in both qualitative and quantitative evaluations when reconstructing data from deep-layer IRs of vision foundation models.

Conclusion: The study highlights urgent privacy protection needs for large models in split inference and provides a baseline diffusion-based attack; encourages development of defense mechanisms and further research into safeguarding SI deployments.

Abstract: With the rise of large foundation models, split inference (SI) has emerged as
a popular computational paradigm for deploying models across lightweight edge
devices and cloud servers, addressing data privacy and computational cost
concerns. However, most existing data reconstruction attacks have focused on
smaller CNN classification models, leaving the privacy risks of foundation
models in SI settings largely unexplored. To address this gap, we propose a
novel data reconstruction attack based on guided diffusion, which leverages the
rich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on
a large-scale dataset. Our method performs iterative reconstruction on the
LDM's learned image prior, effectively generating high-fidelity images
resembling the original data from their intermediate representations (IR).
Extensive experiments demonstrate that our approach significantly outperforms
state-of-the-art methods, both qualitatively and quantitatively, in
reconstructing data from deep-layer IRs of the vision foundation model. The
results highlight the urgent need for more robust privacy protection mechanisms
for large models in SI scenarios. Code is available at:
https://github.com/ntuaislab/DRAG.

</details>


### [370] [Fast and Interpretable Machine Learning Modelling of Atmospheric Molecular Clusters](https://arxiv.org/abs/2509.11728)
*Lauri Seppäläinen,Jakub Kubečka,Jonas Elm,Kai Puolamäki*

Main category: cs.LG

TL;DR: k-NN with chemically informed metrics (kernel-induced and MLKR) using the FCHL19 descriptor rivals kernel ridge regression in accuracy while dramatically reducing compute time, scales to >250k entries, and even extrapolates to larger atmospheric clusters with ~1 kcal/mol error; provides built-in interpretability and uncertainty estimation for atmospheric chemistry applications.


<details>
  <summary>Details</summary>
Motivation: Uncertainty in climate modelling due to the formation and growth of atmospheric aerosol particles; a fast, accurate surrogate model for quantum-chemistry-level insights is needed to enable large-scale exploration of molecular cluster formation.

Method: Compare simple k-NN regression models (with chemically informed distance metrics) to kernel ridge regression using the FCHL19 descriptor across datasets (QM9 and large atmospheric cluster datasets for sulphuric acid–water and sulphuric acid–multibase-base systems). Explore kernel-induced and MLKR metric learning distances, assess accuracy, computational cost, extrapolation potential, and uncertainty estimates.

Result: k-NN matched or rivaled KRR in accuracy but with orders of magnitude faster computation;, achieved near-chemical accuracy on large QM9- and atmospheric-cluster datasets (over 250k entries); demonstrated extrapolation to larger unseen clusters with minimal error (often ~1 kcal/mol); interpretability and uncertainty estimation are built-in, and other descriptors can yield similar performance.

Conclusion: k-NN with chemically informed metrics is a powerful, scalable, and interpretable surrogate tool for atmospheric chemistry and potentially broader molecular discovery, enabling rapid exploration of cluster formation and growth without substantial loss in accuracy.

Abstract: Understanding how atmospheric molecular clusters form and grow is key to
resolving one of the biggest uncertainties in climate modelling: the formation
of new aerosol particles. While quantum chemistry offers accurate insights into
these early-stage clusters, its steep computational costs limit large-scale
exploration. In this work, we present a fast, interpretable, and surprisingly
powerful alternative: $k$-nearest neighbour ($k$-NN) regression model. By
leveraging chemically informed distance metrics, including a kernel-induced
metric and one learned via metric learning for kernel regression (MLKR), we
show that simple $k$-NN models can rival more complex kernel ridge regression
(KRR) models in accuracy, while reducing computational time by orders of
magnitude. We perform this comparison with the well-established
Faber-Christensen-Huang-Lilienfeld (FCHL19) molecular descriptor, but other
descriptors (e.g., FCHL18, MBDF, and CM) can be shown to have similar
performance. Applied to both simple organic molecules in the QM9 benchmark set
and large datasets of atmospheric molecular clusters (sulphuric acid-water and
sulphuric-multibase -base systems), our $k$-NN models achieve near-chemical
accuracy, scale seamlessly to datasets with over 250,000 entries, and even
appears to extrapolate to larger unseen clusters with minimal error (often
nearing 1 kcal/mol). With built-in interpretability and straightforward
uncertainty estimation, this work positions $k$-NN as a potent tool for
accelerating discovery in atmospheric chemistry and beyond.

</details>


### [371] [Data Fusion and Machine Learning for Ship Fuel Consumption Modelling -- A Case of Bulk Carrier Vessel](https://arxiv.org/abs/2509.11750)
*Abdella Mohamed,Xiangyu Hu,Christian Hendricks*

Main category: cs.LG

TL;DR: Fusing voyage data with public climate/sea datasets and applying ML can accurately predict ship fuel consumption, but generalizability across vessel classes needs validation.


<details>
  <summary>Details</summary>
Motivation: Growing regulatory pressure (IMO, EEOI) to reduce bunker fuel and CO2 requires accurate, actionable fuel-prediction models that account for speed, trim, climate, and sea state.

Method: Analysis of 296 bulk carrier voyage reports over one year (28 parameters) combined with CMEMS (19 parameters) and ECMWF (61 parameters); evaluation of data fusion on modeling accuracy and identification of influential factors.

Result: Strong potential for ML to predict fuel consumption when voyage data are fused with climate/sea data; however validation on similar vessel classes is necessary to confirm generalizability.

Conclusion: Public climate/sea data fusion with voyage data is a promising direction for operational fuel prediction and efficiency strategies, pending cross-vessel validation.

Abstract: There is an increasing push for operational measures to reduce ships' bunker
fuel consumption and carbon emissions, driven by the International Maritime
Organization (IMO) mandates. Key performance indicators such as the Energy
Efficiency Operational Indicator (EEOI) focus on fuel efficiency. Strategies
like trim optimization, virtual arrival, and green routing have emerged. The
theoretical basis for these approaches lies in accurate prediction of fuel
consumption as a function of sailing speed, displacement, trim, climate, and
sea state. This study utilized 296 voyage reports from a bulk carrier vessel
over one year (November 16, 2021 to November 21, 2022) and 28 parameters,
integrating hydrometeorological big data from the Copernicus Marine Environment
Monitoring Service (CMEMS) with 19 parameters and the European Centre for
Medium-Range Weather Forecasts (ECMWF) with 61 parameters. The objective was to
evaluate whether fusing external public data sources enhances modeling accuracy
and to highlight the most influential parameters affecting fuel consumption.
The results reveal a strong potential for machine learning techniques to
predict ship fuel consumption accurately by combining voyage reports with
climate and sea data. However, validation on similar classes of vessels remains
necessary to confirm generalizability.

</details>


### [372] [Stabilizing PINNs: A regularization scheme for PINN training to avoid unstable fixed points of dynamical systems](https://arxiv.org/abs/2509.11768)
*Milos Babic,Franz M. Rohrhofer,Bernhard C. Geiger*

Main category: cs.LG

TL;DR: Proposes a stability-aware regularization for PINNs to avoid unstable fixed points in dynamical systems, improving training success and physical accuracy.


<details>
  <summary>Details</summary>
Motivation: PINN loss landscapes can have local minima at fixed points of the underlying dynamical system, which can mislead optimization in forward problems and yield physically incorrect solutions.

Method: Introduce a regularization term grounded in stability theory that penalizes unstable fixed points within the PINN loss; evaluated on multiple dynamical systems, including Lotka–Volterra and van der Pol oscillator.

Result: The regularization reduces the prevalence of unstable fixed-point solutions, improves training success rates, and produces more physically consistent results.

Conclusion: Stability-based regularization enhances PINNs for dynamical-system initial-value problems by steering optimization away from unstable fixed points and improving solution quality.

Abstract: It was recently shown that the loss function used for training
physics-informed neural networks (PINNs) exhibits local minima at solutions
corresponding to fixed points of dynamical systems. In the forward setting,
where the PINN is trained to solve initial value problems, these local minima
can interfere with training and potentially leading to physically incorrect
solutions. Building on stability theory, this paper proposes a regularization
scheme that penalizes solutions corresponding to unstable fixed points.
Experimental results on four dynamical systems, including the Lotka-Volterra
model and the van der Pol oscillator, show that our scheme helps avoiding
physically incorrect solutions and substantially improves the training success
rate of PINNs.

</details>


### [373] [Multimodal Regression for Enzyme Turnover Rates Prediction](https://arxiv.org/abs/2509.11782)
*Bozhen Hu,Cheng Tan,Siyuan Li,Jiangbin Zheng,Sizhe Qiu,Jun Xia,Stan Z. Li*

Main category: cs.LG

TL;DR: A multimodal framework predicts enzyme turnover rate by integrating enzyme sequences, substrate structures, and environmental factors. It uses a pre-trained language model and CNN for proteins, a graph neural network for substrates, an attention mechanism for enzyme–substrate interaction, and Kolmogorov–Arnold Networks for interpretable symbolic regression; it outperforms baselines and enables insights for enzyme engineering.


<details>
  <summary>Details</summary>
Motivation: Enzyme turnover rate is a key kinetic parameter but measurements are costly and scarce; predicting turnover rates would accelerate enzyme discovery, engineering, and industrial biocatalysis.

Method: A multimodal model: protein sequences are encoded with a pre-trained language model and CNN; substrate molecules are encoded with a graph neural network; an attention mechanism fuses enzyme and substrate representations; symbolic regression via Kolmogorov–Arnold Networks yields explicit, interpretable formulas governing turnover rate.

Result: The framework demonstrates superior predictive performance compared to traditional methods and state-of-the-art deep learning baselines across experiments.

Conclusion: The proposed approach offers a robust, interpretable tool for studying enzyme kinetics with potential impact on enzyme engineering and industrial biotechnology.

Abstract: The enzyme turnover rate is a fundamental parameter in enzyme kinetics,
reflecting the catalytic efficiency of enzymes. However, enzyme turnover rates
remain scarce across most organisms due to the high cost and complexity of
experimental measurements. To address this gap, we propose a multimodal
framework for predicting the enzyme turnover rate by integrating enzyme
sequences, substrate structures, and environmental factors. Our model combines
a pre-trained language model and a convolutional neural network to extract
features from protein sequences, while a graph neural network captures
informative representations from substrate molecules. An attention mechanism is
incorporated to enhance interactions between enzyme and substrate
representations. Furthermore, we leverage symbolic regression via
Kolmogorov-Arnold Networks to explicitly learn mathematical formulas that
govern the enzyme turnover rate, enabling interpretable and accurate
predictions. Extensive experiments demonstrate that our framework outperforms
both traditional and state-of-the-art deep learning approaches. This work
provides a robust tool for studying enzyme kinetics and holds promise for
applications in enzyme engineering, biotechnology, and industrial biocatalysis.

</details>


### [374] [Watch Your Step: A Cost-Sensitive Framework for Accelerometer-Based Fall Detection in Real-World Streaming Scenarios](https://arxiv.org/abs/2509.11789)
*Timilehin B. Aderinola,Luca Palmerini,Ilaria D'Ascanio,Lorenzo Chiari,Jochen Klenk,Clemens Becker,Brian Caulfield,Georgiana Ifrim*

Main category: cs.LG

TL;DR: Real-time continuous fall detection with no prior knowledge of fall events using streaming IMU data; cost-sensitive threshold tuning achieves strong recall with good precision and low latency.


<details>
  <summary>Details</summary>
Motivation: Falls are a major health risk for older adults. Real-world fall detection is hampered by reliance on simulated data or assumptions about fall events, and by the need for efficient, robust evaluation in continuous monitoring. A real-time, low-latency framework with minimal prior knowledge would improve practical deployment.

Method: Using over 60 hours of FARSEEING real-world fall IMU data, the approach applies efficient classifiers to compute fall probabilities in streaming mode. It introduces cost-sensitive learning to adjust the decision threshold via a cost function that penalizes missed falls more than false alarms, achieving real-time inference (<5 ms per sample).

Result: Recall 1.00, Precision 0.84, F1 0.91 on FARSEEING; all falls detected with relatively low false alarms; average inference time below 5 ms per sample.

Conclusion: Cost-sensitive threshold tuning enhances robustness and competitiveness of accelerometer-based fall detection, enabling deployment in real-time wearable monitoring systems.

Abstract: Real-time fall detection is crucial for enabling timely interventions and
mitigating the severe health consequences of falls, particularly in older
adults. However, existing methods often rely on simulated data or assumptions
such as prior knowledge of fall events, limiting their real-world
applicability. Practical deployment also requires efficient computation and
robust evaluation metrics tailored to continuous monitoring. This paper
presents a real-time fall detection framework for continuous monitoring without
prior knowledge of fall events. Using over 60 hours of inertial measurement
unit (IMU) data from the FARSEEING real-world falls dataset, we employ recent
efficient classifiers to compute fall probabilities in streaming mode. To
enhance robustness, we introduce a cost-sensitive learning strategy that tunes
the decision threshold using a cost function reflecting the higher risk of
missed falls compared to false alarms. Unlike many methods that achieve high
recall only at the cost of precision, our framework achieved Recall of 1.00,
Precision of 0.84, and an F1 score of 0.91 on FARSEEING, detecting all falls
while keeping false alarms low, with average inference time below 5 ms per
sample. These results demonstrate that cost-sensitive threshold tuning enhances
the robustness of accelerometer-based fall detection. They also highlight the
potential of our computationally efficient framework for deployment in
real-time wearable sensor systems for continuous monitoring.

</details>


### [375] [Visualization and Analysis of the Loss Landscape in Graph Neural Networks](https://arxiv.org/abs/2509.11792)
*Samir Moustafa,Lorenz Kummer,Simon Fetzel,Nils M. Kriege,Wilfried N. Gansterer*

Main category: cs.LG

TL;DR: Introduces a learnable dimensionality-reduction method to visualize GNN loss landscapes, outperforming PCA and enabling memory-efficient reconstruction; analyzes how architecture, sparsification, and optimizer preconditioning affect optimization and performance.


<details>
  <summary>Details</summary>
Motivation: To understand how GNN parameter optimization, expressivity, and generalization relate, and to provide a scalable visualization tool that informs architecture and training choices.

Method: Proposes a learnable projection method for high-dimensional GNN parameters to a low-dimensional visualization space, and analyzes the impact of over-smoothing, jumping knowledge, quantization, sparsification, and preconditioner on the optimization process; compares against PCA and demonstrates memory-efficient, accurate reconstructions.

Result: The learnable projection surpasses PCA in accurately reconstructing high-dimensional parameters with lower memory usage; the findings show that architecture, sparsification, and optimizer preconditioning significantly shape the GNN optimization landscape, training dynamics, and final prediction performance.

Conclusion: These insights enable more efficient design of GNN architectures and training strategies and provide a practical tool for loss-landscape analysis.

Abstract: Graph Neural Networks (GNNs) are powerful models for graph-structured data,
with broad applications. However, the interplay between GNN parameter
optimization, expressivity, and generalization remains poorly understood. We
address this by introducing an efficient learnable dimensionality reduction
method for visualizing GNN loss landscapes, and by analyzing the effects of
over-smoothing, jumping knowledge, quantization, sparsification, and
preconditioner on GNN optimization. Our learnable projection method surpasses
the state-of-the-art PCA-based approach, enabling accurate reconstruction of
high-dimensional parameters with lower memory usage. We further show that
architecture, sparsification, and optimizer's preconditioning significantly
impact the GNN optimization landscape and their training process and final
prediction performance. These insights contribute to developing more efficient
designs of GNN architectures and training strategies.

</details>


### [376] [Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning](https://arxiv.org/abs/2509.11816)
*Filip Sondej,Yushi Yang*

Main category: cs.LG

TL;DR: A selective unlearning method uses PCA on activations and gradients to collapse subspaces tied to dangerous facts, enabling targeted unlearning with minimal impact on general performance and high efficiency.


<details>
  <summary>Details</summary>
Motivation: Current unlearning approaches fail to remove dangerous knowledge without harming model generality. There is a need for targeted, robust unlearning that preserves overall competence while erasing specific facts.

Method: Identify subspaces containing common representations by performing PCA on layer activations and module output gradients. Collapse these subspaces before computing unlearning updates to avoid touching general representations, and apply updates to remove only the desired facts. Evaluated by unlearning facts from WMDP on Llama-3.1-8B.

Result: Post-attack accuracy on biohazardous facts drops 80x more than the best baseline (Circuit Breakers); cyberhazardous facts drop 30x more. General performance degrades far less (0.1% WikiText loss increase). Compute cost is ultra-fast (under 3 GPU-seconds per fact).

Conclusion: A highly selective unlearning approach can robustly erase dangerous knowledge while preserving broad capabilities and achieving very low compute overhead.

Abstract: Current unlearning techniques and safety training consistently fail to remove
dangerous knowledge from language models. We analyze the root causes and
propose a highly selective technique which unlearns robustly and without
disrupting general performance.
  We perform PCA on activations and module output gradients to identify
subspaces containing common representations, and collapse them before
calculating unlearning updates. This way we avoid unlearning general
representations, and only target those specific to the unlearned facts.
  When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack
accuracy 80x more than our best baseline (Circuit Breakers) on biohazardous
facts and 30x more on cyberhazardous facts. Despite this, we disrupt general
performance 30x less (only 0.1% WikiText loss increase), while requiring less
than 3 GPU-seconds per fact.

</details>


### [377] [FedDAF: Federated Domain Adaptation Using Model Functional Distance](https://arxiv.org/abs/2509.11819)
*Mrinmay Sen,Ankita Das,Sidhant Nair,C Krishna Mohan*

Main category: cs.LG

TL;DR: FedDAF tackles federated domain adaptation under domain shift and data scarcity by similarity-based aggregation of the global source model with the target model using mean gradient fields on target data; it outperforms FL, PFL, and FDA on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: FDA often faces two challenges: domain shifts between source and target data and limited labeled data at the target. Existing methods either assume abundant target data or neglect jointly addressing both challenges, and they may not prioritize information sharing toward the target objective.

Method: 1) The server collects local source models and aggregates them by simple averaging to form a global source model. 2) On target data, compute mean gradient fields for the global source model and the target model, measure the angle between these gradient fields, and apply Gompertz normalization to obtain a similarity score. 3) Use this similarity to perform similarity-based aggregation between the global source model and the target model, effectively aligning to the target objective even with limited target data.

Result: Empirical results on real-world datasets show FedDAF achieves higher test accuracy than existing FL, personalized FL (PFL), and FDA methods.

Conclusion: FedDAF provides a target-objective–driven FDA method that handles both domain shifts and target-data scarcity by using gradient-field similarity to weight source-target aggregation, yielding superior performance.

Abstract: Federated Domain Adaptation (FDA) is a federated learning (FL) approach that
improves model performance at the target client by collaborating with source
clients while preserving data privacy. FDA faces two primary challenges: domain
shifts between source and target data and limited labeled data at the target.
Most existing FDA methods focus on domain shifts, assuming ample target data,
yet often neglect the combined challenges of both domain shifts and data
scarcity. Moreover, approaches that address both challenges fail to prioritize
sharing relevant information from source clients according to the target's
objective. In this paper, we propose FedDAF, a novel approach addressing both
challenges in FDA. FedDAF uses similarity-based aggregation of the global
source model and target model by calculating model functional distance from
their mean gradient fields computed on target data. This enables effective
model aggregation based on the target objective, constructed using target data,
even with limited data. While computing model functional distance between these
two models, FedDAF computes the angle between their mean gradient fields and
then normalizes with the Gompertz function. To construct the global source
model, all the local source models are aggregated using simple average in the
server. Experiments on real-world datasets demonstrate FedDAF's superiority
over existing FL, PFL, and FDA methods in terms of achieving better test
accuracy.

</details>


### [378] [Transparent and Fair Profiling in Employment Services: Evidence from Switzerland](https://arxiv.org/abs/2509.11847)
*Tim Räz*

Main category: cs.LG

TL;DR: Interpretable models, specifically explainable boosting machines (EBMs), nearly match the predictive performance of the best black-box models for long-term unemployment risk, while offering improved transparency and fairness; sparsity, feature smoothing, and fairness mitigation can boost transparency with minimal performance loss, supporting interpretable profiling as a trustworthy alternative.


<details>
  <summary>Details</summary>
Motivation: Address the tension between predictive accuracy and transparency/fairness in LTU profiling tools by evaluating interpretable models against traditional statistical and black-box models using Swiss administrative data.

Method: Compare traditional statistical, interpretable, and black-box models across predictive performance, interpretability, and fairness. Assess explainable boosting machines (EBMs) and investigate enhancements via model sparsity, feature smoothing, and fairness mitigation.

Result: EBMs perform nearly as well as the top black-box models. Techniques like sparsity, smoothing, and fairness mitigation can improve interpretability and fairness with only minor decreases in predictive performance.

Conclusion: Interpretable profiling can provide an accountable and trustworthy alternative to black-box models without sacrificing substantial predictive performance, suggesting practical adoption in public employment services.

Abstract: Long-term unemployment (LTU) is a challenge for both jobseekers and public
employment services. Statistical profiling tools are increasingly used to
predict LTU risk. Some profiling tools are opaque, black-box machine learning
models, which raise issues of transparency and fairness. This paper
investigates whether interpretable models could serve as an alternative, using
administrative data from Switzerland. Traditional statistical, interpretable,
and black-box models are compared in terms of predictive performance,
interpretability, and fairness. It is shown that explainable boosting machines,
a recent interpretable model, perform nearly as well as the best black-box
models. It is also shown how model sparsity, feature smoothing, and fairness
mitigation can enhance transparency and fairness with only minor losses in
performance. These findings suggest that interpretable profiling provides an
accountable and trustworthy alternative to black-box models without
compromising performance.

</details>


### [379] [TabStruct: Measuring Structural Fidelity of Tabular Data](https://arxiv.org/abs/2509.11950)
*Xiangjian Jiang,Nikola Simidjievski,Mateja Jamnik*

Main category: cs.LG

TL;DR: Proposes global utility and TabStruct benchmark to jointly evaluate tabular generators on structural fidelity and conventional metrics; demonstrates broad applicability across 29 datasets and 13 generators; code released.


<details>
  <summary>Details</summary>
Motivation: Evaluating synthetic tabular data is hard due to heterogeneous causal structures and lack of ground-truth structures; existing benchmarks miss their interplay; need a holistic, scalable evaluation framework.

Method: Introduce global utility metric that assesses structural fidelity without ground-truth causal structures; build TabStruct benchmark with 29 datasets, 13 generators from nine categories; provide evaluation pipelines and results; publish dataset and code.

Result: Global utility acts as a task-independent, domain-agnostic lens for evaluating tabular generators; TabStruct enables large-scale quantitative analysis across diverse models and datasets.

Conclusion: A holistic evaluation framework for tabular generators is established, enabling assessment of both structural fidelity and conventional performance; the TabStruct benchmark and global utility facilitate robust benchmarking and reproducibility.

Abstract: Evaluating tabular generators remains a challenging problem, as the unique
causal structural prior of heterogeneous tabular data does not lend itself to
intuitive human inspection. Recent work has introduced structural fidelity as a
tabular-specific evaluation dimension to assess whether synthetic data complies
with the causal structures of real data. However, existing benchmarks often
neglect the interplay between structural fidelity and conventional evaluation
dimensions, thus failing to provide a holistic understanding of model
performance. Moreover, they are typically limited to toy datasets, as
quantifying existing structural fidelity metrics requires access to
ground-truth causal structures, which are rarely available for real-world
datasets. In this paper, we propose a novel evaluation framework that jointly
considers structural fidelity and conventional evaluation dimensions. We
introduce a new evaluation metric, $\textbf{global utility}$, which enables the
assessment of structural fidelity even in the absence of ground-truth causal
structures. In addition, we present $\textbf{TabStruct}$, a comprehensive
evaluation benchmark offering large-scale quantitative analysis on 13 tabular
generators from nine distinct categories, across 29 datasets. Our results
demonstrate that global utility provides a task-independent, domain-agnostic
lens for tabular generator performance. We release the TabStruct benchmark
suite, including all datasets, evaluation pipelines, and raw results. Code is
available at https://github.com/SilenceX12138/TabStruct.

</details>


### [380] [Deep operator network for surrogate modeling of poroelasticity with random permeability fields](https://arxiv.org/abs/2509.11966)
*Sangjoon Park,Yeonjong Shin,Jinhyun Choo*

Main category: cs.LG

TL;DR: A DeepONet-based surrogate for poroelasticity with random permeability, using nondimensionalization and KL-based input reduction with a two-step training scheme, delivering fast, accurate predictions for soil consolidation and groundwater-induced subsidence.


<details>
  <summary>Details</summary>
Motivation: Simulations with random permeability fields are expensive, hindering probabilistic analysis, uncertainty quantification, and inverse problems; scalable surrogates for forward solves in poroelasticity are needed for efficiency.

Method: Develop a DeepONet that maps permeability fields (represented via Karhunen–Loève expansion) to transient poroelastic responses. Enhance stability with nondimensionalization, and employ a two-step training procedure that decouples optimization of branch and trunk networks.

Result: Substantial speedup in inference with high predictive accuracy across a wide range of permeability statistics, validated on two benchmark problems (soil consolidation and groundwater-extraction–induced subsidence).

Conclusion: The framework provides a scalable, efficient surrogate modeling approach for poroelastic systems with random permeability fields, enabling rapid probabilistic analyses and inverse problems.

Abstract: Poroelasticity -- coupled fluid flow and elastic deformation in porous media
-- often involves spatially variable permeability, especially in subsurface
systems. In such cases, simulations with random permeability fields are widely
used for probabilistic analysis, uncertainty quantification, and inverse
problems. These simulations require repeated forward solves that are often
prohibitively expensive, motivating the development of efficient surrogate
models. However, efficient surrogate modeling techniques for poroelasticity
with random permeability fields remain scarce. In this study, we propose a
surrogate modeling framework based on the deep operator network (DeepONet), a
neural architecture designed to learn mappings between infinite-dimensional
function spaces. The proposed surrogate model approximates the solution
operator that maps random permeability fields to transient poroelastic
responses. To enhance predictive accuracy and stability, we integrate three
strategies: nondimensionalization of the governing equations, input
dimensionality reduction via Karhunen--Lo\'eve expansion, and a two-step
training procedure that decouples the optimization of branch and trunk
networks. The methodology is evaluated on two benchmark problems in
poroelasticity: soil consolidation and ground subsidence induced by groundwater
extraction. In both cases, the DeepONet achieves substantial speedup in
inference while maintaining high predictive accuracy across a wide range of
permeability statistics. These results highlight the potential of the proposed
approach as a scalable and efficient surrogate modeling technique for
poroelastic systems with random permeability fields.

</details>


### [381] [MillStone: How Open-Minded Are LLMs?](https://arxiv.org/abs/2509.11967)
*Harold Triedman,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: MillStone benchmarks how external arguments affect LLM stances on controversial issues; finds most LLMs are open to opposing arguments but can be swayed by authoritative sources; raises concerns about manipulation in LLM-enabled search/information systems.


<details>
  <summary>Details</summary>
Motivation: As LLMs rely on external documents and sources for information, it is essential to quantify how those sources shape the models' stances and to assess the robustness of LLMs to argument influence, especially on controversial topics.

Method: Apply the MillStone benchmark to nine leading LLMs to measure open-mindedness to opposing arguments, cross-model agreement, persuasive power of different arguments, and whether persuasiveness varies by model. Analyze how source quality affects stance shifts.

Result: LLMs tend to be open-minded on most issues; authoritative sources can sway stances; variability exists across models in persuasiveness and agreement; some arguments are generally persuasive, but results depend on models and topics.

Conclusion: Source selection critically shapes LLM outputs; the information-retrieval systems using LLMs are vulnerable to manipulation via source quality and framing; requires safeguards like source provenance, debiasing, and multi-source cross-validation.

Abstract: Large language models equipped with Web search, information retrieval tools,
and other agentic capabilities are beginning to supplant traditional search
engines. As users start to rely on LLMs for information on many topics,
including controversial and debatable issues, it is important to understand how
the stances and opinions expressed in LLM outputs are influenced by the
documents they use as their information sources.
  In this paper, we present MillStone, the first benchmark that aims to
systematically measure the effect of external arguments on the stances that
LLMs take on controversial issues (not all of them political). We apply
MillStone to nine leading LLMs and measure how ``open-minded'' they are to
arguments supporting opposite sides of these issues, whether different LLMs
agree with each other, which arguments LLMs find most persuasive, and whether
these arguments are the same for different LLMs.
  In general, we find that LLMs are open-minded on most issues. An
authoritative source of information can easily sway an LLM's stance,
highlighting the importance of source selection and the risk that LLM-based
information retrieval and search systems can be manipulated.

</details>


### [382] [Examining the Relationship between Scientific Publishing Activity and Hype-Driven Financial Bubbles: A Comparison of the Dot-Com and AI Eras](https://arxiv.org/abs/2509.11982)
*Aksheytha Chelikavada,Casey C. Bennett*

Main category: cs.LG

TL;DR: Dot-com bubble patterns in yearly publication-citation networks do not reliably forecast AI-era market bubbles; some AI-era scientists exhibit dot-com-like patterns, but overall evidence is inconclusive, suggesting either a new AI bubble or no bubble.


<details>
  <summary>Details</summary>
Motivation: To assess whether signals from scientific publishing/citation data can forecast financial bubbles, and whether such signals generalize from the dot-com era to the AI era.

Method: Use temporal SNA (social network analysis) to detect relationships between publication citation networks and financial market data across two eras (1994-2001 dot-com; 2017-2024 AI). Then apply multiple analysis techniques (LSTM, KNN, ARX/GARCH) to evaluate predictive signals.

Result: Dot-com era patterns did not definitively predict AI bubble; yearly citation networks reflect shifts in publishing behavior; a subset of AI-era scientists mirrored dot-com patterns.

Conclusion: Patterns from the dot-com era do not translate well to predicting AI market dynamics; either AI bubble is of unprecedented form or there is no bubble; new indicators are needed for AI-era forecasting.

Abstract: Financial bubbles often arrive without much warning, but create long-lasting
economic effects. For example, during the dot-com bubble, innovative
technologies created market disruptions through excitement for a promised
bright future. Such technologies originated from research where scientists had
developed them for years prior to their entry into the markets. That raises a
question on the possibility of analyzing scientific publishing data (e.g.
citation networks) leading up to a bubble for signals that may forecast the
rise and fall of similar future bubbles. To that end, we utilized temporal SNAs
to detect possible relationships between the publication citation networks of
scientists and financial market data during two modern eras of rapidly shifting
technology: 1) dot-com era from 1994 to 2001 and 2) AI era from 2017 to 2024.
Results showed that the patterns from the dot-com era (which did end in a
bubble) did not definitively predict the rise and fall of an AI bubble. While
yearly citation networks reflected possible changes in publishing behavior of
scientists between the two eras, there was a subset of AI era scientists whose
publication influence patterns mirrored those during the dot-com era. Upon
further analysis using multiple analysis techniques (LSTM, KNN, AR X/GARCH),
the data seems to suggest two possibilities for the AI era: unprecedented form
of financial bubble unseen or that no bubble exists. In conclusion, our
findings imply that the patterns present in the dot-com era do not effectively
translate in such a manner to apply them to the AI market.

</details>


### [383] [Low-rank Orthogonalization for Large-scale Matrix Optimization with Applications to Foundation Model Training](https://arxiv.org/abs/2509.11983)
*Chuan He,Zhanwang Deng,Zhaosong Lu*

Main category: cs.LG

TL;DR: Low-rank orthogonalization and low-rank variants of Muon improve foundation-model pretraining by exploiting the gradients' low-rank structure; theory provides iteration complexity under heavy-tailed noise; empirical gains observed on GPT-2 and LLaMA pretraining compared to vanilla Muon.


<details>
  <summary>Details</summary>
Motivation: Neural network training is a large-scale matrix optimization problem. The matrix structure of NN parameters has been underexplored. Muon exploits such structure for foundation-model training; this work argues that the gradients during training are low-rank and that explicit low-rank orthogonalization can lead to better optimization and efficiency.

Method: Introduce low-rank orthogonalization to enforce or exploit low-rank gradient structure. Develop low-rank matrix-signed gradient descent and a low-rank Muon variant. Provide theoretical iteration-complexity results for finding approximate stationary solutions (and stochastic stationary solutions under heavy-tailed noise).

Result: Numerical experiments show that low-rank orthogonalization yields superior performance. The low-rank Muon surpasses carefully tuned vanilla Muon in GPT-2 and LLaMA pretraining.

Conclusion: Exploiting the low-rank nature of gradients via orthogonalization and low-rank optimization yields both empirical gains and theoretical guarantees, indicating a promising direction for efficient large-scale NN training and foundation-model pretraining.

Abstract: Neural network (NN) training is inherently a large-scale matrix optimization
problem, yet the matrix structure of NN parameters has long been overlooked.
Recently, the optimizer Muon \cite{jordanmuon}, which explicitly exploits this
structure, has gained significant attention for its strong performance in
foundation model training. A key component contributing to Muon's success is
matrix orthogonalization. In this paper, we propose {\it low-rank
orthogonalization}, which explicitly leverages the low-rank nature of gradients
during NN training. Building on this, we propose low-rank matrix-signed
gradient descent and a low-rank variant of Muon. Our numerical experiments
demonstrate the superior performance of low-rank orthogonalization, with the
low-rank Muon achieving promising results in GPT-2 and LLaMA pretraining --
surpassing the performance of the carefully tuned vanilla Muon. Theoretically,
we establish the iteration complexity of the low-rank matrix-signed gradient
descent for finding an approximate stationary solution, as well as that of
low-rank Muon for finding an approximate stochastic stationary solution under
heavy-tailed noise.

</details>


### [384] [Learning from Uncertain Similarity and Unlabeled Data](https://arxiv.org/abs/2509.11984)
*Meng Wei,Zhongnian Li,Peng Ying,Xinzheng Xu*

Main category: cs.LG

TL;DR: USimUL introduces uncertainty to similarity pairs and unlabeled data to reduce label leakage in similarity-based weak supervision, providing an unbiased risk estimator with theoretical convergence guarantees and strong empirical performance.


<details>
  <summary>Details</summary>
Motivation: Existing similarity-based weakly supervised learning often leaks sensitive label information through precise similarity annotations. There is a need for privacy-preserving methods that still leverage pairwise similarities and unlabeled data to maintain performance.

Method: Propose Uncertain Similarity and Unlabeled Learning (USimUL) that embeds an uncertainty component into each similarity pair. Develop an unbiased risk estimator that learns from uncertain similarities and unlabeled data, and provide theoretical proof of statistically optimal parametric convergence rates. Validate via extensive experiments on benchmark and real-world datasets.

Result: The method achieves superior classification performance compared to conventional similarity-based approaches on both benchmark and real-world datasets.

Conclusion: USimUL offers a privacy-aware, statistically sound framework for weakly supervised learning from similarity data, combining uncertainty-based similarity with unlabeled learning, backed by convergence guarantees and strong empirical results.

Abstract: Existing similarity-based weakly supervised learning approaches often rely on
precise similarity annotations between data pairs, which may inadvertently
expose sensitive label information and raise privacy risks. To mitigate this
issue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel
framework where each similarity pair is embedded with an uncertainty component
to reduce label leakage. In this paper, we propose an unbiased risk estimator
that learns from uncertain similarity and unlabeled data. Additionally, we
theoretically prove that the estimator achieves statistically optimal
parametric convergence rates. Extensive experiments on both benchmark and
real-world datasets show that our method achieves superior classification
performance compared to conventional similarity-based approaches.

</details>


### [385] [Generalizing Behavior via Inverse Reinforcement Learning with Closed-Form Reward Centroids](https://arxiv.org/abs/2509.12010)
*Filippo Lazzati,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: Proposes selecting an average policy among IRL-feasible policies by planning with the reward centroid of a bounded subset of rewards; provides a closed-form centroid and an offline algorithm to estimate it from demonstrations; demonstrates with simulations.


<details>
  <summary>Details</summary>
Motivation: IRL is ill-posed: many reward functions can explain the same behavior, leading to different policies in new settings. A principled criterion is needed to select a policy that generalizes well.

Method: Define a bounded subset of the feasible reward set. Prove that the 'average' policy corresponds to planning with the reward centroid of this subset, and derive a closed-form expression for the centroid. Develop an efficient offline algorithm to estimate the centroid using only expert demonstrations. Validate with numerical simulations showing how the expert behavior relates to the method's output.

Result: The policy obtained by planning with the centroid reward matches the intended average behavior across the subset of rewards. The centroid has a closed-form expression, enabling efficient computation. The offline estimation algorithm uses demonstrations to approximate the centroid, and simulations illustrate the method's effectiveness and its relation to the expert behavior.

Conclusion: The paper provides a principled, computationally efficient approach to generalize expert behavior under IRL ambiguity by reducing the problem to centroid-based planning within a bounded feasible set, with an offline method to estimate the centroid from demonstrations.

Abstract: We study the problem of generalizing an expert agent's behavior, provided
through demonstrations, to new environments and/or additional constraints.
Inverse Reinforcement Learning (IRL) offers a promising solution by seeking to
recover the expert's underlying reward function, which, if used for planning in
the new settings, would reproduce the desired behavior. However, IRL is
inherently ill-posed: multiple reward functions, forming the so-called feasible
set, can explain the same observed behavior. Since these rewards may induce
different policies in the new setting, in the absence of additional
information, a decision criterion is needed to select which policy to deploy.
In this paper, we propose a novel, principled criterion that selects the
"average" policy among those induced by the rewards in a certain bounded subset
of the feasible set. Remarkably, we show that this policy can be obtained by
planning with the reward centroid of that subset, for which we derive a
closed-form expression. We then present a provably efficient algorithm for
estimating this centroid using an offline dataset of expert demonstrations
only. Finally, we conduct numerical simulations that illustrate the
relationship between the expert's behavior and the behavior produced by our
method.

</details>


### [386] [AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models](https://arxiv.org/abs/2509.12019)
*Sangjun Lee,Seung-taek Woo,Jungyu Jin,Changhun Lee,Eunhyeok Park*

Main category: cs.LG

TL;DR: AMQ is a framework for automated mixed-precision weight-only quantization of LLMs that uses pruning, a quantization proxy, a quality predictor, and iterative search to efficiently explore bit-width configurations under memory constraints to reach Pareto-optimal models.


<details>
  <summary>Details</summary>
Motivation: To enable broader deployment of large language models under strict memory constraints, there is a need to identify the best-performing model within an enormous search space (~10^100 configurations). Conventional black-box optimization is infeasible for this combinatorial problem.

Method: Four innovations: (1) search space pruning using prior knowledge to exclude unpromising configurations; (2) quantization proxy to bypass costly format conversions during search; (3) quality predictor to minimize evaluation overhead; (4) iterative search-and-update strategy for fast and stable convergence. Integrates these components to efficiently explore the quality-efficiency landscape and reach the Pareto frontier, yielding compact yet high-performing LLMs.

Result: Claims efficient exploration of the quality-efficiency landscape, achieving Pareto-optimal (or near Pareto) frontiers with LLMs that are both compact and high-performing; includes open-source code at https://github.com/dlwns147/amq.

Conclusion: AMQ provides a practical, scalable approach to memory-aware LLM quantization by combining pruning, proxies, predictive modeling, and iterative search, enabling broader deployment of efficient models.

Abstract: To enable broader deployment of Large Language Models (LLMs), it is essential
to identify the best-performing model under strict memory constraints. We
present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework
that assigns layer-wise quantization bit-widths to optimally balance model
quality and memory usage. However, the combinatorial search space, with over
10^{100} possible configurations, makes conventional black-box optimization
infeasible. AMQ overcomes this challenge through four key innovations:(1)
search space pruning using prior knowledge to exclude unpromising
configurations, (2) quantization proxy to bypass costly format conversions
during search, (3) quality predictor to minimize evaluation overhead, and (4)
iterative search-and-update strategy for fast and stable convergence. By
integrating these components, AMQ efficiently explores the quality-efficiency
landscape, reaching the Pareto frontier and yielding LLMs that are both compact
and high-performing. Our code is available at https://github.com/dlwns147/amq.

</details>


### [387] [Learning non-Markovian Dynamical Systems with Signature-based Encoders](https://arxiv.org/abs/2509.12022)
*Eliott Pradeleix,Rémy Hosseinkhan-Boucher,Alena Shilova,Onofrio Semeraro,Lionel Mathelin*

Main category: cs.LG

TL;DR: The paper proposes using the mathematical signature transform as a continuous-time encoder to capture history in non-Markovian dynamics within neural ODE frameworks, replacing discrete RNN encoders, and shows improved test performance on synthetic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Neural ODEs rely on a Markov assumption that ignores history; real dynamic systems have memory, delays, and history-dependent behavior. RNN encoders, while common, are discrete and may hinder continuous modeling and training stability. A continuous-time, theoretically grounded encoder could better capture temporal dependencies.

Method: Introduce a signature-transform-based encoding scheme used within encoder–decoder dynamics models. Integrate the signature encoder into the neural ODE framework to summarize historical trajectory information into features that inform the continuous-time vector field. Compare against RNN-based encoders on synthetic benchmarks.

Result: Signature-based encoding outperforms RNN-based encoders in test performance on synthetic benchmarks.

Conclusion: A continuous-time, signature-based encoder is a viable and theoretically grounded alternative for non-Markovian dynamics in neural ODEs, offering improved performance over traditional RNN encoders on synthetic tasks; further work could extend to real-world data and assess computational trade-offs.

Abstract: Neural ordinary differential equations offer an effective framework for
modeling dynamical systems by learning a continuous-time vector field. However,
they rely on the Markovian assumption - that future states depend only on the
current state - which is often untrue in real-world scenarios where the
dynamics may depend on the history of past states. This limitation becomes
especially evident in settings involving the continuous control of complex
systems with delays and memory effects. To capture historical dependencies,
existing approaches often rely on recurrent neural network (RNN)-based
encoders, which are inherently discrete and struggle with continuous modeling.
In addition, they may exhibit poor training behavior. In this work, we
investigate the use of the signature transform as an encoder for learning
non-Markovian dynamics in a continuous-time setting. The signature transform
offers a continuous-time alternative with strong theoretical foundations and
proven efficiency in summarizing multidimensional information in time. We
integrate a signature-based encoding scheme into encoder-decoder dynamics
models and demonstrate that it outperforms RNN-based alternatives in test
performance on synthetic benchmarks.

</details>


### [388] [Imitation Learning as Return Distribution Matching](https://arxiv.org/abs/2509.12026)
*Filippo Lazzati,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: We study training a risk-sensitive imitation learning agent by matching the expert's return distribution (via Wasserstein distance) in a tabular RL setting. Markovian policies are shown to be insufficient, so a non-Markovian policy subclass is proposed. Two algorithms, RS-BC (unknown transitions) and RS-KT (known transitions), are developed; RS-KT enjoys lower sample complexity by leveraging dynamics, and an oracle-based variant handles unknown expert rewards. Empirical results show sample efficiency and advantages of non-Markovian policies over standard IL baselines.


<details>
  <summary>Details</summary>
Motivation: Standard imitation learning typically matches the expert's expected return (average performance) but ignores distributional risk attributes. The goal is to imitate not just the mean but the entire return distribution, enabling control over risk attitudes. Wasserstein distance provides a principled way to compare distributions. In the tabular setting, Markovian policies are limited in expressivity for matching distributions, motivating a more expressive policy class and efficient algorithms.

Method: Formulate risk-sensitive imitation learning as distribution matching between the expert and learner return distributions using Wasserstein distance. Work in a tabular RL setup with known expert reward. Show Markovian policies are insufficient and introduce a sufficiently expressive non-Markovian policy subclass tailored to this task. Develop two algorithms: RS-BC for unknown transition dynamics and RS-KT for known dynamics, with RS-KT exhibiting lower sample complexity by exploiting dynamics. Also discuss an oracle-based variant of RS-KT for the setting where the expert reward is unknown.

Result: RS-KT achieves substantially lower sample complexity than RS-BC by leveraging dynamics information. Numerical simulations demonstrate the sample efficiency of return distribution matching and illustrate the advantages of non-Markovian policies over standard IL baselines. An oracle-based variant further confirms the method’s utility when the expert reward is unknown.

Conclusion: Matching the expert’s return distribution is a viable and advantageous direction for risk-sensitive imitation learning. A carefully designed non-Markovian policy class enables the necessary expressivity, and RS-KT provides a more sample-efficient solution when dynamics are available. The work highlights the benefits of distributional objectives in IL and offers practical algorithms with theoretical guarantees.

Abstract: We study the problem of training a risk-sensitive reinforcement learning (RL)
agent through imitation learning (IL). Unlike standard IL, our goal is not only
to train an agent that matches the expert's expected return (i.e., its average
performance) but also its risk attitude (i.e., other features of the return
distribution, such as variance). We propose a general formulation of the
risk-sensitive IL problem in which the objective is to match the expert's
return distribution in Wasserstein distance. We focus on the tabular setting
and assume the expert's reward is known. After demonstrating the limited
expressivity of Markovian policies for this task, we introduce an efficient and
sufficiently expressive subclass of non-Markovian policies tailored to it.
Building on this subclass, we develop two provably efficient algorithms, RS-BC
and RS-KT, for solving the problem when the transition model is unknown and
known, respectively. We show that RS-KT achieves substantially lower sample
complexity than RS-BC by exploiting dynamics information. We further
demonstrate the sample efficiency of return distribution matching in the
setting where the expert's reward is unknown by designing an oracle-based
variant of RS-KT. Finally, we complement our theoretical analysis of RS-KT and
RS-BC with numerical simulations, highlighting both their sample efficiency and
the advantages of non-Markovian policies over standard sample-efficient IL
algorithms.

</details>


### [389] [Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework](https://arxiv.org/abs/2509.12043)
*Mayur Patil,Qadeer Ahmed,Shawn Midlam-Mohler*

Main category: cs.LG

TL;DR: A GNN-based traffic forecasting framework using adaptive stochastic adjacency (log-normal, CV) and weather-adjusted edge weights, with Adaptive Conformal Prediction for uncertainty; validated via SUMO simulations and INRIX data.


<details>
  <summary>Details</summary>
Motivation: Urban traffic is highly stochastic and influenced by environmental factors; static graphs and point forecasts fail to capture variability and require reliable uncertainty quantification for safe, efficient transportation planning.

Method: A Graph Neural Network with adaptive adjacency matrices parameterized by log-normal distributions and CV to reflect travel-time variability; weather factors (temperature, wind, precipitation) adjust edge weights to capture evolving spatio-temporal dependencies; Adaptive Conformal Prediction (ACP) provides calibrated prediction intervals; validation via SUMO-based traffic scenarios and Monte Carlo travel-time distribution for a Vehicle Under Test, with comparison to INRIX historical data.

Result: The model achieves better prediction accuracy and tighter/valid uncertainty bounds compared with baselines; the Monte Carlo simulated mean travel time falls within INRIX intervals, indicating robustness and practical reliability.

Conclusion: Adaptive stochastic graph structure combined with ACP enables accurate, uncertainty-aware travel-time forecasting under stochastic and weather-influenced conditions.

Abstract: Traffic flow forecasting is essential for managing congestion, improving
safety, and optimizing various transportation systems. However, it remains a
prevailing challenge due to the stochastic nature of urban traffic and
environmental factors. Better predictions require models capable of
accommodating the traffic variability influenced by multiple dynamic and
complex interdependent factors. In this work, we propose a Graph Neural Network
(GNN) framework to address the stochasticity by leveraging adaptive adjacency
matrices using log-normal distributions and Coefficient of Variation (CV)
values to reflect real-world travel time variability. Additionally, weather
factors such as temperature, wind speed, and precipitation adjust edge weights
and enable GNN to capture evolving spatio-temporal dependencies across traffic
stations. This enhancement over the static adjacency matrix allows the model to
adapt effectively to traffic stochasticity and changing environmental
conditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP)
framework to provide reliable uncertainty quantification, achieving target
coverage while maintaining acceptable prediction intervals. Experimental
results demonstrate that the proposed model, in comparison with baseline
methods, showed better prediction accuracy and uncertainty bounds. We, then,
validate this method by constructing traffic scenarios in SUMO and applying
Monte-Carlo simulation to derive a travel time distribution for a Vehicle Under
Test (VUT) to reflect real-world variability. The simulated mean travel time of
the VUT falls within the intervals defined by INRIX historical data, verifying
the model's robustness.

</details>


### [390] [Hi-DARTS: Hierarchical Dynamically Adapting Reinforcement Trading System](https://arxiv.org/abs/2509.12048)
*Hoon Sagong,Heesu Kim,Hanbeen Hong*

Main category: cs.LG

TL;DR: Hi-DARTS is a hierarchical multi-agent reinforcement learning framework that dynamically selects between high- and low-frequency trading via a volatility-aware meta-agent, balancing speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional fixed-frequency autonomous trading trades off computational efficiency against market responsiveness; a dynamic, hierarchical approach aims to adapt execution frequency to market conditions.

Method: A meta-agent analyzes market volatility to activate specialized Time Frame Agents for high- or low-frequency trading. The system is evaluated via back-testing on AAPL from Jan 2024 to May 2025.

Result: Cumulative return of 25.17% with a Sharpe Ratio of 0.75, outperforming a passive buy-and-hold on AAPL (12.19%) and SPY (20.01%).

Conclusion: Dynamic, hierarchical agents can achieve superior risk-adjusted returns while preserving computational efficiency.

Abstract: Conventional autonomous trading systems struggle to balance computational
efficiency and market responsiveness due to their fixed operating frequency. We
propose Hi-DARTS, a hierarchical multi-agent reinforcement learning framework
that addresses this trade-off. Hi-DARTS utilizes a meta-agent to analyze market
volatility and dynamically activate specialized Time Frame Agents for
high-frequency or low-frequency trading as needed. During back-testing on AAPL
stock from January 2024 to May 2025, Hi-DARTS yielded a cumulative return of
25.17% with a Sharpe Ratio of 0.75. This performance surpasses standard
benchmarks, including a passive buy-and-hold strategy on AAPL (12.19% return)
and the S&P 500 ETF (SPY) (20.01% return). Our work demonstrates that dynamic,
hierarchical agents can achieve superior risk-adjusted returns while
maintaining high computational efficiency.

</details>


### [391] [Foundational theory for optimal decision tree problems. II. Optimal hypersurface decision tree algorithm](https://arxiv.org/abs/2509.12057)
*Xi He*

Main category: cs.LG

TL;DR: This two-part work extends optimal decision trees to hypersurface splits (HODT), defines a rigorous model, and demonstrates superior accuracy and robustness over axis-parallel ODTs, without external solvers.


<details>
  <summary>Details</summary>
Motivation: ODT is NP-hard and axis-parallel splits limit accuracy. Existing optimal methods rely on generic solvers. The authors propose a formal hypersurface framework and a practical solver-free algorithm to improve expressiveness and performance.

Method: Part I establishes four axioms for a proper ODT model, derives four generic ODT algorithms, and analyzes the geometry of hypersurfaces (showing polynomial hypersurfaces satisfy the axioms). Part II presents the HODT algorithm for hypersurface splits without external solvers, and evaluates it on synthetic data (varying size, dimensionality, noise) and 30 real-world datasets, comparing to axis-parallel baselines.

Result: HODT better recovers ground truth than axis-parallel trees and is more robust to noise on synthetic data. On real data, it achieves up to 30% higher accuracy than the state-of-the-art axis-parallel ODT when controlling tree complexity.

Conclusion: Hypersurface decision trees generalize ODT beyond axis-parallel splits. The HODT algorithm provides a practical, solver-free method that can yield higher accuracy and robustness, with demonstrated generalization gains on real datasets.

Abstract: Decision trees are a ubiquitous model for classification and regression tasks
due to their interpretability and efficiency. However, solving the optimal
decision tree (ODT) problem remains a challenging combinatorial optimization
task. Even for the simplest splitting rules--axis-parallel hyperplanes--it is
NP-hard to optimize. In Part I of this series, we rigorously defined the proper
decision tree model through four axioms and, based on these, introduced four
formal definitions of the ODT problem. From these definitions, we derived four
generic algorithms capable of solving ODT problems for arbitrary decision trees
satisfying the axioms. We also analyzed the combinatorial geometric properties
of hypersurfaces, showing that decision trees defined by polynomial
hypersurface splitting rules satisfy the proper axioms that we proposed.
  In this second paper (Part II) of this two-part series, building on the
algorithmic and geometric foundations established in Part I, we introduce the
first hypersurface decision tree (HODT) algorithm. To the best of our
knowledge, existing optimal decision tree methods are, to date, limited to
hyperplane splitting rules--a special case of hypersurfaces--and rely on
general-purpose solvers. In contrast, our HODT algorithm addresses the general
hypersurface decision tree model without requiring external solvers.
  Using synthetic datasets generated from ground-truth hyperplane decision
trees, we vary tree size, data size, dimensionality, and label and feature
noise. Results showing that our algorithm recovers the ground truth more
accurately than axis-parallel trees and exhibits greater robustness to noise.
We also analyzed generalization performance across 30 real-world datasets,
showing that HODT can achieve up to 30% higher accuracy than the
state-of-the-art optimal axis-parallel decision tree algorithm when tree
complexity is properly controlled.

</details>


### [392] [Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation in Tomato Crops Using Leaf Spectral Analysis and Machine Learning](https://arxiv.org/abs/2509.12074)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Parastoo Farajpoor,Hamid Jafarbiglu,Mohsen B. Mesgaran*

Main category: cs.LG

TL;DR: Early detection of branched broomrape (Phelipanche ramosa) in tomato using leaf spectral reflectance (400–2500 nm) and ensemble ML; an 89% accuracy model at 585 growing-degree days (GDD) enabling pre-symptomatic intervention, with performance declining as plants age.


<details>
  <summary>Details</summary>
Motivation: Parasitic broomrape causes yield losses in tomato production; there is a need for early, field-based detection before canopy symptoms appear to enable targeted control.

Method: Field study with 300 tomato plants. Growth stages defined by growing-degree-days (GDD). Leaf reflectance measured with a portable spectrometer (400–2500 nm). Preprocessing included band denoising, 1 nm interpolation, Savitzky–Golay smoothing, and correlation-based band reduction. An ensemble classifier combining Random Forest, XGBoost, SVM with RBF kernel, and Naive Bayes.

Result: At 585 GDD the ensemble achieved 89% accuracy with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy declined at later stages (e.g., 69% at 1568 GDD), likely due to leaf senescence and weed interference. Despite limited infected samples and environmental confounders, proximal sensing with ensemble ML enabled pre-symptomatic detection of broomrape and potential targeted intervention to reduce yield losses.

Conclusion: Proximal spectral sensing coupled with ensemble ML can detect broomrape before visible canopy symptoms, supporting timely management and reduced yield losses. Further work with larger infected cohorts and broader conditions could improve generalizability.

Abstract: Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic
weed that threatens tomato production by extracting nutrients from the host. We
investigate early detection using leaf-level spectral reflectance (400-2500 nm)
and ensemble machine learning. In a field experiment in Woodland, California,
we tracked 300 tomato plants across growth stages defined by growing degree
days (GDD). Leaf reflectance was acquired with a portable spectrometer and
preprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing,
correlation-based band reduction). Clear class differences were observed near
1500 nm and 2000 nm water absorption features, consistent with reduced leaf
water content in infected plants at early stages. An ensemble combining Random
Forest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at
585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy
declined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and
weed interference. Despite the small number of infected plants and
environmental confounders, results show that proximal sensing with ensemble
learning enables timely detection of broomrape before canopy symptoms are
visible, supporting targeted interventions and reduced yield losses.

</details>


### [393] [A Time-Series Foundation Model by Universal Delay Embedding](https://arxiv.org/abs/2509.12080)
*Zijian Wang,Peng Tao,Jifan Shi,Rui Bao,Rui Liu,Luonan Chen*

Main category: cs.LG

TL;DR: Proposes Universal Delay Embedding (UDE), a pretrained foundation model combining delay-embedding (Takens) with Koopman operator learning to forecast nonlinear time series, achieving better MSE and interpretability via dynamical/topological representations.


<details>
  <summary>Details</summary>
Motivation: Time-series forecasting benefits from principled dynamical representations; many models lack interpretability and universality; leveraging Takens embedding and Koopman operator to linearize dynamics and improve generalization.

Method: Construct Hankel (delay) embeddings into 2D patches treated as images; feed into self-attention encoder; learn a finite-dimensional Koopman operator in latent space to predict future states; evaluate on benchmarks and climate data; emphasize interpretability via topologically informative subspaces.

Result: >20% average reduction in mean squared error vs. state-of-the-art foundation models; better generalization in fine-tuning; patches yield interpretable dynamical representations; robust encoding of domain-invariant dynamics; scalable across scientific/industrial domains.

Conclusion: UDE offers scalable, interpretable universal time-series modeling by integrating delay embedding with Koopman predictions, enabling principled, better forecasting and interpretability.

Abstract: This study introduces Universal Delay Embedding (UDE), a pretrained
foundation model designed to revolutionize time-series forecasting through
principled integration of delay embedding representation and Koopman operator
prediction. Leveraging Takens' embedding theorem, UDE as a dynamical
representation of observed data constructs two-dimensional subspace patches
from Hankel matrices, theoretically preserving dynamical and topological
properties of underlying dynamical systems. Such patches are viewed as images,
which can be efficiently processed by exploiting advanced deep learning
technologies. Computationally, these patches further serve as tokens for
learning a self-attention encoder, thus enabling accurate prediction of
nonlinear time-series by a finite-dimensional Koopman operator in a linear
manner in a latent space. Extensive evaluations across various benchmarks and
real-world climate datasets demonstrate over 20% average reduction in mean
squared error versus state-of-the-art foundation models, alongside superior
generalization in fine-tuning scenarios. In particular, the learned dynamical
representations and Koopman operator prediction forms from the patches exhibit
exceptional interpretability, with consistent identification of topologically
informative subspaces and robust encoding of domain-invariant dynamics,
establishing UDE as a scalable, interpretable framework for universal
time-series modeling and forecasting with broad scientific and industrial
applicability.

</details>


### [394] [Draw a Portrait of Your Graph Data: An Instance-Level Profiling Framework for Graph-Structured Data](https://arxiv.org/abs/2509.12094)
*Tianqi Zhao,Russa Biswas,Megha Khosla*

Main category: cs.LG

TL;DR: NodePro provides node-level profiling for graph ML to diagnose models beyond aggregate metrics by combining data-centric signals with model-centric training signals, enabling fine-grained, generalizable judgments of node reliability and identifying corrupted or inconsistent nodes in real-world graphs.


<details>
  <summary>Details</summary>
Motivation: Standard metrics like accuracy obscure per-node failures, making it hard to diagnose when/where graph models fail. There is a need for interpretable, node-level diagnostics that reveal systematic model differences and support reliability without ground-truth labels.

Method: Compute per-node profile scores by fusing data-centric cues (feature dissimilarity, label uncertainty, structural ambiguity) with model-centric cues (prediction confidence, training consistency). Use these profiles to compare models, align behavior with profiles, and detect systematic node-level differences. Validate generalization to unseen nodes and apply to real-world graphs (e.g., knowledge graphs) to identify inconsistent/corrupted nodes.

Result: NodePro exposes systematic, node-level differences between models that aggregate metrics miss; profiles generalize to unseen nodes, enabling reliability assessment without labels; it can flag semantically inconsistent or corrupted nodes in structured knowledge graphs, demonstrating practical usefulness.

Conclusion: NodePro enables fine-grained, interpretable diagnosis of graph models at the node level, supporting better model selection, reliability estimation, and data quality checks, with generalization to unseen nodes and applicability to real-world graphs.

Abstract: Graph machine learning models often achieve similar overall performance yet
behave differently at the node level, failing on different subsets of nodes
with varying reliability. Standard evaluation metrics such as accuracy obscure
these fine grained differences, making it difficult to diagnose when and where
models fail. We introduce NodePro, a node profiling framework that enables
fine-grained diagnosis of model behavior by assigning interpretable profile
scores to individual nodes. These scores combine data-centric signals, such as
feature dissimilarity, label uncertainty, and structural ambiguity, with
model-centric measures of prediction confidence and consistency during
training. By aligning model behavior with these profiles, NodePro reveals
systematic differences between models, even when aggregate metrics are
indistinguishable. We show that node profiles generalize to unseen nodes,
supporting prediction reliability without ground-truth labels. Finally, we
demonstrate the utility of NodePro in identifying semantically inconsistent or
corrupted nodes in a structured knowledge graph, illustrating its effectiveness
in real-world settings.

</details>


### [395] [$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.12117)
*Aryaman Reddi,Gabriele Tiboni,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: A new K-Level Policy Gradient (KPG) method for deep MARL updates agents against updated policies of others, accelerating coordination. It provides a finite-iterate monotonic convergence guarantee to a local Nash equilibrium under certain conditions, and applies to MAPPO, MADDPG, and FACMAC, with superior results on StarCraft II and multi-agent MuJoCo.


<details>
  <summary>Details</summary>
Motivation: Standard actor-critic MARL updates respond to other agents' current policies without accounting for their concurrent updates, causing miscoordination. There is a need for methods that anticipate others' updates to speed up coordinated policies.

Method: Introduce K-Level Policy Gradient (KPG) that recursively updates each agent against the updated policies of others. Prove monotonic convergence to a local Nash equilibrium for finite iterations under certain conditions. Implement KPG by integrating it with popular deep MARL algorithms MAPPO, MADDPG, and FACMAC. Validate empirically on StarCraft II and multi-agent MuJoCo.

Result: KPG yields superior performance compared to existing deep MARL algorithms on StarCraft II and multi-agent MuJoCo.

Conclusion: KPG provides a principled, theoretically grounded approach to account for other agents' updates in MARL, improving coordination and performance, and is compatible with widely-used MARL algorithms.

Abstract: Actor-critic algorithms for deep multi-agent reinforcement learning (MARL)
typically employ a policy update that responds to the current strategies of
other agents. While being straightforward, this approach does not account for
the updates of other agents at the same update step, resulting in
miscoordination. In this paper, we introduce the $K$-Level Policy Gradient
(KPG), a method that recursively updates each agent against the updated
policies of other agents, speeding up the discovery of effective coordinated
policies. We theoretically prove that KPG with finite iterates achieves
monotonic convergence to a local Nash equilibrium under certain conditions. We
provide principled implementations of KPG by applying it to the deep MARL
algorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior
performance over existing deep MARL algorithms in StarCraft II and multi-agent
MuJoCo.

</details>


### [396] [Do machine learning climate models work in changing climate dynamics?](https://arxiv.org/abs/2509.12147)
*Maria Conchita Agana Navarro,Geng Li,Theo Wolf,María Pérez-Ortiz*

Main category: cs.LG

TL;DR: Systematic evaluation of ML-based climate models under distribution shifts (OOD) using adapted evaluation methods; results show notable performance variability across scenarios; underscores need for robust evaluation frameworks and practical guidance for climate risk forecasting.


<details>
  <summary>Details</summary>
Motivation: Climate change intensifies unprecedented events; ensuring ML models generalize under distribution shifts is crucial for risk assessment and adaptation, yet this area is underexplored in climate science.

Method: Adapt established OOD evaluation methodologies to climate data and benchmarks; evaluate state-of-the-art ML climate models across diverse OOD scenarios using large-scale datasets.

Result: Experiments reveal substantial performance variability across OOD scenarios; some models generalize well in certain shifts but fail in others, highlighting strengths and limitations and the need for robust evaluation.

Conclusion: Robust OOD evaluation frameworks are essential for reliable climate risk forecasting with ML; findings provide actionable guidance to improve and adopting ML responsibly in climate risk applications.

Abstract: Climate change is accelerating the frequency and severity of unprecedented
events, deviating from established patterns. Predicting these
out-of-distribution (OOD) events is critical for assessing risks and guiding
climate adaptation. While machine learning (ML) models have shown promise in
providing precise, high-speed climate predictions, their ability to generalize
under distribution shifts remains a significant limitation that has been
underexplored in climate contexts. This research systematically evaluates
state-of-the-art ML-based climate models in diverse OOD scenarios by adapting
established OOD evaluation methodologies to climate data. Experiments on
large-scale datasets reveal notable performance variability across scenarios,
shedding light on the strengths and limitations of current models. These
findings underscore the importance of robust evaluation frameworks and provide
actionable insights to guide the reliable application of ML for climate risk
forecasting.

</details>


### [397] [Learning Neural Networks by Neuron Pursuit](https://arxiv.org/abs/2509.12154)
*Akshay Kumar,Jarvis Haupt*

Main category: cs.LG

TL;DR: Gradient flow near sparsity-structured saddles in homogeneous nets stays near the saddle and aligns small-norm weights, motivating a greedy 'Neuron Pursuit' training method that expands the network and minimizes loss; validated numerically.


<details>
  <summary>Details</summary>
Motivation: To understand the optimization dynamics of gradient flow around structured saddles in homogeneous neural networks and to derive a practical training algorithm from these dynamics.

Method: Analyze gradient flow for homogeneous networks near a class of sparsity-exhibiting saddles; prove prolonged proximity and directional convergence of small-norm weights; propose Neuron Pursuit (NP), an iterative procedure that expands the network by adding carefully chosen neurons and refines it via loss minimization; demonstrate with numerical experiments.

Result: Gradient flow remains near the saddle for an extended period, with small-norm weights staying small but converging in direction; NP effectively grows the network and reduces training loss, supported by numerical validation.

Conclusion: The study links optimization dynamics near sparsity saddles to a practical greedy training algorithm, showing both theoretical insights and empirical efficacy for NP.

Abstract: The first part of this paper studies the evolution of gradient flow for
homogeneous neural networks near a class of saddle points exhibiting a sparsity
structure. The choice of these saddle points is motivated from previous works
on homogeneous networks, which identified the first saddle point encountered by
gradient flow after escaping the origin. It is shown here that, when
initialized sufficiently close to such saddle points, gradient flow remains
near the saddle point for a sufficiently long time, during which the set of
weights with small norm remain small but converge in direction. Furthermore,
important empirical observations are made on the behavior of gradient descent
after escaping these saddle points. The second part of the paper, motivated by
these results, introduces a greedy algorithm to train deep neural networks
called Neuron Pursuit (NP). It is an iterative procedure which alternates
between expanding the network by adding neuron(s) with carefully chosen
weights, and minimizing the training loss using this augmented network. The
efficacy of the proposed algorithm is validated using numerical experiments.

</details>


### [398] [From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via Adversarial Learning](https://arxiv.org/abs/2509.12176)
*Collin Guo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human face synthesis and manipulation are increasingly important in
entertainment and AI, with a growing demand for highly realistic,
identity-preserving images even when only unpaired, unaligned datasets are
available. We study unpaired face manipulation via adversarial learning, moving
from autoencoder baselines to a robust, guided CycleGAN framework. While
autoencoders capture coarse identity, they often miss fine details. Our
approach integrates spectral normalization for stable training, identity- and
perceptual-guided losses to preserve subject identity and high-level structure,
and landmark-weighted cycle constraints to maintain facial geometry across pose
and illumination changes. Experiments show that our adversarial trained
CycleGAN improves realism (FID), perceptual quality (LPIPS), and identity
preservation (ID-Sim) over autoencoders, with competitive cycle-reconstruction
SSIM and practical inference times, which achieved high quality without paired
datasets and approaching pix2pix on curated paired subsets. These results
demonstrate that guided, spectrally normalized CycleGANs provide a practical
path from autoencoders to robust unpaired face manipulation.

</details>


### [399] [All that structure matches does not glitter](https://arxiv.org/abs/2509.12178)
*Maya M. Martirossyan,Thomas Egg,Philipp Hoellmer,George Karypis,Mark Transtrum,Adrian Roitberg,Mingjie Liu,Richard G. Hennig,Ellad B. Tadmor,Stefano Martiniani*

Main category: cs.LG

TL;DR: Critical examination of crystal-structure-generation datasets (carbon-24, perov-5): reveals duplicates, inappropriate random splits, and misleading benchmarks; proposes fixes including deduplicated datasets, polymorph-aware splits, and new metrics METRe and cRMSE.


<details>
  <summary>Details</summary>
Motivation: Benchmark reliability is essential for evaluating generative materials models; current datasets and metrics may distort performance due to duplicates and ill-suited splits.

Method: Empirical analysis of datasets: measure uniqueness, assess splits; implement deduplicated versions of carbon-24 (duplicate-removed, N-split, unit-cell variants); propose perov-5 polymorph-aware split; introduce METRe and cRMSE evaluation metrics.

Result: Findings: carbon-24 is only ~40% unique; random splits are problematic for polymorph-rich datasets; benchmarks can mislead with naive match-rate metrics; deliver revised datasets and new evaluation metrics (METRe, cRMSE) to correct issues.

Conclusion: Sound benchmarking requires curated datasets and robust metrics; the proposed revisions enable more meaningful evaluation of crystal-structure generative models and should be adopted by the community.

Abstract: Generative models for materials, especially inorganic crystals, hold
potential to transform the theoretical prediction of novel compounds and
structures. Advancement in this field depends critically on robust benchmarks
and minimal, information-rich datasets that enable meaningful model evaluation.
This paper critically examines common datasets and reported metrics for a
crystal structure prediction task$\unicode{x2014}$generating the most likely
structures given the chemical composition of a material. We focus on three key
issues: First, materials datasets should contain unique crystal structures; for
example, we show that the widely-utilized carbon-24 dataset only contains
$\approx$40% unique structures. Second, materials datasets should not be split
randomly if polymorphs of many different compositions are numerous, which we
find to be the case for the perov-5 dataset. Third, benchmarks can mislead if
used uncritically, e.g., reporting a match rate metric without considering the
structural variety exhibited by identical building blocks. To address these
oft-overlooked issues, we introduce several fixes. We provide revised versions
of the carbon-24 dataset: one with duplicates removed, one deduplicated and
split by number of atoms $N$, and two containing only identical structures but
with different unit cells. We also propose a new split for the perov-5 dataset
which ensures polymorphs are grouped within each split subset, setting a more
sensible standard for benchmarking model performance. Finally, we present METRe
and cRMSE, new model evaluation metrics that can correct existing issues with
the match rate metric.

</details>


### [400] [Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences](https://arxiv.org/abs/2509.12188)
*Antonin Sulc*

Main category: cs.LG

TL;DR: Event2Vec learns representations of discrete event sequences using a simple additive recurrent structure, achieving a linear additive (sum-of-events) representation in Euclidean space; a hyperbolic-space variant captures hierarchical/tree-like structures where it shows improved performance on hierarchical data.


<details>
  <summary>Details</summary>
Motivation: Motivated by the role of geometric/topological structures in neural representations, the work seeks interpretable, composable embeddings for sequences and explores how geometry (Euclidean vs hyperbolic) impacts their ability to capture additive and hierarchical relationships.

Method: Proposes an additive recurrent architecture that learns embeddings as a sum over constituent events. Provides a theoretical analysis showing convergence to an ideal additive structure under specific training objectives. Extends to a hyperbolic variant to better embed tree-like hierarchies.

Result: Empirical validation supports the linear additive hypothesis; the hyperbolic model particularly improves performance on hierarchical event sequences.

Conclusion: Sequence representations can converge to a vector sum of their events under suitable objectives; hyperbolic geometry offers a natural fit for hierarchical data, complementing Euclidean additive representations.

Abstract: The study of neural representations, both in biological and artificial
systems, is increasingly revealing the importance of geometric and topological
structures. Inspired by this, we introduce Event2Vec, a novel framework for
learning representations of discrete event sequences. Our model leverages a
simple, additive recurrent structure to learn composable, interpretable
embeddings. We provide a theoretical analysis demonstrating that, under
specific training objectives, our model's learned representations in a
Euclidean space converge to an ideal additive structure. This ensures that the
representation of a sequence is the vector sum of its constituent events, a
property we term the linear additive hypothesis. To address the limitations of
Euclidean geometry for hierarchical data, we also introduce a variant of our
model in hyperbolic space, which is naturally suited to embedding tree-like
structures with low distortion. We present experiments to validate our
hypothesis and demonstrate the benefits of each geometry, highlighting the
improved performance of the hyperbolic model on hierarchical event sequences.

</details>


### [401] [Dynamic Relational Priming Improves Transformer in Multivariate Time Series](https://arxiv.org/abs/2509.12196)
*Hunjae Lee,Corey Clark*

Main category: cs.LG

TL;DR: Prime attention introduces dynamic, per-interaction token modulation to tailor relationships in transformers for multivariate time series, outperforming standard attention while preserving complexity.


<details>
  <summary>Details</summary>
Motivation: Standard attention uses static token representations, which underfit heterogeneous inter-channel dynamics in MTS; there's a need for relation-aware mechanisms that adapt per token-pair.

Method: Propose attention with dynamic relational priming (prime attention) that applies learnable modulations to each token-pair interaction, enabling per-pair tailoring without increasing asymptotic complexity.

Result: Empirical benchmarks show prime attention consistently outperforms standard attention, with up to 6.5% improvement in forecasting accuracy; can achieve comparable or better performance with up to 40% shorter sequence lengths.

Conclusion: Dynamic relational priming enhances relational modeling in MTS, improving accuracy and data efficiency while maintaining efficiency; it's a promising approach for heterogeneous relational dynamics.

Abstract: Standard attention mechanisms in transformers employ static token
representations that remain unchanged across all pair-wise computations in each
layer. This limits their representational alignment with the potentially
diverse relational dynamics of each token-pair interaction. While they excel in
domains with relatively homogeneous relationships, standard attention's static
relational learning struggles to capture the diverse, heterogeneous
inter-channel dependencies of multivariate time series (MTS) data--where
different channel-pair interactions within a single system may be governed by
entirely different physical laws or temporal dynamics. To better align the
attention mechanism for such domain phenomena, we propose attention with
dynamic relational priming (prime attention). Unlike standard attention where
each token presents an identical representation across all of its pair-wise
interactions, prime attention tailors each token dynamically (or per
interaction) through learnable modulations to best capture the unique
relational dynamics of each token pair, optimizing each pair-wise interaction
for that specific relationship. This representational plasticity of prime
attention enables effective extraction of relationship-specific information in
MTS while maintaining the same asymptotic computational complexity as standard
attention. Our results demonstrate that prime attention consistently
outperforms standard attention across benchmarks, achieving up to 6.5\%
improvement in forecasting accuracy. In addition, we find that prime attention
achieves comparable or superior performance using up to 40\% less sequence
length compared to standard attention, further demonstrating its superior
relational modeling capabilities.

</details>
